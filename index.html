<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-20T01:30:00Z">09-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08186">
<div class="article-summary-box-inner">
<span><p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.
FaST-VGS is a Transformer-based model for learning the associations between raw
speech waveforms and visual images. The model unifies dual-encoder and
cross-attention architectures into a single model, reaping the superior
retrieval speed of the former along with the accuracy of the latter. FaST-VGS
achieves state-of-the-art speech-image retrieval accuracy on benchmark
datasets, and its learned representations exhibit strong performance on the
ZeroSpeech 2021 phonetic and semantic tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Numerical reasoning in machine reading comprehension tasks: are we there yet?. (arXiv:2109.08207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08207">
<div class="article-summary-box-inner">
<span><p>Numerical reasoning based machine reading comprehension is a task that
involves reading comprehension along with using arithmetic operations such as
addition, subtraction, sorting, and counting. The DROP benchmark (Dua et al.,
2019) is a recent dataset that has inspired the design of NLP models aimed at
solving this task. The current standings of these models in the DROP
leaderboard, over standard metrics, suggest that the models have achieved
near-human performance. However, does this mean that these models have learned
to reason? In this paper, we present a controlled study on some of the
top-performing model architectures for the task of numerical reasoning. Our
observations suggest that the standard metrics are incapable of measuring
progress towards such tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08214">
<div class="article-summary-box-inner">
<span><p>When humans conceive how to perform a particular task, they do so
hierarchically: splitting higher-level tasks into smaller sub-tasks. However,
in the literature on natural language (NL) command of situated agents, most
works have treated the procedures to be executed as flat sequences of simple
actions, or any hierarchies of procedures have been shallow at best. In this
paper, we propose a formalism of procedures as programs, a powerful yet
intuitive method of representing hierarchical procedural knowledge for agent
command and control. We further propose a modeling paradigm of hierarchical
modular networks, which consist of a planner and reactors that convert NL
intents to predictions of executable programs and probe the environment for
information necessary to complete the program execution. We instantiate this
framework on the IQA and ALFRED datasets for NL instruction following. Our
model outperforms reactive baselines by a large margin on both datasets. We
also demonstrate that our framework is more data-efficient, and that it allows
for fast iterative development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bag of Tricks for Dialogue Summarization. (arXiv:2109.08232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08232">
<div class="article-summary-box-inner">
<span><p>Dialogue summarization comes with its own peculiar challenges as opposed to
news or scientific articles summarization. In this work, we explore four
different challenges of the task: handling and differentiating parts of the
dialogue belonging to multiple speakers, negation understanding, reasoning
about the situation, and informal language understanding. Using a pretrained
sequence-to-sequence language model, we explore speaker name substitution,
negation scope highlighting, multi-task learning with relevant tasks, and
pretraining on in-domain data. Our experiments show that our proposed
techniques indeed improve summarization performance, outperforming strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularized Training of Nearest Neighbor Language Models. (arXiv:2109.08249v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08249">
<div class="article-summary-box-inner">
<span><p>Including memory banks in a natural language processing architecture
increases model capacity by equipping it with additional data at inference
time. In this paper, we build upon $k$NN-LM \citep{khandelwal20generalization},
which uses a pre-trained language model together with an exhaustive $k$NN
search through the training data (memory bank) to achieve state-of-the-art
results. We investigate whether we can improve the $k$NN-LM performance by
instead training a LM with the knowledge that we will be using a $k$NN
post-hoc. We achieved significant improvement using our method on language
modeling tasks on \texttt{WIKI-2} and \texttt{WIKI-103}. The main phenomenon
that we encounter is that adding a simple L2 regularization on the activations
(not weights) of the model, a transformer, improves the post-hoc $k$NN
classification performance. We explore some possible reasons for this
improvement. In particular, we find that the added L2 regularization seems to
improve the performance for high-frequency words without deteriorating the
performance for low frequency ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing out Bias: Achieving Fairness Through Training Reweighting. (arXiv:2109.08253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08253">
<div class="article-summary-box-inner">
<span><p>Bias in natural language processing arises primarily from models learning
characteristics of the author such as gender and race when modelling tasks such
as sentiment and syntactic parsing. This problem manifests as disparities in
error rates across author demographics, typically disadvantaging minority
groups. Existing methods for mitigating and measuring bias do not directly
account for correlations between author demographics and linguistic variables.
Moreover, evaluation of bias has been inconsistent in previous work, in terms
of dataset balance and evaluation methods. This paper introduces a very simple
but highly effective method for countering bias using instance reweighting,
based on the frequency of both task labels and author demographics. We extend
the method in the form of a gated model which incorporates the author
demographic as an input, and show that while it is highly vulnerable to input
data bias, it provides debiased predictions through demographic input
perturbation, and outperforms all other bias mitigation techniques when
combined with instance reweighting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08256">
<div class="article-summary-box-inner">
<span><p>The importance and pervasiveness of emotions in our lives makes affective
computing a tremendously important and vibrant line of work. Systems for
automatic emotion recognition (AER) and sentiment analysis can be facilitators
of enormous progress (e.g., in improving public health and commerce) but also
enablers of great harm (e.g., for suppressing dissidents and manipulating
voters). Thus, it is imperative that the affective computing community actively
engage with the ethical ramifications of their creations. In this paper, I have
synthesized and organized information from AI Ethics and Emotion Recognition
literature to present fifty ethical considerations relevant to AER. Notably,
the sheet fleshes out assumptions hidden in how AER is commonly framed, and in
the choices often made regarding the data, method, and evaluation. Special
attention is paid to the implications of AER on privacy and social groups. The
objective of the sheet is to facilitate and encourage more thoughtfulness on
why to automate, how to automate, and how to judge success well before the
building of AER systems. Additionally, the sheet acts as a useful introductory
document on emotion recognition (complementing survey articles).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-training with Few-shot Rationalization: Teacher Explanations Aid Student in Few-shot NLU. (arXiv:2109.08259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08259">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models have obtained state-of-the-art performance
for several natural language understanding tasks, they are quite opaque in
terms of their decision-making process. While some recent works focus on
rationalizing neural predictions by highlighting salient concepts in the text
as justifications or rationales, they rely on thousands of labeled training
examples for both task labels as well as an-notated rationales for every
instance. Such extensive large-scale annotations are infeasible to obtain for
many tasks. To this end, we develop a multi-task teacher-student framework
based on self-training language models with limited task-specific labels and
rationales, and judicious sample selection to learn from informative
pseudo-labeled examples1. We study several characteristics of what constitutes
a good rationale and demonstrate that the neural model performance can be
significantly improved by making it aware of its rationalized predictions,
particularly in low-resource settings. Extensive experiments in several
bench-mark datasets demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08270">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are sentence-completion engines trained on massive
corpora. LMs have emerged as a significant breakthrough in natural-language
processing, providing capabilities that go far beyond sentence completion
including question answering, summarization, and natural-language inference.
While many of these capabilities have potential application to cognitive
systems, exploiting language models as a source of task knowledge, especially
for task learning, offers significant, near-term benefits. We introduce
language models and the various tasks to which they have been applied and then
review methods of knowledge extraction from language models. The resulting
analysis outlines both the challenges and opportunities for using language
models as a new knowledge source for cognitive systems. It also identifies
possible ways to improve knowledge extraction from language models using the
capabilities provided by cognitive systems. Central to success will be the
ability of a cognitive agent to itself learn an abstract model of the knowledge
implicit in the LM as well as methods to extract high-quality knowledge
effectively and efficiently. To illustrate, we introduce a hypothetical robot
agent and describe how language models could extend its task knowledge and
improve its performance and the kinds of knowledge and methods the agent can
use to exploit the knowledge within a language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis. (arXiv:2109.08306v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08306">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment
analysis task that aims to extract aspects, classify corresponding sentiment
polarities and find opinions as the causes of sentiment. The latest research
tends to solve the ABSA task in a unified way with end-to-end frameworks. Yet,
these frameworks get fine-tuned from downstream tasks without any task-adaptive
modification. Specifically, they do not use task-related knowledge well or
explicitly model relations between aspect and opinion terms, hindering them
from better performance. In this paper, we propose SentiPrompt to use sentiment
knowledge enhanced prompts to tune the language model in the unified framework.
We inject sentiment knowledge regarding aspects, opinions, and polarities into
prompt and explicitly model term relations via constructing consistency and
polarity judgment templates from the ground truth triplets. Experimental
results demonstrate that our approach can outperform strong baselines on
Triplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment
Classification by a notable margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Multimodal Sentiment Dataset for Video Recommendation. (arXiv:2109.08333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08333">
<div class="article-summary-box-inner">
<span><p>Recently, multimodal sentiment analysis has seen remarkable advance and a lot
of datasets are proposed for its development. In general, current multimodal
sentiment analysis datasets usually follow the traditional system of
sentiment/emotion, such as positive, negative and so on. However, when applied
in the scenario of video recommendation, the traditional sentiment/emotion
system is hard to be leveraged to represent different contents of videos in the
perspective of visual senses and language understanding. Based on this, we
propose a multimodal sentiment analysis dataset, named baiDu Video Sentiment
dataset (DuVideoSenti), and introduce a new sentiment system which is designed
to describe the sentimental style of a video on recommendation scenery.
Specifically, DuVideoSenti consists of 5,630 videos which displayed on Baidu,
each video is manually annotated with a sentimental style label which describes
the user's real feeling of a video. Furthermore, we propose UNIMO as our
baseline for DuVideoSenti. Experimental results show that DuVideoSenti brings
new challenges to multimodal sentiment analysis, and could be used as a new
benchmark for evaluating approaches designed for video understanding and
multimodal fusion. We also expect our proposed DuVideoSenti could further
improve the development of multimodal sentiment analysis and its application to
video recommendations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-adaptive Pre-training of Language Models with Word Embedding Regularization. (arXiv:2109.08354v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08354">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PTLMs) acquire domain-independent linguistic
knowledge through pre-training with massive textual resources. Additional
pre-training is effective in adapting PTLMs to domains that are not well
covered by the pre-training corpora. Here, we focus on the static word
embeddings of PTLMs for domain adaptation to teach PTLMs domain-specific
meanings of words. We propose a novel fine-tuning process: task-adaptive
pre-training with word embedding regularization (TAPTER). TAPTER runs
additional pre-training by making the static word embeddings of a PTLM close to
the word embeddings obtained in the target domain with fastText. TAPTER
requires no additional corpus except for the training data of the downstream
task. We confirmed that TAPTER improves the performance of the standard
fine-tuning and the task-adaptive pre-training on BioASQ (question answering in
the biomedical domain) and on SQuAD (the Wikipedia domain) when their
pre-training corpora were not dominated by in-domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling Linguistic Context for Language Model Compression. (arXiv:2109.08359v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08359">
<div class="article-summary-box-inner">
<span><p>A computationally expensive and memory intensive neural network lies behind
the recent success of language representation learning. Knowledge distillation,
a major technique for deploying such a vast language model in resource-scarce
environments, transfers the knowledge on individual word representations
learned without restrictions. In this paper, inspired by the recent
observations that language representations are relatively positioned and have
more semantic knowledge as a whole, we present a new knowledge distillation
objective for language representation learning that transfers the contextual
knowledge via two types of relationships across representations: Word Relation
and Layer Transforming Relation. Unlike other recent distillation techniques
for the language models, our contextual distillation does not have any
restrictions on architectural changes between teacher and student. We validate
the effectiveness of our method on challenging benchmarks of language
understanding tasks, not only in architectures of various sizes, but also in
combination with DynaBERT, the recently proposed adaptive size pruning method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CodeQA: A Question Answering Dataset for Source Code Comprehension. (arXiv:2109.08365v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08365">
<div class="article-summary-box-inner">
<span><p>We propose CodeQA, a free-form question answering dataset for the purpose of
source code comprehension: given a code snippet and a question, a textual
answer is required to be generated. CodeQA contains a Java dataset with 119,778
question-answer pairs and a Python dataset with 70,085 question-answer pairs.
To obtain natural and faithful questions and answers, we implement syntactic
rules and semantic analysis to transform code comments into question-answer
pairs. We present the construction process and conduct systematic analysis of
our dataset. Experiment results achieved by several neural baselines on our
dataset are shown and discussed. While research on question-answering and
machine reading comprehension develops rapidly, few prior work has drawn
attention to code question answering. This new dataset can serve as a useful
research benchmark for source code comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To be Closer: Learning to Link up Aspects with Opinions. (arXiv:2109.08382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08382">
<div class="article-summary-box-inner">
<span><p>Dependency parse trees are helpful for discovering the opinion words in
aspect-based sentiment analysis (ABSA). However, the trees obtained from
off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA.
This is because the syntactic trees are not designed for capturing the
interactions between opinion words and aspect words. In this work, we aim to
shorten the distance between aspects and corresponding opinion words by
learning an aspect-centric tree structure. The aspect and opinion words are
expected to be closer along such tree structure compared to the standard
dependency parse tree. The learning process allows the tree structure to
adaptively correlate the aspect and opinion words, enabling us to better
identify the polarity in the ABSA task. We conduct experiments on five
aspect-based sentiment datasets, and the proposed model significantly
outperforms recent strong baselines. Furthermore, our thorough analysis
demonstrates the average distance between aspect and opinion words are
shortened by at least 19% on the standard SemEval Restaurant14 dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">reproducing "ner and pos when nothing is capitalized". (arXiv:2109.08396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08396">
<div class="article-summary-box-inner">
<span><p>Capitalization is an important feature in many NLP tasks such as Named Entity
Recognition (NER) or Part of Speech Tagging (POS). We are trying to reproduce
results of paper which shows how to mitigate a significant performance drop
when casing is mismatched between training and testing data. In particular we
show that lowercasing 50% of the dataset provides the best performance,
matching the claims of the original paper. We also show that we got slightly
lower performance in almost all experiments we have tried to reproduce,
suggesting that there might be some hidden factors impacting our performance.
Lastly, we make all of our work available in a public github repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08406">
<div class="article-summary-box-inner">
<span><p>Despite the success of fine-tuning pretrained language encoders like BERT for
downstream natural language understanding (NLU) tasks, it is still poorly
understood how neural networks change after fine-tuning. In this work, we use
centered kernel alignment (CKA), a method for comparing learned
representations, to measure the similarity of representations in task-tuned
models across layers. In experiments across twelve NLU tasks, we discover a
consistent block diagonal structure in the similarity of representations within
fine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of
earlier and later layers, but not between them. The similarity of later layer
representations implies that later layers only marginally contribute to task
performance, and we verify in experiments that the top few layers of fine-tuned
Transformers can be discarded without hurting performance, even with no further
tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis. (arXiv:2109.08412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08412">
<div class="article-summary-box-inner">
<span><p>Chatbot is increasingly thriving in different domains, however, because of
unexpected discourse complexity and training data sparseness, its potential
distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff
(MHCH), predicting chatbot failure and enabling human-algorithm collaboration
to enhance chatbot quality, has attracted increasing attention from industry
and academia. In this study, we propose a novel model, Role-Selected Sharing
Network (RSSN), which integrates both dialogue satisfaction estimation and
handoff prediction in one multi-task learning framework. Unlike prior efforts
in dialog mining, by utilizing local user satisfaction as a bridge, global
satisfaction detector and handoff predictor can effectively exchange critical
information. Specifically, we decouple the relation and interaction between the
two tasks by the role information after the shared encoder. Extensive
experiments on two public datasets demonstrate the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">New Students on Sesame Street: What Order-Aware Matrix Embeddings Can Learn from BERT. (arXiv:2109.08449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08449">
<div class="article-summary-box-inner">
<span><p>Large-scale pretrained language models (PreLMs) are revolutionizing natural
language processing across all benchmarks. However, their sheer size is
prohibitive in low-resource or large-scale applications. While common
approaches reduce the size of PreLMs via same-architecture distillation or
pruning, we explore distilling PreLMs into more efficient order-aware embedding
models. Our results on the GLUE benchmark show that embedding-centric students,
which have learned from BERT, yield scores comparable to DistilBERT on QQP and
RTE, often match or exceed the scores of ELMo, and only fall behind on
detecting linguistic acceptability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Unification for Logic Reasoning over Natural Language. (arXiv:2109.08460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08460">
<div class="article-summary-box-inner">
<span><p>Automated Theorem Proving (ATP) deals with the development of computer
programs being able to show that some conjectures (queries) are a logical
consequence of a set of axioms (facts and rules). There exists several
successful ATPs where conjectures and axioms are formally provided (e.g.
formalised as First Order Logic formulas). Recent approaches, such as (Clark et
al., 2020), have proposed transformer-based architectures for deriving
conjectures given axioms expressed in natural language (English). The
conjecture is verified through a binary text classifier, where the transformers
model is trained to predict the truth value of a conjecture given the axioms.
The RuleTaker approach of (Clark et al., 2020) achieves appealing results both
in terms of accuracy and in the ability to generalize, showing that when the
model is trained with deep enough queries (at least 3 inference steps), the
transformers are able to correctly answer the majority of queries (97.6%) that
require up to 5 inference steps. In this work we propose a new architecture,
namely the Neural Unifier, and a relative training procedure, which achieves
state-of-the-art results in term of generalisation, showing that mimicking a
well-known inference procedure, the backward chaining, it is possible to answer
deep queries even when the model is trained only on shallow ones. The approach
is demonstrated in experiments using a diverse set of benchmark data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08475">
<div class="article-summary-box-inner">
<span><p>Visual dialog, which aims to hold a meaningful conversation with humans about
a given image, is a challenging task that requires models to reason the complex
dependencies among visual content, dialog history, and current questions. Graph
neural networks are recently applied to model the implicit relations between
objects in an image or dialog. However, they neglect the importance of 1)
coreference relations among dialog history and dependency relations between
words for the question representation; and 2) the representation of the image
based on the fully represented question. Therefore, we propose a novel
relation-aware graph-over-graph network (GoG) for visual dialog. Specifically,
GoG consists of three sequential graphs: 1) H-Graph, which aims to capture
coreference relations among dialog history; 2) History-aware Q-Graph, which
aims to fully understand the question through capturing dependency relations
between words based on coreference resolution on the dialog history; and 3)
Question-aware I-Graph, which aims to capture the relations between objects in
an image based on fully question representation. As an additional feature
representation module, we add GoG to the existing visual dialogue model.
Experimental results show that our model outperforms the strong baseline in
both generative and discriminative settings by a significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation. (arXiv:2109.08478v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08478">
<div class="article-summary-box-inner">
<span><p>Visual dialogue is a challenging task since it needs to answer a series of
coherent questions on the basis of understanding the visual environment.
Previous studies focus on the implicit exploration of multimodal co-reference
by implicitly attending to spatial image features or object-level image
features but neglect the importance of locating the objects explicitly in the
visual content, which is associated with entities in the textual content.
Therefore, in this paper we propose a {\bf M}ultimodal {\bf I}ncremental {\bf
T}ransformer with {\bf V}isual {\bf G}rounding, named MITVG, which consists of
two key parts: visual grounding and multimodal incremental transformer. Visual
grounding aims to explicitly locate related objects in the image guided by
textual entities, which helps the model exclude the visual content that does
not need attention. On the basis of visual grounding, the multimodal
incremental transformer encodes the multi-turn dialogue history combined with
visual scene step by step according to the order of the dialogue and then
generates a contextually and visually coherent response. Experimental results
on the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the
proposed model, which achieves comparable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08535">
<div class="article-summary-box-inner">
<span><p>Open-domain question answering has exploded in popularity recently due to the
success of dense retrieval models, which have surpassed sparse models using
only a few supervised training examples. However, in this paper, we demonstrate
current dense models are not yet the holy grail of retrieval. We first
construct EntityQuestions, a set of simple, entity-rich questions based on
facts from Wikidata (e.g., "Where was Arve Furset born?"), and observe that
dense retrievers drastically underperform sparse methods. We investigate this
issue and uncover that dense retrievers can only generalize to common entities
unless the question pattern is explicitly observed during training. We discuss
two simple solutions towards addressing this critical problem. First, we
demonstrate that data augmentation is unable to fix the generalization problem.
Second, we argue a more robust passage encoder helps facilitate better question
adaptation using specialized question encoders. We hope our work can shed light
on the challenges in creating a robust, universal dense retriever that works
well across different input distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules. (arXiv:2109.08544v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08544">
<div class="article-summary-box-inner">
<span><p>One of the challenges faced by conversational agents is their inability to
identify unstated presumptions of their users' commands, a task trivial for
humans due to their common sense. In this paper, we propose a zero-shot
commonsense reasoning system for conversational agents in an attempt to achieve
this. Our reasoner uncovers unstated presumptions from user commands satisfying
a general template of if-(state), then-(action), because-(goal). Our reasoner
uses a state-of-the-art transformer-based generative commonsense knowledge base
(KB) as its source of background knowledge for reasoning. We propose a novel
and iterative knowledge query mechanism to extract multi-hop reasoning chains
from the neural KB which uses symbolic logic rules to significantly reduce the
search space. Similar to any KBs gathered to date, our commonsense KB is prone
to missing knowledge. Therefore, we propose to conversationally elicit the
missing knowledge from human users with our novel dynamic question generation
strategy, which generates and presents contextualized queries to human users.
We evaluate the model with a user study with human users that achieves a 35%
higher success rate compared to SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Slot Filling for Biomedical Information Extraction. (arXiv:2109.08564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08564">
<div class="article-summary-box-inner">
<span><p>Information Extraction (IE) from text refers to the task of extracting
structured knowledge from unstructured text. The task typically consists of a
series of sub-tasks such as Named Entity Recognition and Relation Extraction.
Sourcing entity and relation type specific training data is a major bottleneck
in the above sub-tasks.In this work we present a slot filling approach to the
task of biomedical IE, effectively replacing the need for entity and
relation-specific training data, allowing to deal with zero-shot settings. We
follow the recently proposed paradigm of coupling a Tranformer-based
bi-encoder, Dense Passage Retrieval, with a Transformer-based reader model to
extract relations from biomedical text. We assemble a biomedical slot filling
dataset for both retrieval and reading comprehension and conduct a series of
experiments demonstrating that our approach outperforms a number of simpler
baselines. We also evaluate our approach end-to-end for standard as well as
zero-shot settings. Our work provides a fresh perspective on how to solve
biomedical IE tasks, in the absence of relevant training data. Our code, models
and pretrained data are available at
https://github.com/healx/biomed-slot-filling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Multitask Learning for Low-Resource AbstractiveSummarization. (arXiv:2109.08565v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08565">
<div class="article-summary-box-inner">
<span><p>This paper explores the effect of using multitask learning for abstractive
summarization in the context of small training corpora. In particular, we
incorporate four different tasks (extractive summarization, language modeling,
concept detection, and paraphrase detection) both individually and in
combination, with the goal of enhancing the target task of abstractive
summarization via multitask learning. We show that for many task combinations,
a model trained in a multitask setting outperforms a model trained only for
abstractive summarization, with no additional summarization data introduced.
Additionally, we do a comprehensive search and find that certain tasks (e.g.
paraphrase detection) consistently benefit abstractive summarization, not only
when combined with other tasks but also when using different architectures and
training corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization. (arXiv:2109.08569v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08569">
<div class="article-summary-box-inner">
<span><p>This paper explores three simple data manipulation techniques (synthesis,
augmentation, curriculum) for improving abstractive summarization models
without the need for any additional data. We introduce a method of data
synthesis with paraphrasing, a data augmentation technique with sample mixing,
and curriculum learning with two new difficulty metrics based on specificity
and abstractiveness. We conduct experiments to show that these three techniques
can help improve abstractive summarization across two summarization models and
two different small datasets. Furthermore, we show that these techniques can
improve performance when applied in isolation and when combined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchy-Aware T5 with Path-Adaptive Mask Mechanism for Hierarchical Text Classification. (arXiv:2109.08585v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08585">
<div class="article-summary-box-inner">
<span><p>Hierarchical Text Classification (HTC), which aims to predict text labels
organized in hierarchical space, is a significant task lacking in investigation
in natural language processing. Existing methods usually encode the entire
hierarchical structure and fail to construct a robust label-dependent model,
making it hard to make accurate predictions on sparse lower-level labels and
achieving low Macro-F1. In this paper, we propose a novel PAMM-HiA-T5 model for
HTC: a hierarchy-aware T5 model with path-adaptive mask mechanism that not only
builds the knowledge of upper-level labels into low-level ones but also
introduces path dependency information in label prediction. Specifically, we
generate a multi-level sequential label structure to exploit hierarchical
dependency across different levels with Breadth-First Search (BFS) and T5
model. To further improve label dependency prediction within each path, we then
propose an original path-adaptive mask mechanism (PAMM) to identify the label's
path information, eliminating sources of noises from other paths. Comprehensive
experiments on three benchmark datasets show that our novel PAMM-HiA-T5 model
greatly outperforms all state-of-the-art HTC approaches especially in Macro-F1.
The ablation studies show that the improvements mainly come from our innovative
approach instead of T5.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Commonsense help in detecting Sarcasm?. (arXiv:2109.08588v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08588">
<div class="article-summary-box-inner">
<span><p>Sarcasm detection is important for several NLP tasks such as sentiment
identification in product reviews, user feedback, and online forums. It is a
challenging task requiring a deep understanding of language, context, and world
knowledge. In this paper, we investigate whether incorporating commonsense
knowledge helps in sarcasm detection. For this, we incorporate commonsense
knowledge into the prediction process using a graph convolution network with
pre-trained language model embeddings as input. Our experiments with three
sarcasm detection datasets indicate that the approach does not outperform the
baseline model. We perform an exhaustive set of experiments to analyze where
commonsense support adds value and where it hurts classification. Our
implementation is publicly available at:
https://github.com/brcsomnath/commonsense-sarcasm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Transformers for Job Expression Extraction and Classification in a Low-Resource Setting. (arXiv:2109.08597v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08597">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore possible improvements of transformer models in a
low-resource setting. In particular, we present our approaches to tackle the
first two of three subtasks of the MEDDOPROF competition, i.e., the extraction
and classification of job expressions in Spanish clinical texts. As neither
language nor domain experts, we experiment with the multilingual XLM-R
transformer model and tackle these low-resource information extraction tasks as
sequence-labeling problems. We explore domain- and language-adaptive
pretraining, transfer learning and strategic datasplits to boost the
transformer model. Our results show strong improvements using these methods by
up to 5.3 F1 points compared to a fine-tuned XLM-R model. Our best models
achieve 83.2 and 79.3 F1 for the first two tasks, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The futility of STILTs for the classification of lexical borrowings in Spanish. (arXiv:2109.08607v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08607">
<div class="article-summary-box-inner">
<span><p>The first edition of the IberLEF 2021 shared task on automatic detection of
borrowings (ADoBo) focused on detecting lexical borrowings that appeared in the
Spanish press and that have recently been imported into the Spanish language.
In this work, we tested supplementary training on intermediate labeled-data
tasks (STILTs) from part of speech (POS), named entity recognition (NER),
code-switching, and language identification approaches to the classification of
borrowings at the token level using existing pre-trained transformer-based
language models. Our extensive experimental results suggest that STILTs do not
provide any improvement over direct fine-tuning of multilingual models.
However, multilingual models trained on small subsets of languages perform
reasonably better than multilingual BERT but not as good as multilingual
RoBERTa for the given dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Scrubbing of Demographic Information for Text Classification. (arXiv:2109.08613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08613">
<div class="article-summary-box-inner">
<span><p>Contextual representations learned by language models can often encode
undesirable attributes, like demographic associations of the users, while being
trained for an unrelated target task. We aim to scrub such undesirable
attributes and learn fair representations while maintaining performance on the
target task. In this paper, we present an adversarial learning framework
"Adversarial Scrubber" (ADS), to debias contextual representations. We perform
theoretical analysis to show that our framework converges without leaking
demographic information under certain conditions. We extend previous evaluation
techniques by evaluating debiasing performance using Minimum Description Length
(MDL) probing. Experimental evaluations on 8 datasets show that ADS generates
representations with minimal information about demographic attributes while
being maximally informative about the target task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CKMorph: A Comprehensive Morphological Analyzer for Central Kurdish. (arXiv:2109.08615v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08615">
<div class="article-summary-box-inner">
<span><p>A morphological analyzer, which is a significant component of many natural
language processing applications especially for morphologically rich languages,
divides an input word into all its composing morphemes and identifies their
morphological roles. In this paper, we introduce a comprehensive morphological
analyzer for Central Kurdish (CK), a low-resourced language with a rich
morphology. Building upon the limited existing literature, we first assembled
and systematically categorized a comprehensive collection of the morphological
and morphophonological rules of the language. Additionally, we collected and
manually labeled a generative lexicon containing nearly 10,000 verb, noun and
adjective stems, named entities, and other types of word stems. We used these
rule sets and resources to implement CKMorph Analyzer based on finite-state
transducers. In order to provide a benchmark for future research, we collected,
manually labeled, and publicly shared test sets for evaluating accuracy and
coverage of the analyzer. CKMorph was able to correctly analyze 95.9% of the
accuracy test set, containing 1,000 CK words morphologically analyzed according
to the context. Moreover, CKMorph gave at least one analysis for 95.5% of 4.22M
CK tokens of the coverage test set. The demonstration of the application and
resources including CK verb database and test sets are openly accessible at
https://github.com/CKMorph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications. (arXiv:2109.08627v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08627">
<div class="article-summary-box-inner">
<span><p>Sentence-level Quality estimation (QE) of machine translation is
traditionally formulated as a regression task, and the performance of QE models
is typically measured by Pearson correlation with human labels. Recent QE
models have achieved previously-unseen levels of correlation with human
judgments, but they rely on large multilingual contextualized language models
that are computationally expensive and make them infeasible for real-world
applications. In this work, we evaluate several model compression techniques
for QE and find that, despite their popularity in other NLP tasks, they lead to
poor performance in this regression setting. We observe that a full model
parameterization is required to achieve SoTA results in a regression task.
However, we argue that the level of expressiveness of a model in a continuous
range is unnecessary given the downstream applications of QE, and show that
reframing QE as a classification problem and evaluating QE models using
classification metrics would better reflect their actual performance in
real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?. (arXiv:2109.08634v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08634">
<div class="article-summary-box-inner">
<span><p>Models designed for intelligent process automation are required to be capable
of grounding user interface elements. This task of interface element grounding
is centred on linking instructions in natural language to their target
referents. Even though BERT and similar pre-trained language models have
excelled in several NLP tasks, their use has not been widely explored for the
UI grounding domain. This work concentrates on testing and probing the
grounding abilities of three different transformer-based models: BERT, RoBERTa
and LayoutLM. Our primary focus is on these models' spatial reasoning skills,
given their importance in this domain. We observe that LayoutLM has a promising
advantage for applications in this domain, even though it was created for a
different original purpose (representing scanned documents): the learned
spatial features appear to be transferable to the UI grounding setting,
especially as they demonstrate the ability to discriminate between target
directions in natural language instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Measuring of Readability to Improve Documents Accessibility for Arabic Language Learners. (arXiv:2109.08648v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08648">
<div class="article-summary-box-inner">
<span><p>This paper presents an approach based on supervised machine learning methods
to build a classifier that can identify text complexity in order to present
Arabic language learners with texts suitable to their levels. The approach is
based on machine learning classification methods to discriminate between the
different levels of difficulty in reading and understanding a text. Several
models were trained on a large corpus mined from online Arabic websites and
manually annotated. The model uses both Count and TF-IDF representations and
applies five machine learning algorithms; Multinomial Naive Bayes, Bernoulli
Naive Bayes, Logistic Regression, Support Vector Machine and Random Forest,
using unigrams and bigrams features. With the goal of extracting the text
complexity, the problem is usually addressed by formulating the level
identification as a classification task. Experimental results showed that
n-gram features could be indicative of the reading level of a text and could
substantially improve performance, and showed that SVM and Multinomial Naive
Bayes are the most accurate in predicting the complexity level. Best results
were achieved using TF-IDF Vectors trained by a combination of word-based
unigrams and bigrams with an overall accuracy of 87.14% over four classes of
complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Handling Unconstrained User Preferences in Dialogue. (arXiv:2109.08650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08650">
<div class="article-summary-box-inner">
<span><p>A user input to a schema-driven dialogue information navigation system, such
as venue search, is typically constrained by the underlying database which
restricts the user to specify a predefined set of preferences, or slots,
corresponding to the database fields. We envision a more natural information
navigation dialogue interface where a user has flexibility to specify
unconstrained preferences that may not match a predefined schema. We propose to
use information retrieval from unstructured knowledge to identify entities
relevant to a user request. We update the Cambridge restaurants database with
unstructured knowledge snippets (reviews and information from the web) for each
of the restaurants and annotate a set of query-snippet pairs with a relevance
label. We use the annotated dataset to train and evaluate snippet relevance
classifiers, as a proxy to evaluating recommendation accuracy. We show that
with a pretrained transformer model as an encoder, an unsupervised/supervised
classifier achieves a weighted F1 of .661/.856.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Primer: Searching for Efficient Transformers for Language Modeling. (arXiv:2109.08668v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08668">
<div class="article-summary-box-inner">
<span><p>Large Transformer models have been central to recent advances in natural
language processing. The training and inference costs of these models, however,
have grown rapidly and become prohibitively expensive. Here we aim to reduce
the costs of Transformers by searching for a more efficient variant. Compared
to previous approaches, our search is performed at a lower level, over the
primitives that define a Transformer TensorFlow program. We identify an
architecture, named Primer, that has a smaller training cost than the original
Transformer and other variants for auto-regressive language modeling. Primer's
improvements can be mostly attributed to two simple modifications: squaring
ReLU activations and adding a depthwise convolution layer after each Q, K, and
V projection in self-attention.
</p>
<p>Experiments show Primer's gains over Transformer increase as compute scale
grows and follow a power law with respect to quality at optimal model sizes. We
also verify empirically that Primer can be dropped into different codebases to
significantly speed up training without additional tuning. For example, at a
500M parameter size, Primer improves the original T5 architecture on C4
auto-regressive language modeling, reducing the training cost by 4X.
Furthermore, the reduced training cost means Primer needs much less compute to
reach a target one-shot performance. For instance, in a 1.9B parameter
configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to
achieve the same one-shot performance as Transformer. We open source our models
and several comparisons in T5 to help with reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. (arXiv:2109.08678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08678">
<div class="article-summary-box-inner">
<span><p>Existing KBQA approaches, despite achieving strong performance on i.i.d. test
data, often struggle in generalizing to questions involving unseen KB schema
items. Prior ranking-based approaches have shown some success in
generalization, but suffer from the coverage issue. We present RnG-KBQA, a
Rank-and-Generate approach for KBQA, which remedies the coverage issue with a
generation model while preserving a strong generalization capability. Our
approach first uses a contrastive ranker to rank a set of candidate logical
forms obtained by searching over the knowledge graph. It then introduces a
tailored generation model conditioned on the question and the top-ranked
candidates to compose the final logical form. We achieve new state-of-the-art
results on GrailQA and WebQSP datasets. In particular, our method surpasses the
prior state-of-the-art by a large margin on the GrailQA leaderboard. In
addition, RnG-KBQA outperforms all prior approaches on the popular WebQSP
benchmark, even including the ones that use the oracle entity linking. The
experimental results demonstrate the effectiveness of the interplay between
ranking and generation, which leads to the superior performance of our proposed
approach across all settings with especially strong improvements in zero-shot
generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Global Informativeness in Open Domain Keyphrase Extraction. (arXiv:2004.13639v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13639">
<div class="article-summary-box-inner">
<span><p>Open-domain KeyPhrase Extraction (KPE) aims to extract keyphrases from
documents without domain or quality restrictions, e.g., web pages with variant
domains and qualities. Recently, neural methods have shown promising results in
many KPE tasks due to their powerful capacity for modeling contextual semantics
of the given documents. However, we empirically show that most neural KPE
methods prefer to extract keyphrases with good phraseness, such as short and
entity-style n-grams, instead of globally informative keyphrases from
open-domain documents. This paper presents JointKPE, an open-domain KPE
architecture built on pre-trained language models, which can capture both local
phraseness and global informativeness when extracting keyphrases. JointKPE
learns to rank keyphrases by estimating their informativeness in the entire
document and is jointly trained on the keyphrase chunking task to guarantee the
phraseness of keyphrase candidates. Experiments on two large KPE datasets with
diverse domains, OpenKP and KP20k, demonstrate the effectiveness of JointKPE on
different pre-trained variants in open-domain scenarios. Further analyses
reveal the significant advantages of JointKPE in predicting long and non-entity
keyphrases, which are challenging for previous neural KPE methods. Our code is
publicly available at https://github.com/thunlp/BERT-KPE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15779">
<div class="article-summary-box-inner">
<span><p>Pretraining large neural language models, such as BERT, has led to impressive
gains on many natural language processing (NLP) tasks. However, most
pretraining efforts focus on general domain corpora, such as newswire and Web.
A prevailing assumption is that even domain-specific pretraining can benefit by
starting from general-domain language models. In this paper, we challenge this
assumption by showing that for domains with abundant unlabeled text, such as
biomedicine, pretraining language models from scratch results in substantial
gains over continual pretraining of general-domain language models. To
facilitate this investigation, we compile a comprehensive biomedical NLP
benchmark from publicly-available datasets. Our experiments show that
domain-specific pretraining serves as a solid foundation for a wide range of
biomedical NLP tasks, leading to new state-of-the-art results across the board.
Further, in conducting a thorough evaluation of modeling choices, both for
pretraining and task-specific fine-tuning, we discover that some common
practices are unnecessary with BERT models, such as using complex tagging
schemes in named entity recognition (NER). To help accelerate research in
biomedical NLP, we have released our state-of-the-art pretrained and
task-specific models for the community, and created a leaderboard featuring our
BLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning
Benchmark) at https://aka.ms/BLURB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Document Clustering Based on BERT with Data Augment. (arXiv:2011.08523v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08523">
<div class="article-summary-box-inner">
<span><p>Contrastive learning is a promising approach to unsupervised learning, as it
inherits the advantages of well-studied deep models without a dedicated and
complex model design. In this paper, based on bidirectional encoder
representations from transformers, we propose self-supervised contrastive
learning (SCL) as well as few-shot contrastive learning (FCL) with unsupervised
data augmentation (UDA) for text clustering. SCL outperforms state-of-the-art
unsupervised clustering approaches for short texts and those for long texts in
terms of several clustering evaluation measures. FCL achieves performance close
to supervised learning, and FCL with UDA further improves the performance for
short texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To what extent do human explanations of model behavior align with actual model behavior?. (arXiv:2012.13354v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13354">
<div class="article-summary-box-inner">
<span><p>Given the increasingly prominent role NLP models (will) play in our lives, it
is important for human expectations of model behavior to align with actual
model behavior. Using Natural Language Inference (NLI) as a case study, we
investigate the extent to which human-generated explanations of models'
inference decisions align with how models actually make these decisions. More
specifically, we define three alignment metrics that quantify how well natural
language explanations align with model sensitivity to input words, as measured
by integrated gradients. Then, we evaluate eight different models (the base and
large versions of BERT, RoBERTa and ELECTRA, as well as anRNN and bag-of-words
model), and find that the BERT-base model has the highest alignment with
human-generated explanations, for all alignment metrics. Focusing in on
transformers, we find that the base versions tend to have higher alignment with
human-generated explanations than their larger counterparts, suggesting that
increasing the number of model parameters leads, in some cases, to worse
alignment with human explanations. Finally, we find that a model's alignment
with human explanations is not predicted by the model's accuracy, suggesting
that accuracy and alignment are complementary ways to evaluate models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15283">
<div class="article-summary-box-inner">
<span><p>While pre-trained language models (PTLMs) have achieved noticeable success on
many NLP tasks, they still struggle for tasks that require event temporal
reasoning, which is essential for event-centric applications. We present a
continual pre-training approach that equips PTLMs with targeted knowledge about
event temporal relations. We design self-supervised learning objectives to
recover masked-out event and temporal indicators and to discriminate sentences
from their corrupted counterparts (where event or temporal indicators got
replaced). By further pre-training a PTLM with these objectives jointly, we
reinforce its attention to event and temporal information, yielding enhanced
capability on event temporal reasoning. This effective continual pre-training
framework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning
performances across five relation extraction and question answering tasks and
achieves new or on-par state-of-the-art performances in most of our downstream
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora. (arXiv:2012.15674v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15674">
<div class="article-summary-box-inner">
<span><p>Recent studies have demonstrated that pre-trained cross-lingual models
achieve impressive performance in downstream cross-lingual tasks. This
improvement benefits from learning a large amount of monolingual and parallel
corpora. Although it is generally acknowledged that parallel corpora are
critical for improving the model performance, existing methods are often
constrained by the size of parallel corpora, especially for low-resource
languages. In this paper, we propose ERNIE-M, a new training method that
encourages the model to align the representation of multiple languages with
monolingual corpora, to overcome the constraint that the parallel corpus size
places on the model performance. Our key insight is to integrate
back-translation into the pre-training process. We generate pseudo-parallel
sentence pairs on a monolingual corpus to enable the learning of semantic
alignments between different languages, thereby enhancing the semantic modeling
of cross-lingual models. Experimental results show that ERNIE-M outperforms
existing cross-lingual models and delivers new state-of-the-art results in
various cross-lingual downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Politics via Contextualized Discourse Processing. (arXiv:2012.15784v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15784">
<div class="article-summary-box-inner">
<span><p>Politicians often have underlying agendas when reacting to events. Arguments
in contexts of various events reflect a fairly consistent set of agendas for a
given entity. In spite of recent advances in Pretrained Language Models (PLMs),
those text representations are not designed to capture such nuanced patterns.
In this paper, we propose a Compositional Reader model consisting of encoder
and composer modules, that attempts to capture and leverage such information to
generate more effective representations for entities, issues, and events. These
representations are contextualized by tweets, press releases, issues, news
articles, and participating entities. Our model can process several documents
at once and generate composed representations for multiple entities over
several issues or events. Via qualitative and quantitative empirical analysis,
we show that these representations are meaningful and effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Everything in Order? A Simple Way to Order Sentences. (arXiv:2104.07064v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07064">
<div class="article-summary-box-inner">
<span><p>The task of organizing a shuffled set of sentences into a coherent text has
been used to evaluate a machine's understanding of causal and temporal
relations. We formulate the sentence ordering task as a conditional
text-to-marker generation problem. We present Reorder-BART (Re-BART) that
leverages a pre-trained Transformer-based model to identify a coherent order
for a given set of shuffled sentences. The model takes a set of shuffled
sentences with sentence-specific markers as input and generates a sequence of
position markers of the sentences in the ordered text. Re-BART achieves the
state-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and
Kendall's tau ($\tau$). We perform evaluations in a zero-shot setting,
showcasing that our model is able to generalize well across other datasets. We
additionally perform several experiments to understand the functioning and
limitations of our framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Learning for Generation with Long Source Sequences. (arXiv:2104.07545v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07545">
<div class="article-summary-box-inner">
<span><p>One of the challenges for current sequence to sequence (seq2seq) models is
processing long sequences, such as those in summarization and document level
machine translation tasks. These tasks require the model to reason at the token
level as well as the sentence and paragraph level. We design and study a new
Hierarchical Attention Transformer-based architecture (HAT) that outperforms
standard Transformers on several sequence to sequence tasks. Furthermore, our
model achieves state-of-the-art ROUGE scores on four summarization tasks,
including PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms
document-level machine translation baseline on the WMT20 English to German
translation task. We investigate what the hierarchical layers learn by
visualizing the hierarchical encoder-decoder attention. Finally, we study
hierarchical learning on encoder-only pre-training and analyze its performance
on classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04484">
<div class="article-summary-box-inner">
<span><p>Deep learning algorithms have shown promising results in visual question
answering (VQA) tasks, but a more careful look reveals that they often do not
understand the rich signal they are being fed with. To understand and better
measure the generalization capabilities of VQA systems, we look at their
robustness to counterfactually augmented data. Our proposed augmentations are
designed to make a focused intervention on a specific property of the question
such that the answer changes. Using these augmentations, we propose a new
robustness measure, Robustness to Augmented Data (RAD), which measures the
consistency of model predictions between original and augmented examples.
Through extensive experimentation, we show that RAD, unlike classical accuracy
measures, can quantify when state-of-the-art systems are not robust to
counterfactuals. We find substantial failure cases which reveal that current
VQA systems are still brittle. Finally, we connect between robustness and
generalization, demonstrating the predictive power of RAD for performance on
unseen augmentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13375">
<div class="article-summary-box-inner">
<span><p>Information overload is a prevalent challenge in many high-value domains. A
prominent case in point is the explosion of the biomedical literature on
COVID-19, which swelled to hundreds of thousands of papers in a matter of
months. In general, biomedical literature expands by two papers every minute,
totalling over a million new papers every year. Search in the biomedical realm,
and many other vertical domains is challenging due to the scarcity of direct
supervision from click logs. Self-supervised learning has emerged as a
promising direction to overcome the annotation bottleneck. We propose a general
approach for vertical search based on domain-specific pretraining and present a
case study for the biomedical domain. Despite being substantially simpler and
not using any relevance labels for training or development, our method performs
comparably or better than the best systems in the official TREC-COVID
evaluation, a COVID-related biomedical search competition. Using distributed
computing in modern cloud infrastructure, our system can scale to tens of
millions of articles on PubMed and has been deployed as Microsoft Biomedical
Search, a new search experience for biomedical literature:
https://aka.ms/biomedsearch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ethics Sheets for AI Tasks. (arXiv:2107.01183v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01183">
<div class="article-summary-box-inner">
<span><p>Recent innovations such as Datasheets for Datasets and Model Cards for Model
Reporting have made useful contributions to furthering ethical research. Yet,
several high-profile events, such as the mass testing of emotion recognition
systems on vulnerable sub-populations, have highlighted how technology will
often lead to more adverse outcomes for those that are already marginalized. In
this paper, I will make a case for thinking about ethical considerations not
just at the level of individual models and datasets, but also at the level of
AI tasks. I will present a new form of such an effort, Ethics Sheets for AI
Tasks, dedicated to fleshing out the assumptions and ethical considerations
hidden in how a task is commonly framed and in the choices we make regarding
the data, method, and evaluation. Finally, I will provide an example ethics
sheet for automatic emotion recognition. Ethics sheets are a mechanism to
document ethical considerations \textit{before} building datasets and systems.
Such pre-production activities (e.g., ethics analyses) and associated artifacts
(e.g., accessible documentation) are crucial for responsible AI: for
communicating risks to all stakeholders, to help decision and policy making,
and for developing more effective post-production documents such as Data Sheets
and Model Cards.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pattern-based Acquisition of Scientific Entities from Scholarly Article Titles. (arXiv:2109.00199v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00199">
<div class="article-summary-box-inner">
<span><p>We describe a rule-based approach for the automatic acquisition of salient
scientific entities from Computational Linguistics (CL) scholarly article
titles. Two observations motivated the approach: (i) noting salient aspects of
an article's contribution in its title; and (ii) pattern regularities capturing
the salient terms that could be expressed in a set of rules. Only those
lexico-syntactic patterns were selected that were easily recognizable, occurred
frequently, and positionally indicated a scientific entity type. The rules were
developed on a collection of 50,237 CL titles covering all articles in the ACL
Anthology. In total, 19,799 research problems, 18,111 solutions, 20,033
resources, 1,059 languages, 6,878 tools, and 21,687 methods were extracted at
an average precision of 75%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00590">
<div class="article-summary-box-inner">
<span><p>Web search is fundamentally multimodal and multihop. Often, even before
asking a question we choose to go directly to image search to find our answers.
Further, rarely do we find an answer from a single source but aggregate
information and reason through implications. Despite the frequency of this
everyday occurrence, at present, there is no unified question answering
benchmark that requires a single model to answer long-form natural language
questions from text and open-ended visual sources -- akin to a human's
experience. We propose to bridge this gap between the natural language and
computer vision communities with WebQA. We show that A. our multihop text
queries are difficult for a large-scale transformer model, and B. existing
multi-modal transformers and visual representations do not perform well on
open-domain visual queries. Our challenge for the community is to create a
unified multimodal reasoning model that seamlessly transitions and reasons
regardless of the source modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00627">
<div class="article-summary-box-inner">
<span><p>Contextual knowledge is important for real-world automatic speech recognition
(ASR) applications. In this paper, a novel tree-constrained pointer generator
(TCPGen) component is proposed that incorporates such knowledge as a list of
biasing words into both attention-based encoder-decoder and transducer
end-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing
words into an efficient prefix tree to serve as its symbolic input and creates
a neural shortcut between the tree and the final ASR output distribution to
facilitate recognising biasing words during decoding. Systems were trained and
evaluated on the Librispeech corpus where biasing words were extracted at the
scales of an utterance, a chapter, or a book to simulate different application
scenarios. Experimental results showed that TCPGen consistently improved word
error rates (WERs) compared to the baselines, and in particular, achieved
significant WER reductions on the biasing words. TCPGen is highly efficient: it
can handle 5,000 biasing words and distractors and only add a small overhead to
memory use and computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03570">
<div class="article-summary-box-inner">
<span><p>This work presents biomedical and clinical language models for Spanish by
experimenting with different pretraining choices, such as masking at word and
subword level, varying the vocabulary size and testing with domain data,
looking for better language representations. Interestingly, in the absence of
enough clinical data to train a model from scratch, we applied mixed-domain
pretraining and cross-domain transfer approaches to generate a performant
bio-clinical model suitable for real-world clinical data. We evaluated our
models on Named Entity Recognition (NER) tasks for biomedical documents and
challenging hospital discharge reports. When compared against the competitive
mBERT and BETO models, we outperform them in all NER tasks by a significant
margin. Finally, we studied the impact of the model's vocabulary on the NER
performances by offering an interesting vocabulary-centric analysis. The
results confirm that domain-specific pretraining is fundamental to achieving
higher performances in downstream NER tasks, even within a mid-resource
scenario. To the best of our knowledge, we provide the first biomedical and
clinical transformer-based pretrained language models for Spanish, intending to
boost native Spanish NLP applications in biomedicine. Our best models are
freely available in the HuggingFace hub: https://huggingface.co/BSC-TeMU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04114">
<div class="article-summary-box-inner">
<span><p>We apply imitation learning (IL) to tackle the NMT exposure bias problem with
error-correcting oracles, and evaluate an SMT lattice-based oracle which,
despite its excellent performance in an unconstrained oracle translation task,
turned out to be too pruned and idiosyncratic to serve as the oracle for IL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cartography Active Learning. (arXiv:2109.04282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04282">
<div class="article-summary-box-inner">
<span><p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)
algorithm that exploits the behavior of the model on individual instances
during training as a proxy to find the most informative instances for labeling.
CAL is inspired by data maps, which were recently proposed to derive insights
into dataset quality (Swayamdipta et al., 2020). We compare our method on
popular text classification tasks to commonly used AL strategies, which instead
rely on post-training behavior. We demonstrate that CAL is competitive to other
common AL methods, showing that training dynamics derived from small seed data
can be successfully used for AL. We provide insights into our new AL method by
analyzing batch-level statistics utilizing the data maps. Our results further
show that CAL results in a more data-efficient learning strategy, achieving
comparable or better results with considerably less training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05179">
<div class="article-summary-box-inner">
<span><p>Generating high quality question-answer pairs is a hard but meaningful task.
Although previous works have achieved great results on answer-aware question
generation, it is difficult to apply them into practical application in the
education field. This paper for the first time addresses the question-answer
pair generation task on the real-world examination data, and proposes a new
unified framework on RACE. To capture the important information of the input
passage we first automatically generate(rather than extracting) keyphrases,
thus this task is reduced to keyphrase-question-answer triplet joint
generation. Accordingly, we propose a multi-agent communication model to
generate and optimize the question and keyphrases iteratively, and then apply
the generated question and keyphrases to guide the generation of answers. To
establish a solid benchmark, we build our model on the strong generative
pre-training model. Experimental results show that our model makes great
breakthroughs in the question-answer pair generation task. Moreover, we make a
comprehensive analysis on our model, suggesting new directions for this
challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. (arXiv:2109.07383v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07383">
<div class="article-summary-box-inner">
<span><p>This paper addresses the efficiency challenge of Neural Architecture Search
(NAS) by formulating the task as a ranking problem. Previous methods require
numerous training examples to estimate the accurate performance of
architectures, although the actual goal is to find the distinction between
"good" and "bad" candidates. Here we do not resort to performance predictors.
Instead, we propose a performance ranking method (RankNAS) via pairwise
ranking. It enables efficient architecture search using much fewer training
examples. Moreover, we develop an architecture selection method to prune the
search space and concentrate on more promising candidates. Extensive
experiments on machine translation and language modeling tasks show that
RankNAS can design high-performance architectures while being orders of
magnitude faster than state-of-the-art NAS systems.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-20 23:07:37.860670258 UTC">2021-09-20 23:07:37 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>