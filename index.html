<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-17T01:30:00Z">08-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Entity Extraction Method Based on Machine Reading Comprehension. (arXiv:2108.06444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06444">
<div class="article-summary-box-inner">
<span><p>Entity extraction is a key technology for obtaining information from massive
texts in natural language processing. The further interaction between them does
not meet the standards of human reading comprehension, thus limiting the
understanding of the model, and also the omission or misjudgment of the answer
(ie the target entity) due to the reasoning question. An effective MRC-based
entity extraction model-MRC-I2DP, which uses the proposed gated
attention-attracting mechanism to adjust the restoration of each part of the
text pair, creating problems and thinking for multi-level interactive attention
calculations to increase the target entity It also uses the proposed 2D
probability coding module, TALU function and mask mechanism to strengthen the
detection of all possible targets of the target, thereby improving the
probability and accuracy of prediction. Experiments have proved that MRC-I2DP
represents an overall state-of-the-art model in 7 from the scientific and
public domains, achieving a performance improvement of 2.1% ~ 10.4% compared to
the model model in F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Bias In Automatic Toxic Comment Detection: An Empirical Study. (arXiv:2108.06487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06487">
<div class="article-summary-box-inner">
<span><p>With surge in online platforms, there has been an upsurge in the user
engagement on these platforms via comments and reactions. A large portion of
such textual comments are abusive, rude and offensive to the audience. With
machine learning systems in-place to check such comments coming onto platform,
biases present in the training data gets passed onto the classifier leading to
discrimination against a set of classes, religion and gender. In this work, we
evaluate different classifiers and feature to estimate the bias in these
classifiers along with their performance on downstream task of toxicity
classification. Results show that improvement in performance of automatic toxic
comment detection models is positively correlated to mitigating biases in these
models. In our work, LSTM with attention mechanism proved to be a better
modelling strategy than a CNN model. Further analysis shows that fasttext
embeddings is marginally preferable than glove embeddings on training models
for toxicity comment detection. Deeper analysis reveals the findings that such
automatic models are particularly biased to specific identity groups even
though the model has a high AUC score. Finally, in effort to mitigate bias in
toxicity detection models, a multi-task setup trained with auxiliary task of
toxicity sub-types proved to be useful leading to upto 0.26% (6% relative) gain
in AUC scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Sample Named Entity Recognition for Security Vulnerability Reports by Fine-Tuning Pre-Trained Language Models. (arXiv:2108.06590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06590">
<div class="article-summary-box-inner">
<span><p>Public security vulnerability reports (e.g., CVE reports) play an important
role in the maintenance of computer and network systems. Security companies and
administrators rely on information from these reports to prioritize tasks on
developing and deploying patches to their customers. Since these reports are
unstructured texts, automatic information extraction (IE) can help scale up the
processing by converting the unstructured reports to structured forms, e.g.,
software names and versions and vulnerability types. Existing works on
automated IE for security vulnerability reports often rely on a large number of
labeled training samples. However, creating massive labeled training set is
both expensive and time consuming. In this work, for the first time, we propose
to investigate this problem where only a small number of labeled training
samples are available. In particular, we investigate the performance of
fine-tuning several state-of-the-art pre-trained language models on our small
training dataset. The results show that with pre-trained language models and
carefully tuned hyperparameters, we have reached or slightly outperformed the
state-of-the-art system on this task. Consistent with previous two-step process
of first fine-tuning on main category and then transfer learning to others as
in [7], if otherwise following our proposed approach, the number of required
labeled samples substantially decrease in both stages: 90% reduction in
fine-tuning from 5758 to 576,and 88.8% reduction in transfer learning with 64
labeled samples per category. Our experiments thus demonstrate the
effectiveness of few-sample learning on NER for security vulnerability report.
This result opens up multiple research opportunities for few-sample learning
for security vulnerability reports, which is discussed in the paper. Code:
https://github.com/guanqun-yang/FewVulnerability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages. (arXiv:2108.06598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06598">
<div class="article-summary-box-inner">
<span><p>We present the findings of the LoResMT 2021 shared task which focuses on
machine translation (MT) of COVID-19 data for both low-resource spoken and sign
languages. The organization of this task was conducted as part of the fourth
workshop on technologies for machine translation of low resource languages
(LoResMT). Parallel corpora is presented and publicly available which includes
the following directions: English$\leftrightarrow$Irish,
English$\leftrightarrow$Marathi, and Taiwanese Sign
language$\leftrightarrow$Traditional Chinese. Training data consists of 8112,
20933 and 128608 segments, respectively. There are additional monolingual data
sets for Marathi and English that consist of 21901 segments. The results
presented here are based on entries from a total of eight teams. Three teams
submitted systems for English$\leftrightarrow$Irish while five teams submitted
systems for English$\leftrightarrow$Marathi. Unfortunately, there were no
systems submissions for the Taiwanese Sign language$\leftrightarrow$Traditional
Chinese task. Maximum system performance was computed using BLEU and follow as
36.0 for English--Irish, 34.6 for Irish--English, 24.2 for English--Marathi,
and 31.3 for Marathi--English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The SelectGen Challenge: Finding the Best Training Samples for Few-Shot Neural Text Generation. (arXiv:2108.06614v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06614">
<div class="article-summary-box-inner">
<span><p>We propose a shared task on training instance selection for few-shot neural
text generation. Large-scale pretrained language models have led to dramatic
improvements in few-shot text generation. Nonetheless, almost all previous work
simply applies random sampling to select the few-shot training instances.
Little to no attention has been paid to the selection strategies and how they
would affect model performance. The study of the selection strategy can help us
to (1) make the most use of our annotation budget in downstream tasks and (2)
better benchmark few-shot text generative models. We welcome submissions that
present their selection strategies and the effects on the generation quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEXTER: Deep Encoding of External Knowledge for Named Entity Recognition in Virtual Assistants. (arXiv:2108.06633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06633">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is usually developed and tested on text from
well-written sources. However, in intelligent voice assistants, where NER is an
important component, input to NER may be noisy because of user or speech
recognition error. In applications, entity labels may change frequently, and
non-textual properties like topicality or popularity may be needed to choose
among alternatives.
</p>
<p>We describe a NER system intended to address these problems. We test and
train this system on a proprietary user-derived dataset. We compare with a
baseline text-only NER system; the baseline enhanced with external gazetteers;
and the baseline enhanced with the search and indirect labelling techniques we
describe below. The final configuration gives around 6% reduction in NER error
rate. We also show that this technique improves related tasks, such as semantic
parsing, with an improvement of up to 5% in error rate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation. (arXiv:2108.06643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06643">
<div class="article-summary-box-inner">
<span><p>We motivate and propose a suite of simple but effective improvements for
concept-to-text generation called SAPPHIRE: Set Augmentation and Post-hoc
PHrase Infilling and REcombination. We demonstrate their effectiveness on
generative commonsense reasoning, a.k.a. the CommonGen task, through
experiments using both BART and T5 models. Through extensive automatic and
human evaluation, we show that SAPPHIRE noticeably improves model performance.
An in-depth qualitative analysis illustrates that SAPPHIRE effectively
addresses many issues of the baseline model generations, including lack of
commonsense, insufficient specificity, and poor fluency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models. (arXiv:2108.06665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06665">
<div class="article-summary-box-inner">
<span><p>Consistency, which refers to the capability of generating the same
predictions for semantically similar contexts, is a highly desirable property
for a sound language understanding model. Although recent pretrained language
models (PLMs) deliver outstanding performance in various downstream tasks, they
should exhibit consistent behaviour provided the models truly understand
language. In this paper, we propose a simple framework named consistency
analysis on language understanding models (CALUM)} to evaluate the model's
lower-bound consistency ability. Through experiments, we confirmed that current
PLMs are prone to generate inconsistent predictions even for semantically
identical inputs. We also observed that multi-task training with paraphrase
identification tasks is of benefit to improve consistency, increasing the
consistency by 13% on average.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Knowledge Base Question Answering: A Survey. (arXiv:2108.06688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06688">
<div class="article-summary-box-inner">
<span><p>Knowledge base question answering (KBQA) aims to answer a question over a
knowledge base (KB). Early studies mainly focused on answering simple questions
over KBs and achieved great success. However, their performance on complex
questions is still far from satisfactory. Therefore, in recent years,
researchers propose a large number of novel methods, which looked into the
challenges of answering complex questions. In this survey, we review recent
advances on KBQA with the focus on solving complex questions, which usually
contain multiple subjects, express compound relations, or involve numerical
operations. In detail, we begin with introducing the complex KBQA task and
relevant background. Then, we describe benchmark datasets for complex KBQA task
and introduce the construction process of these datasets. Next, we present two
mainstream categories of methods for complex KBQA, namely semantic
parsing-based (SP-based) methods and information retrieval-based (IR-based)
methods. Specifically, we illustrate their procedures with flow designs and
discuss their major differences and similarities. After that, we summarize the
challenges that these two categories of methods encounter when answering
complex questions, and explicate advanced solutions and techniques used in
existing work. Finally, we conclude and discuss several promising directions
related to complex KBQA for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation. (arXiv:2108.06712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06712">
<div class="article-summary-box-inner">
<span><p>Tables are often created with hierarchies, but existing works on table
reasoning mainly focus on flat tables and neglect hierarchical tables.
Hierarchical tables challenge existing methods by hierarchical indexing, as
well as implicit relationships of calculation and semantics. This work presents
HiTab, a free and open dataset for the research community to study question
answering (QA) and natural language generation (NLG) over hierarchical tables.
HiTab is a cross-domain dataset constructed from a wealth of statistical
reports and Wikipedia pages, and has unique characteristics: (1) nearly all
tables are hierarchical, and (2) both target sentences for NLG and questions
for QA are revised from high-quality descriptions in statistical reports that
are meaningful and diverse. (3) HiTab provides fine-grained annotations on both
entity and quantity alignment. Targeting hierarchical structure, we devise a
novel hierarchy-aware logical form for symbolic reasoning over tables, which
shows high effectiveness. Then given annotations of entity and quantity
alignment, we propose partially supervised training, which helps models to
largely reduce spurious predictions in the QA task. In the NLG task, we find
that entity and quantity alignment also helps NLG models to generate better
results in a conditional generation setting. Experiment results of
state-of-the-art baselines suggest that this dataset presents a strong
challenge and a valuable benchmark for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning. (arXiv:2108.06743v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06743">
<div class="article-summary-box-inner">
<span><p>To quantitatively and intuitively explore the generalization ability of
pre-trained language models (PLMs), we have designed several tasks of
arithmetic and logical reasoning. We both analyse how well PLMs generalize when
the test data is in the same distribution as the train data and when it is
different, for the latter analysis, we have also designed a cross-distribution
test set other than the in-distribution test set. We conduct experiments on one
of the most advanced and publicly released generative PLM - BART. Our research
finds that the PLMs can easily generalize when the distribution is the same,
however, it is still difficult for them to generalize out of the distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What can Neural Referential Form Selectors Learn?. (arXiv:2108.06806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06806">
<div class="article-summary-box-inner">
<span><p>Despite achieving encouraging results, neural Referring Expression Generation
models are often thought to lack transparency. We probed neural Referential
Form Selection (RFS) models to find out to what extent the linguistic features
influencing the RE form are learnt and captured by state-of-the-art RFS models.
The results of 8 probing tasks show that all the defined features were learnt
to some extent. The probing tasks pertaining to referential status and
syntactic position exhibited the highest performance. The lowest performance
was achieved by the probing models designed to predict discourse structure
properties beyond the sentence level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Maps Search Misspelling Detection Leveraging Domain-Augmented Contextual Representations. (arXiv:2108.06842v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06842">
<div class="article-summary-box-inner">
<span><p>Building an independent misspelling detector and serve it before correction
can bring multiple benefits to speller and other search components, which is
particularly true for the most commonly deployed noisy-channel based speller
systems. With rapid development of deep learning and substantial advancement in
contextual representation learning such as BERTology, building a decent
misspelling detector without having to rely on hand-crafted features associated
with noisy-channel architecture becomes more-than-ever accessible. However
BERTolgy models are trained with natural language corpus but Maps Search is
highly domain specific, would BERTology continue its success. In this paper we
design 4 stages of models for misspeling detection ranging from the most basic
LSTM to single-domain augmented fine-tuned BERT. We found for Maps Search in
our case, other advanced BERTology family model such as RoBERTa does not
necessarily outperform BERT, and a classic cross-domain fine-tuned full BERT
even underperforms a smaller single-domain fine-tuned BERT. We share more
findings through comprehensive modeling experiments and analysis, we also
briefly cover the data generation algorithm breakthrough.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering Filipino Disaster-Related Tweets Using Incremental and Density-Based Spatiotemporal Algorithm with Support Vector Machines for Needs Assessment 2. (arXiv:2108.06853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06853">
<div class="article-summary-box-inner">
<span><p>Social media has played a huge part on how people get informed and
communicate with one another. It has helped people express their needs due to
distress especially during disasters. Because posts made through it are
publicly accessible by default, Twitter is among the most helpful social media
sites in times of disaster. With this, the study aims to assess the needs
expressed during calamities by Filipinos on Twitter. Data were gathered and
classified as either disaster-related or unrelated with the use of Na\"ive
Bayes classifier. After this, the disaster-related tweets were clustered per
disaster type using Incremental Clustering Algorithm, and then sub-clustered
based on the location and time of the tweet using Density-based Spatiotemporal
Clustering Algorithm. Lastly, using Support Vector Machines, the tweets were
classified according to the expressed need, such as shelter, rescue, relief,
cash, prayer, and others. After conducting the study, results showed that the
Incremental Clustering Algorithm and Density-Based Spatiotemporal Clustering
Algorithm were able to cluster the tweets with f-measure scores of 47.20% and
82.28% respectively. Also, the Na\"ive Bayes and Support Vector Machines were
able to classify with an average f-measure score of 97% and an average accuracy
of 77.57% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AutoChart: A Dataset for Chart-to-Text Generation Task. (arXiv:2108.06897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06897">
<div class="article-summary-box-inner">
<span><p>The analytical description of charts is an exciting and important research
area with many applications in academia and industry. Yet, this challenging
task has received limited attention from the computational linguistics research
community. This paper proposes \textsf{AutoChart}, a large dataset for the
analytical description of charts, which aims to encourage more research into
this important area. Specifically, we offer a novel framework that generates
the charts and their analytical description automatically. We conducted
extensive human and machine evaluations on the generated charts and
descriptions and demonstrate that the generated texts are informative,
coherent, and relevant to the corresponding charts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Mood Analysis with Knowledge Graph Representation for Hindi Song Lyrics in Devanagari Script. (arXiv:2108.06947v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06947">
<div class="article-summary-box-inner">
<span><p>Lyrics play a significant role in conveying the song's mood and are
information to understand and interpret music communication. Conventional
natural language processing approaches use translation of the Hindi text into
English for analysis. This approach is not suitable for lyrics as it is likely
to lose the inherent intended contextual meaning. Thus, the need was identified
to develop a system for Devanagari text analysis. The data set of 300 song
lyrics with equal distribution in five different moods is used for the
experimentation. The proposed system performs contextual mood analysis of Hindi
song lyrics in Devanagari text format. The contextual analysis is stored as a
knowledge base, updated using an incremental learning approach with new data.
Contextual knowledge graph with moods and associated important contextual terms
provides the graphical representation of the lyric data set used. The testing
results show 64% accuracy for the mood prediction. This work can be easily
extended to applications related to Hindi literary work such as summarization,
indexing, contextual retrieval, context-based classification and grouping of
documents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MobIE: A German Dataset for Named Entity Recognition, Entity Linking and Relation Extraction in the Mobility Domain. (arXiv:2108.06955v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06955">
<div class="article-summary-box-inner">
<span><p>We present MobIE, a German-language dataset, which is human-annotated with 20
coarse- and fine-grained entity types and entity linking information for
geographically linkable entities. The dataset consists of 3,232 social media
texts and traffic reports with 91K tokens, and contains 20.5K annotated
entities, 13.1K of which are linked to a knowledge base. A subset of the
dataset is human-annotated with seven mobility-related, n-ary relation types,
while the remaining documents are annotated using a weakly-supervised labeling
approach implemented with the Snorkel framework. To the best of our knowledge,
this is the first German-language dataset that combines annotations for NER, EL
and RE, and thus can be used for joint and multi-task learning of these
fundamental information extraction tasks. We make MobIE public at
https://github.com/dfki-nlp/mobie.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Effective System for Multi-format Information Extraction. (arXiv:2108.06957v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06957">
<div class="article-summary-box-inner">
<span><p>The multi-format information extraction task in the 2021 Language and
Intelligence Challenge is designed to comprehensively evaluate information
extraction from different dimensions. It consists of an multiple slots relation
extraction subtask and two event extraction subtasks that extract events from
both sentence-level and document-level. Here we describe our system for this
multi-format information extraction competition task. Specifically, for the
relation extraction subtask, we convert it to a traditional triple extraction
task and design a voting based method that makes full use of existing models.
For the sentence-level event extraction subtask, we convert it to a NER task
and use a pointer labeling based method for extraction. Furthermore,
considering the annotated trigger information may be helpful for event
extraction, we design an auxiliary trigger recognition model and use the
multi-task learning mechanism to integrate the trigger features into the event
extraction model. For the document-level event extraction subtask, we design an
Encoder-Decoder based method and propose a Transformer-alike decoder.
Finally,our system ranks No.4 on the test set leader-board of this multi-format
information extraction task, and its F1 scores for the subtasks of relation
extraction, event extractions of sentence-level and document-level are 79.887%,
85.179%, and 70.828% respectively. The codes of our model are available at
{https://github.com/neukg/MultiIE}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Single Example Can Improve Zero-Shot Data Generation. (arXiv:2108.06991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06991">
<div class="article-summary-box-inner">
<span><p>Sub-tasks of intent classification, such as robustness to distribution shift,
adaptation to specific user groups and personalization, out-of-domain
detection, require extensive and flexible datasets for experiments and
evaluation. As collecting such datasets is time- and labor-consuming, we
propose to use text generation methods to gather datasets. The generator should
be trained to generate utterances that belong to the given intent. We explore
two approaches to generating task-oriented utterances. In the zero-shot
approach, the model is trained to generate utterances from seen intents and is
further used to generate utterances for intents unseen during training. In the
one-shot approach, the model is presented with a single utterance from a test
intent. We perform a thorough automatic, and human evaluation of the dataset
generated utilizing two proposed approaches. Our results reveal that the
attributes of the generated data are close to original test sets, collected via
crowd-sourcing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Effective Non-Autoregressive Model for Spoken Language Understanding. (arXiv:2108.07005v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07005">
<div class="article-summary-box-inner">
<span><p>Spoken Language Understanding (SLU), a core component of the task-oriented
dialogue system, expects a shorter inference latency due to the impatience of
humans. Non-autoregressive SLU models clearly increase the inference speed but
suffer uncoordinated-slot problems caused by the lack of sequential dependency
information among each slot chunk. To gap this shortcoming, in this paper, we
propose a novel non-autoregressive SLU model named Layered-Refine Transformer,
which contains a Slot Label Generation (SLG) task and a Layered Refine
Mechanism (LRM). SLG is defined as generating the next slot label with the
token sequence and generated slot labels. With SLG, the non-autoregressive
model can efficiently obtain dependency information during training and spend
no extra time in inference. LRM predicts the preliminary SLU results from
Transformer's middle states and utilizes them to guide the final prediction.
Experiments on two public datasets indicate that our model significantly
improves SLU performance (1.5\% on Overall accuracy) while substantially speed
up (more than 10 times) the inference process over the state-of-the-art
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration. (arXiv:2108.07073v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07073">
<div class="article-summary-box-inner">
<span><p>Vision-and-language pretraining (VLP) aims to learn generic multimodal
representations from massive image-text pairs. While various successful
attempts have been proposed, learning fine-grained semantic alignments between
image-text pairs plays a key role in their approaches. Nevertheless, most
existing VLP approaches have not fully utilized the intrinsic knowledge within
the image-text pairs, which limits the effectiveness of the learned alignments
and further restricts the performance of their models. To this end, we
introduce a new VLP method called ROSITA, which integrates the cross- and
intra-modal knowledge in a unified scene graph to enhance the semantic
alignments. Specifically, we introduce a novel structural knowledge masking
(SKM) strategy to use the scene graph structure as a priori to perform masked
language (region) modeling, which enhances the semantic alignments by
eliminating the interference information within and across modalities.
Extensive ablation studies and comprehensive analysis verifies the
effectiveness of ROSITA in semantic alignments. Pretrained with both in-domain
and out-of-domain datasets, ROSITA significantly outperforms existing
state-of-the-art VLP methods on three typical vision-and-language tasks over
six benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages. (arXiv:2108.07127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07127">
<div class="article-summary-box-inner">
<span><p>We translate a closed text that is known in advance and available in many
languages into a new and severely low resource language. Most human translation
efforts adopt a portion-based approach to translate consecutive pages/chapters
in order, which may not suit machine translation. We compare the portion-based
approach that optimizes coherence of the text locally with the random sampling
approach that increases coverage of the text globally. Our results show that
the random sampling approach performs better. When training on a seed corpus of
~1,000 lines from the Bible and testing on the rest of the Bible (~30,000
lines), random sampling gives a performance gain of +11.0 BLEU using English as
a simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a
Mayan language. Furthermore, we compare three ways of updating machine
translation models with increasing amount of human post-edited data through
iterations. We find that adding newly post-edited data to training after
vocabulary update without self-supervision performs the best. We propose an
algorithm for human and machine to work together seamlessly to translate a
closed text into a severely low resource language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label-Agnostic Sequence Labeling by Copying Nearest Neighbors. (arXiv:1906.04225v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.04225">
<div class="article-summary-box-inner">
<span><p>Retrieve-and-edit based approaches to structured prediction, where structures
associated with retrieved neighbors are edited to form new structures, have
recently attracted increased interest. However, much recent work merely
conditions on retrieved structures (e.g., in a sequence-to-sequence framework),
rather than explicitly manipulating them. We show we can perform accurate
sequence labeling by explicitly (and only) copying labels from retrieved
neighbors. Moreover, because this copying is label-agnostic, we can achieve
impressive performance in zero-shot sequence-labeling tasks. We additionally
consider a dynamic programming approach to sequence labeling in the presence of
retrieved neighbors, which allows for controlling the number of distinct
(copied) segments used to form a prediction, and leads to both more
interpretable and accurate predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01777">
<div class="article-summary-box-inner">
<span><p>We propose a novel interpretable deep neural network for text classification,
called ProtoryNet, based on a new concept of prototype trajectories. Motivated
by the prototype theory in modern linguistics, ProtoryNet makes a prediction by
finding the most similar prototype for each sentence in a text sequence and
feeding an RNN backbone with the proximity of each sentence to the
corresponding active prototype. The RNN backbone then captures the temporal
pattern of the prototypes, which we refer to as prototype trajectories.
Prototype trajectories enable intuitive and fine-grained interpretation of the
reasoning process of the RNN model, in resemblance to how humans analyze texts.
We also design a prototype pruning procedure to reduce the total number of
prototypes used by the model for better interpretability. Experiments on
multiple public data sets show that ProtoryNet is more accurate than the
baseline prototype-based deep neural net and reduces the performance gap
compared to state-of-the-art black-box models. In addition, after prototype
pruning, the resulting ProtoryNet models only need less than or around 20
prototypes for all datasets, which significantly benefits interpretability.
Furthermore, we report a survey result indicating that human users find
ProtoryNet more intuitive and easier to understand than other prototype-based
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Streaming Approach For Efficient Batched Beam Search. (arXiv:2010.02164v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02164">
<div class="article-summary-box-inner">
<span><p>We propose an efficient batching strategy for variable-length decoding on GPU
architectures. During decoding, when candidates terminate or are pruned
according to heuristics, our streaming approach periodically "refills" the
batch before proceeding with a selected subset of candidates. We apply our
method to variable-width beam search on a state-of-the-art machine translation
model. Our method decreases runtime by up to 71% compared to a fixed-width beam
search baseline and 17% compared to a variable-width baseline, while matching
baselines' BLEU. Finally, experiments show that our method can speed up
decoding in other domains, such as semantic and syntactic parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Based Construction of Confusion Matrices for Multi-Label Classification Algorithms using Semantic Similarity Measures. (arXiv:2011.00109v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00109">
<div class="article-summary-box-inner">
<span><p>So far, multi-label classification algorithms have been evaluated using
statistical methods that do not consider the semantics of the considered
classes and that fully depend on abstract computations such as Bayesian
Reasoning. Currently, there are several attempts to develop ontology-based
methods for a better assessment of supervised classification algorithms. In
this research paper, we define a novel approach that aligns expected labels
with predicted labels in multi-label classification using ontology-driven
feature-based semantic similarity measures and we use it to develop a method
for creating precise confusion matrices for a more effective evaluation of
multi-label classification algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lessons from Computational Modelling of Reference Production in Mandarin and English. (arXiv:2011.07398v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07398">
<div class="article-summary-box-inner">
<span><p>Referring expression generation (REG) algorithms offer computational models
of the production of referring expressions. In earlier work, a corpus of
referring expressions (REs) in Mandarin was introduced. In the present paper,
we annotate this corpus, evaluate classic REG algorithms on it, and compare the
results with earlier results on the evaluation of REG for English referring
expressions. Next, we offer an in-depth analysis of the corpus, focusing on
issues that arise from the grammar of Mandarin. We discuss shortcomings of
previous REG evaluations that came to light during our investigation and we
highlight some surprising results. Perhaps most strikingly, we found a much
higher proportion of under-specified expressions than previous studies had
suggested, not just in Mandarin but in English as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Integrated Approach for Improving Brand Consistency of Web Content: Modeling, Analysis and Recommendation. (arXiv:2011.09754v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09754">
<div class="article-summary-box-inner">
<span><p>A consumer-dependent (business-to-consumer) organization tends to present
itself as possessing a set of human qualities, which is termed as the brand
personality of the company. The perception is impressed upon the consumer
through the content, be it in the form of advertisement, blogs or magazines,
produced by the organization. A consistent brand will generate trust and retain
customers over time as they develop an affinity towards regularity and common
patterns. However, maintaining a consistent messaging tone for a brand has
become more challenging with the virtual explosion in the amount of content
which needs to be authored and pushed to the Internet to maintain an edge in
the era of digital marketing. To understand the depth of the problem, we
collect around 300K web page content from around 650 companies. We develop
trait-specific classification models by considering the linguistic features of
the content. The classifier automatically identifies the web articles which are
not consistent with the mission and vision of a company and further helps us to
discover the conditions under which the consistency cannot be maintained. To
address the brand inconsistency issue, we then develop a sentence ranking
system that outputs the top three sentences that need to be changed for making
a web article more consistent with the company's brand personality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLG-Net: Video-Language Graph Matching Network for Video Grounding. (arXiv:2011.10132v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10132">
<div class="article-summary-box-inner">
<span><p>Grounding language queries in videos aims at identifying the time interval
(or moment) semantically relevant to a language query. The solution to this
challenging task demands understanding videos' and queries' semantic content
and the fine-grained reasoning about their multi-modal interactions. Our key
idea is to recast this challenge into an algorithmic graph matching problem.
Fueled by recent advances in Graph Neural Networks, we propose to leverage
Graph Convolutional Networks to model video and textual information as well as
their semantic alignment. To enable the mutual exchange of information across
the modalities, we design a novel Video-Language Graph Matching Network
(VLG-Net) to match video and query graphs. Core ingredients include
representation graphs built atop video snippets and query tokens separately and
used to model intra-modality relationships. A Graph Matching layer is adopted
for cross-modal context modeling and multi-modal fusion. Finally, moment
candidates are created using masked moment attention pooling by fusing the
moment's enriched snippet features. We demonstrate superior performance over
state-of-the-art grounding methods on three widely used datasets for temporal
localization of moments in videos with language queries: ActivityNet-Captions,
TACoS, and DiDeMo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vocabulary Learning via Optimal Transport for Neural Machine Translation. (arXiv:2012.15671v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15671">
<div class="article-summary-box-inner">
<span><p>The choice of token vocabulary affects the performance of machine
translation. This paper aims to figure out what is a good vocabulary and
whether one can find the optimal vocabulary without trial training. To answer
these questions, we first provide an alternative understanding of the role of
vocabulary from the perspective of information theory. Motivated by this, we
formulate the quest of vocabularization -- finding the best token dictionary
with a proper size -- as an optimal transport (OT) problem. We propose VOLT, a
simple and efficient solution without trial training. Empirical results show
that VOLT outperforms widely-used vocabularies in diverse scenarios, including
WMT-14 English-German and TED's 52 translation directions. For example, VOLT
achieves almost 70% vocabulary size reduction and 0.5 BLEU gain on
English-German translation. Also, compared to BPE-search, VOLT reduces the
search time from 384 GPU hours to 30 GPU hours on English-German translation.
Codes are available at https://github.com/Jingjing-NLP/VOLT .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Defense of Scene Graphs for Image Captioning. (arXiv:2102.04990v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04990">
<div class="article-summary-box-inner">
<span><p>The mainstream image captioning models rely on Convolutional Neural Network
(CNN) image features to generate captions via recurrent models. Recently, image
scene graphs have been used to augment captioning models so as to leverage
their structural semantics, such as object entities, relationships and
attributes. Several studies have noted that the naive use of scene graphs from
a black-box scene graph generator harms image captioning performance and that
scene graph-based captioning models have to incur the overhead of explicit use
of image features to generate decent captions. Addressing these challenges, we
propose \textbf{SG2Caps}, a framework that utilizes only the scene graph labels
for competitive image captioning performance. The basic idea is to close the
semantic gap between the two scene graphs - one derived from the input image
and the other from its caption. In order to achieve this, we leverage the
spatial location of objects and the Human-Object-Interaction (HOI) labels as an
additional HOI graph. SG2Caps outperforms existing scene graph-only captioning
models by a large margin, indicating scene graphs as a promising representation
for image captioning. Direct utilization of scene graph labels avoids expensive
graph convolutions over high-dimensional CNN features resulting in 49% fewer
trainable parameters. Our code is available at:
https://github.com/Kien085/SG2Caps
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferability of Neural Network-based De-identification Systems. (arXiv:2102.08517v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08517">
<div class="article-summary-box-inner">
<span><p>Methods and Materials: We investigated transferability of neural
network-based de-identification sys-tems with and without domain
generalization. We used two domain generalization approaches: a novel approach
Joint-Domain Learning (JDL) as developed in this paper, and a state-of-the-art
domain general-ization approach Common-Specific Decomposition (CSD) from the
literature. First, we measured trans-ferability from a single external source.
Second, we used two external sources and evaluated whether domain
generalization can improve transferability of de-identification models across
domains which rep-resent different note types from the same institution. Third,
using two external sources with in-domain training data, we studied whether
external source data are useful even in cases where sufficient in-domain
training data are available. Finally, we investigated transferability of the
de-identification mod-els across institutions. Results and Conclusions: We
found transferability from a single external source gave inconsistent re-sults.
Using additional external sources consistently yielded an F1-score of
approximately 80%, but domain generalization was not always helpful to improve
transferability. We also found that external sources were useful even in cases
where in-domain training data were available by reducing the amount of needed
in-domain training data or by improving performance. Transferability across
institutions was differed by note type and annotation label. External sources
from a different institution were also useful to further improve performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Effect of Domain and Diacritics in Yor\`ub\'a-English Neural Machine Translation. (arXiv:2103.08647v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08647">
<div class="article-summary-box-inner">
<span><p>Massively multilingual machine translation (MT) has shown impressive
capabilities, including zero and few-shot translation between low-resource
language pairs. However, these models are often evaluated on high-resource
languages with the assumption that they generalize to low-resource ones. The
difficulty of evaluating MT models on low-resource pairs is often due to lack
of standardized evaluation datasets. In this paper, we present MENYO-20k, the
first multi-domain parallel corpus with a special focus on clean orthography
for Yor\`ub\'a--English with standardized train-test splits for benchmarking.
We provide several neural MT benchmarks and compare them to the performance of
popular pre-trained (massively multilingual) MT models both for the
heterogeneous test set and its subdomains. Since these pre-trained models use
huge amounts of data with uncertain quality, we also analyze the effect of
diacritics, a major characteristic of Yor\`ub\'a, in the training data. We
investigate how and when this training condition affects the final quality and
intelligibility of a translation. Our models outperform massively multilingual
models such as Google ($+8.7$ BLEU) and Facebook M2M ($+9.1$ BLEU) when
translating to Yor\`ub\'a, setting a high quality benchmark for future
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. (arXiv:2104.04473v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04473">
<div class="article-summary-box-inner">
<span><p>Large language models have led to state-of-the-art accuracies across a range
of tasks. However, training these models efficiently is challenging for two
reasons: a) GPU memory capacity is limited, making it impossible to fit large
models on even a multi-GPU server, and b) the number of compute operations
required to train these models can result in unrealistically long training
times. Consequently, new methods of model parallelism such as tensor and
pipeline parallelism have been proposed. Unfortunately, naive usage of these
methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to
expensive cross-node communication or devices spending significant time waiting
on other devices to make progress.
</p>
<p>In this paper, we show how different types of parallelism methods (tensor,
pipeline, and data parallelism) can be composed to scale to thousands of GPUs
and models with trillions of parameters. We survey techniques for pipeline
parallelism and propose a novel interleaved pipeline parallelism schedule that
can improve throughput by 10+% with memory footprint comparable to existing
approaches. We quantitatively study the trade-offs between tensor, pipeline,
and data parallelism, and provide intuition as to how to configure distributed
training of a large model. Our approach allows us to perform training
iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs
with achieved per-GPU throughput of 52% of theoretical peak. Our code is open
sourced at https://github.com/nvidia/megatron-lm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FUDGE: Controlled Text Generation With Future Discriminators. (arXiv:2104.05218v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05218">
<div class="article-summary-box-inner">
<span><p>We propose Future Discriminators for Generation (FUDGE), a flexible and
modular method for controlled text generation. Given a pre-existing model G for
generating text from a distribution of interest, FUDGE enables conditioning on
a desired attribute a (for example, formality) while requiring access only to
G's output logits. FUDGE learns an attribute predictor operating on a partial
sequence, and uses this predictor's outputs to adjust G's original
probabilities. We show that FUDGE models terms corresponding to a Bayesian
decomposition of the conditional distribution of G given attribute a. Moreover,
FUDGE can easily compose predictors for multiple desired attributes. We
evaluate FUDGE on three tasks -- couplet completion in poetry, topic control in
language generation, and formality change in machine translation -- and observe
gains in all three tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09106">
<div class="article-summary-box-inner">
<span><p>Subword units are commonly used for end-to-end automatic speech recognition
(ASR), while a fully acoustic-oriented subword modeling approach is somewhat
missing. We propose an acoustic data-driven subword modeling (ADSM) approach
that adapts the advantages of several text-based and acoustic-based subword
methods into one pipeline. With a fully acoustic-oriented label design and
learning process, ADSM produces acoustic-structured subword units and
acoustic-matched target sequence for further ASR training. The obtained ADSM
labels are evaluated with different end-to-end ASR approaches including CTC,
RNN-Transducer and attention models. Experiments on the LibriSpeech corpus show
that ADSM clearly outperforms both byte pair encoding (BPE) and
pronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis
shows that ADSM achieves acoustically more logical word segmentation and more
balanced sequence length, and thus, is suitable for both time-synchronous and
label-synchronous models. We also briefly describe how to apply acoustic-based
subword regularization and unseen text segmentation using ADSM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection. (arXiv:2104.09261v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09261">
<div class="article-summary-box-inner">
<span><p>The existence of multiple datasets for sarcasm detection prompts us to apply
transfer learning to exploit their commonality. The adversarial neural transfer
(ANT) framework utilizes multiple loss terms that encourage the source-domain
and the target-domain feature distributions to be similar while optimizing for
domain-specific performance. However, these objectives may be in conflict,
which can lead to optimization difficulties and sometimes diminished transfer.
We propose a generalized latent optimization strategy that allows different
losses to accommodate each other and improves training dynamics. The proposed
method outperforms transfer learning and meta-learning baselines. In
particular, we achieve 10.02% absolute performance gain over the previous state
of the art on the iSarcasm dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07340">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the original BERT (i.e., BASE BERT), researchers
have developed various customized BERT models with improved performance for
specific domains and tasks by exploiting the benefits of {\em transfer
learning}. Due to the nature of mathematical texts, which often use domain
specific vocabulary along with equations and math symbols, we posit that the
development of a new BERT model for mathematics would be useful for many
mathematical downstream tasks. In this resource paper, we introduce our
multi-institutional effort (i.e., two learning platforms and three academic
institutions in the US) toward this need: MathBERT, a model created by
pre-training the BASE BERT model on a large mathematical corpus ranging from
pre-kindergarten (pre-k), to high-school, to college graduate level
mathematical content. In addition, we select three general NLP tasks that are
often used in mathematics education: prediction of knowledge component,
auto-grading open-ended Q\&amp;A, and knowledge tracing, to demonstrate the
superiority of MathBERT over BASE BERT. Our experiments show that MathBERT
outperforms prior best methods by 1.2-22\% and BASE BERT by 2-8\% on these
tasks. In addition, we build a mathematics specific vocabulary `mathVocab' to
train with MathBERT. We discover that MathBERT pre-trained with `mathVocab'
outperforms MathBERT trained with the BASE BERT vocabulary (i.e., `origVocab').
MathBERT is currently being adopted at the participated leaning platforms:
Stride, Inc, a commercial educational resource provider, and ASSISTments.org, a
free online educational platform. We release MathBERT for public usage at:
https://github.com/tbs17/MathBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FST: the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task. (arXiv:2107.06959v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06959">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe our end-to-end multilingual speech translation
system submitted to the IWSLT 2021 evaluation campaign on the Multilingual
Speech Translation shared task. Our system is built by leveraging transfer
learning across modalities, tasks and languages. First, we leverage
general-purpose multilingual modules pretrained with large amounts of
unlabelled and labelled data. We further enable knowledge transfer from the
text task to the speech task by training two tasks jointly. Finally, our
multilingual model is finetuned on speech translation task-specific data to
achieve the best translation results. Experimental results show our system
outperforms the reported systems, including both end-to-end and cascaded based
approaches, by a large margin.
</p>
<p>In some translation directions, our speech translation results evaluated on
the public Multilingual TEDx test set are even comparable with the ones from a
strong text-to-text translation system, which uses the oracle speech
transcripts as input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clinical Relation Extraction Using Transformer-based Models. (arXiv:2107.08957v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08957">
<div class="article-summary-box-inner">
<span><p>The newly emerged transformer technology has a tremendous impact on NLP
research. In the general English domain, transformer-based models have achieved
state-of-the-art performances on various NLP benchmarks. In the clinical
domain, researchers also have investigated transformer models for clinical
applications. The goal of this study is to systematically explore three widely
used transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical
relation extraction and develop an open-source package with clinical
pre-trained transformer-based models to facilitate information extraction in
the clinical domain. We developed a series of clinical RE models based on three
transformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these
models using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2
challenges. We compared two classification strategies (binary vs. multi-class
classification) and investigated two approaches to generate candidate relations
in different experimental settings. In this study, we compared three
transformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We
demonstrated that the RoBERTa-clinical RE model achieved the best performance
on the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2
dataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our
results indicated that the binary classification strategy consistently
outperformed the multi-class classification strategy for clinical relation
extraction. Our methods and models are publicly available at
https://github.com/uf-hobi-informatics-lab/ClinicalTransformerRelationExtraction.
We believe this work will improve current practice on clinical relation
extraction and other related NLP tasks in the biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03857">
<div class="article-summary-box-inner">
<span><p>"Art is the lie that enables us to realize the truth." - Pablo Picasso. For
centuries, humans have dedicated themselves to producing arts to convey their
imagination. The advancement in technology and deep learning in particular, has
caught the attention of many researchers trying to investigate whether art
generation is possible by computers and algorithms. Using generative
adversarial networks (GANs), applications such as synthesizing photorealistic
human faces and creating captions automatically from images were realized. This
survey takes a comprehensive look at the recent works using GANs for generating
visual arts, music, and literary text. A performance comparison and description
of the various GAN architecture are also presented. Finally, some of the key
challenges in art generation using GANs are highlighted along with
recommendations for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Noisy Channel Language Model Prompting for Few-Shot Text Classification. (arXiv:2108.04106v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04106">
<div class="article-summary-box-inner">
<span><p>We introduce a noisy channel approach for language model prompting in
few-shot text classification. Instead of computing the likelihood of the label
given the input (referred as direct models), channel models compute the
conditional probability of the input given the label, and are thereby required
to explain every word in the input. We use channel models for recently proposed
few-shot learning methods with no or very limited updates to the language model
parameters, via either in-context demonstration or prompt tuning. Our
experiments show that, for both methods, channel models significantly
outperform their direct counterparts, which we attribute to their stability,
i.e., lower variance and higher worst-case accuracy. We also present extensive
ablations that provide recommendations for when to use channel prompt tuning
instead of other competitive models (e.g., direct head tuning): channel prompt
tuning is preferred when the number of training examples is small, labels in
the training data are imbalanced, or generalization to unseen labels is
required.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generation Challenges: Results of the Accuracy Evaluation Shared Task. (arXiv:2108.05644v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05644">
<div class="article-summary-box-inner">
<span><p>The Shared Task on Evaluating Accuracy focused on techniques (both manual and
automatic) for evaluating the factual accuracy of texts produced by neural NLG
systems, in a sports-reporting domain. Four teams submitted evaluation
techniques for this task, using very different approaches and techniques. The
best-performing submissions did encouragingly well at this difficult task.
However, all automatic submissions struggled to detect factual errors which are
semantically or pragmatically complex (for example, based on incorrect
computation or inference).
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-17 23:08:56.470724971 UTC">2021-08-17 23:08:56 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>