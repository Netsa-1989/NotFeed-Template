<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-18T01:30:00Z">08-18</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Generative Relation Linking for Question Answering over Knowledge Bases. (arXiv:2108.07337v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07337">
<div class="article-summary-box-inner">
<span><p>Relation linking is essential to enable question answering over knowledge
bases. Although there are various efforts to improve relation linking
performance, the current state-of-the-art methods do not achieve optimal
results, therefore, negatively impacting the overall end-to-end question
answering performance. In this work, we propose a novel approach for relation
linking framing it as a generative problem facilitating the use of pre-trained
sequence-to-sequence models. We extend such sequence-to-sequence models with
the idea of infusing structured data from the target knowledge base, primarily
to enable these models to handle the nuances of the knowledge base. Moreover,
we train the model with the aim to generate a structured output consisting of a
list of argument-relation pairs, enabling a knowledge validation step. We
compared our method against the existing relation linking systems on four
different datasets derived from DBpedia and Wikidata. Our method reports large
improvements over the state-of-the-art while using a much simpler model that
can be easily adapted to different knowledge bases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IsoScore: Measuring the Uniformity of Vector Space Utilization. (arXiv:2108.07344v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07344">
<div class="article-summary-box-inner">
<span><p>The recent success of distributed word representations has led to an
increased interest in analyzing the properties of their spatial distribution.
Current metrics suggest that contextualized word embedding models do not
uniformly utilize all dimensions when embedding tokens in vector space. Here we
argue that existing metrics are fragile and tend to obfuscate the true spatial
distribution of point clouds. To ameliorate this issue, we propose IsoScore: a
novel metric which quantifies the degree to which a point cloud uniformly
utilizes the ambient vector space. We demonstrate that IsoScore has several
desirable properties such as mean invariance and direct correspondence to the
number of dimensions used, which are properties that existing scores do not
possess. Furthermore, IsoScore is conceptually intuitive and computationally
efficient, making it well suited for analyzing the distribution of point clouds
in arbitrary vector spaces, not necessarily limited to those of word embeddings
alone. Additionally, we use IsoScore to demonstrate that a number of recent
conclusions in the NLP literature that have been derived using brittle metrics
of spatial distribution, such as average cosine similarity, may be incomplete
or altogether inaccurate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An NLP approach to quantify dynamic salience of predefined topics in a text corpus. (arXiv:2108.07345v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07345">
<div class="article-summary-box-inner">
<span><p>The proliferation of news media available online simultaneously presents a
valuable resource and significant challenge to analysts aiming to profile and
understand social and cultural trends in a geographic location of interest.
While an abundance of news reports documenting significant events, trends, and
responses provides a more democratized picture of the social characteristics of
a location, making sense of an entire corpus to extract significant trends is a
steep challenge for any one analyst or team. Here, we present an approach using
natural language processing techniques that seeks to quantify how a set of
pre-defined topics of interest change over time across a large corpus of text.
We found that, given a predefined topic, we can identify and rank sets of
terms, or n-grams, that map to those topics and have usage patterns that
deviate from a normal baseline. Emergence, disappearance, or significant
variations in n-gram usage present a ground-up picture of a topic's dynamic
salience within a corpus of interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards. (arXiv:2108.07374v1 [cs.DB])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07374">
<div class="article-summary-box-inner">
<span><p>Developing documentation guidelines and easy-to-use templates for datasets
and models is a challenging task, especially given the variety of backgrounds,
skills, and incentives of the people involved in the building of natural
language processing (NLP) tools. Nevertheless, the adoption of standard
documentation practices across the field of NLP promotes more accessible and
detailed descriptions of NLP datasets and models, while supporting researchers
and developers in reflecting on their work. To help with the standardization of
documentation, we present two case studies of efforts that aim to develop
reusable documentation templates -- the HuggingFace data card, a general
purpose card for datasets in NLP, and the GEM benchmark data and model cards
with a focus on natural language generation. We describe our process for
developing these templates, including the identification of relevant
stakeholder groups, the definition of a set of guiding principles, the use of
existing templates as our foundation, and iterative revisions based on
feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Protein Using Large-scale Pretrain Language Model. (arXiv:2108.07435v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07435">
<div class="article-summary-box-inner">
<span><p>Protein is linked to almost every life process. Therefore, analyzing the
biological structure and property of protein sequences is critical to the
exploration of life, as well as disease detection and drug discovery.
Traditional protein analysis methods tend to be labor-intensive and
time-consuming. The emergence of deep learning models makes modeling data
patterns in large quantities of data possible. Interdisciplinary researchers
have begun to leverage deep learning methods to model large biological
datasets, e.g. using long short-term memory and convolutional neural network
for protein sequence classification. After millions of years of evolution,
evolutionary information is encoded in protein sequences. Inspired by the
similarity between natural language and protein sequences, we use large-scale
language models to model evolutionary-scale protein sequences, encoding protein
biology information in representation. Significant improvements are observed in
both token-level and sequence-level tasks, demonstrating that our large-scale
model can accurately capture evolution information from pretraining on
evolutionary-scale individual sequences. Our code and model are available at
https://github.com/THUDM/ProteinLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems. (arXiv:2108.07493v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07493">
<div class="article-summary-box-inner">
<span><p>It's challenging to customize transducer-based automatic speech recognition
(ASR) system with context information which is dynamic and unavailable during
model training. In this work, we introduce a light-weight contextual spelling
correction model to correct context-related recognition errors in
transducer-based ASR systems. We incorporate the context information into the
spelling correction model with a shared context encoder and use a filtering
algorithm to handle large-size context lists. Experiments show that the model
improves baseline ASR model performance with about 50% relative word error rate
reduction, which also significantly outperforms the baseline method such as
contextual LM biasing. The model also shows excellent performance for
out-of-vocabulary terms not seen during training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation Guidelines for the Turku Paraphrase Corpus. (arXiv:2108.07499v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07499">
<div class="article-summary-box-inner">
<span><p>This document describes the annotation guidelines used to construct the Turku
Paraphrase Corpus. These guidelines were developed together with the corpus
annotation, revising and extending the guidelines regularly during the
annotation work. Our paraphrase annotation scheme uses the base scale 1-4,
where labels 1 and 2 are used for negative candidates (not paraphrases), while
labels 3 and 4 are paraphrases at least in the given context if not everywhere.
In addition to base labeling, the scheme is enriched with additional
subcategories (flags) for categorizing different types of paraphrases inside
the two positive labels, making the annotation scheme suitable for more
fine-grained paraphrase categorization. The annotation scheme is used to
annotate over 100,000 Finnish paraphrase pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPMoE: Generate Multiple Pattern-Aware Outputs with Sparse Pattern Mixture of Expert. (arXiv:2108.07535v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07535">
<div class="article-summary-box-inner">
<span><p>Many generation tasks follow a one-to-many mapping relationship: each input
could be associated with multiple outputs. Existing methods like Conditional
Variational AutoEncoder(CVAE) employ a latent variable to model this
one-to-many relationship. However, this high-dimensional and dense latent
variable lacks explainability and usually leads to poor and uncontrollable
generations. In this paper, we innovatively introduce the linguistic concept of
pattern to decompose the one-to-many mapping into multiple one-to-one mappings
and further propose a model named Sparse Pattern Mixture of Experts(SPMoE).
Each one-to-one mapping is associated with a conditional generation pattern and
is modeled with an expert in SPMoE. To ensure each language pattern can be
exclusively handled with an expert model for better explainability and
diversity, a sparse mechanism is employed to coordinate all the expert models
in SPMoE. We assess the performance of our SPMoE on the paraphrase generation
task and the experiment results prove that SPMoE can achieve a good balance in
terms of quality, pattern-level diversity, and corpus-level diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Capsule Aggregation for Unaligned Multimodal Sequences. (arXiv:2108.07543v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07543">
<div class="article-summary-box-inner">
<span><p>Humans express their opinions and emotions through multiple modalities which
mainly consist of textual, acoustic and visual modalities. Prior works on
multimodal sentiment analysis mostly apply Recurrent Neural Network (RNN) to
model aligned multimodal sequences. However, it is unpractical to align
multimodal sequences due to different sample rates for different modalities.
Moreover, RNN is prone to the issues of gradient vanishing or exploding and it
has limited capacity of learning long-range dependency which is the major
obstacle to model unaligned multimodal sequences. In this paper, we introduce
Graph Capsule Aggregation (GraphCAGE) to model unaligned multimodal sequences
with graph-based neural model and Capsule Network. By converting sequence data
into graph, the previously mentioned problems of RNN are avoided. In addition,
the aggregation capability of Capsule Network and the graph-based structure
enable our model to be interpretable and better solve the problem of long-range
dependency. Experimental results suggest that GraphCAGE achieves
state-of-the-art performance on two benchmark datasets with representations
refined by Capsule Network and interpretation provided.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Linearizations Are Equally Data-Hungry in Sequence Labeling Parsing. (arXiv:2108.07556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07556">
<div class="article-summary-box-inner">
<span><p>Different linearizations have been proposed to cast dependency parsing as
sequence labeling and solve the task as: (i) a head selection problem, (ii)
finding a representation of the token arcs as bracket strings, or (iii)
associating partial transition sequences of a transition-based parser to words.
Yet, there is little understanding about how these linearizations behave in
low-resource setups. Here, we first study their data efficiency, simulating
data-restricted setups from a diverse set of rich-resource treebanks. Second,
we test whether such differences manifest in truly low-resource setups. The
results show that head selection encodings are more data-efficient and perform
better in an ideal (gold) framework, but that such advantage greatly vanishes
in favour of bracketing formats when the running setup resembles a real-world
low-resource configuration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. (arXiv:2108.07571v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07571">
<div class="article-summary-box-inner">
<span><p>Citation recommendation is intended to assist researchers in the process of
searching for relevant papers to cite by recommending appropriate citations for
a given input text. Existing test collections for this task are noisy and
unreliable since they are built automatically from parsed PDF papers. In this
paper, we present our ongoing effort at creating a publicly available, manually
annotated test collection for citation recommendation. We also conduct a series
of experiments to evaluate the effectiveness of content-based baseline models
on the test collection, providing results for future work to improve upon. Our
test collection and code to replicate experiments are available at
https://github.com/boudinfl/acm-cr
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MigrationsKB: A Knowledge Base of Public Attitudes towards Migrations and their Driving Factors. (arXiv:2108.07593v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07593">
<div class="article-summary-box-inner">
<span><p>With the increasing trend in the topic of migration in Europe, the public is
now more engaged in expressing their opinions through various platforms such as
Twitter. Understanding the online discourses is therefore essential to capture
the public opinion. The goal of this study is the analysis of social media
platform to quantify public attitudes towards migrations and the identification
of different factors causing these attitudes. The tweets spanning from 2013 to
Jul-2021 in the European countries which are hosts to immigrants are collected,
pre-processed, and filtered using advanced topic modeling technique. BERT-based
entity linking and sentiment analysis, and attention-based hate speech
detection are performed to annotate the curated tweets. Moreover, the external
databases are used to identify the potential social and economic factors
causing negative attitudes of the people about migration. To further promote
research in the interdisciplinary fields of social science and computer
science, the outcomes are integrated into a Knowledge Base (KB), i.e.,
MigrationsKB which significantly extends the existing models to take into
account the public attitudes towards migrations and the economic indicators.
This KB is made public using FAIR principles, which can be queried through
SPARQL endpoint. Data dumps are made available on Zenodo.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study. (arXiv:2108.07627v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07627">
<div class="article-summary-box-inner">
<span><p>An independent ethical assessment of an artificial intelligence system is an
impartial examination of the system's development, deployment, and use in
alignment with ethical values. System-level qualitative frameworks that
describe high-level requirements and component-level quantitative metrics that
measure individual ethical dimensions have been developed over the past few
years. However, there exists a gap between the two, which hinders the execution
of independent ethical assessments in practice. This study bridges this gap and
designs a holistic independent ethical assessment process for a text
classification model with a special focus on the task of hate speech detection.
The assessment is further augmented with protected attributes mining and
counterfactual-based analysis to enhance bias assessment. It covers assessments
of technical performance, data bias, embedding bias, classification bias, and
interpretability. The proposed process is demonstrated through an assessment of
a deep hate speech detection model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weak Supervised Dataset of Fine-Grained Emotions in Portuguese. (arXiv:2108.07638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07638">
<div class="article-summary-box-inner">
<span><p>Affective Computing is the study of how computers can recognize, interpret
and simulate human affects. Sentiment Analysis is a common task in NLP related
to this topic, but it focuses only on emotion valence (positive, negative,
neutral). An emerging approach in NLP is Emotion Recognition, which relies on
fined-grained classification. This research describes an approach to create a
lexical-based weak supervised corpus for fine-grained emotion in Portuguese. We
evaluate our dataset by fine-tuning a transformer-based language model (BERT)
and validating it on a Golden Standard annotated validation set. Our results
(F1-score= .64) suggest lexical-based weak supervision as an appropriate
strategy for initial work in low resources environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Incorrectness Logic and Kleene Algebra With Top and Tests. (arXiv:2108.07707v1 [cs.PL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07707">
<div class="article-summary-box-inner">
<span><p>Kleene algebra with tests (KAT) is a foundational equational framework for
reasoning about programs, which has found applications in program
transformations, networking and compiler optimizations, among many other areas.
In his seminal work, Kozen proved that KAT subsumes propositional Hoare logic,
showing that one can reason about the (partial) correctness of while programs
by means of the equational theory of KAT.
</p>
<p>In this work, we investigate the support that KAT provides for reasoning
about \emph{incorrectness}, instead, as embodied by Ohearn's recently proposed
incorrectness logic. We show that KAT cannot directly express incorrectness
logic. The main reason for this limitation can be traced to the fact that KAT
cannot express explicitly the notion of codomain, which is essential to express
incorrectness triples. To address this issue, we study Kleene algebra with Top
and Tests (TopKAT), an extension of KAT with a top element. We show that TopKAT
is powerful enough to express a codomain operation, to express incorrectness
triples, and to prove all the rules of incorrectness logic sound. This shows
that one can reason about the incorrectness of while-like programs by means of
the equational theory of TopKAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Game Interface to Study Semantic Grounding in Text-Based Models. (arXiv:2108.07708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07708">
<div class="article-summary-box-inner">
<span><p>Can language models learn grounded representations from text distribution
alone? This question is both central and recurrent in natural language
processing; authors generally agree that grounding requires more than textual
distribution. We propose to experimentally test this claim: if any two words
have different meanings and yet cannot be distinguished from distribution
alone, then grounding is out of the reach of text-based models. To that end, we
present early work on an online game for the collection of human judgments on
the distributional similarity of word pairs in five languages. We further
report early results of our data collection campaign.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Combining speakers of multiple languages to improve quality of neural voices. (arXiv:2108.07737v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07737">
<div class="article-summary-box-inner">
<span><p>In this work, we explore multiple architectures and training procedures for
developing a multi-speaker and multi-lingual neural TTS system with the goals
of a) improving the quality when the available data in the target language is
limited and b) enabling cross-lingual synthesis. We report results from a large
experiment using 30 speakers in 8 different languages across 15 different
locales. The system is trained on the same amount of data per speaker. Compared
to a single-speaker model, when the suggested system is fine tuned to a
speaker, it produces significantly better quality in most of the cases while it
only uses less than $40\%$ of the speaker's data used to build the
single-speaker model. In cross-lingual synthesis, on average, the generated
quality is within $80\%$ of native single-speaker models, in terms of Mean
Opinion Score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Multi-scale Convolution for Dialect Identification. (arXiv:2108.07787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07787">
<div class="article-summary-box-inner">
<span><p>Time Delay Neural Networks (TDNN)-based methods are widely used in dialect
identification. However, in previous work with TDNN application, subtle variant
is being neglected in different feature scales. To address this issue, we
propose a new architecture, named dynamic multi-scale convolution, which
consists of dynamic kernel convolution, local multi-scale learning, and global
multi-scale pooling. Dynamic kernel convolution captures features between
short-term and long-term context adaptively. Local multi-scale learning, which
represents multi-scale features at a granular level, is able to increase the
range of receptive fields for convolution operation. Besides, global
multi-scale pooling is applied to aggregate features from different bottleneck
layers in order to collect information from multiple aspects. The proposed
architecture significantly outperforms state-of-the-art system on the
AP20-OLR-dialect-task of oriental language recognition (OLR) challenge 2020,
with the best average cost performance (Cavg) of 0.067 and the best equal error
rate (EER) of 6.52%. Compared with the known best results, our method achieves
9% of Cavg and 45% of EER relative improvement, respectively. Furthermore, the
parameters of proposed model are 91% fewer than the best known model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition. (arXiv:2108.07789v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07789">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) pre-trained on massive amounts of text, in particular
bidirectional encoder representations from Transformers (BERT), generative
pre-training (GPT), and GPT-2, have become a key technology for many natural
language processing tasks. In this paper, we present results using fine-tuned
GPT, GPT-2, and their combination for automatic speech recognition (ASR).
Unlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct
product of the output probabilities is no longer a valid language prior
probability. A conversion method is proposed to compute the correct language
prior probability based on bidirectional LM outputs in a mathematically exact
way. Experimental results on the widely used AMI and Switchboard ASR tasks
showed that the combination of the fine-tuned GPT and GPT-2 outperformed the
combination of three neural LMs with different architectures trained from
scratch on the in-domain text by up to a 12% relative word error rate reduction
(WERR). Furthermore, the proposed conversion for language prior probabilities
enables BERT to receive an extra 3% relative WERR, and the combination of BERT,
GPT and GPT-2 results in further improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07790">
<div class="article-summary-box-inner">
<span><p>Language models trained on large-scale unfiltered datasets curated from the
open web acquire systemic biases, prejudices, and harmful views from their
training data. We present a methodology for programmatically identifying and
removing harmful text from web-scale datasets. A pretrained language model is
used to calculate the log-likelihood of researcher-written trigger phrases
conditioned on a specific document, which is used to identify and filter
documents from the dataset. We demonstrate that models trained on this filtered
dataset exhibit lower propensity to generate harmful text, with a marginal
decrease in performance on standard language modeling benchmarks compared to
unfiltered baselines. We provide a partial explanation for this performance gap
by surfacing examples of hate speech and other undesirable content from
standard language modeling benchmarks. Finally, we discuss the generalization
of this method and how trigger phrases which reflect specific values can be
used by researchers to build language models which are more closely aligned
with their values.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Resources for Bhojpuri, Magahi and Maithili: Statistics about them, their Similarity Estimates, and Baselines for Three Applications. (arXiv:2004.13945v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13945">
<div class="article-summary-box-inner">
<span><p>Corpus preparation for low-resource languages and for development of human
language technology to analyze or computationally process them is a laborious
task, primarily due to the unavailability of expert linguists who are native
speakers of these languages and also due to the time and resources required.
Bhojpuri, Magahi, and Maithili, languages of the Purvanchal region of India (in
the north-eastern parts), are low-resource languages belonging to the
Indo-Aryan (or Indic) family. They are closely related to Hindi, which is a
relatively high-resource language, which is why we compare with Hindi. We
collected corpora for these three languages from various sources and cleaned
them to the extent possible, without changing the data in them. The text
belongs to different domains and genres. We calculated some basic statistical
measures for these corpora at character, word, syllable, and morpheme levels.
These corpora were also annotated with parts-of-speech (POS) and chunk tags.
The basic statistical measures were both absolute and relative and were
exptected to indicate of linguistic properties such as morphological, lexical,
phonological, and syntactic complexities (or richness). The results were
compared with a standard Hindi corpus. For most of the measures, we tried to
the corpus size the same across the languages to avoid the effect of corpus
size, but in some cases it turned out that using the full corpus was better,
even if sizes were very different. Although the results are not very clear, we
try to draw some conclusions about the languages and the corpora. For POS
tagging and chunking, the BIS tagset was used to manually annotate the data.
The POS tagged data sizes are 16067, 14669 and 12310 sentences, respectively,
for Bhojpuri, Magahi and Maithili. The sizes for chunking are 9695 and 1954
sentences for Bhojpuri and Maithili, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering. (arXiv:2007.15207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15207">
<div class="article-summary-box-inner">
<span><p>Progress in cross-lingual modeling depends on challenging, realistic, and
diverse evaluation sets. We introduce Multilingual Knowledge Questions and
Answers (MKQA), an open-domain question answering evaluation set comprising 10k
question-answer pairs aligned across 26 typologically diverse languages (260k
question-answer pairs in total). Answers are based on a heavily curated,
language-independent data representation, making results comparable across
languages and independent of language-specific passages. With 26 languages,
this dataset supplies the widest range of languages to-date for evaluating
question answering. We benchmark a variety of state-of-the-art methods and
baselines for generative and extractive question answering, trained on Natural
Questions, in zero shot and translation settings. Results indicate this dataset
is challenging even in English, but especially in low-resource languages
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01377">
<div class="article-summary-box-inner">
<span><p>Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a
key requirement for both linguistic research and subsequent automated natural
language processing (NLP) tasks. This problem is commonly tackled using machine
learning methods, i.e., by training a POS tagger on a sufficiently large corpus
of labeled data. While the problem of POS tagging can essentially be considered
as solved for modern languages, historical corpora turn out to be much more
difficult, especially due to the lack of native speakers and sparsity of
training data. Moreover, most texts have no sentences as we know them today,
nor a common orthography. These irregularities render the task of automated POS
tagging more difficult and error-prone. Under these circumstances, instead of
forcing the POS tagger to predict and commit to a single tag, it should be
enabled to express its uncertainty. In this paper, we consider POS tagging
within the framework of set-valued prediction, which allows the POS tagger to
express its uncertainty via predicting a set of candidate POS tags instead of
guessing a single one. The goal is to guarantee a high confidence that the
correct POS tag is included while keeping the number of candidates small. In
our experimental study, we find that extending state-of-the-art POS taggers to
set-valued prediction yields more precise and robust taggings, especially for
unknown words, i.e., words not occurring in the training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Chemistry 101: Learning to Reason about Social and Moral Norms. (arXiv:2011.00620v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00620">
<div class="article-summary-box-inner">
<span><p>Social norms -- the unspoken commonsense rules about acceptable social
behavior -- are crucial in understanding the underlying causes and intents of
people's actions in narratives. For example, underlying an action such as
"wanting to call cops on my neighbors" are social norms that inform our
conduct, such as "It is expected that you report crimes."
</p>
<p>We present Social Chemistry, a new conceptual formalism to study people's
everyday social norms and moral judgments over a rich spectrum of real life
situations described in natural language. We introduce Social-Chem-101, a
large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run
a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further
broken down with 12 different dimensions of people's judgments, including
social judgments of good and bad, moral foundations, expected cultural
pressure, and assumed legality, which together amount to over 4.5 million
annotations of categorical labels and free-text descriptions.
</p>
<p>Comprehensive empirical results based on state-of-the-art neural models
demonstrate that computational modeling of social norms is a promising research
direction. Our model framework, Neural Norm Transformer, learns and generalizes
Social-Chem-101 to successfully reason about previously unseen situations,
generating relevant (and potentially novel) attribute-aware social
rules-of-thumb.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching. (arXiv:2101.06423v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06423">
<div class="article-summary-box-inner">
<span><p>Neural text matching models have been widely used in community question
answering, information retrieval, and dialogue. However, these models designed
for short texts cannot well address the long-form text matching problem,
because there are many contexts in long-form texts can not be directly aligned
with each other, and it is difficult for existing models to capture the key
matching signals from such noisy data. Besides, these models are
computationally expensive for simply use all textual data indiscriminately. To
tackle the effectiveness and efficiency problem, we propose a novel
hierarchical noise filtering model, namely Match-Ignition. The main idea is to
plug the well-known PageRank algorithm into the Transformer, to identify and
filter both sentence and word level noisy information in the matching process.
Noisy sentences are usually easy to detect because previous work has shown that
their similarity can be explicitly evaluated by the word overlapping, so we
directly use PageRank to filter such information based on a sentence similarity
graph. Unlike sentences, words rely on their contexts to express concrete
meanings, so we propose to jointly learn the filtering and matching process, to
well capture the critical word-level matching signals. Specifically, a word
graph is first built based on the attention scores in each self-attention block
of Transformer, and key words are then selected by applying PageRank on this
graph. In this way, noisy words will be filtered out layer by layer in the
matching process. Experimental results show that Match-Ignition outperforms
both SOTA short text matching models and recent long-form text matching models.
We also conduct detailed analysis to show that Match-Ignition efficiently
captures important sentences and words, to facilitate the long-form text
matching process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">In Defense of Scene Graphs for Image Captioning. (arXiv:2102.04990v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04990">
<div class="article-summary-box-inner">
<span><p>The mainstream image captioning models rely on Convolutional Neural Network
(CNN) image features to generate captions via recurrent models. Recently, image
scene graphs have been used to augment captioning models so as to leverage
their structural semantics, such as object entities, relationships and
attributes. Several studies have noted that the naive use of scene graphs from
a black-box scene graph generator harms image captioning performance and that
scene graph-based captioning models have to incur the overhead of explicit use
of image features to generate decent captions. Addressing these challenges, we
propose \textbf{SG2Caps}, a framework that utilizes only the scene graph labels
for competitive image captioning performance. The basic idea is to close the
semantic gap between the two scene graphs - one derived from the input image
and the other from its caption. In order to achieve this, we leverage the
spatial location of objects and the Human-Object-Interaction (HOI) labels as an
additional HOI graph. SG2Caps outperforms existing scene graph-only captioning
models by a large margin, indicating scene graphs as a promising representation
for image captioning. Direct utilization of scene graph labels avoids expensive
graph convolutions over high-dimensional CNN features resulting in 49% fewer
trainable parameters. Our code is available at:
https://github.com/Kien085/SG2Caps
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferability of Neural Network Clinical De-identification Systems. (arXiv:2102.08517v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08517">
<div class="article-summary-box-inner">
<span><p>Objective: Neural network de-identification studies have focused on
individual datasets. These studies assume the availability of a sufficient
amount of human-annotated data to train models that can generalize to
corresponding test data. In real-world situations, however, researchers often
have limited or no in-house training data. Existing systems and external data
can help jump-start de-identification on in-house data; however, the most
efficient way of utilizing existing systems and external data is unclear. This
article investigates the transferability of a state-of-the-art neural clinical
de-identification system, NeuroNER, across a variety of datasets, when it is
modified architecturally for domain generalization and when it is trained
strategically for domain transfer. Methods and Materials: We conducted a
comparative study of the transferability of NeuroNER using four clinical note
corpora with multiple note types from two institutions. We modified NeuroNER
architecturally to integrate two types of domain generalization approaches. We
evaluated each architecture using three training strategies. We measured:
transferability from external sources; transferability across note types; the
contribution of external source data when in-domain training data are
available; and transferability across institutions. Results and Conclusions:
Transferability from a single external source gave inconsistent results. Using
additional external sources consistently yielded an F1-score of approximately
80%. Fine-tuning emerged as a dominant transfer strategy, with or without
domain generalization. We also found that external sources were useful even in
cases where in-domain training data were available. Transferability across
institutions differed by note type and annotation label but resulted in
improved performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06132">
<div class="article-summary-box-inner">
<span><p>Answering questions about why characters perform certain actions is central
to understanding and reasoning about narratives. Despite recent progress in QA,
it is not clear if existing models have the ability to answer "why" questions
that may require commonsense knowledge external to the input narrative. In this
work, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more
than 30k questions and free-form answers concerning why characters in short
narratives perform the actions described. For a third of this dataset, the
answers are not present within the narrative. Given the limitations of
automated evaluation for this task, we also present a systematized human
evaluation interface for this dataset. Our evaluation of state-of-the-art
models show that they are far below human performance on answering such
questions. They are especially worse on questions whose answers are external to
the narrative, thus providing a challenge for future QA and narrative
understanding research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02605">
<div class="article-summary-box-inner">
<span><p>This report presents the results of the EENLP project, done as a part of EEML
2021 summer school.
</p>
<p>It presents a broad index of NLP resources for Eastern European languages,
which, we hope, could be helpful for the NLP community; several new
hand-crafted cross-lingual datasets focused on Eastern European languages, and
a sketch evaluation of cross-lingual transfer learning abilities of several
modern multilingual Transformer-based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07253">
<div class="article-summary-box-inner">
<span><p>We present a task and benchmark dataset for person-centric visual grounding,
the problem of linking between people named in a caption and people pictured in
an image. In contrast to prior work in visual grounding, which is predominantly
object-based, our new task masks out the names of people in captions in order
to encourage methods trained on such image-caption pairs to focus on contextual
cues (such as rich interactions between multiple people), rather than learning
associations between names and appearances. To facilitate this task, we
introduce a new dataset, Who's Waldo, mined automatically from image-caption
data on Wikimedia Commons. We propose a Transformer-based method that
outperforms several strong baselines on this task, and are releasing our data
to the research community to spur work on contextual models that consider both
vision and language.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-18 23:09:01.125735881 UTC">2021-08-18 23:09:01 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>