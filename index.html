<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-23T01:30:00Z">09-23</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammatical Profiling for Semantic Change Detection. (arXiv:2109.10397v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10397">
<div class="article-summary-box-inner">
<span><p>Semantics, morphology and syntax are strongly interdependent. However, the
majority of computational methods for semantic change detection use
distributional word representations which encode mostly semantics. We
investigate an alternative method, grammatical profiling, based entirely on
changes in the morphosyntactic behaviour of words. We demonstrate that it can
be used for semantic change detection and even outperforms some distributional
semantic methods. We present an in-depth qualitative and quantitative analysis
of the predictions made by our grammatical profiling system, showing that they
are plausible and interpretable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RETRONLU: Retrieval Augmented Task-Oriented Semantic Parsing. (arXiv:2109.10410v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10410">
<div class="article-summary-box-inner">
<span><p>While large pre-trained language models accumulate a lot of knowledge in
their parameters, it has been demonstrated that augmenting it with
non-parametric retrieval-based memory has a number of benefits from accuracy
improvements to data efficiency for knowledge-focused tasks, such as question
answering. In this paper, we are applying retrieval-based modeling ideas to the
problem of multi-domain task-oriented semantic parsing for conversational
assistants. Our approach, RetroNLU, extends a sequence-to-sequence model
architecture with a retrieval component, used to fetch existing similar
examples and provide them as an additional input to the model. In particular,
we analyze two settings, where we augment an input with (a) retrieved nearest
neighbor utterances (utterance-nn), and (b) ground-truth semantic parses of
nearest neighbor utterances (semparse-nn). Our technique outperforms the
baseline method by 1.5% absolute macro-F1, especially at the low resource
setting, matching the baseline model accuracy with only 40% of the data.
Furthermore, we analyze the nearest neighbor retrieval component's quality,
model sensitivity and break down the performance for semantic parses of
different utterance complexity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Would it Take to get Biomedical QA Systems into Practice?. (arXiv:2109.10415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10415">
<div class="article-summary-box-inner">
<span><p>Medical question answering (QA) systems have the potential to answer
clinicians uncertainties about treatment and diagnosis on demand, informed by
the latest evidence. However, despite the significant progress in general QA
made by the NLP community, medical QA systems are still not widely used in
clinical environments. One likely reason for this is that clinicians may not
readily trust QA system outputs, in part because transparency, trustworthiness,
and provenance have not been key considerations in the design of such models.
In this paper we discuss a set of criteria that, if met, we argue would likely
increase the utility of biomedical QA systems, which may in turn lead to
adoption of such systems in practice. We assess existing models, tasks, and
datasets with respect to these criteria, highlighting shortcomings of
previously proposed approaches and pointing toward what might be more usable QA
systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Debiasing Techniques for Intersectional Biases. (arXiv:2109.10441v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10441">
<div class="article-summary-box-inner">
<span><p>Bias is pervasive in NLP models, motivating the development of automatic
debiasing techniques. Evaluation of NLP debiasing methods has largely been
limited to binary attributes in isolation, e.g., debiasing with respect to
binary gender or race, however many corpora involve multiple such attributes,
possibly with higher cardinality. In this paper we argue that a truly fair
model must consider `gerrymandering' groups which comprise not only single
attributes, but also intersectional groups. We evaluate a form of
bias-constrained model which is new to NLP, as well an extension of the
iterative nullspace projection technique which can handle multiple protected
attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fairness-aware Class Imbalanced Learning. (arXiv:2109.10444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10444">
<div class="article-summary-box-inner">
<span><p>Class imbalance is a common challenge in many NLP tasks, and has clear
connections to bias, in that bias in training data often leads to higher
accuracy for majority groups at the expense of minority groups. However there
has traditionally been a disconnect between research on class-imbalanced
learning and mitigating bias, and only recently have the two been looked at
through a common lens. In this work we evaluate long-tail learning methods for
tweet sentiment and occupation classification, and extend a margin-loss based
approach with methods to enforce fairness. We empirically show through
controlled experiments that the proposed approaches help mitigate both class
imbalance and demographic biases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results. (arXiv:2109.10453v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10453">
<div class="article-summary-box-inner">
<span><p>Recent transformer-based approaches demonstrate promising results on
relational scientific information extraction. Existing datasets focus on
high-level description of how research is carried out. Instead we focus on the
subtleties of how experimental associations are presented by building SciClaim,
a dataset of scientific claims drawn from Social and Behavior Science (SBS),
PubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not
only coarse-grained entity spans as nodes and relations as edges between them,
but also fine-grained attributes that modify entities and their relations, for
a total of 12,738 labels in the corpus. By including more label types and more
than twice the label density of previous datasets, SciClaim captures causal,
comparative, predictive, statistical, and proportional associations over
experimental variables along with their qualifications, subtypes, and evidence.
We extend work in transformer-based joint entity and relation extraction to
effectively infer our schema, showing the promise of fine-grained knowledge
graphs in scientific claims and beyond.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable and Efficient MoE Training for Multitask Multilingual Models. (arXiv:2109.10465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10465">
<div class="article-summary-box-inner">
<span><p>The Mixture of Experts (MoE) models are an emerging class of sparsely
activated deep learning models that have sublinear compute costs with respect
to their parameters. In contrast with dense models, the sparse architecture of
MoE offers opportunities for drastically growing model size with significant
accuracy gain while consuming much lower compute budget. However, supporting
large scale MoE training also has its own set of system and modeling
challenges. To overcome the challenges and embrace the opportunities of MoE, we
first develop a system capable of scaling MoE models efficiently to trillions
of parameters. It combines multi-dimensional parallelism and heterogeneous
memory technologies harmoniously with MoE to empower 8x larger models on the
same hardware compared with existing work. Besides boosting system efficiency,
we also present new training methods to improve MoE sample efficiency and
leverage expert pruning strategy to improve inference time efficiency. By
combining the efficient system and training methods, we are able to
significantly scale up large multitask multilingual models for language
generation which results in a great improvement in model accuracy. A model
trained with 10 billion parameters on 50 languages can achieve state-of-the-art
performance in Machine Translation (MT) and multilingual natural language
generation tasks. The system support of efficient MoE training has been
implemented and open-sourced with the DeepSpeed library.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salience-Aware Event Chain Modeling for Narrative Understanding. (arXiv:2109.10475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10475">
<div class="article-summary-box-inner">
<span><p>Storytelling, whether via fables, news reports, documentaries, or memoirs,
can be thought of as the communication of interesting and related events that,
taken together, form a concrete process. It is desirable to extract the event
chains that represent such processes. However, this extraction remains a
challenging problem. We posit that this is due to the nature of the texts from
which chains are discovered. Natural language text interleaves a narrative of
concrete, salient events with background information, contextualization,
opinion, and other elements that are important for a variety of necessary
discourse and pragmatics acts but are not part of the principal chain of events
being communicated. We introduce methods for extracting this principal chain
from natural language text, by filtering away non-salient events and supportive
sentences. We demonstrate the effectiveness of our methods at isolating
critical event chains by comparing their effect on downstream tasks. We show
that by pre-training large language models on our extracted chains, we obtain
improvements in two tasks that benefit from a clear understanding of event
chains: narrative prediction and event-based temporal question answering. The
demonstrated improvements and ablative studies confirm that our extraction
method isolates critical event chains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DialogueBERT: A Self-Supervised Learning based Dialogue Pre-training Encoder. (arXiv:2109.10480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10480">
<div class="article-summary-box-inner">
<span><p>With the rapid development of artificial intelligence, conversational bots
have became prevalent in mainstream E-commerce platforms, which can provide
convenient customer service timely. To satisfy the user, the conversational
bots need to understand the user's intention, detect the user's emotion, and
extract the key entities from the conversational utterances. However,
understanding dialogues is regarded as a very challenging task. Different from
common language understanding, utterances in dialogues appear alternately from
different roles and are usually organized as hierarchical structures. To
facilitate the understanding of dialogues, in this paper, we propose a novel
contextual dialogue encoder (i.e. DialogueBERT) based on the popular
pre-trained language model BERT. Five self-supervised learning pre-training
tasks are devised for learning the particularity of dialouge utterances. Four
different input embeddings are integrated to catch the relationship between
utterances, including turn embedding, role embedding, token embedding and
position embedding. DialogueBERT was pre-trained with 70 million dialogues in
real scenario, and then fine-tuned in three different downstream dialogue
understanding tasks. Experimental results show that DialogueBERT achieves
exciting results with 88.63% accuracy for intent recognition, 94.25% accuracy
for emotion recognition and 97.04% F1 score for named entity recognition, which
outperforms several strong baselines by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The NiuTrans Machine Translation Systems for WMT21. (arXiv:2109.10485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10485">
<div class="article-summary-box-inner">
<span><p>This paper describes NiuTrans neural machine translation systems of the WMT
2021 news translation tasks. We made submissions to 9 language directions,
including English$\leftrightarrow$$\{$Chinese, Japanese, Russian, Icelandic$\}$
and English$\rightarrow$Hausa tasks. Our primary systems are built on several
effective variants of Transformer, e.g., Transformer-DLCL, ODE-Transformer. We
also utilize back-translation, knowledge distillation, post-ensemble, and
iterative fine-tuning techniques to enhance the model performance further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the OBQA Context. (arXiv:2109.10497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10497">
<div class="article-summary-box-inner">
<span><p>In the open question answering (OBQA) task, how to select the relevant
information from a large corpus is a crucial problem for reasoning and
inference. Some datasets (e.g, HotpotQA) mainly focus on testing the model's
reasoning ability at the sentence level. To overcome this challenge, many
existing frameworks use a deep learning model to select relevant passages and
then answer each question by matching a sentence in the corresponding passage.
However, such frameworks require long inference time and fail to take advantage
of the relationship between passages and sentences. In this work, we present a
simple yet effective framework to address these problems by jointly ranking
passages and selecting sentences. We propose consistency and similarity
constraints to promote the correlation and interaction between passage ranking
and sentence selection. In our experiments, we demonstrate that our framework
can achieve competitive results and outperform the baseline by 28\% in terms of
exact matching of relevant sentences on the HotpotQA dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning. (arXiv:2109.10500v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10500">
<div class="article-summary-box-inner">
<span><p>Taxonomies are valuable resources for many applications, but the limited
coverage due to the expensive manual curation process hinders their general
applicability. Prior works attempt to automatically expand existing taxonomies
to improve their coverage by learning concept embeddings in Euclidean space,
while taxonomies, inherently hierarchical, more naturally align with the
geometric properties of a hyperbolic space. In this paper, we present
HyperExpan, a taxonomy expansion algorithm that seeks to preserve the structure
of a taxonomy in a more expressive hyperbolic embedding space and learn to
represent concepts and their relations with a Hyperbolic Graph Neural Network
(HGNN). Specifically, HyperExpan leverages position embeddings to exploit the
structure of the existing taxonomies, and characterizes the concept profile
information to support the inference on unseen concepts during training.
Experiments show that our proposed HyperExpan outperforms baseline models with
representation learning in a Euclidean feature space and achieves
state-of-the-art performance on the taxonomy expansion benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tecnologica cosa: Modeling Storyteller Personalities in Boccaccio's Decameron. (arXiv:2109.10506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10506">
<div class="article-summary-box-inner">
<span><p>We explore Boccaccio's Decameron to see how digital humanities tools can be
used for tasks that have limited data in a language no longer in contemporary
use: medieval Italian. We focus our analysis on the question: Do the different
storytellers in the text exhibit distinct personalities? To answer this
question, we curate and release a dataset based on the authoritative edition of
the text. We use supervised classification methods to predict storytellers
based on the stories they tell, confirming the difficulty of the task, and
demonstrate that topic modeling can extract thematic storyteller "profiles."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Contextualized Document Representation. (arXiv:2109.10509v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10509">
<div class="article-summary-box-inner">
<span><p>Several NLP tasks need the effective representation of text documents. Arora
et. al., 2017 demonstrate that simple weighted averaging of word vectors
frequently outperforms neural models. SCDV (Mekala et. al., 2017) further
extends this from sentences to documents by employing soft and sparse
clustering over pre-computed word vectors. However, both techniques ignore the
polysemy and contextual character of words. In this paper, we address this
issue by proposing SCDV+BERT(ctxd), a simple and effective unsupervised
representation that combines contextualized BERT (Devlin et al., 2019) based
word embedding for word sense disambiguation with SCDV soft clustering
approach. We show that our embeddings outperform original SCDV, pre-train BERT,
and several other baselines on many classification datasets. We also
demonstrate our embeddings effectiveness on other tasks, such as concept
matching and sentence similarity. In addition, we show that SCDV+BERT(ctxd)
outperforms fine-tune BERT and different embedding approaches in scenarios with
limited data and only few shots examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCM: A Fine-grained Comparison Model forMulti-turn Dialogue Reasoning. (arXiv:2109.10510v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10510">
<div class="article-summary-box-inner">
<span><p>Despite the success of neural dialogue systems in achieving high performance
on the leader-board, they cannot meet users' requirements in practice, due to
their poor reasoning skills. The underlying reason is that most neural dialogue
models only capture the syntactic and semantic information, but fail to model
the logical consistency between the dialogue history and the generated
response. Recently, a new multi-turn dialogue reasoning task has been proposed,
to facilitate dialogue reasoning research. However, this task is challenging,
because there are only slight differences between the illogical response and
the dialogue history. How to effectively solve this challenge is still worth
exploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle
this problem. Inspired by human's behavior in reading comprehension, a
comparison mechanism is proposed to focus on the fine-grained differences in
the representation of each response candidate. Specifically, each candidate
representation is compared with the whole history to obtain a history
consistency representation. Furthermore, the consistency signals between each
candidate and the speaker's own history are considered to drive a model to
prefer a candidate that is logically consistent with the speaker's history
logic. Finally, the above consistency representations are employed to output a
ranking list of the candidate responses for multi-turn dialogue reasoning.
Experimental results on two public dialogue datasets show that our method
obtains higher ranking scores than the baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards The Automatic Coding of Medical Transcripts to Improve Patient-Centered Communication. (arXiv:2109.10514v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10514">
<div class="article-summary-box-inner">
<span><p>This paper aims to provide an approach for automatic coding of
physician-patient communication transcripts to improve patient-centered
communication (PCC). PCC is a central part of high-quality health care. To
improve PCC, dialogues between physicians and patients have been recorded and
tagged with predefined codes. Trained human coders have manually coded the
transcripts. Since it entails huge labor costs and poses possible human errors,
automatic coding methods should be considered for efficiency and effectiveness.
We adopted three machine learning algorithms (Na\"ive Bayes, Random Forest, and
Support Vector Machine) to categorize lines in transcripts into corresponding
codes. The result showed that there is evidence to distinguish the codes, and
this is considered to be sufficient for training of human annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages. (arXiv:2109.10534v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10534">
<div class="article-summary-box-inner">
<span><p>We explore the impact of leveraging the relatedness of languages that belong
to the same family in NLP models using multilingual fine-tuning. We hypothesize
and validate that multilingual fine-tuning of pre-trained language models can
yield better performance on downstream NLP applications, compared to models
fine-tuned on individual languages. A first of its kind detailed study is
presented to track performance change as languages are added to a base language
in a graded and greedy (in the sense of best boost of performance) manner;
which reveals that careful selection of subset of related languages can
significantly improve performance than utilizing all related languages. The
Indo-Aryan (IA) language family is chosen for the study, the exact languages
being Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script
barrier is crossed by simple rule-based transliteration of the text of all
languages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL
and two RoBERTa-based LMs, the last two being pre-trained by us. Low resource
languages, such as Oriya and Punjabi, are found to be the largest beneficiaries
of multilingual fine-tuning. Textual Entailment, Entity Classification, Section
Title Prediction, tasks of IndicGLUE and POS tagging form our test bed.
Compared to monolingual fine tuning we get relative performance improvement of
up to 150% in the downstream tasks. The surprise take-away is that for any
language there is a particular combination of other languages which yields the
best performance, and any additional language is in fact detrimental.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing. (arXiv:2109.10540v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10540">
<div class="article-summary-box-inner">
<span><p>Recent years pretrained language models (PLMs) hit a success on several
downstream tasks, showing their power on modeling language. To better
understand and leverage what PLMs have learned, several techniques have emerged
to explore syntactic structures entailed by PLMs. However, few efforts have
been made to explore grounding capabilities of PLMs, which are also essential.
In this paper, we highlight the ability of PLMs to discover which token should
be grounded to which concept, if combined with our proposed
erasing-then-awakening approach. Empirical studies on four datasets demonstrate
that our approach can awaken latent grounding which is understandable to human
experts, even if it is not exposed to such labels during training. More
importantly, our approach shows great potential to benefit downstream semantic
parsing models. Taking text-to-SQL as a case study, we successfully couple our
approach with two off-the-shelf parsers, obtaining an absolute improvement of
up to 9.8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diarisation using Location tracking with agglomerative clustering. (arXiv:2109.10598v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10598">
<div class="article-summary-box-inner">
<span><p>Previous works have shown that spatial location information can be
complementary to speaker embeddings for a speaker diarisation task. However,
the models used often assume that speakers are fairly stationary throughout a
meeting. This paper proposes to relax this assumption, by explicitly modelling
the movements of speakers within an Agglomerative Hierarchical Clustering (AHC)
diarisation framework. Kalman filters, which track the locations of speakers,
are used to compute log-likelihood ratios that contribute to the cluster
affinity computations for the AHC merging and stopping decisions. Experiments
show that the proposed approach is able to yield improvements on a Microsoft
rich meeting transcription task, compared to methods that do not use location
information or that make stationarity assumptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. (arXiv:2109.10604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10604">
<div class="article-summary-box-inner">
<span><p>While diverse question answering (QA) datasets have been proposed and
contributed significantly to the development of deep learning models for QA
tasks, the existing datasets fall short in two aspects. First, we lack QA
datasets covering complex questions that involve answers as well as the
reasoning processes to get the answers. As a result, the state-of-the-art QA
research on numerical reasoning still focuses on simple calculations and does
not provide the mathematical expressions or evidences justifying the answers.
Second, the QA community has contributed much effort to improving the
interpretability of QA models. However, these models fail to explicitly show
the reasoning process, such as the evidence order for reasoning and the
interactions between different pieces of evidence. To address the above
shortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset
with questions requiring numerical reasoning with compound mathematical
expressions. With NOAHQA, we develop an interpretable reasoning graph as well
as the appropriate evaluation metric to measure the answer quality. We evaluate
the state-of-the-art QA models trained using existing QA datasets on NOAHQA and
show that the best among them can only achieve 55.5 exact match scores, while
the human performance is 89.7. We also present a new QA model for generating a
reasoning graph where the reasoning graph metric still has a large gap compared
with that of humans, e.g., 28 scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVR: A test-bed for Visually Grounded Compositional Generalization with real images. (arXiv:2109.10613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10613">
<div class="article-summary-box-inner">
<span><p>While interest in models that generalize at test time to new compositions has
risen in recent years, benchmarks in the visually-grounded domain have thus far
been restricted to synthetic images. In this work, we propose COVR, a new
test-bed for visually-grounded compositional generalization with real images.
To create COVR, we use real images annotated with scene graphs, and propose an
almost fully automatic procedure for generating question-answer pairs along
with a set of context images. COVR focuses on questions that require complex
reasoning, including higher-order operations such as quantification and
aggregation. Due to the automatic generation process, COVR facilitates the
creation of compositional splits, where models at test time need to generalize
to new concepts and compositions in a zero- or few-shot setting. We construct
compositional splits using COVR and demonstrate a myriad of cases where
state-of-the-art pre-trained language-and-vision models struggle to
compositionally generalize.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriching and Controlling Global Semantics for Text Summarization. (arXiv:2109.10616v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10616">
<div class="article-summary-box-inner">
<span><p>Recently, Transformer-based models have been proven effective in the
abstractive summarization task by creating fluent and informative summaries.
Nevertheless, these models still suffer from the short-range dependency
problem, causing them to produce summaries that miss the key points of
document. In this paper, we attempt to address this issue by introducing a
neural topic model empowered with normalizing flow to capture the global
semantics of the document, which are then integrated into the summarization
model. In addition, to avoid the overwhelming effect of global semantics on
contextualized representation, we introduce a mechanism to control the amount
of global semantics supplied to the text generation module. Our method
outperforms state-of-the-art summarization models on five common text
summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and
PubMed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Learning for Fair Representations. (arXiv:2109.10645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10645">
<div class="article-summary-box-inner">
<span><p>Trained classification models can unintentionally lead to biased
representations and predictions, which can reinforce societal preconceptions
and stereotypes. Existing debiasing methods for classification models, such as
adversarial training, are often expensive to train and difficult to optimise.
In this paper, we propose a method for mitigating bias in classifier training
by incorporating contrastive learning, in which instances sharing the same
class label are encouraged to have similar representations, while instances
sharing a protected attribute are forced further apart. In such a way our
method learns representations which capture the task label in focused regions,
while ensuring the protected attribute has diverse spread, and thus has limited
impact on prediction and thereby results in fairer models. Extensive
experimental results across four tasks in NLP and computer vision show (a) that
our proposed method can achieve fairer representations and realises bias
reductions compared with competitive baselines; and (b) that it can do so
without sacrificing main task performance; (c) that it sets a new
state-of-the-art performance in one task despite reducing the bias. Finally,
our method is conceptually simple and agnostic to network architectures, and
incurs minimal additional compute cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization. (arXiv:2109.10650v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10650">
<div class="article-summary-box-inner">
<span><p>One of the most challenging aspects of current single-document news
summarization is that the summary often contains 'extrinsic hallucinations',
i.e., facts that are not present in the source document, which are often
derived via world knowledge. This causes summarization systems to act more like
open-ended language models tending to hallucinate facts that are erroneous. In
this paper, we mitigate this problem with the help of multiple supplementary
resource documents assisting the task. We present a new dataset MiRANews and
benchmark existing summarization models. In contrast to multi-document
summarization, which addresses multiple events from several source documents,
we still aim at generating a summary for a single document. We show via data
analysis that it's not only the models which are to blame: more than 27% of
facts mentioned in the gold summaries of MiRANews are better grounded on
assisting documents than in the main source articles. An error analysis of
generated summaries from pretrained models fine-tuned on MiRANews reveals that
this has an even bigger effects on models: assisted summarization reduces 55%
of hallucinations when compared to single-document summarization models trained
on the main article only. Our code and data are available at
https://github.com/XinnuoXu/MiRANews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10686">
<div class="article-summary-box-inner">
<span><p>There remain many open questions pertaining to the scaling behaviour of
Transformer architectures. These scaling decisions and findings can be
critical, as training runs often come with an associated computational cost
which have both financial and/or environmental impact. The goal of this paper
is to present scaling insights from pretraining and finetuning Transformers.
While Kaplan et al. presents a comprehensive study of the scaling behaviour of
Transformer language models, the scope is only on the upstream (pretraining)
loss. Therefore, it is still unclear if these set of findings transfer to
downstream task within the context of the pretrain-finetune paradigm. The key
findings of this paper are as follows: (1) we show that aside from only the
model size, model shape matters for downstream fine-tuning, (2) scaling
protocols operate differently at different compute regions, (3) widely adopted
T5-base and T5-large sizes are Pareto-inefficient. To this end, we present
improved scaling protocols whereby our redesigned models achieve similar
downstream fine-tuning quality while having 50\% fewer parameters and training
40\% faster compared to the widely adopted T5-base model. We publicly release
over 100 pretrained checkpoints of different T5 configurations to facilitate
future research and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simulated Annealing for Emotional Dialogue Systems. (arXiv:2109.10715v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10715">
<div class="article-summary-box-inner">
<span><p>Explicitly modeling emotions in dialogue generation has important
applications, such as building empathetic personal companions. In this study,
we consider the task of expressing a specific emotion for dialogue generation.
Previous approaches take the emotion as an input signal, which may be ignored
during inference. We instead propose a search-based emotional dialogue system
by simulated annealing (SA). Specifically, we first define a scoring function
that combines contextual coherence and emotional correctness. Then, SA
iteratively edits a general response and searches for a sentence with a higher
score, enforcing the presence of the desired emotion. We evaluate our system on
the NLPCC2017 dataset. Our proposed method shows 12% improvements in emotion
accuracy compared with the previous state-of-the-art method, without hurting
the generation quality (measured by BLEU).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Latency Incremental Text-to-Speech Synthesis with Distilled Context Prediction Network. (arXiv:2109.10724v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10724">
<div class="article-summary-box-inner">
<span><p>Incremental text-to-speech (TTS) synthesis generates utterances in small
linguistic units for the sake of real-time and low-latency applications. We
previously proposed an incremental TTS method that leverages a large
pre-trained language model to take unobserved future context into account
without waiting for the subsequent segment. Although this method achieves
comparable speech quality to that of a method that waits for the future
context, it entails a huge amount of processing for sampling from the language
model at each time step. In this paper, we propose an incremental TTS method
that directly predicts the unobserved future context with a lightweight model,
instead of sampling words from the large-scale language model. We perform
knowledge distillation from a GPT2-based context prediction network into a
simple recurrent model by minimizing a teacher-student loss defined between the
context embedding vectors of those models. Experimental results show that the
proposed method requires about ten times less inference time to achieve
comparable synthetic speech quality to that of our previous method, and it can
perform incremental synthesis much faster than the average speaking speed of
human English speakers, demonstrating the availability of our method to
real-time applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing. (arXiv:2109.10847v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10847">
<div class="article-summary-box-inner">
<span><p>Recent progress in the Natural Language Processing domain has given us
several State-of-the-Art (SOTA) pretrained models which can be finetuned for
specific tasks. These large models with billions of parameters trained on
numerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In
this paper, we discuss the need for a benchmark for cost and time effective
smaller models trained on a single GPU. This will enable researchers with
resource constraints experiment with novel and innovative ideas on
tokenization, pretraining tasks, architecture, fine tuning methods etc. We set
up Small-Bench NLP, a benchmark for small efficient neural language models
trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks
on the publicly available GLUE datasets and a leaderboard to track the progress
of the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture
achieves an average score of 81.53 which is comparable to that of BERT-Base's
82.20 (110M parameters). Our models, code and leaderboard are available at
https://github.com/smallbenchnlp
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10852">
<div class="article-summary-box-inner">
<span><p>This paper presents Pix2Seq, a simple and generic framework for object
detection. Unlike existing approaches that explicitly integrate prior knowledge
about the task, we simply cast object detection as a language modeling task
conditioned on the observed pixel inputs. Object descriptions (e.g., bounding
boxes and class labels) are expressed as sequences of discrete tokens, and we
train a neural net to perceive the image and generate the desired sequence. Our
approach is based mainly on the intuition that if a neural net knows about
where and what the objects are, we just need to teach it how to read them out.
Beyond the use of task-specific data augmentations, our approach makes minimal
assumptions about the task, yet it achieves competitive results on the
challenging COCO dataset, compared to highly specialized and well optimized
detection algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BFClass: A Backdoor-free Text Classification Framework. (arXiv:2109.10855v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10855">
<div class="article-summary-box-inner">
<span><p>Backdoor attack introduces artificial vulnerabilities into the model by
poisoning a subset of the training data via injecting triggers and modifying
labels. Various trigger design strategies have been explored to attack text
classifiers, however, defending such attacks remains an open problem. In this
work, we propose BFClass, a novel efficient backdoor-free training framework
for text classification. The backbone of BFClass is a pre-trained discriminator
that predicts whether each token in the corrupted input was replaced by a
masked language model. To identify triggers, we utilize this discriminator to
locate the most suspicious token from each training sample and then distill a
concise set by considering their association strengths with particular labels.
To recognize the poisoned subset, we examine the training samples with these
identified triggers as the most suspicious token, and check if removing the
trigger will change the poisoned model's prediction. Extensive experiments
demonstrate that BFClass can identify all the triggers, remove 95% poisoned
training samples with very limited false alarms, and achieve almost the same
performance as the models trained on the benign training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data. (arXiv:2109.10856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10856">
<div class="article-summary-box-inner">
<span><p>Existing text classification methods mainly focus on a fixed label set,
whereas many real-world applications require extending to new fine-grained
classes as the number of samples per label increases. To accommodate such
requirements, we introduce a new problem called coarse-to-fine grained
classification, which aims to perform fine-grained classification on coarsely
annotated data. Instead of asking for new fine-grained human annotations, we
opt to leverage label surface names as the only human guidance and weave in
rich pre-trained generative language models into the iterative weak supervision
strategy. Specifically, we first propose a label-conditioned finetuning
formulation to attune these generators for our task. Furthermore, we devise a
regularization objective based on the coarse-fine label constraints derived
from our problem setting, giving us even further improvements over the prior
formulation. Our framework uses the fine-tuned generative models to sample
pseudo-training data for training the classifier, and bootstraps on real
unlabeled data for model refinement. Extensive experiments and case studies on
two real-world datasets demonstrate superior performance over SOTA zero-shot
classification baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pushing the Right Buttons: Adversarial Evaluation of Quality Estimation. (arXiv:2109.10859v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10859">
<div class="article-summary-box-inner">
<span><p>Current Machine Translation (MT) systems achieve very good results on a
growing variety of language pairs and datasets. However, they are known to
produce fluent translation outputs that can contain important meaning errors,
thus undermining their reliability in practice. Quality Estimation (QE) is the
task of automatically assessing the performance of MT systems at test time.
Thus, in order to be useful, QE systems should be able to detect such errors.
However, this ability is yet to be tested in the current evaluation practices,
where QE systems are assessed only in terms of their correlation with human
judgements. In this work, we bridge this gap by proposing a general methodology
for adversarial testing of QE for MT. First, we show that despite a high
correlation with human judgements achieved by the recent SOTA, certain types of
meaning errors are still problematic for QE to detect. Second, we show that on
average, the ability of a given model to discriminate between
meaning-preserving and meaning-altering perturbations is predictive of its
overall performance, thus potentially allowing for comparing QE systems without
relying on manual quality annotation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursively Summarizing Books with Human Feedback. (arXiv:2109.10862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10862">
<div class="article-summary-box-inner">
<span><p>A major challenge for scaling machine learning is training models to perform
tasks that are very difficult or time-consuming for humans to evaluate. We
present progress on this problem on the task of abstractive summarization of
entire fiction novels. Our method combines learning from human feedback with
recursive task decomposition: we use models trained on smaller parts of the
task to assist humans in giving feedback on the broader task. We collect a
large volume of demonstrations and comparisons from human labelers, and
fine-tune GPT-3 using behavioral cloning and reward modeling to do
summarization recursively. At inference time, the model first summarizes small
sections of the book and then recursively summarizes these summaries to produce
a summary of the entire book. Our human labelers are able to supervise and
evaluate the models quickly, despite not having read the entire books
themselves. Our resulting model generates sensible summaries of entire books,
even matching the quality of human-written summaries in a few cases ($\sim5\%$
of books). We achieve state-of-the-art results on the recent BookSum dataset
for book-length summarization. A zero-shot question-answering model using these
summaries achieves state-of-the-art results on the challenging NarrativeQA
benchmark for answering questions about books and movie scripts. We release
datasets of samples from our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OTEANN: Estimating the Transparency of Orthographies with an Artificial Neural Network. (arXiv:1912.13321v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.13321">
<div class="article-summary-box-inner">
<span><p>To transcribe spoken language to written medium, most alphabets enable an
unambiguous sound-to-letter rule. However, some writing systems have distanced
themselves from this simple concept and little work exists in Natural Language
Processing (NLP) on measuring such distance. In this study, we use an
Artificial Neural Network (ANN) model to evaluate the transparency between
written words and their pronunciation, hence its name Orthographic Transparency
Estimation with an ANN (OTEANN). Based on datasets derived from Wikimedia
dictionaries, we trained and tested this model to score the percentage of
correct predictions in phoneme-to-grapheme and grapheme-to-phoneme translation
tasks. The scores obtained on 17 orthographies were in line with the
estimations of other studies. Interestingly, the model also provided insight
into typical mistakes made by learners who only consider the phonemic rule in
reading and writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"Wait, I'm Still Talking!" Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model. (arXiv:2002.09616v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.09616">
<div class="article-summary-box-inner">
<span><p>Producing natural and accurate responses like human beings is the ultimate
goal of intelligent dialogue agents. So far, most of the past works concentrate
on selecting or generating one pertinent and fluent response according to
current query and its context. These models work on a one-to-one environment,
making one response to one utterance each round. However, in real human-human
conversations, human often sequentially sends several short messages for
readability instead of a long message in one turn. Thus messages will not end
with an explicit ending signal, which is crucial for agents to decide when to
reply. So the first step for an intelligent dialogue agent is not replying but
deciding if it should reply at the moment. To address this issue, in this
paper, we propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to
help the agent decide whether to wait or to make a response directly. Our
method has two imaginator modules and an arbitrator module. The two imaginators
will learn the agent's and user's speaking style respectively, generate
possible utterances as the input of the arbitrator, combining with dialogue
history. And the arbitrator decides whether to wait or to make a response to
the user directly. To verify the performance and effectiveness of our method,
we prepared two dialogue datasets and compared our approach with several
popular models. Experimental results show that our model performs well on
addressing ending prediction issue and outperforms baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00033">
<div class="article-summary-box-inner">
<span><p>With the emergence of the COVID-19 pandemic, the political and the medical
aspects of disinformation merged as the problem got elevated to a whole new
level to become the first global infodemic. Fighting this infodemic has been
declared one of the most important focus areas of the World Health
Organization, with dangers ranging from promoting fake cures, rumors, and
conspiracy theories to spreading xenophobia and panic. Addressing the issue
requires solving a number of challenging problems such as identifying messages
containing claims, determining their check-worthiness and factuality, and their
potential to do harm as well as the nature of that harm, to mention just a few.
To address this gap, we release a large dataset of 16K manually annotated
tweets for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests of journalists, fact-checkers,
social media platforms, policy makers, and society, and (iii) covers Arabic,
Bulgarian, Dutch, and English. Finally, we show strong evaluation results using
pretrained Transformers, thus confirming the practical utility of the dataset
in monolingual vs. multilingual, and single task vs. multitask settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predict-then-Decide: A Predictive Approach for Wait or Answer Task in Dialogue Systems. (arXiv:2005.13119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.13119">
<div class="article-summary-box-inner">
<span><p>Different people have different habits of describing their intents in
conversations. Some people tend to deliberate their intents in several
successive utterances, i.e., they use several consistent messages for
readability instead of a long sentence to express their question. This creates
a predicament faced by the application of dialogue systems, especially in
real-world industry scenarios, in which the dialogue system is unsure whether
it should answer the query of user immediately or wait for further
supplementary input. Motivated by such an interesting predicament, we define a
novel Wait-or-Answer task for dialogue systems. We shed light on a new research
topic about how the dialogue system can be more intelligent to behave in this
Wait-or-Answer quandary. Further, we propose a predictive approach named
Predict-then-Decide (PTD) to tackle this Wait-or-Answer task. More
specifically, we take advantage of a decision model to help the dialogue system
decide whether to wait or answer. The decision of decision model is made with
the assistance of two ancillary prediction models: a user prediction and an
agent prediction. The user prediction model tries to predict what the user
would supplement and uses its prediction to persuade the decision model that
the user has some information to add, so the dialogue system should wait. The
agent prediction model tries to predict the answer of the dialogue system and
convince the decision model that it is a superior choice to answer the query of
user immediately since the input of user has come to an end. We conduct our
experiments on two real-life scenarios and three public datasets. Experimental
results on five datasets show our proposed PTD approach significantly
outperforms the existing models in solving this Wait-or-Answer problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GIPFA: Generating IPA Pronunciation from Audio. (arXiv:2006.07573v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07573">
<div class="article-summary-box-inner">
<span><p>Transcribing spoken audio samples into the International Phonetic Alphabet
(IPA) has long been reserved for experts. In this study, we examine the use of
an Artificial Neural Network (ANN) model to automatically extract the IPA
phonemic pronunciation of a word based on its audio pronunciation, hence its
name Generating IPA Pronunciation From Audio (GIPFA). Based on the French
Wikimedia dictionary, we trained our model which then correctly predicted 75%
of the IPA pronunciations tested. Interestingly, by studying inference errors,
the model made it possible to highlight possible errors in the dataset as well
as to identify the closest phonemes in French.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures. (arXiv:2007.08970v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08970">
<div class="article-summary-box-inner">
<span><p>While mainstream machine learning methods are known to have limited ability
to compositionally generalize, new architectures and techniques continue to be
proposed to address this limitation. We investigate state-of-the-art techniques
and architectures in order to assess their effectiveness in improving
compositional generalization in semantic parsing tasks based on the SCAN and
CFQ datasets. We show that masked language model (MLM) pre-training rivals
SCAN-inspired architectures on primitive holdout splits. On a more complex
compositional task, we show that pre-training leads to significant improvements
in performance vs. comparable non-pre-trained models, whereas architectures
proposed to encourage compositional generalization on SCAN or in the area of
algorithm learning fail to lead to significant improvements. We establish a new
state of the art on the CFQ compositional generalization benchmark using MLM
pre-training together with an intermediate representation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Sound Change: Deep and Iterative Learning, Convolutional Neural Networks, and Language Change. (arXiv:2011.05463v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.05463">
<div class="article-summary-box-inner">
<span><p>This paper proposes a framework for modeling sound change that combines deep
learning and iterative learning. Acquisition and transmission of speech is
modeled by training generations of Generative Adversarial Networks (GANs) on
unannotated raw speech data. The paper argues that several properties of sound
change emerge from the proposed architecture. GANs (Goodfellow et al. 2014
<a href="/abs/1406.2661">arXiv:1406.2661</a>, Donahue et al. 2019 <a href="/abs/1705.07904">arXiv:1705.07904</a>) are uniquely appropriate
for modeling language change because the networks are trained on raw
unsupervised acoustic data, contain no language-specific features and, as
argued in Begu\v{s} (2020 <a href="/abs/2006.03965">arXiv:2006.03965</a>), encode phonetic and phonological
representations in their latent space and generate linguistically informative
innovative data. The first generation of networks is trained on the relevant
sequences in human speech from TIMIT. The subsequent generations are not
trained on TIMIT, but on generated outputs from the previous generation and
thus start learning from each other in an iterative learning task. The initial
allophonic distribution is progressively being lost with each generation,
likely due to pressures from the global distribution of aspiration in the
training data. The networks show signs of a gradual shift in phonetic targets
characteristic of a gradual phonetic sound change. At endpoints, the outputs
superficially resemble a phonological change -- rule loss.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Transferability of Adversarial Attacksagainst Neural Text Classifier. (arXiv:2011.08558v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08558">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are vulnerable to adversarial attacks, where a small
perturbation to an input alters the model prediction. In many cases, malicious
inputs intentionally crafted for one model can fool another model. In this
paper, we present the first study to systematically investigate the
transferability of adversarial examples for text classification models and
explore how various factors, including network architecture, tokenization
scheme, word embedding, and model capacity, affect the transferability of
adversarial examples. Based on these studies, we propose a genetic algorithm to
find an ensemble of models that can be used to induce adversarial examples to
fool almost all existing models. Such adversarial examples reflect the defects
of the learning process and the data bias in the training set. Finally, we
derive word replacement rules that can be used for model diagnostics from these
adversarial examples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach. (arXiv:2102.10242v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10242">
<div class="article-summary-box-inner">
<span><p>Reliable automatic evaluation of dialogue systems under an interactive
environment has long been overdue. An ideal environment for evaluating dialog
systems, also known as the Turing test, needs to involve human interaction,
which is usually not affordable for large-scale experiments. Though researchers
have attempted to use metrics (e.g., perplexity, BLEU) in language generation
tasks or some model-based reinforcement learning methods (e.g., self-play
evaluation) for automatic evaluation, these methods only show a very weak
correlation with the actual human evaluation in practice. To bridge such a gap,
we propose a new framework named ENIGMA for estimating human evaluation scores
based on recent advances of off-policy evaluation in reinforcement learning.
ENIGMA only requires a handful of pre-collected experience data, and therefore
does not involve human interaction with the target policy during the
evaluation, making automatic evaluations feasible. More importantly, ENIGMA is
model-free and agnostic to the behavior policies for collecting the experience
data (see details in Section 2), which significantly alleviates the technical
difficulties of modeling complex dialogue environments and human behaviors. Our
experiments show that ENIGMA significantly outperforms existing methods in
terms of correlation with human evaluation scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Classifiers: Promises, Shortcomings, and Advances. (arXiv:2102.12452v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12452">
<div class="article-summary-box-inner">
<span><p>Probing classifiers have emerged as one of the prominent methodologies for
interpreting and analyzing deep neural network models of natural language
processing. The basic idea is simple -- a classifier is trained to predict some
linguistic property from a model's representations -- and has been used to
examine a wide variety of models and properties. However, recent studies have
demonstrated various methodological limitations of this approach. This article
critically reviews the probing classifiers framework, highlighting their
promises, shortcomings, and advances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. (arXiv:2104.05893v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05893">
<div class="article-summary-box-inner">
<span><p>Online misinformation is a prevalent societal issue, with adversaries relying
on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated
by the threat scenario where an image is used out of context to support a
certain narrative. While some prior datasets for detecting image-text
inconsistency generate samples via text manipulation, we propose a dataset
where both image and text are unmanipulated but mismatched. We introduce
several strategies for automatically retrieving convincing images for a given
caption, capturing cases with inconsistent entities or semantic context. Our
large-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates
that machine-driven image repurposing is now a realistic threat, and (2)
provides samples that represent challenging instances of mismatch between text
and image in news that are able to mislead humans. We benchmark several
state-of-the-art multimodal models on our dataset and analyze their performance
across different pretraining domains and visual backbones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Attention Free Transformer. (arXiv:2105.14103v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14103">
<div class="article-summary-box-inner">
<span><p>We introduce Attention Free Transformer (AFT), an efficient variant of
Transformers that eliminates the need for dot product self attention. In an AFT
layer, the key and value are first combined with a set of learned position
biases, the result of which is multiplied with the query in an element-wise
fashion. This new operation has a memory complexity linear w.r.t. both the
context size and the dimension of features, making it compatible to both large
input and model sizes. We also introduce AFT-local and AFT-conv, two model
variants that take advantage of the idea of locality and spatial weight sharing
while maintaining global connectivity. We conduct extensive experiments on two
autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image
recognition task (ImageNet-1K classification). We show that AFT demonstrates
competitive performance on all the benchmarks, while providing excellent
efficiency at the same time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07634">
<div class="article-summary-box-inner">
<span><p>Keyword spotting (KWS) is an important technique for speech applications,
which enables users to activate devices by speaking a keyword phrase. Although
a phoneme classifier can be used for KWS, exploiting a large amount of
transcribed data for automatic speech recognition (ASR), there is a mismatch
between the training criterion (phoneme recognition) and the target task (KWS).
Recently, multi-task learning has been applied to KWS to exploit both ASR and
KWS training data. In this approach, an output of an acoustic model is split
into two branches for the two tasks, one for phoneme transcription trained with
the ASR data and one for keyword classification trained with the KWS data. In
this paper, we introduce a cross attention decoder in the multi-task learning
framework. Unlike the conventional multi-task learning approach with the simple
split of the output layer, the cross attention decoder summarizes information
from a phonetic encoder by performing cross attention between the encoder
outputs and a trainable query sequence to predict a confidence score for the
KWS task. Experimental results on KWS tasks show that the proposed approach
achieves a 12% relative reduction in the false reject ratios compared to the
conventional multi-task learning with split branches and a bi-directional long
short-team memory decoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00590">
<div class="article-summary-box-inner">
<span><p>Web search is fundamentally multimodal and multihop. Often, even before
asking a question we choose to go directly to image search to find our answers.
Further, rarely do we find an answer from a single source but aggregate
information and reason through implications. Despite the frequency of this
everyday occurrence, at present, there is no unified question answering
benchmark that requires a single model to answer long-form natural language
questions from text and open-ended visual sources -- akin to a human's
experience. We propose to bridge this gap between the natural language and
computer vision communities with WebQA. We show that A. our multihop text
queries are difficult for a large-scale transformer model, and B. existing
multi-modal transformers and visual representations do not perform well on
open-domain visual queries. Our challenge for the community is to create a
unified multimodal reasoning model that seamlessly transitions and reasons
regardless of the source modality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
<div class="article-summary-box-inner">
<span><p>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v2 [cs.MM] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05184">
<div class="article-summary-box-inner">
<span><p>Internet memes have become powerful means to transmit political,
psychological, and socio-cultural ideas. Although memes are typically humorous,
recent days have witnessed an escalation of harmful memes used for trolling,
cyberbullying, and abuse. Detecting such memes is challenging as they can be
highly satirical and cryptic. Moreover, while previous work has focused on
specific aspects of memes such as hate speech and propaganda, there has been
little work on harm in general. Here, we aim to bridge this gap. We focus on
two tasks: (i)detecting harmful memes, and (ii)identifying the social entities
they target. We further extend a recently released HarMeme dataset, which
covered COVID-19, with additional memes and a new topic: US politics. To solve
these tasks, we propose MOMENTA (MultimOdal framework for detecting harmful
MemEs aNd Their tArgets), a novel multimodal deep neural network that uses
global and local perspectives to detect harmful memes. MOMENTA systematically
analyzes the local and the global perspective of the input meme (in both
modalities) and relates it to the background context. MOMENTA is interpretable
and generalizable, and our experiments show that it outperforms several strong
rivaling approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Keyword Extraction for Improved Document Retrieval in Conversational Search. (arXiv:2109.05979v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05979">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that mixed-initiative conversational search, based
on the interaction between users and computers to clarify and improve a query,
provides enormous advantages. Nonetheless, incorporating additional information
provided by the user from the conversation poses some challenges. In fact,
further interactions could confuse the system as a user might use words
irrelevant to the information need but crucial for correct sentence
construction in the context of multi-turn conversations. To this aim, in this
paper, we have collected two conversational keyword extraction datasets and
propose an end-to-end document retrieval pipeline incorporating them.
Furthermore, we study the performance of two neural keyword extraction models,
namely, BERT and sequence to sequence, in terms of extraction accuracy and
human annotation. Finally, we study the effect of keyword extraction on the
end-to-end neural IR performance and show that our approach beats
state-of-the-art IR models. We make the two datasets publicly available to
foster research in this area.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense-Focused Dialogues for Response Generation: An Empirical Study. (arXiv:2109.06427v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06427">
<div class="article-summary-box-inner">
<span><p>Smooth and effective communication requires the ability to perform latent or
explicit commonsense inference. Prior commonsense reasoning benchmarks (such as
SocialIQA and CommonsenseQA) mainly focus on the discriminative task of
choosing the right answer from a set of candidates, and do not involve
interactive language generation as in dialogue. Moreover, existing dialogue
datasets do not explicitly focus on exhibiting commonsense as a facet. In this
paper, we present an empirical study of commonsense in dialogue response
generation. We first auto-extract commonsensical dialogues from existing
dialogue datasets by leveraging ConceptNet, a commonsense knowledge graph.
Furthermore, building on social contexts/situations in SocialIQA, we collect a
new dialogue dataset with 25K dialogues aimed at exhibiting social commonsense
in an interactive setting. We evaluate response generation models trained using
these datasets and find that models trained on both extracted and our collected
data produce responses that consistently exhibit more commonsense than
baselines. Finally we propose an approach for automatic evaluation of
commonsense that relies on features derived from ConceptNet and pre-trained
language and dialog models, and show reasonable correlation with human
evaluation of responses' commonsense quality. We are releasing a subset of our
collected data, Commonsense-Dialogues, containing about 11K dialogs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Clinical Information Extraction with Transferred Contextual Embeddings. (arXiv:2109.07243v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07243">
<div class="article-summary-box-inner">
<span><p>The Bidirectional Encoder Representations from Transformers (BERT) model has
achieved the state-of-the-art performance for many natural language processing
(NLP) tasks. Yet, limited research has been contributed to studying its
effectiveness when the target domain is shifted from the pre-training corpora,
for example, for biomedical or clinical NLP applications. In this paper, we
applied it to a widely studied a hospital information extraction (IE) task and
analyzed its performance under the transfer learning setting. Our application
became the new state-of-the-art result by a clear margin, compared with a range
of existing IE models. Specifically, on this nursing handover data set, the
macro-average F1 score from our model was 0.438, whilst the previous best deep
learning models had 0.416. In conclusion, we showed that BERT based
pre-training models can be transferred to health-related documents under mild
conditions and with a proper fine-tuning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08818">
<div class="article-summary-box-inner">
<span><p>Incorporating lexical knowledge into deep learning models has been proved to
be very effective for sequence labeling tasks. However, previous works commonly
have difficulty dealing with large-scale dynamic lexicons which often cause
excessive matching noise and problems of frequent updates. In this paper, we
propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence
labeling tasks. Instead of leveraging embeddings of words in the lexicon as in
conventional methods, we adopt word-agnostic tag embeddings to avoid
re-training the representation while updating the lexicon. Moreover, we employ
an effective supervised lexical knowledge denoising method to smooth out
matching noise. Finally, we introduce a col-wise attention based knowledge
fusion mechanism to guarantee the pluggability of the proposed framework.
Experiments on ten datasets of three tasks show that the proposed framework
achieves new SOTA, even with very large scale lexicons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Joint Intent Detection and Slot Filling via Higher-order Attention. (arXiv:2109.08890v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08890">
<div class="article-summary-box-inner">
<span><p>Intent detection (ID) and Slot filling (SF) are two major tasks in spoken
language understanding (SLU). Recently, attention mechanism has been shown to
be effective in jointly optimizing these two tasks in an interactive manner.
However, latest attention-based works concentrated only on the first-order
attention design, while ignoring the exploration of higher-order attention
mechanisms. In this paper, we propose a BiLinear attention block, which
leverages bilinear pooling to simultaneously exploit both the contextual and
channel-wise bilinear attention distributions to capture the second-order
interactions between the input intent or slot features. Higher and even
infinity order interactions are built by stacking numerous blocks and assigning
Exponential Linear Unit (ELU) to blocks. Before the decoding stage, we
introduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot
information in a more effective way. Technically, instead of simply
concatenating intent and slot features, we first compute two correlation
matrices to weight on two features. Furthermore, we present Higher-order
Attention Network for the SLU tasks. Experiments on two benchmark datasets show
that our approach yields improvements compared with the state-of-the-art
approach. We also provide discussion to demonstrate the effectiveness of the
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-23 23:09:12.449670795 UTC">2021-09-23 23:09:12 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>