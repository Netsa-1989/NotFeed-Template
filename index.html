<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-05T01:30:00Z">10-05</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment and structure in word co-occurrence networks on Twitter. (arXiv:2110.00587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00587">
<div class="article-summary-box-inner">
<span><p>We explore the relationship between context and happiness scores in political
tweets using word co-occurrence networks, where nodes in the network are the
words, and the weight of an edge is the number of tweets in the corpus for
which the two connected words co-occur. In particular, we consider tweets with
hashtags #imwithher and #crookedhillary, both relating to Hillary Clinton's
presidential bid in 2016. We then analyze the network properties in conjunction
with the word scores by comparing with null models to separate the effects of
the network structure and the score distribution. Neutral words are found to be
dominant and most words, regardless of polarity, tend to co-occur with neutral
words. We do not observe any score homophily among positive and negative words.
However, when we perform network backboning, community detection results in
word groupings with meaningful narratives, and the happiness scores of the
words in each group correspond to its respective theme. Thus, although we
observe no clear relationship between happiness scores and co-occurrence at the
node or edge level, a community-centric approach can isolate themes of
competing sentiments in a corpus.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expected Validation Performance and Estimation of a Random Variable's Maximum. (arXiv:2110.00613v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00613">
<div class="article-summary-box-inner">
<span><p>Research in NLP is often supported by experimental results, and improved
reporting of such results can lead to better understanding and more
reproducible science. In this paper we analyze three statistical estimators for
expected validation performance, a tool used for reporting performance (e.g.,
accuracy) as a function of computational budget (e.g., number of hyperparameter
tuning experiments). Where previous work analyzing such estimators focused on
the bias, we also examine the variance and mean squared error (MSE). In both
synthetic and realistic scenarios, we evaluate three estimators and find the
unbiased estimator has the highest variance, and the estimator with the
smallest variance has the largest bias; the estimator with the smallest MSE
strikes a balance between bias and variance, displaying a classic bias-variance
tradeoff. We use expected validation performance to compare between different
models, and analyze how frequently each estimator leads to drawing incorrect
conclusions about which of two models performs best. We find that the two
biased estimators lead to the fewest incorrect conclusions, which hints at the
importance of minimizing variance and MSE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Attentive Constituency Parsing for UCCA-based Semantic Parsing. (arXiv:2110.00621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00621">
<div class="article-summary-box-inner">
<span><p>Semantic parsing provides a way to extract the semantic structure of a text
that could be understood by machines. It is utilized in various NLP
applications that require text comprehension such as summarization and question
answering. Graph-based representation is one of the semantic representation
approaches to express the semantic structure of a text. Such representations
generate expressive and adequate graph-based target structures. In this paper,
we focus primarily on UCCA graph-based semantic representation. The paper not
only presents the existing approaches proposed for UCCA representation, but
also proposes a novel self-attentive neural parsing model for the UCCA
representation. We present the results for both single-lingual and
cross-lingual tasks using zero-shot and few-shot learning for low-resource
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALBU: An approximate Loopy Belief message passing algorithm for LDA to improve performance on small data sets. (arXiv:2110.00635v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00635">
<div class="article-summary-box-inner">
<span><p>Variational Bayes (VB) applied to latent Dirichlet allocation (LDA) has
become the most popular algorithm for aspect modeling. While sufficiently
successful in text topic extraction from large corpora, VB is less successful
in identifying aspects in the presence of limited data. We present a novel
variational message passing algorithm as applied to Latent Dirichlet Allocation
(LDA) and compare it with the gold standard VB and collapsed Gibbs sampling. In
situations where marginalisation leads to non-conjugate messages, we use ideas
from sampling to derive approximate update equations. In cases where conjugacy
holds, Loopy Belief update (LBU) (also known as Lauritzen-Spiegelhalter) is
used. Our algorithm, ALBU (approximate LBU), has strong similarities with
Variational Message Passing (VMP) (which is the message passing variant of VB).
To compare the performance of the algorithms in the presence of limited data,
we use data sets consisting of tweets and news groups. Additionally, to perform
more fine grained evaluations and comparisons, we use simulations that enable
comparisons with the ground truth via Kullback-Leibler divergence (KLD). Using
coherence measures for the text corpora and KLD with the simulations we show
that ALBU learns latent distributions more accurately than does VB, especially
for smaller data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models. (arXiv:2110.00672v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00672">
<div class="article-summary-box-inner">
<span><p>We use a dataset of U.S. first names with labels based on predominant gender
and racial group to examine the effect of training corpus frequency on
tokenization, contextualization, similarity to initial representation, and bias
in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white
names are less frequent in the training corpora of these four language models.
We find that infrequent names are more self-similar across contexts, with
Spearman's r between frequency and self-similarity as low as -.763. Infrequent
names are also less similar to initial representation, with Spearman's r
between frequency and linear centered kernel alignment (CKA) similarity to
initial representation as high as .702. Moreover, we find Spearman's r between
racial bias and name frequency in BERT of .492, indicating that lower-frequency
minority group names are more associated with unpleasantness. Representations
of infrequent names undergo more processing, but are more self-similar,
indicating that models rely on less context-informed representations of
uncommon and minority names which are overfit to a lower number of observed
contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning. (arXiv:2110.00678v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00678">
<div class="article-summary-box-inner">
<span><p>To address the performance gap of English ASR models on L2 English speakers,
we evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;
Xu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,
2018) under different training settings. We compare \textbf{(a)} models trained
with a combination of diverse accents to ones trained with only specific
accents and \textbf{(b)} results from different single-accent models. Our
experiments demonstrate the promise of developing ASR models for non-native
English speakers, even with small amounts of L2 training data and even without
a language model. Our models also excel in the zero-shot setting where we train
on multiple L2 datasets and test on a blind L2 test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Robustness of Dialog Models to Popular Figurative Language Constructs. (arXiv:2110.00687v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00687">
<div class="article-summary-box-inner">
<span><p>Humans often employ figurative language use in communication, including
during interactions with dialog systems. Thus, it is important for real-world
dialog systems to be able to handle popular figurative language constructs like
metaphor and simile. In this work, we analyze the performance of existing
dialog models in situations where the input dialog context exhibits use of
figurative language. We observe large gaps in handling of figurative language
when evaluating the models on two open domain dialog datasets. When faced with
dialog contexts consisting of figurative language, some models show very large
drops in performance compared to contexts without figurative language. We
encourage future research in dialog modeling to separately analyze and report
results on figurative language in order to better test model capabilities
relevant to real-world use. Finally, we propose lightweight solutions to help
existing models become more robust to figurative language by simply using an
external resource to translate figurative language to literal (non-figurative)
forms while preserving the meaning to the best extent possible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Clustering and Network Analysis for the Embedding Spaces of Sentences and Sub-Sentences. (arXiv:2110.00697v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00697">
<div class="article-summary-box-inner">
<span><p>Sentence embedding methods offer a powerful approach for working with short
textual constructs or sequences of words. By representing sentences as dense
numerical vectors, many natural language processing (NLP) applications have
improved their performance. However, relatively little is understood about the
latent structure of sentence embeddings. Specifically, research has not
addressed whether the length and structure of sentences impact the sentence
embedding space and topology. This paper reports research on a set of
comprehensive clustering and network analyses targeting sentence and
sub-sentence embedding spaces. Results show that one method generates the most
clusterable embeddings. In general, the embeddings of span sub-sentences have
better clustering properties than the original sentences. The results have
implications for future sentence embedding models and applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Zero-shot Multilingual Neural Machine Translation for Low-Resource Languages. (arXiv:2110.00712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00712">
<div class="article-summary-box-inner">
<span><p>Although the multilingual Neural Machine Translation(NMT), which extends
Google's multilingual NMT, has ability to perform zero-shot translation and the
iterative self-learning algorithm can improve the quality of zero-shot
translation, it confronts with two problems: the multilingual NMT model is
prone to generate wrong target language when implementing zero-shot
translation; the self-learning algorithm, which uses beam search to generate
synthetic parallel data, demolishes the diversity of the generated source
language and amplifies the impact of the same noise during the iterative
learning process. In this paper, we propose the tagged-multilingual NMT model
and improve the self-learning algorithm to handle these two problems. Firstly,
we extend the Google's multilingual NMT model and add target tokens to the
target languages, which associates the start tag with the target language to
ensure that the source language can be translated to the required target
language. Secondly, we improve the self-learning algorithm by replacing beam
search with random sample to increases the diversity of the generated data and
makes it properly cover the true data distribution. Experimental results on
IWSLT show that the adjusted tagged-multilingual NMT separately obtains 9.41
and 7.85 BLEU scores over the multilingual NMT on 2010 and 2017
Romanian-Italian test sets. Similarly, it obtains 9.08 and 7.99 BLEU scores on
Italian-Romanian zero-shot translation. Furthermore, the improved self-learning
algorithm shows its superiorities over the conventional self-learning algorithm
on zero-shot translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is There More Pattern in Knowledge Graph? Exploring Proximity Pattern for Knowledge Graph Embedding. (arXiv:2110.00720v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00720">
<div class="article-summary-box-inner">
<span><p>Modeling of relation pattern is the core focus of previous Knowledge Graph
Embedding works, which represents how one entity is related to another
semantically by some explicit relation. However, there is a more natural and
intuitive relevancy among entities being always ignored, which is that how one
entity is close to another semantically, without the consideration of any
explicit relation. We name such semantic phenomenon in knowledge graph as
proximity pattern. In this work, we explore the problem of how to define and
represent proximity pattern, and how it can be utilized to help knowledge graph
embedding. Firstly, we define the proximity of any two entities according to
their statistically shared queries, then we construct a derived graph structure
and represent the proximity pattern from global view. Moreover, with the
original knowledge graph, we design a Chained couPle-GNN (CP-GNN) architecture
to deeply merge the two patterns (graphs) together, which can encode a more
comprehensive knowledge embedding. Being evaluated on FB15k-237 and WN18RR
datasets, CP-GNN achieves state-of-the-art results for Knowledge Graph
Completion task, and can especially boost the modeling capacity for complex
queries that contain multiple answer entities, proving the effectiveness of
introduced proximity pattern.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simplify Your Law: Using Information Theory to Deduplicate Legal Documents. (arXiv:2110.00735v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00735">
<div class="article-summary-box-inner">
<span><p>Textual redundancy is one of the main challenges to ensuring that legal texts
remain comprehensible and maintainable. Drawing inspiration from the
refactoring literature in software engineering, which has developed methods to
expose and eliminate duplicated code, we introduce the duplicated phrase
detection problem for legal texts and propose the Dupex algorithm to solve it.
Leveraging the Minimum Description Length principle from information theory,
Dupex identifies a set of duplicated phrases, called patterns, that together
best compress a given input text. Through an extensive set of experiments on
the Titles of the United States Code, we confirm that our algorithm works well
in practice: Dupex will help you simplify your law.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TopiOCQA: Open-domain Conversational Question Answeringwith Topic Switching. (arXiv:2110.00768v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00768">
<div class="article-summary-box-inner">
<span><p>In a conversational question answering scenario, a questioner seeks to
extract information about a topic through a series of interdependent questions
and answers. As the conversation progresses, they may switch to related topics,
a phenomenon commonly observed in information-seeking search sessions. However,
current datasets for conversational question answering are limiting in two
ways: 1) they do not contain topic switches; and 2) they assume the reference
text for the conversation is given, i.e., the setting is not open-domain. We
introduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset
with topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with
information-seeking questions and free-form answers. TopiOCQA poses a
challenging test-bed for models, where efficient retrieval is required on
multiple turns of the same conversation, in conjunction with constructing valid
responses using conversational history. We evaluate several baselines, by
combining state-of-the-art document retrieval methods with neural reader
models. Our best models achieves F1 of 51.9, and BLEU score of 42.1 which falls
short of human performance by 18.3 points and 17.6 points respectively,
indicating the difficulty of our dataset. Our dataset and code will be
available at https://mcgill-nlp.github.io/topiocqa
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimizing LR(1) State Machines is NP-Hard. (arXiv:2110.00776v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00776">
<div class="article-summary-box-inner">
<span><p>LR(1) parsing was a focus of extensive research in the past 50 years. Though
most fundamental mysteries have been resolved, a few remain hidden in the dark
corners. The one we bumped into is the minimization of the LR(1) state
machines, which we prove is NP-hard. It is the node-coloring problem that is
reduced to the minimization puzzle. The reduction makes use of two technique:
indirect reduction and incremental construction. Indirect reduction means the
graph to be colored is not reduced to an LR(1) state machine directly. Instead,
it is reduced to a context-free grammar from which an LR(1) state machine is
derived. Furthermore, by considering the nodes in the graph to be colored one
at a time, the context-free grammar is incrementally extended from a template
context-free grammar that is for a two-node graph. The extension is done by
adding new grammar symbols and rules. A minimized LR(1) machine can be used to
recover a minimum coloring of the original graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect Sentiment Quad Prediction as Paraphrase Generation. (arXiv:2110.00796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00796">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) has been extensively studied in recent
years, which typically involves four fundamental sentiment elements, including
the aspect category, aspect term, opinion term, and sentiment polarity.
Existing studies usually consider the detection of partial sentiment elements,
instead of predicting the four elements in one shot. In this work, we introduce
the Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all
sentiment elements in quads for a given opinionated sentence, which can reveal
a more comprehensive and complete aspect-level sentiment structure. We further
propose a novel \textsc{Paraphrase} modeling paradigm to cast the ASQP task to
a paraphrase generation process. On one hand, the generation formulation allows
solving ASQP in an end-to-end manner, alleviating the potential error
propagation in the pipeline solution. On the other hand, the semantics of the
sentiment elements can be fully exploited by learning to generate them in the
natural language form. Extensive experiments on benchmark datasets show the
superiority of our proposed method and the capacity of cross-task transfer with
the proposed unified \textsc{Paraphrase} modeling framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Swiss-Judgment-Prediction: A Multilingual Legal Judgment Prediction Benchmark. (arXiv:2110.00806v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00806">
<div class="article-summary-box-inner">
<span><p>In many jurisdictions, the excessive workload of courts leads to high delays.
Suitable predictive AI models can assist legal professionals in their work, and
thus enhance and speed up the process. So far, Legal Judgment Prediction (LJP)
datasets have been released in English, French, and Chinese. We publicly
release a multilingual (German, French, and Italian), diachronic (2000-2020)
corpus of 85K cases from the Federal Supreme Court of Switzerland (FSCS). We
evaluate state-of-the-art BERT-based methods including two variants of BERT
that overcome the BERT input (text) length limitation (up to 512 tokens).
Hierarchical BERT has the best performance (approx. 68-70% Macro-F1-Score in
German and French). Furthermore, we study how several factors (canton of
origin, year of publication, text length, legal area) affect performance. We
release both the benchmark dataset and our code to accelerate future research
and ensure reproducibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning. (arXiv:2110.00842v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00842">
<div class="article-summary-box-inner">
<span><p>Mapping natural language instructions to programs that computers can process
is a fundamental challenge. Existing approaches focus on likelihood-based
training or using reinforcement learning to fine-tune models based on a single
reward. In this paper, we pose program generation from language as Inverse
Reinforcement Learning. We introduce several interpretable reward components
and jointly learn (1) a reward function that linearly combines them, and (2) a
policy for program generation. Fine-tuning with our approach achieves
significantly better performance than competitive methods using Reinforcement
Learning (RL). On the VirtualHome framework, we get improvements of up to 9.0%
on the Longest Common Subsequence metric and 14.7% on recall-based metrics over
previous work on this framework (Puig et al., 2018). The approach is
data-efficient, showing larger gains in performance in the low-data regime.
Generated programs are also preferred by human evaluators over an RL-based
approach, and rated higher on relevance, completeness, and human-likeness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Sentiment Analysis Using NLP and Different Machine Learning Techniques on US Airline Twitter Data. (arXiv:2110.00859v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00859">
<div class="article-summary-box-inner">
<span><p>Today's business ecosystem has become very competitive. Customer satisfaction
has become a major focus for business growth. Business organizations are
spending a lot of money and human resources on various strategies to understand
and fulfill their customer's needs. But, because of defective manual analysis
on multifarious needs of customers, many organizations are failing to achieve
customer satisfaction. As a result, they are losing customer's loyalty and
spending extra money on marketing. We can solve the problems by implementing
Sentiment Analysis. It is a combined technique of Natural Language Processing
(NLP) and Machine Learning (ML). Sentiment Analysis is broadly used to extract
insights from wider public opinion behind certain topics, products, and
services. We can do it from any online available data. In this paper, we have
introduced two NLP techniques (Bag-of-Words and TF-IDF) and various ML
classification algorithms (Support Vector Machine, Logistic Regression,
Multinomial Naive Bayes, Random Forest) to find an effective approach for
Sentiment Analysis on a large, imbalanced, and multi-classed dataset. Our best
approaches provide 77% accuracy using Support Vector Machine and Logistic
Regression with Bag-of-Words technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Case Study to Reveal if an Area of Interest has a Trend in Ongoing Tweets Using Word and Sentence Embeddings. (arXiv:2110.00866v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00866">
<div class="article-summary-box-inner">
<span><p>In the field of Natural Language Processing, information extraction from
texts has been the objective of many researchers for years. Many different
techniques have been applied in order to reveal the opinion that a tweet might
have, thus understanding the sentiment of the small writing up to 280
characters. Other than figuring out the sentiment of a tweet, a study can also
focus on finding the correlation of the tweets with a certain area of interest,
which constitutes the purpose of this study. In order to reveal if an area of
interest has a trend in ongoing tweets, we have proposed an easily applicable
automated methodology in which the Daily Mean Similarity Scores that show the
similarity between the daily tweet corpus and the target words representing our
area of interest is calculated by using a na\"ive correlation-based technique
without training any Machine Learning Model. The Daily Mean Similarity Scores
have mainly based on cosine similarity and word/sentence embeddings computed by
Multilanguage Universal Sentence Encoder and showed main opinion stream of the
tweets with respect to a certain area of interest, which proves that an ongoing
trend of a specific subject on Twitter can easily be captured in almost real
time by using the proposed methodology in this study. We have also compared the
effectiveness of using word versus sentence embeddings while applying our
methodology and realized that both give almost the same results, whereas using
word embeddings requires less computational time than sentence embeddings, thus
being more effective. This paper will start with an introduction followed by
the background information about the basics, then continue with the explanation
of the proposed methodology and later on finish by interpreting the results and
concluding the findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subtractive mountain clustering algorithm applied to a chatbot to assist elderly people in medication intake. (arXiv:2110.00933v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00933">
<div class="article-summary-box-inner">
<span><p>Errors in medication intake among elderly people are very common. One of the
main causes for this is their loss of ability to retain information. The high
amount of medicine intake required by the advanced age is another limiting
factor. Thence, the design of an interactive aid system, preferably using
natural language, to help the older population with medication is in demand. A
chatbot based on a subtractive cluster algorithm, included in unsupervised
learned models, is the chosen solution since the processing of natural
languages is a necessary step in view to construct a chatbot able to answer
questions that older people may pose upon themselves concerning a particular
drug. In this work, the subtractive mountain clustering algorithm has been
adapted to the problem of natural languages processing. This algorithm version
allows for the association of a set of words into clusters. After finding the
centre of every cluster -- the most relevant word, all the others are
aggregated according to a defined metric adapted to the language processing
realm. All the relevant stored information is processed, as well as the
questions, by the algorithm. The correct processing of the text enables the
chatbot to produce answers that relate to the posed queries. To validate the
method, we use the package insert of a drug as the available information and
formulate associated questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Likelihood Ratio Estimation for High- to Zero-frequency N-grams. (arXiv:2110.00946v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00946">
<div class="article-summary-box-inner">
<span><p>Likelihood ratios (LRs), which are commonly used for probabilistic data
processing, are often estimated based on the frequency counts of individual
elements obtained from samples. In natural language processing, an element can
be a continuous sequence of $N$ items, called an $N$-gram, in which each item
is a word, letter, etc. In this paper, we attempt to estimate LRs based on
$N$-gram frequency information. A naive estimation approach that uses only
$N$-gram frequencies is sensitive to low-frequency (rare) $N$-grams and not
applicable to zero-frequency (unobserved) $N$-grams; these are known as the
low- and zero-frequency problems, respectively. To address these problems, we
propose a method for decomposing $N$-grams into item units and then applying
their frequencies along with the original $N$-gram frequencies. Our method can
obtain the estimates of unobserved $N$-grams by using the unit frequencies.
Although using only unit frequencies ignores dependencies between items, our
method takes advantage of the fact that certain items often co-occur in
practice and therefore maintains their dependencies by using the relevant
$N$-gram frequencies. We also introduce a regularization to achieve robust
estimation for rare $N$-grams. Our experimental results demonstrate that our
method is effective at solving both problems and can effectively control
dependencies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00976">
<div class="article-summary-box-inner">
<span><p>Law, interpretations of law, legal arguments, agreements, etc. are typically
expressed in writing, leading to the production of vast corpora of legal text.
Their analysis, which is at the center of legal practice, becomes increasingly
elaborate as these collections grow in size. Natural language understanding
(NLU) technologies can be a valuable tool to support legal practitioners in
these endeavors. Their usefulness, however, largely depends on whether current
state-of-the-art models can generalize across various tasks in the legal
domain. To answer this currently open question, we introduce the Legal General
Language Understanding Evaluation (LexGLUE) benchmark, a collection of datasets
for evaluating model performance across a diverse set of legal NLU tasks in a
standardized way. We also provide an evaluation and analysis of several generic
and legal-oriented models demonstrating that the latter consistently offer
performance improvements across multiple tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple Recurrent Neural Networks is all we need for clinical events predictions using EHR data. (arXiv:2110.00998v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00998">
<div class="article-summary-box-inner">
<span><p>Recently, there is great interest to investigate the application of deep
learning models for the prediction of clinical events using electronic health
records (EHR) data. In EHR data, a patient's history is often represented as a
sequence of visits, and each visit contains multiple events. As a result, deep
learning models developed for sequence modeling, like recurrent neural networks
(RNNs) are common architecture for EHR-based clinical events predictive models.
While a large variety of RNN models were proposed in the literature, it is
unclear if complex architecture innovations will offer superior predictive
performance. In order to move this field forward, a rigorous evaluation of
various methods is needed. In this study, we conducted a thorough benchmark of
RNN architectures in modeling EHR data. We used two prediction tasks: the risk
for developing heart failure and the risk of early readmission for inpatient
hospitalization. We found that simple gated RNN models, including GRUs and
LSTMs, often offer competitive results when properly tuned with Bayesian
Optimization, which is in line with similar to findings in the natural language
processing (NLP) domain. For reproducibility, Our codebase is shared at
https://github.com/ZhiGroup/pytorch_ehr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01013">
<div class="article-summary-box-inner">
<span><p>Today's VQA models still tend to capture superficial linguistic correlations
in the training set and fail to generalize to the test set with different QA
distributions. To reduce these language biases, recent VQA works introduce an
auxiliary question-only model to regularize the training of targeted VQA model,
and achieve dominating performance on diagnostic benchmarks for
out-of-distribution testing. However, due to complex model design, these
ensemble-based methods are unable to equip themselves with two indispensable
characteristics of an ideal VQA model: 1) Visual-explainable: The model should
rely on the right visual regions when making decisions. 2) Question-sensitive:
The model should be sensitive to the linguistic variations in questions. To
this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing
and Training (CSST) strategy. After training with CSST, VQA models are forced
to focus on all critical objects and words, which significantly improves both
visual-explainable and question-sensitive abilities. Specifically, CSST is
composed of two parts: Counterfactual Samples Synthesizing (CSS) and
Counterfactual Samples Training (CST). CSS generates counterfactual samples by
carefully masking critical objects in images or words in questions and
assigning pseudo ground-truth answers. CST not only trains the VQA models with
both complementary samples to predict respective ground-truth answers, but also
urges the VQA models to further distinguish the original samples and
superficially similar counterfactual ones. To facilitate the CST training, we
propose two variants of supervised contrastive loss for VQA, and design an
effective positive and negative sample selection mechanism based on CSS.
Extensive experiments have shown the effectiveness of CSST. Particularly, by
building on top of model LMH+SAR, we achieve record-breaking performance on all
OOD benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Project Debater APIs: Decomposing the AI Grand Challenge. (arXiv:2110.01029v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01029">
<div class="article-summary-box-inner">
<span><p>Project Debater was revealed in 2019 as the first AI system that can debate
human experts on complex topics. Engaging in a live debate requires a diverse
set of skills, and Project Debater has been developed accordingly as a
collection of components, each designed to perform a specific subtask. Project
Debater APIs provide access to many of these capabilities, as well as to more
recently developed ones. This diverse set of web services, publicly available
for academic use, includes core NLP services, argument mining and analysis
capabilities, and higher-level services for content summarization. We describe
these APIs and their performance, and demonstrate how they can be used for
building practical solutions. In particular, we will focus on Key Point
Analysis, a novel technology that identifies the main points and their
prevalence in a collection of texts such as survey responses and user reviews.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Document Keyphrase Extraction: A Literature Review and the First Dataset. (arXiv:2110.01073v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01073">
<div class="article-summary-box-inner">
<span><p>Keyphrase extraction has been comprehensively researched within the
single-document setting, with an abundance of methods and a wealth of datasets.
In contrast, multi-document keyphrase extraction has been infrequently studied,
despite its utility for describing sets of documents, and its use in
summarization. Moreover, no dataset existed for multi-document keyphrase
extraction, hindering the progress of the task. Recent advances in multi-text
processing make the task an even more appealing challenge to pursue. To
initiate this pursuit, we present here the first literature review and the
first dataset for the task, MK-DUC-01, which can serve as a new benchmark. We
test several keyphrase extraction baselines on our data and show their results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Voice-Activated Framework using Self-supervised Learning. (arXiv:2110.01077v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01077">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning methods such as wav2vec 2.0 have shown promising
results in learning speech representations from unlabelled and untranscribed
speech data that are useful for speech recognition. Since these representations
are learned without any task-specific supervision, they can also be useful for
other voice-activated tasks like speaker verification, keyword spotting,
emotion classification etc. In our work, we propose a general purpose framework
for adapting a pre-trained wav2vec 2.0 model for different voice-activated
tasks. We develop downstream network architectures that operate on the
contextualized speech representations of wav2vec 2.0 to adapt the
representations for solving a given task. Finally, we extend our framework to
perform multi-task learning by jointly optimizing the network parameters on
multiple voice activated tasks using a shared transformer backbone. Both of our
single and multi-task frameworks achieve state-of-the-art results in speaker
verification and keyword spotting benchmarks. Our best performing models
achieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and
VoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0
keyword spotting dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Understanding Persuasion in Computational Argumentation. (arXiv:2110.01078v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01078">
<div class="article-summary-box-inner">
<span><p>Opinion formation and persuasion in argumentation are affected by three major
factors: the argument itself, the source of the argument, and the properties of
the audience. Understanding the role of each and the interplay between them is
crucial for obtaining insights regarding argument interpretation and
generation. It is particularly important for building effective argument
generation systems that can take both the discourse and the audience
characteristics into account. Having such personalized argument generation
systems would be helpful to expose individuals to different viewpoints and help
them make a more fair and informed decision on an issue. Even though studies in
Social Sciences and Psychology have shown that source and audience effects are
essential components of the persuasion process, most research in computational
persuasion has focused solely on understanding the characteristics of
persuasive language. In this thesis, we make several contributions to
understand the relative effect of the source, audience, and language in
computational persuasion. We first introduce a large-scale dataset with
extensive user information to study these factors' effects simultaneously.
Then, we propose models to understand the role of the audience's prior beliefs
on their perception of arguments. We also investigate the role of social
interactions and engagement in understanding users' success in online debating
over time. We find that the users' prior beliefs and social interactions play
an essential role in predicting their success in persuasion. Finally, we
explore the importance of incorporating contextual information to predict
argument impact and show improvements compared to encoding only the text of the
arguments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Examples Generation for Reducing Implicit Gender Bias in Pre-trained Models. (arXiv:2110.01094v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01094">
<div class="article-summary-box-inner">
<span><p>Over the last few years, Contextualized Pre-trained Neural Language Models,
such as BERT, GPT, have shown significant gains in various NLP tasks. To
enhance the robustness of existing pre-trained models, one way is adversarial
examples generation and evaluation for conducting data augmentation or
adversarial learning. In the meanwhile, gender bias embedded in the models
seems to be a serious problem in practical applications. Many researches have
covered the gender bias produced by word-level information(e.g.
gender-stereotypical occupations), while few researchers have investigated the
sentence-level cases and implicit cases.
</p>
<p>In this paper, we proposed a method to automatically generate implicit gender
bias samples at sentence-level and a metric to measure gender bias. Samples
generated by our method will be evaluated in terms of accuracy. The metric will
be used to guide the generation of examples from Pre-trained models. Therefore,
those examples could be used to impose attacks on Pre-trained Models. Finally,
we discussed the evaluation efficacy of our generated examples on reducing
gender bias for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Language Models for Understanding of Temporal Expressions. (arXiv:2110.01113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01113">
<div class="article-summary-box-inner">
<span><p>We present three Natural Language Inference (NLI) challenge sets that can
evaluate NLI models on their understanding of temporal expressions. More
specifically, we probe these models for three temporal properties: (a) the
order between points in time, (b) the duration between two points in time, (c)
the relation between the magnitude of times specified in different units. We
find that although large language models fine-tuned on MNLI have some basic
perception of the order between points in time, at large, these models do not
have a thorough understanding of the relation between temporal expressions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured abbreviation expansion in context. (arXiv:2110.01140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01140">
<div class="article-summary-box-inner">
<span><p>Ad hoc abbreviations are commonly found in informal communication channels
that favor shorter messages. We consider the task of reversing these
abbreviations in context to recover normalized, expanded versions of
abbreviated messages. The problem is related to, but distinct from, spelling
correction, in that ad hoc abbreviations are intentional and may involve
substantial differences from the original words. Ad hoc abbreviations are
productively generated on-the-fly, so they cannot be resolved solely by
dictionary lookup. We generate a large, open-source data set of ad hoc
abbreviations. This data is used to study abbreviation strategies and to
develop two strong baselines for abbreviation expansion
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Interplay Between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis. (arXiv:2110.01147v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01147">
<div class="article-summary-box-inner">
<span><p>Are end-to-end text-to-speech (TTS) models over-parametrized? To what extent
can these models be pruned, and what happens to their synthesis capabilities?
This work serves as a starting point to explore pruning both spectrogram
prediction networks and vocoders. We thoroughly investigate the tradeoffs
between sparstiy and its subsequent effects on synthetic speech. Additionally,
we explored several aspects of TTS pruning: amount of finetuning data versus
sparsity, TTS-Augmentation to utilize unspoken text, and combining knowledge
distillation and pruning. Our findings suggest that not only are end-to-end TTS
models highly prunable, but also, perhaps surprisingly, pruned TTS models can
produce synthetic speech with equal or higher naturalness and intelligibility,
with similar prosody. All of our experiments are conducted on publicly
available models, and findings in this work are backed by large-scale
subjective tests and objective measures. Code and 200 pruned models are made
available to facilitate future research on efficiency in TTS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts. (arXiv:2110.01159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01159">
<div class="article-summary-box-inner">
<span><p>Recent models in developing summarization systems consist of millions of
parameters and the model performance is highly dependent on the abundance of
training data. While most existing summarization corpora contain data in the
order of thousands to one million, generation of large-scale summarization
datasets in order of couple of millions is yet to be explored. Practically,
more data is better at generalizing the training patterns to unseen data. In
this paper, we introduce TLDR9+ -- a large-scale summarization dataset --
containing over 9 million training instances extracted from Reddit discussion
forum (https://github.com/sajastu/reddit_collector). This dataset is
specifically gathered to perform extreme summarization (i.e., generating
one-sentence summary in high compression and abstraction) and is more than
twice larger than the previously proposed dataset. We go one step further and
with the help of human annotations, we distill a more fine-grained dataset by
sampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We
further pinpoint different state-of-the-art summarization models on our
proposed datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Topics: Discovering Latent Healthcare Objectives from Event Sequences. (arXiv:2110.01160v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01160">
<div class="article-summary-box-inner">
<span><p>A meaningful understanding of clinical protocols and patient pathways helps
improve healthcare outcomes. Electronic health records (EHR) reflect real-world
treatment behaviours that are used to enhance healthcare management but present
challenges; protocols and pathways are often loosely defined and with elements
frequently not recorded in EHRs, complicating the enhancement. To solve this
challenge, healthcare objectives associated with healthcare management
activities can be indirectly observed in EHRs as latent topics. Topic models,
such as Latent Dirichlet Allocation (LDA), are used to identify latent patterns
in EHR data. However, they do not examine the ordered nature of EHR sequences,
nor do they appraise individual events in isolation. Our novel approach, the
Categorical Sequence Encoder (CaSE) addresses these shortcomings. The
sequential nature of EHRs is captured by CaSE's event-level representations,
revealing latent healthcare objectives. In synthetic EHR sequences, CaSE
outperforms LDA by up to 37% at identifying healthcare objectives. In the
real-world MIMIC-III dataset, CaSE identifies meaningful representations that
could critically enhance protocol and pathway development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Metric for Evaluating Semantics Preservation. (arXiv:2110.01176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01176">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage pre-trained language models (PLMs) to precisely
evaluate the semantics preservation of edition process on sentences. Our
metric, Neighbor Distribution Divergence (NDD), evaluates the disturbance on
predicted distribution of neighboring words from mask language model (MLM). NDD
is capable of detecting precise changes in semantics which are easily ignored
by text similarity. By exploiting the property of NDD, we implement a
unsupervised and even training-free algorithm for extractive sentence
compression. We show that our NDD-based algorithm outperforms previous
perplexity-based unsupervised algorithm by a large margin. For further
exploration on interpretability, we evaluate NDD by pruning on syntactic
dependency treebanks and apply NDD for predicate detection as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The state-of-the-art in text-based automatic personality prediction. (arXiv:2110.01186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01186">
<div class="article-summary-box-inner">
<span><p>Personality detection is an old topic in psychology and Automatic Personality
Prediction (or Perception) (APP) is the automated (computationally) forecasting
of the personality on different types of human generated/exchanged contents
(such as text, speech, image, video). The principal objective of this study is
to offer a shallow (overall) review of natural language processing approaches
on APP since 2010. With the advent of deep learning and following it
transfer-learning and pre-trained model in NLP, APP research area has been a
hot topic, so in this review, methods are categorized into three; pre-trained
independent, pre-trained model based, multimodal approaches. Also, to achieve a
comprehensive comparison, reported results are informed by datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LawSum: A weakly supervised approach for Indian Legal Document Summarization. (arXiv:2110.01188v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01188">
<div class="article-summary-box-inner">
<span><p>Unlike the courts in western countries, public records of Indian judiciary
are completely unstructured and noisy. No large scale publicly available
annotated datasets of Indian legal documents exist till date. This limits the
scope for legal analytics research. In this work, we propose a new dataset
consisting of over 10,000 judgements delivered by the supreme court of India
and their corresponding hand written summaries. The proposed dataset is
pre-processed by normalising common legal abbreviations, handling spelling
variations in named entities, handling bad punctuations and accurate sentence
tokenization. Each sentence is tagged with their rhetorical roles. We also
annotate each judgement with several attributes like date, names of the
plaintiffs, defendants and the people representing them, judges who delivered
the judgement, acts/statutes that are cited and the most common citations used
to refer the judgement. Further, we propose an automatic labelling technique
for identifying sentences which have summary worthy information. We demonstrate
that this auto labeled data can be used effectively to train a weakly
supervised sentence extractor with high accuracy. Some possible applications of
this dataset besides legal document summarization can be in retrieval, citation
analysis and prediction of decisions by a particular judge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeBERTa: Decoding-enhanced BERT with Disentangled Attention. (arXiv:2006.03654v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03654">
<div class="article-summary-box-inner">
<span><p>Recent progress in pre-trained neural language models has significantly
improved the performance of many natural language processing (NLP) tasks. In
this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT
with disentangled attention) that improves the BERT and RoBERTa models using
two novel techniques. The first is the disentangled attention mechanism, where
each word is represented using two vectors that encode its content and
position, respectively, and the attention weights among words are computed
using disentangled matrices on their contents and relative positions,
respectively. Second, an enhanced mask decoder is used to incorporate absolute
positions in the decoding layer to predict the masked tokens in model
pre-training. In addition, a new virtual adversarial training method is used
for fine-tuning to improve models' generalization. We show that these
techniques significantly improve the efficiency of model pre-training and the
performance of both natural language understanding (NLU) and natural langauge
generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model
trained on half of the training data performs consistently better on a wide
range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),
on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).
Notably, we scale up DeBERTa by training a larger version that consists of 48
Transform layers with 1.5 billion parameters. The significant performance boost
makes the single DeBERTa model surpass the human performance on the SuperGLUE
benchmark (Wang et al., 2019a) for the first time in terms of macro-average
score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the
SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline
by a decent margin (90.3 versus 89.8).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Named Entity Recognition for Kazakh. (arXiv:2007.13626v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.13626">
<div class="article-summary-box-inner">
<span><p>We present several neural networks to address the task of named entity
recognition for morphologically complex languages (MCL). Kazakh is a
morphologically complex language in which each root/stem can produce hundreds
or thousands of variant word forms. This nature of the language could lead to a
serious data sparsity problem, which may prevent the deep learning models from
being well trained for under-resourced MCLs. In order to model the MCLs' words
effectively, we introduce root and entity tag embedding plus tensor layer to
the neural networks. The effects of those are significant for improving NER
model performance of MCLs. The proposed models outperform state-of-the-art
including character-based approaches, and can be potentially applied to other
morphologically complex languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Text Generation with Pattern-Exploiting Training. (arXiv:2012.11926v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11926">
<div class="article-summary-box-inner">
<span><p>Providing pretrained language models with simple task descriptions in natural
language enables them to solve some tasks in a fully unsupervised fashion.
Moreover, when combined with regular learning from examples, this idea yields
impressive few-shot results for a wide range of text classification tasks. It
is also a promising direction to improve data efficiency in generative
settings, but there are several challenges to using a combination of task
descriptions and example-based learning for text generation. In particular, it
is crucial to find task descriptions that are easy to understand for the
pretrained model and to ensure that it actually makes good use of them;
furthermore, effective measures against overfitting have to be implemented. In
this paper, we show how these challenges can be tackled: We introduce GenPET, a
method for text generation that is based on pattern-exploiting training, a
recent approach for combining textual instructions with supervised learning
that only works for classification tasks. On several summarization and headline
generation datasets, GenPET gives consistent improvements over strong baselines
in few-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast WordPiece Tokenization. (arXiv:2012.15524v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15524">
<div class="article-summary-box-inner">
<span><p>Tokenization is a fundamental preprocessing step for almost all NLP tasks. In
this paper, we propose efficient algorithms for the WordPiece tokenization used
in BERT, from single-word tokenization to general text (e.g., sentence)
tokenization. When tokenizing a single word, WordPiece uses a
longest-match-first strategy, known as maximum matching. The best known
algorithms so far are O(n^2) (where n is the input length) or O(nm) (where m is
the maximum vocabulary token length). We propose a novel algorithm whose
tokenization complexity is strictly O(n). Our method is inspired by the
Aho-Corasick algorithm. We introduce additional linkages on top of the trie
built from the vocabulary, allowing smart transitions when the trie matching
cannot continue. For general text, we further propose an algorithm that
combines pre-tokenization (splitting the text into words) and our linear-time
WordPiece method into a single pass. Experimental results show that our method
is 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text
on average for general text tokenization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vy\=akarana: A Colorless Green Benchmark for Syntactic Evaluation in Indic Languages. (arXiv:2103.00854v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00854">
<div class="article-summary-box-inner">
<span><p>While there has been significant progress towards developing NLU resources
for Indic languages, syntactic evaluation has been relatively less explored.
Unlike English, Indic languages have rich morphosyntax, grammatical genders,
free linear word-order, and highly inflectional morphology. In this paper, we
introduce Vy\=akarana: a benchmark of Colorless Green sentences in Indic
languages for syntactic evaluation of multilingual language models. The
benchmark comprises four syntax-related tasks: PoS Tagging, Syntax Tree-depth
Prediction, Grammatical Case Marking, and Subject-Verb Agreement. We use the
datasets from the evaluation tasks to probe five multilingual language models
of varying architectures for syntax in Indic languages. Due to its prevalence,
we also include a code-switching setting in our experiments. Our results show
that the token-level and sentence-level representations from the Indic language
models (IndicBERT and MuRIL) do not capture the syntax in Indic languages as
efficiently as the other highly multilingual language models. Further, our
layer-wise probing experiments reveal that while mBERT, DistilmBERT, and XLM-R
localize the syntax in middle layers, the Indic language models do not show
such syntactic localization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-autoregressive Mandarin-English Code-switching Speech Recognition. (arXiv:2104.02258v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02258">
<div class="article-summary-box-inner">
<span><p>Mandarin-English code-switching (CS) is frequently used among East and
Southeast Asian people. However, the intra-sentence language switching of the
two very different languages makes recognizing CS speech challenging.
Meanwhile, the recent successful non-autoregressive (NAR) ASR models remove the
need for left-to-right beam decoding in autoregressive (AR) models and achieved
outstanding performance and fast inference speed, but it has not been applied
to Mandarin-English CS speech recognition. This paper takes advantage of the
Mask-CTC NAR ASR framework to tackle the CS speech recognition issue. We
further propose to change the Mandarin output target of the encoder to Pinyin
for faster encoder training and introduce the Pinyin-to-Mandarin decoder to
learn contextualized information. Moreover, we use word embedding label
smoothing to regularize the decoder with contextualized information and
projection matrix regularization to bridge that gap between the encoder and
decoder. We evaluate these methods on the SEAME corpus and achieved exciting
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Datasets with Pretrained Language Models. (arXiv:2104.07540v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07540">
<div class="article-summary-box-inner">
<span><p>To obtain high-quality sentence embeddings from pretrained language models
(PLMs), they must either be augmented with additional pretraining objectives or
finetuned on a large set of labeled text pairs. While the latter approach
typically outperforms the former, it requires great human effort to generate
suitable datasets of sufficient size. In this paper, we show how PLMs can be
leveraged to obtain high-quality sentence embeddings without the need for
labeled data, finetuning or modifications to the pretraining objective: We
utilize the generative abilities of large and high-performing PLMs to generate
entire datasets of labeled text pairs from scratch, which we then use for
finetuning much smaller and more efficient models. Our fully unsupervised
approach outperforms strong baselines on several semantic textual similarity
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation. (arXiv:2104.08200v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08200">
<div class="article-summary-box-inner">
<span><p>A benchmark provides an ecosystem to measure the advancement of models with
standard datasets and automatic and human evaluation metrics. We introduce
IndoNLG, the first such benchmark for the Indonesian language for natural
language generation (NLG). It covers six tasks: summarization, question
answering, open chitchat, as well as three different language-pairs of machine
translation tasks. We provide a vast and clean pre-training corpus of
Indonesian, Sundanese, and Javanese datasets called Indo4B-Plus, which is used
to train our pre-trained NLG model, IndoBART. We evaluate the effectiveness and
efficiency of IndoBART by conducting extensive evaluation on all IndoNLG tasks.
Our findings show that IndoBART achieves competitive performance on Indonesian
tasks with five times fewer parameters compared to the largest multilingual
model in our benchmark, mBART-LARGE (Liu et al., 2020), and an almost 4x and
2.5x faster inference time on the CPU and GPU respectively. We additionally
demonstrate the ability of IndoBART to learn Javanese and Sundanese, and it
achieves decent performance on machine translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Human-Imitating Metrics for Training and Evaluating Privacy Preserving Emotion Recognition Models Using Sociolinguistic Knowledge. (arXiv:2104.08792v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08792">
<div class="article-summary-box-inner">
<span><p>Privacy preservation is a crucial component of any real-world application.
But, in applications relying on machine learning backends, privacy is
challenging because models often capture more than what the model was initially
trained for, resulting in the potential leakage of sensitive information. In
this paper, we propose an automatic and quantifiable metric that allows us to
evaluate humans' perception of a model's ability to preserve privacy with
respect to sensitive variables. In this paper, we focus on saliency-based
explanations, explanations that highlight regions of the input text, to infer
internal workings of a black box model. We use the degree with which
differences in interpretation of general vs privacy preserving models correlate
with sociolinguistic biases to inform metric design. We show how certain
commonly-used methods that seek to preserve privacy do not align with human
perception of privacy preservation leading to distrust about model's claims. We
demonstrate the versatility of our proposed metric by validating its utility
for measuring cross corpus generalization for both privacy and emotion.
Finally, we conduct crowdsourcing experiments to evaluate the inclination of
the evaluators to choose a particular model for a given purpose when model
explanations are provided, and show a positive relationship with the proposed
metric. To the best of our knowledge, we take the first step in proposing
automatic and quantifiable metrics that best align with human perception of
model's ability for privacy preservation, allowing for cost-effective model
development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks. (arXiv:2104.08815v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08815">
<div class="article-summary-box-inner">
<span><p>Increasing concerns and regulations about data privacy and sparsity
necessitate the study of privacy-preserving, decentralized learning methods for
natural language processing (NLP) tasks. Federated learning (FL) provides
promising approaches for a large number of clients (e.g., personal devices or
organizations) to collaboratively learn a shared global model to benefit all
clients while allowing users to keep their data locally. Despite interest in
studying FL methods for NLP tasks, a systematic comparison and analysis is
lacking in the literature. Herein, we present the FedNLP, a benchmarking
framework for evaluating federated learning methods on four different task
formulations: text classification, sequence tagging, question answering, and
seq2seq. We propose a universal interface between Transformer-based language
models (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under
various non-IID partitioning strategies. Our extensive experiments with FedNLP
provide empirical comparisons between FL methods and helps us better understand
the inherent challenges of this direction. The comprehensive analysis points to
intriguing and exciting future research aimed at developing FL methods for NLP
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Predicting Gender by First Name Using Character-level Machine Learning. (arXiv:2106.10156v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10156">
<div class="article-summary-box-inner">
<span><p>Predicting gender by the first name is not a simple task. In many
applications, especially in the natural language processing (NLP) field, this
task may be necessary, mainly when considering foreign names. In this paper, we
examined and implemented several machine learning algorithms, such as extra
trees, KNN, Naive Bayes, SVM, random forest, gradient boosting, light GBM,
logistic regression, ridge classifier, and deep neural network models, such as
MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first name. A
dataset of Brazilian names is used to train and evaluate the models. We
analyzed the accuracy, recall, precision, f1 score, and confusion matrix to
measure the models' performances. The results indicate that the gender
prediction can be performed from the feature extraction strategy looking at the
names as a set of strings. Some models accurately predict gender in more than
95% of the cases. The recurrent models overcome the feedforward models in this
binary classification problem.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03228">
<div class="article-summary-box-inner">
<span><p>Recent studies on compression of pretrained language models (e.g., BERT)
usually use preserved accuracy as the metric for evaluation. In this paper, we
propose two new metrics, label loyalty and probability loyalty that measure how
closely a compressed model (i.e., student) mimics the original model (i.e.,
teacher). We also explore the effect of compression with regard to robustness
under adversarial attacks. We benchmark quantization, pruning, knowledge
distillation and progressive module replacing with loyalty and robustness. By
combining multiple compression techniques, we provide a practical strategy to
achieve better accuracy, loyalty and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Search for a Search Method -- Simple Heuristics Suffice for Adversarial Text Attacks. (arXiv:2109.07926v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07926">
<div class="article-summary-box-inner">
<span><p>Recently more attention has been given to adversarial attacks on neural
networks for natural language processing (NLP). A central research topic has
been the investigation of search algorithms and search constraints, accompanied
by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth
order optimization-based attacks and compare with the benchmark results in the
TextAttack framework. Surprisingly, we find that optimization-based methods do
not yield any improvement in a constrained setup and slightly benefit from
approximate gradient information only in unconstrained setups where search
spaces are larger. In contrast, simple heuristics exploiting nearest neighbors
without querying the target function yield substantial success rates in
constrained setups, and nearly full success rate in unconstrained setups, at an
order of magnitude fewer queries. We conclude from these results that current
TextAttack benchmark tasks are too easy and constraints are too strict,
preventing meaningful research on black-box adversarial text attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08722">
<div class="article-summary-box-inner">
<span><p>Prerequisite chain learning helps people acquire new knowledge efficiently.
While people may quickly determine learning paths over concepts in a domain,
finding such paths in other domains can be challenging. We introduce
Domain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this
cross-domain prerequisite chain learning task efficiently. Our novel model
consists of a variational graph autoencoder (VGAE) and a domain discriminator.
The VGAE is trained to predict concept relations through link prediction, while
the domain discriminator takes both source and target domain data as input and
is trained to predict domain labels. Most importantly, this method only needs
simple homogeneous graphs as input, compared with the current state-of-the-art
model. We evaluate our model on the LectureBankCD dataset, and results show
that our model outperforms recent graph-based benchmarks while using only 1/10
of graph scale and 1/3 computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From None to Severe: Predicting Severity in Movie Scripts. (arXiv:2109.09276v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09276">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the task of predicting severity of age-restricted
aspects of movie content based solely on the dialogue script. We first
investigate categorizing the ordinal severity of movies on 5 aspects: Sex,
Violence, Profanity, Substance consumption, and Frightening scenes. The problem
is handled using a siamese network-based multitask framework which concurrently
improves the interpretability of the predictions. The experimental results show
that our method outperforms the previous state-of-the-art model and provides
useful information to interpret model predictions. The proposed dataset and
source code are publicly available at our GitHub repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation. (arXiv:2109.10164v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10164">
<div class="article-summary-box-inner">
<span><p>Intermediate layer knowledge distillation (KD) can improve the standard KD
technique (which only targets the output of teacher and student models)
especially over large pre-trained language models. However, intermediate layer
distillation suffers from excessive computational burdens and engineering
efforts required for setting up a proper layer mapping. To address these
problems, we propose a RAndom Intermediate Layer Knowledge Distillation
(RAIL-KD) approach in which, intermediate layers from the teacher model are
selected randomly to be distilled into the intermediate layers of the student
model. This randomized selection enforce that: all teacher layers are taken
into account in the training process, while reducing the computational cost of
intermediate layer distillation. Also, we show that it act as a regularizer for
improving the generalizability of the student model. We perform extensive
experiments on GLUE tasks as well as on out-of-domain test sets. We show that
our proposed RAIL-KD approach outperforms other state-of-the-art intermediate
layer KD methods considerably in both performance and training-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. (arXiv:2109.11491v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11491">
<div class="article-summary-box-inner">
<span><p>We present a method for exploring regions around individual points in a
contextualized vector space (particularly, BERT space), as a way to investigate
how these regions correspond to word senses. By inducing a contextualized
"pseudoword" as a stand-in for a static embedding in the input layer, and then
performing masked prediction of a word in the sentence, we are able to
investigate the geometry of the BERT-space in a controlled manner around
individual instances. Using our method on a set of carefully constructed
sentences targeting ambiguous English words, we find substantial regularity in
the contextualized space, with regions that correspond to distinct word senses;
but between these regions there are occasionally "sense voids" -- regions that
do not correspond to any intelligible sense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14927">
<div class="article-summary-box-inner">
<span><p>Temporal expressions in text play a significant role in language
understanding and correctly identifying them is fundamental to various
retrieval and natural language processing systems. Previous works have slowly
shifted from rule-based to neural architectures, capable of tagging expressions
with higher accuracy. However, neural models can not yet distinguish between
different expression types at the same level as their rule-based counterparts.
In this work, we aim to identify the most suitable transformer architecture for
joint temporal tagging and type classification, as well as, investigating the
effect of semi-supervised training on the performance of these systems. Based
on our study of token classification variants and encoder-decoder
architectures, we present a transformer encoder-decoder model using the RoBERTa
language model as our best performing system. By supplementing training
resources with weakly labeled data from rule-based systems, our model surpasses
previous works in temporal tagging and type classification, especially on rare
classes. Our code and pre-trained experiments are available at:
https://github.com/satya77/Transformer_Temporal_Tagger
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-05 23:09:33.344019721 UTC">2021-10-05 23:09:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>