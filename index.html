<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-11T01:30:00Z">10-11</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Sentence Classification: Detecting Sustainability Initiatives in Company Reports. (arXiv:2110.03727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03727">
<div class="article-summary-box-inner">
<span><p>We introduce the novel task of detecting sustainability initiatives in
company reports. Given a full report, the aim is to automatically identify
mentions of practical activities that a company has performed in order to
tackle specific societal issues. As a single initiative can often be described
over multiples sentences, new methods for identifying continuous sentence spans
needs to be developed. We release a new dataset of company reports in which the
text has been manually annotated with sustainability initiatives. We also
evaluate different models for initiative detection, introducing a novel
aggregation and evaluation methodology. Our proposed architecture uses
sequences of five consecutive sentences to account for contextual information
when making classification decisions at the individual sentence level.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UoB at SemEval-2021 Task 5: Extending Pre-Trained Language Models to Include Task and Domain-Specific Information for Toxic Span Prediction. (arXiv:2110.03730v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03730">
<div class="article-summary-box-inner">
<span><p>Toxicity is pervasive in social media and poses a major threat to the health
of online communities. The recent introduction of pre-trained language models,
which have achieved state-of-the-art results in many NLP tasks, has transformed
the way in which we approach natural language processing. However, the inherent
nature of pre-training means that they are unlikely to capture task-specific
statistical information or learn domain-specific knowledge. Additionally, most
implementations of these models typically do not employ conditional random
fields, a method for simultaneous token classification. We show that these
modifications can improve model performance on the Toxic Spans Detection task
at SemEval-2021 to achieve a score within 4 percentage points of the top
performing team.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference. (arXiv:2110.03742v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03742">
<div class="article-summary-box-inner">
<span><p>Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling
multilingual translation models to billions of parameters without a
proportional increase in training computation. However, MoE models are
prohibitively large and practitioners often resort to methods such as
distillation for serving. In this work, we investigate routing strategies at
different granularity (token, sentence, task) in MoE models to bypass
distillation. Experiments on WMT and a web-scale dataset suggest that
task-level routing (task-MoE) enables us to extract smaller, ready-to-deploy
sub-networks from large sparse models. On WMT, our task-MoE with 32 experts
(533M parameters) outperforms the best performing token-level MoE model
(token-MoE) by +1.0 BLEU on average across 30 language pairs. The peak
inference throughput is also improved by a factor of 1.9x when we route by
tasks instead of tokens. While distilling a token-MoE to a smaller dense model
preserves only 32% of the BLEU gains, our sub-network task-MoE, by design,
preserves all the gains with the same inference cost as the distilled student
model. Finally, when scaling up to 200 language pairs, our 128-expert task-MoE
(13B parameters) performs competitively with a token-level counterpart, while
improving the peak inference throughput by a factor of 2.6x.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sonorant spectra and coarticulation distinguish speakers with different dialects. (arXiv:2110.03756v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03756">
<div class="article-summary-box-inner">
<span><p>The aim of this study is to determine the effect of language varieties on the
spectral distribution of stressed and unstressed sonorants (nasals /m, n/,
lateral approximants /l/, and rhotics /r/) and on their coarticulatory effects
on adjacent sounds. To quantify the shape of the spectral distribution, we
calculated the spectral moments from the sonorant spectra of nasals /m, n/,
lateral approximants /l/, and rhotics /r/ produced by Athenian Greek and
Cypriot Greek speakers. To estimate the co-articulatory effects of sonorants on
the adjacent vowels' F1 - F4 formant frequencies, we developed polynomial
models of the adjacent vowel's formant contours. We found significant effects
of language variety (sociolinguistic information) on the spectral moments of
each sonorant /m/, /n/, /l/, /r/ (except between /m/ and /n/) and on the
formant contours of the adjacent vowel. All sonorants (including /m/ and /n/)
had distinct effects on adjacent vowel's formant contours, especially for F3
and F4. The study highlights that the combination of spectral moments and
coarticulatory effects of sonorants determines linguistic (stress and phonemic
category) and sociolinguistic (language variety) characteristics of sonorants.
It also provides the first comparative acoustic analysis of Athenian Greek and
Cypriot Greek sonorants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Input Length Matters: An Empirical Study Of RNN-T And MWER Training For Long-form Telephony Speech Recognition. (arXiv:2110.03841v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03841">
<div class="article-summary-box-inner">
<span><p>End-to-end models have achieved state-of-the-art results on several automatic
speech recognition tasks. However, they perform poorly when evaluated on
long-form data, e.g., minutes long conversational telephony audio. One reason
the model fails on long-form speech is that it has only seen short utterances
during training. This paper presents an empirical study on the effect of
training utterance length on the word error rate (WER) for RNN-transducer
(RNN-T) model. We compare two widely used training objectives, log loss (or
RNN-T loss) and minimum word error rate (MWER) loss. We conduct experiments on
telephony datasets in four languages. Our experiments show that for both
losses, the WER on long-form speech reduces substantially as the training
utterance length increases. The average relative WER gain is 15.7% for log loss
and 8.8% for MWER loss. When training on short utterances, MWER loss leads to a
lower WER than the log loss. Such difference between the two losses diminishes
when the input length increases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine Translation Verbosity Control for Automatic Dubbing. (arXiv:2110.03847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03847">
<div class="article-summary-box-inner">
<span><p>Automatic dubbing aims at seamlessly replacing the speech in a video document
with synthetic speech in a different language. The task implies many
challenges, one of which is generating translations that not only convey the
original content, but also match the duration of the corresponding utterances.
In this paper, we focus on the problem of controlling the verbosity of machine
translation output, so that subsequent steps of our automatic dubbing pipeline
can generate dubs of better quality. We propose new methods to control the
verbosity of MT output and compare them against the state of the art with both
intrinsic and extrinsic evaluations. For our experiments we use a public data
set to dub English speeches into French, Italian, German and Spanish. Finally,
we report extensive subjective tests that measure the impact of MT verbosity
control on the final quality of dubbed video clips.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speeding up Deep Model Training by Sharing Weights and Then Unsharing. (arXiv:2110.03848v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03848">
<div class="article-summary-box-inner">
<span><p>We propose a simple and efficient approach for training the BERT model. Our
approach exploits the special structure of BERT that contains a stack of
repeated modules (i.e., transformer encoders). Our proposed approach first
trains BERT with the weights shared across all the repeated modules till some
point. This is for learning the commonly shared component of weights across all
repeated layers. We then stop weight sharing and continue training until
convergence. We present theoretic insights for training by sharing weights then
unsharing with analysis for simplified models. Empirical experiments on the
BERT model show that our method yields better performance of trained models,
and significantly reduces the number of training iterations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A study on the efficacy of model pre-training in developing neural text-to-speech system. (arXiv:2110.03857v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03857">
<div class="article-summary-box-inner">
<span><p>In the development of neural text-to-speech systems, model pre-training with
a large amount of non-target speakers' data is a common approach. However, in
terms of ultimately achieved system performance for target speaker(s), the
actual benefits of model pre-training are uncertain and unstable, depending
very much on the quantity and text content of training data. This study aims to
understand better why and how model pre-training can positively contribute to
TTS system performance. It is postulated that the pre-training process plays a
critical role in learning text-related variation in speech, while further
training with the target speaker's data aims to capture the speaker-related
variation. Different test sets are created with varying degrees of similarity
to target speaker data in terms of text content. Experiments show that
leveraging a speaker-independent TTS trained on speech data with diverse text
content can improve the target speaker TTS on domain-mismatched text. We also
attempt to reduce the amount of pre-training data for a new text domain and
improve the data and computational efficiency. It is found that the TTS system
could achieve comparable performance when the pre-training data is reduced to
1/8 of its original size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks. (arXiv:2110.03861v1 [quant-ph])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03861">
<div class="article-summary-box-inner">
<span><p>The advent of noisy intermediate-scale quantum (NISQ) computers raises a
crucial challenge to design quantum neural networks for fully quantum learning
tasks. To bridge the gap, this work proposes an end-to-end learning framework
named QTN-VQC, by introducing a trainable quantum tensor network (QTN) for
quantum embedding on a variational quantum circuit (VQC). The architecture of
QTN is composed of a parametric tensor-train network for feature extraction and
a tensor product encoding for quantum encoding. We highlight the QTN for
quantum embedding in terms of two perspectives: (1) we theoretically
characterize QTN by analyzing its representation power of input features; (2)
QTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the
generation of quantum embedding to the output measurement. Our experiments on
the MNIST dataset demonstrate the advantages of QTN for quantum embedding over
other quantum embedding approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Cross-Lingual Transfer of Structured Predictors without Source Data. (arXiv:2110.03866v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03866">
<div class="article-summary-box-inner">
<span><p>Providing technologies to communities or domains where training data is
scarce or protected e.g., for privacy reasons, is becoming increasingly
important. To that end, we generalise methods for unsupervised transfer from
multiple input models for structured prediction. We show that the means of
aggregating over the input models is critical, and that multiplying marginal
probabilities of substructures to obtain high-probability structures for
distant supervision is substantially better than taking the union of such
structures over the input models, as done in prior work. Testing on 18
languages, we demonstrate that the method works in a cross-lingual setting,
considering both dependency parsing and part-of-speech structured prediction
problems. Our analyses show that the proposed method produces less noisy labels
for the distant supervision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation of professions in entertainment media: Insights into frequency and sentiment trends through computational text analysis. (arXiv:2110.03873v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03873">
<div class="article-summary-box-inner">
<span><p>Societal ideas and trends dictate media narratives and cinematic depictions
which in turn influences people's beliefs and perceptions of the real world.
Media portrayal of culture, education, government, religion, and family affect
their function and evolution over time as people interpret and perceive these
representations and incorporate them into their beliefs and actions. It is
important to study media depictions of these social structures so that they do
not propagate or reinforce negative stereotypes, or discriminate against any
demographic section. In this work, we examine media representation of
professions and provide computational insights into their incidence, and
sentiment expressed, in entertainment media content. We create a searchable
taxonomy of professional groups and titles to facilitate their retrieval from
speaker-agnostic text passages like movie and television (TV) show subtitles.
We leverage this taxonomy and relevant natural language processing (NLP) models
to create a corpus of professional mentions in media content, spanning more
than 136,000 IMDb titles over seven decades (1950-2017). We analyze the
frequency and sentiment trends of different occupations, study the effect of
media attributes like genre, country of production, and title type on these
trends, and investigate if the incidence of professions in media subtitles
correlate with their real-world employment statistics. We observe increased
media mentions of STEM, arts, sports, and entertainment occupations in the
analyzed subtitles, and a decreased frequency of manual labor jobs and military
occupations. The sentiment expressed toward lawyers, police, and doctors is
becoming negative over time, whereas astronauts, musicians, singers, and
engineers are mentioned favorably. Professions that employ more people have
increased media frequency, supporting our hypothesis that media acts as a
mirror to society.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phone-to-audio alignment without text: A Semi-supervised Approach. (arXiv:2110.03876v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03876">
<div class="article-summary-box-inner">
<span><p>The task of phone-to-audio alignment has many applications in speech
research. Here we introduce two Wav2Vec2-based models for both text-dependent
and text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a
semi-supervised model, directly learns phone-to-audio alignment through
contrastive learning and a forward sum loss, and can be coupled with a
pretrained phone recognizer to achieve text-independent alignment. The other
model, Wav2Vec2-FC, is a frame classification model trained on forced aligned
labels that can both perform forced alignment and text-independent
segmentation. Evaluation results suggest that both proposed methods, even when
transcriptions are not available, generate highly close results to existing
forced alignment tools. Our work presents a neural pipeline of fully automated
phone-to-audio alignment. Code and pretrained models are available at
https://github.com/lingjzhu/charsiu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining the Attention Mechanism of End-to-End Speech Recognition Using Decision Trees. (arXiv:2110.03879v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03879">
<div class="article-summary-box-inner">
<span><p>The attention mechanism has largely improved the performance of end-to-end
speech recognition systems. However, the underlying behaviours of attention is
not yet clearer. In this study, we use decision trees to explain how the
attention mechanism impact itself in speech recognition. The results indicate
that attention levels are largely impacted by their previous states rather than
the encoder and decoder patterns. Additionally, the default attention mechanism
seems to put more weights on closer states, but behaves poorly on modelling
long-term dependencies of attention states.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. (arXiv:2110.03888v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03888">
<div class="article-summary-box-inner">
<span><p>Recent expeditious developments in deep learning algorithms, distributed
training, and even hardware design for large models have enabled training
extreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of
billions or even trillions of parameters. However, under limited resources,
extreme-scale model training that requires enormous amounts of computes and
memory footprint suffers from frustratingly low efficiency in model
convergence. In this paper, we propose a simple training strategy called
"Pseudo-to-Real" for high-memory-footprint-required large models.
Pseudo-to-Real is compatible with large models with architecture of sequential
layers. We demonstrate a practice of pretraining unprecedented
10-trillion-parameter model, an order of magnitude larger than the
state-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the
application of Pseudo-to-Real, we also provide a technique, Granular CPU
offloading, to manage CPU memory for training large model and maintain high GPU
utilities. Fast training of extreme-scale models on a decent amount of
resources can bring much smaller carbon footprint and contribute to greener AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer Assessments. (arXiv:2110.03895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03895">
<div class="article-summary-box-inner">
<span><p>Peer assessment has been widely applied across diverse academic fields over
the last few decades and has demonstrated its effectiveness. However, the
advantages of peer assessment can only be achieved with high-quality peer
reviews. Previous studies have found that high-quality review comments usually
comprise several features (e.g., contain suggestions, mention problems, use a
positive tone). Thus, researchers have attempted to evaluate peer-review
comments by detecting different features using various machine learning and
deep learning models. However, there is no single study that investigates using
a multi-task learning (MTL) model to detect multiple features simultaneously.
This paper presents two MTL models for evaluating peer-review comments by
leveraging the state-of-the-art pre-trained language representation models BERT
and DistilBERT. Our results demonstrate that BERT-based models significantly
outperform previous GloVe-based methods by around 6% in F1-score on tasks of
detecting a single feature, and MTL further improves performance while reducing
model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CheerBots: Chatbots toward Empathy and Emotionusing Reinforcement Learning. (arXiv:2110.03949v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03949">
<div class="article-summary-box-inner">
<span><p>Apart from the coherence and fluency of responses, an empathetic chatbot
emphasizes more on people's feelings. By considering altruistic behaviors
between human interaction, empathetic chatbots enable people to get a better
interactive and supportive experience. This study presents a framework whereby
several empathetic chatbots are based on understanding users' implied feelings
and replying empathetically for multiple dialogue turns. We call these chatbots
CheerBots. CheerBots can be retrieval-based or generative-based and were
finetuned by deep reinforcement learning. To respond in an empathetic way, we
develop a simulating agent, a Conceptual Human Model, as aids for CheerBots in
training with considerations on changes in user's emotional states in the
future to arouse sympathy. Finally, automatic metrics and human rating results
demonstrate that CheerBots outperform other baseline chatbots and achieves
reciprocal altruism. The code and the pre-trained models will be made
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perceived and Intended Sarcasm Detection with Graph Attention Networks. (arXiv:2110.04001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04001">
<div class="article-summary-box-inner">
<span><p>Existing sarcasm detection systems focus on exploiting linguistic markers,
context, or user-level priors. However, social studies suggest that the
relationship between the author and the audience can be equally relevant for
the sarcasm usage and interpretation. In this work, we propose a framework
jointly leveraging (1) a user context from their historical tweets together
with (2) the social information from a user's conversational neighborhood in an
interaction graph, to contextualize the interpretation of the post. We use
graph attention networks (GAT) over users and tweets in a conversation thread,
combined with dense user history representations. Apart from achieving
state-of-the-art results on the recently published dataset of 19k Twitter users
with 30K labeled tweets, adding 10M unlabeled tweets as context, our results
indicate that the model contributes to interpreting the sarcastic intentions of
an author more than to predicting the sarcasm perception by others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Math-Aware Automated Classification and Similarity Search of Scientific Publications: Methods of Mathematical Content Representations. (arXiv:2110.04040v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04040">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate mathematical content representations suitable
for the automated classification of and the similarity search in STEM documents
using standard machine learning algorithms: the Latent Dirichlet Allocation
(LDA) and the Latent Semantic Indexing (LSI). The methods are evaluated on a
subset of arXiv.org papers with the Mathematics Subject Classification (MSC) as
a reference classification and using the standard precision/recall/F1-measure
metrics. The results give insight into how different math representations may
influence the performance of the classification and similarity search tasks in
STEM repositories. Non-surprisingly, machine learning methods are able to grab
distributional semantics from textual tokens. A proper selection of weighted
tokens representing math may improve the quality of the results slightly. A
structured math representation that imitates successful text-processing
techniques with math is shown to yield better results than flat TeX tokens.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Do Things without Words: Modeling Semantic Drift of Emoji. (arXiv:2110.04093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04093">
<div class="article-summary-box-inner">
<span><p>Emoji have become a significant part of our informal textual communication.
Previous work addressing the societal and linguistic functions of emoji
overlook the evolving meaning of the symbol. This evolution could be addressed
through the framework of semantic drifts. In this paper we model and analyze
the semantic drift of emoji and discuss the features that may be contributing
to the drift, some are unique to emoji and some are more general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Conditional End-to-End ASR with CTC and Multi-Granular Subword Units. (arXiv:2110.04109v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04109">
<div class="article-summary-box-inner">
<span><p>In end-to-end automatic speech recognition (ASR), a model is expected to
implicitly learn representations suitable for recognizing a word-level
sequence. However, the huge abstraction gap between input acoustic signals and
output linguistic tokens makes it challenging for a model to learn the
representations. In this work, to promote the word-level representation
learning in end-to-end ASR, we propose a hierarchical conditional model that is
based on connectionist temporal classification (CTC). Our model is trained by
auxiliary CTC losses applied to intermediate layers, where the vocabulary size
of each target subword sequence is gradually increased as the layer becomes
close to the word-level output. Here, we make each level of sequence prediction
explicitly conditioned on the previous sequences predicted at lower levels.
With the proposed approach, we expect the proposed model to learn the
word-level representations effectively by exploiting a hierarchy of linguistic
structures. Experimental results on LibriSpeech-{100h, 960h} and TEDLIUM2
demonstrate that the proposed model improves over a standard CTC-based model
and other competitive models from prior work. We further analyze the results to
confirm the effectiveness of the intended representation learning with our
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I Do Not Understand What I Cannot Define: Automatic Question Generation With Pedagogically-Driven Content Selection. (arXiv:2110.04123v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04123">
<div class="article-summary-box-inner">
<span><p>Most learners fail to develop deep text comprehension when reading textbooks
passively. Posing questions about what learners have read is a well-established
way of fostering their text comprehension. However, many textbooks lack
self-assessment questions because authoring them is timeconsuming and
expensive. Automatic question generators may alleviate this scarcity by
generating sound pedagogical questions. However, generating questions
automatically poses linguistic and pedagogical challenges. What should we ask?
And, how do we phrase the question automatically? We address those challenges
with an automatic question generator grounded in learning theory. The paper
introduces a novel pedagogically meaningful content selection mechanism to find
question-worthy sentences and answers in arbitrary textbook contents. We
conducted an empirical evaluation study with educational experts, annotating
150 generated questions in six different domains. Results indicate a high
linguistic quality of the generated questions. Furthermore, the evaluation
results imply that the majority of the generated questions inquire central
information related to the given text and may foster text comprehension in
specific learning scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text analysis and deep learning: A network approach. (arXiv:2110.04151v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04151">
<div class="article-summary-box-inner">
<span><p>Much information available to applied researchers is contained within written
language or spoken text. Deep language models such as BERT have achieved
unprecedented success in many applications of computational linguistics.
However, much less is known about how these models can be used to analyze
existing text. We propose a novel method that combines transformer models with
network analysis to form a self-referential representation of language use
within a corpus of interest. Our approach produces linguistic relations
strongly consistent with the underlying model as well as mathematically
well-defined operations on them, while reducing the amount of discretionary
choices of representation and distance measures. It represents, to the best of
our knowledge, the first unsupervised method to extract semantic networks
directly from deep language models. We illustrate our approach in a semantic
analysis of the term "founder". Using the entire corpus of Harvard Business
Review from 1980 to 2020, we find that ties in our network track the semantics
of discourse over time, and across contexts, identifying and relating clusters
of semantic and syntactic relations. Finally, we discuss how this method can
also complement and inform analyses of the behavior of deep learning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Iterative Decoding for Compositional Generalization in Transformers. (arXiv:2110.04169v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04169">
<div class="article-summary-box-inner">
<span><p>Deep learning models do well at generalizing to in-distribution data but
struggle to generalize compositionally, i.e., to combine a set of learned
primitives to solve more complex tasks. In particular, in sequence-to-sequence
(seq2seq) learning, transformers are often unable to predict correct outputs
for even marginally longer examples than those seen during training. This paper
introduces iterative decoding, an alternative to seq2seq learning that (i)
improves transformer compositional generalization and (ii) evidences that, in
general, seq2seq transformers do not learn iterations that are not unrolled.
Inspired by the idea of compositionality -- that complex tasks can be solved by
composing basic primitives -- training examples are broken down into a sequence
of intermediate steps that the transformer then learns iteratively. At
inference time, the intermediate outputs are fed back to the transformer as
intermediate inputs until an end-of-iteration token is predicted. Through
numerical experiments, we show that transfomers trained via iterative decoding
outperform their seq2seq counterparts on the PCFG dataset, and solve the
problem of calculating Cartesian products between vectors longer than those
seen during training with 100% accuracy, a task at which seq2seq models have
been shown to fail. We also illustrate a limitation of iterative decoding,
specifically, that it can make sorting harder to learn on the CFQ dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Development of an Extractive Title Generation System Using Titles of Papers of Top Conferences for Intermediate English Students. (arXiv:2110.04204v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04204">
<div class="article-summary-box-inner">
<span><p>The formulation of good academic paper titles in English is challenging for
intermediate English authors (particularly students). This is because such
authors are not aware of the type of titles that are generally in use. We aim
to realize a support system for formulating more effective English titles for
intermediate English and beginner authors. This study develops an extractive
title generation system that formulates titles from keywords extracted from an
abstract. Moreover, we realize a title evaluation model that can evaluate the
appropriateness of paper titles. We train the model with titles of
top-conference papers by using BERT. This paper describes the training data,
implementation, and experimental results. The results show that our evaluation
model can identify top-conference titles more effectively than intermediate
English and beginner students.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive String Representation Learning using Synthetic Data. (arXiv:2110.04217v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04217">
<div class="article-summary-box-inner">
<span><p>String representation Learning (SRL) is an important task in the field of
Natural Language Processing, but it remains under-explored. The goal of SRL is
to learn dense and low-dimensional vectors (or embeddings) for encoding
character sequences. The learned representation from this task can be used in
many downstream application tasks such as string similarity matching or lexical
normalization. In this paper, we propose a new method for to train a SRL model
by only using synthetic data. Our approach makes use of Contrastive Learning in
order to maximize similarity between related strings while minimizing it for
unrelated strings. We demonstrate the effectiveness of our approach by
evaluating the learned representation on the task of string similarity
matching. Codes, data and pretrained models will be made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">lambeq: An Efficient High-Level Python Library for Quantum NLP. (arXiv:2110.04236v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04236">
<div class="article-summary-box-inner">
<span><p>We present lambeq, the first high-level Python library for Quantum Natural
Language Processing (QNLP). The open-source toolkit offers a detailed hierarchy
of modules and classes implementing all stages of a pipeline for converting
sentences to string diagrams, tensor networks, and quantum circuits ready to be
used on a quantum computer. lambeq supports syntactic parsing, rewriting and
simplification of string diagrams, ansatz creation and manipulation, as well as
a number of compositional models for preparing quantum-friendly representations
of sentences, employing various degrees of syntax sensitivity. We present the
generic architecture and describe the most important modules in detail,
demonstrating the usage with illustrative examples. Further, we test the
toolkit in practice by using it to perform a number of experiments on simple
NLP tasks, implementing both classical and quantum pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VieSum: How Robust Are Transformer-based Models on Vietnamese Summarization?. (arXiv:2110.04257v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04257">
<div class="article-summary-box-inner">
<span><p>Text summarization is a challenging task within natural language processing
that involves text generation from lengthy input sequences. While this task has
been widely studied in English, there is very limited research on summarization
for Vietnamese text. In this paper, we investigate the robustness of
transformer-based encoder-decoder architectures for Vietnamese abstractive
summarization. Leveraging transfer learning and self-supervised learning, we
validate the performance of the methods on two Vietnamese datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04260">
<div class="article-summary-box-inner">
<span><p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can
easily scale to have outrageously large amounts of parameters without
significant increase in computational cost. However, SAMs are reported to be
parameter inefficient such that larger models do not always lead to better
performance. While most on-going research focuses on improving SAMs models by
exploring methods of routing inputs to experts, our analysis reveals that such
research might not lead to the solution we expect, i.e., the commonly-used
routing methods based on gating mechanisms do not work better than randomly
routing inputs to experts. In this paper, we propose a new expert-based model,
THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,
such as the Switch Transformer, experts in THOR are randomly activated for each
input during training and inference. THOR models are trained using a
consistency regularized loss, where experts learn not only from training data
but also from other experts as teachers, such that all the experts make
consistent predictions. We validate the effectiveness of THOR on machine
translation tasks. Results show that THOR models are more parameter efficient
in that they significantly outperform the Transformer and MoE models across
various settings. For example, in multilingual translation, THOR outperforms
the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as
that of a state-of-the-art MoE model that is 18 times larger. Our code is
publicly available at: github.com/microsoft/Stochastic-Mixture-of-Experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Heterogeneous Characteristics of Layers in ASR Models for More Efficient Training. (arXiv:2110.04267v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04267">
<div class="article-summary-box-inner">
<span><p>Transformer-based architectures have been the subject of research aimed at
understanding their overparameterization and the non-uniform importance of
their layers. Applying these approaches to Automatic Speech Recognition, we
demonstrate that the state-of-the-art Conformer models generally have multiple
ambient layers. We study the stability of these layers across runs and model
sizes, propose that group normalization may be used without disrupting their
formation, and examine their correlation with model weight updates in each
layer. Finally, we apply these findings to Federated Learning in order to
improve the training procedure, by targeting Federated Dropout to layers by
importance. This allows us to reduce the model size optimized by clients
without quality degradation, and shows potential for future exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Local and Global Context-Based Pairwise Models for Sentence Ordering. (arXiv:2110.04291v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04291">
<div class="article-summary-box-inner">
<span><p>Sentence Ordering refers to the task of rearranging a set of sentences into
the appropriate coherent order. For this task, most previous approaches have
explored global context-based end-to-end methods using Sequence Generation
techniques. In this paper, we put forward a set of robust local and global
context-based pairwise ordering strategies, leveraging which our prediction
strategies outperform all previous works in this domain. Our proposed encoding
method utilizes the paragraph's rich global contextual information to predict
the pairwise order using novel transformer architectures. Analysis of the two
proposed decoding strategies helps better explain error propagation in pairwise
models. This approach is the most accurate pure pairwise model and our encoding
strategy also significantly improves the performance of other recent approaches
that use pairwise models, including the previous state-of-the-art,
demonstrating the research novelty and generalizability of this work.
Additionally, we show how the pre-training task for ALBERT helps it to
significantly outperform BERT, despite having considerably lesser parameters.
The extensive experimental results, architectural analysis and ablation studies
demonstrate the effectiveness and superiority of the proposed models compared
to the previous state-of-the-art, besides providing a much better understanding
of the functioning of pairwise models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From SCAN to Real Data: Systematic Generalization via Meaningful Learning. (arXiv:2003.06658v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.06658">
<div class="article-summary-box-inner">
<span><p>Humans can systematically generalize to novel compositions of existing
concepts. There have been extensive conjectures into the extent to which neural
networks can do the same. Recent arguments supported by evidence on the SCAN
dataset claim that neural networks are inherently ineffective in such cognitive
capacity. In this paper, we revisit systematic generalization from the
perspective of meaningful learning, an exceptional capability of humans to
learn new concepts by connecting them with other previously known knowledge. We
propose to augment a training dataset in either an inductive or deductive
manner to build semantic links between new and old concepts. Our observations
on SCAN suggest that, following the meaningful learning principle, modern
sequence-to-sequence models, including RNNs, CNNs, and Transformers, can
successfully generalize to compositions of new concepts. We further validate
our findings on two real-world datasets on semantic parsing and consistent
compositional generalization is also observed. Moreover, our experiments
demonstrate that both prior knowledge and semantic linking play a key role to
achieve systematic generalization. Meanwhile, inductive learning generally
works better than deductive learning in our experiments. Finally, we provide an
explanation for data augmentation techniques by concluding them into either
inductive-based or deductive-based meaningful learning. We hope our findings
will encourage excavating existing neural networks' potential in systematic
generalization through more advanced learning schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">English Machine Reading Comprehension Datasets: A Survey. (arXiv:2101.10421v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10421">
<div class="article-summary-box-inner">
<span><p>This paper surveys 60 English Machine Reading Comprehension datasets, with a
view to providing a convenient resource for other researchers interested in
this problem. We categorize the datasets according to their question and answer
form and compare them across various dimensions including size, vocabulary,
data source, method of creation, human performance level, and first question
word. Our analysis reveals that Wikipedia is by far the most common data source
and that there is a relative lack of why, when, and where questions across
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention. (arXiv:2103.15722v4 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15722">
<div class="article-summary-box-inner">
<span><p>Self-attention (SA), which encodes vector sequences according to their
pairwise similarity, is widely used in speech recognition due to its strong
context modeling ability. However, when applied to long sequence data, its
accuracy is reduced. This is caused by the fact that its weighted average
operator may lead to the dispersion of the attention distribution, which
results in the relationship between adjacent signals ignored. To address this
issue, in this paper, we introduce relative-position-awareness self-attention
(RPSA). It not only maintains the global-range dependency modeling ability of
self-attention, but also improves the localness modeling ability. Because the
local window length of the original RPSA is fixed and sensitive to different
test data, here we propose Gaussian-based self-attention (GSA) whose window
length is learnable and adaptive to the test data automatically. We further
generalize GSA to a new residual Gaussian self-attention (resGSA) for the
performance improvement. We apply RPSA, GSA, and resGSA to Transformer-based
speech recognition respectively. Experimental results on the AISHELL-1 Mandarin
speech recognition corpus demonstrate the effectiveness of the proposed
methods. For example, the resGSA-Transformer achieves a character error rate
(CER) of 5.86% on the test set, which is relative 7.8% lower than that of the
SA-Transformer. Although the performance of the proposed resGSA-Transformer is
only slightly better than that of the RPSA-Transformer, it does not have to
tune the window length manually.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relaxing the Conditional Independence Assumption of CTC-based ASR by Conditioning on Intermediate Predictions. (arXiv:2104.02724v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02724">
<div class="article-summary-box-inner">
<span><p>This paper proposes a method to relax the conditional independence assumption
of connectionist temporal classification (CTC)-based automatic speech
recognition (ASR) models. We train a CTC-based ASR model with auxiliary CTC
losses in intermediate layers in addition to the original CTC loss in the last
layer. During both training and inference, each generated prediction in the
intermediate layers is summed to the input of the next layer to condition the
prediction of the last layer on those intermediate predictions. Our method is
easy to implement and retains the merits of CTC-based ASR: a simple model
architecture and fast decoding speed. We conduct experiments on three different
ASR corpora. Our proposed method improves a standard CTC model significantly
(e.g., more than 20 % relative word error rate reduction on the WSJ corpus)
with a little computational overhead. Moreover, for the TEDLIUM2 corpus and the
AISHELL-1 corpus, it achieves a comparable performance to a strong
autoregressive model with beam search, but the decoding speed is at least 30
times faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Weakly Supervised Dataset of Fine-Grained Emotions in Portuguese. (arXiv:2108.07638v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07638">
<div class="article-summary-box-inner">
<span><p>Affective Computing is the study of how computers can recognize, interpret
and simulate human affects. Sentiment Analysis is a common task inNLP related
to this topic, but it focuses only on emotion valence (positive, negative,
neutral). An emerging approach in NLP is Emotion Recognition, which relies on
fined-grained classification. This research describes an approach to create a
lexical-based weakly supervised corpus for fine-grained emotion in Portuguese.
We evaluated our dataset by fine-tuning a transformer-based language model
(BERT) and validating it on a Gold Standard annotated validation set. Our
results (F1-score=.64) suggest lexical-based weak supervision as an appropriate
strategy for initial work in low resourced environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04620">
<div class="article-summary-box-inner">
<span><p>Current approaches to incorporating terminology constraints in machine
translation (MT) typically assume that the constraint terms are provided in
their correct morphological forms. This limits their application to real-world
scenarios where constraint terms are provided as lemmas. In this paper, we
introduce a modular framework for incorporating lemma constraints in neural MT
(NMT) in which linguistic knowledge and diverse types of NMT models can be
flexibly applied. It is based on a novel cross-lingual inflection module that
inflects the target lemma constraints based on the source context. We explore
linguistically motivated rule-based and data-driven neural-based inflection
modules and design English-German health and English-Lithuanian news test
suites to evaluate them in domain adaptation and low-resource MT settings.
Results show that our rule-based inflection module helps NMT models incorporate
lemma constraints more accurately than a neural module and outperforms the
existing end-to-end approach with lower training costs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. (arXiv:2109.05729v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05729">
<div class="article-summary-box-inner">
<span><p>In this paper, we take the advantage of previous pre-trained models (PTMs)
and propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different
from previous Chinese PTMs, CPT is designed for both natural language
understanding (NLU) and natural language generation (NLG) tasks. CPT consists
of three parts: a shared encoder, an understanding decoder, and a generation
decoder. Two specific decoders with a shared encoder are pre-trained with
masked language modeling (MLM) and denoising auto-encoding (DAE) tasks,
respectively. With the partially shared architecture and multi-task
pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks
with two decoders and (2) be fine-tuned flexibly that fully exploits the
potential of the model. Moreover, the unbalanced Transformer saves the
computational and storage cost, which makes CPT competitive and greatly
accelerates the inference of text generation. Experimental results on a wide
range of Chinese NLU and NLG tasks show the effectiveness of CPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Critic: Language Models for Unsupervised Grammatical Error Correction. (arXiv:2109.06822v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06822">
<div class="article-summary-box-inner">
<span><p>Training a model for grammatical error correction (GEC) requires a set of
labeled ungrammatical / grammatical sentence pairs, but manually annotating
such pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has
demonstrated strong results on learning to repair a broken program without any
labeled examples, but this relies on a perfect critic (e.g., a compiler) that
returns whether an example is valid or not, which does not exist for the GEC
task. In this work, we show how to leverage a pretrained language model (LM) in
defining an LM-Critic, which judges a sentence to be grammatical if the LM
assigns it a higher probability than its local perturbations. We apply this
LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap
realistic ungrammatical / grammatical pairs for training a corrector. We
evaluate our approach on GEC datasets across multiple domains (CoNLL-2014,
BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing
methods in both the unsupervised setting (+7.7 F0.5) and the supervised setting
(+0.5 F0.5).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11797">
<div class="article-summary-box-inner">
<span><p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising
capabilities in grounding natural language in image data, facilitating a broad
variety of cross-modal tasks. However, we note that there exists a significant
gap between the objective forms of model pre-training and fine-tuning,
resulting in a need for large amounts of labeled data to stimulate the visual
grounding capability of VL-PTMs for downstream tasks. To address the challenge,
we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt
Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual
grounding into a fill-in-the-blank problem with color-based co-referential
markers in image and text, maximally mitigating the gap. In this way, CPT
enables strong few-shot and even zero-shot visual grounding capabilities of
VL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs
outperform their fine-tuned counterparts by a large margin (e.g., 17.3%
absolute accuracy improvement, and 73.8% relative standard deviation reduction
on average with one shot in RefCOCO evaluation). All the data and codes will be
available to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FoodChem: A food-chemical relation extraction model. (arXiv:2110.02019v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02019">
<div class="article-summary-box-inner">
<span><p>In this paper, we present FoodChem, a new Relation Extraction (RE) model for
identifying chemicals present in the composition of food entities, based on
textual information provided in biomedical peer-reviewed scientific literature.
The RE task is treated as a binary classification problem, aimed at identifying
whether the contains relation exists between a food-chemical entity pair. This
is accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models.
For evaluation purposes, a novel dataset with annotated contains relations in
food-chemical entity pairs is generated, in a golden and silver version. The
models are integrated into a voting scheme in order to produce the silver
version of the dataset which we use for augmenting the individual models, while
the manually annotated golden version is used for their evaluation. Out of the
three evaluated models, the BioBERT model achieves the best results, with a
macro averaged F1 score of 0.902 in the unbalanced augmentation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Conditional Text Generation for Aspect-Based Sentiment Analysis. (arXiv:2110.02334v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02334">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) is an NLP task that entails processing
user-generated reviews to determine (i) the target being evaluated, (ii) the
aspect category to which it belongs, and (iii) the sentiment expressed towards
the target and aspect pair. In this article, we propose transforming ABSA into
an abstract summary-like conditional text generation task that uses targets,
aspects, and polarities to generate auxiliary statements. To demonstrate the
efficacy of our task formulation and a proposed system, we fine-tune a
pre-trained model for conditional text generation tasks to get new
state-of-the-art results on a few restaurant domains and urban neighborhoods
domain benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cut the CARP: Fishing for zero-shot story evaluation. (arXiv:2110.03111v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03111">
<div class="article-summary-box-inner">
<span><p>Recent advances in large-scale language models (Raffel et al., 2019; Brown et
al., 2020) have brought significant qualitative and quantitative improvements
in machine-driven text generation. Despite this, generation and evaluation of
machine-generated narrative text remains a challenging problem. Objective
evaluation of computationally-generated stories may be prohibitively expensive,
require meticulously annotated datasets, or may not adequately measure the
logical coherence of a generated story's narratological structure.
</p>
<p>Informed by recent advances in contrastive learning (Radford et al., 2021),
we present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,
efficient method for performing qualitatively superior, zero-shot evaluation of
stories. We show a strong correlation between human evaluation of stories and
those of CARP. Model outputs more significantly correlate with corresponding
human input than those language-model based methods which utilize finetuning or
prompt engineering approaches. We also present and analyze the Story-Critique
Dataset, a new corpora composed of 1.3 million aligned story-critique pairs
derived from over 80,000 stories. We expect this corpus to be of interest to
NLP researchers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Continual Knowledge Learning of Language Models. (arXiv:2110.03215v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03215">
<div class="article-summary-box-inner">
<span><p>Large Language Models (LMs) are known to encode world knowledge in their
parameters as they pretrain on a vast amount of web corpus, which is often
utilized for performing knowledge-dependent downstream tasks such as question
answering, fact-checking, and open dialogue. In real-world scenarios, the world
knowledge stored in the LMs can quickly become outdated as the world changes,
but it is non-trivial to avoid catastrophic forgetting and reliably acquire new
knowledge while preserving invariant knowledge. To push the community towards
better maintenance of ever-changing LMs, we formulate a new continual learning
(CL) problem called Continual Knowledge Learning (CKL). We construct a new
benchmark and metric to quantify the retention of time-invariant world
knowledge, the update of outdated knowledge, and the acquisition of new
knowledge. We adopt applicable recent methods from literature to create several
strong baselines. Through extensive experiments, we find that CKL exhibits
unique challenges that are not addressed in previous CL setups, where parameter
expansion is necessary to reliably retain and learn knowledge simultaneously.
By highlighting the critical causes of knowledge forgetting, we show that CKL
is a challenging and important problem that helps us better understand and
train ever-changing LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Logic-Based Framework for Natural Language Inference in Dutch. (arXiv:2110.03323v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03323">
<div class="article-summary-box-inner">
<span><p>We present a framework for deriving inference relations between Dutch
sentence pairs. The proposed framework relies on logic-based reasoning to
produce inspectable proofs leading up to inference labels; its judgements are
therefore transparent and formally verifiable. At its core, the system is
powered by two ${\lambda}$-calculi, used as syntactic and semantic theories,
respectively. Sentences are first converted to syntactic proofs and terms of
the linear ${\lambda}$-calculus using a choice of two parsers: an Alpino-based
pipeline, and Neural Proof Nets. The syntactic terms are then converted to
semantic terms of the simply typed ${\lambda}$-calculus, via a set of hand
designed type- and term-level transformations. Pairs of semantic terms are then
fed to an automated theorem prover for natural logic which reasons with them
while using lexical relations found in the Open Dutch WordNet. We evaluate the
reasoning pipeline on the recently created Dutch natural language inference
dataset, and achieve promising results, remaining only within a $1.1-3.2{\%}$
performance margin to strong neural baselines. To the best of our knowledge,
the reasoning pipeline is the first logic-based system for Dutch.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03611">
<div class="article-summary-box-inner">
<span><p>Current dense text retrieval models face two typical challenges. First, it
adopts a siamese dual-encoder architecture to encode query and document
independently for fast indexing and searching, whereas neglecting the
finer-grained term-wise interactions. This results in a sub-optimal recall
performance. Second, it highly relies on a negative sampling technique to build
up the negative documents in its contrastive loss. To address these challenges,
we present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder
retriever plus a cross-encoder ranker. The two models are jointly optimized
according to a minimax adversarial objective: the retriever learns to retrieve
negative documents to cheat the ranker, while the ranker learns to rank a
collection of candidates including both the ground-truth and the retrieved
ones, as well as providing progressive direct feedback to the dual-encoder
retriever. Through this adversarial game, the retriever gradually produces
harder negative documents to train a better ranker, whereas the cross-encoder
ranker provides progressive feedback to improve retriever. We evaluate AR2 on
three benchmarks. Experimental results show that AR2 consistently and
significantly outperforms existing dense retriever methods and achieves new
state-of-the-art results on all of them. This includes the improvements on
Natural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and
MS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data
publicly available.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-11 23:09:23.267033008 UTC">2021-10-11 23:09:23 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>