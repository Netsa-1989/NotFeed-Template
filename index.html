<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<link href="https://cdn.lineicons.com/2.0/LineIcons.css" rel="stylesheet">
<section class="header-container">
<div style="display:flex; justify-content:space-between; align-items:center;">
<div>
<a href="https://github.com/NotCraft/NotFeed-Template" style="text-decoration: none;">
<div class="header-title">
<span class="header-title-preffix">NotCraft</span>//NotFeed
</div>
</a>
</div>
<div class=icons>
<label class="theme-switch" for="checkbox">
<input type="checkbox" id="checkbox"/>
<i class="lni lni-32 lni-sun" rel="noopener noreferrer"></i>
</label>
</div>
</div>
</section>
<section class="day-container">
<div class="date">2021-08-11</div>
<article>
<details>
<Summary>cs.CL updates on arXiv.org</Summary>
<div class="details-content">
<p style="text-align:center;">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</p>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ FairyTailor: A Multimodal Generative Framework for Storytelling.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1">Eden Bensaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1">Mauro Martino</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1">Benjamin Hoover</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a>, <a href="http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1">Hendrik Strobelt</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04324">
<div class="article-summary-box-inner">
<span><p>Storytelling is an open-ended task that entails creative thinking and
requires a constant flow of ideas. Natural language generation (NLG) for
storytelling is especially challenging because it requires the generated text
to follow an overall theme while remaining creative and diverse to engage the
reader. In this work, we introduce a system and a web-based demo, FairyTailor,
for human-in-the-loop visual story co-creation. Users can create a cohesive
children's fairytale by weaving generated texts and retrieved images with their
input. FairyTailor adds another modality and modifies the text generation
process to produce a coherent and creative sequence of text and images. To our
knowledge, this is the first dynamic tool for multimodal story generation that
allows interactive co-formation of both texts and images. It allows users to
give feedback on co-created stories and share their results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ COMPARE: A Taxonomy and Dataset of Comparison Discussions in Peer Reviews.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shruti Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Mayank Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04366">
<div class="article-summary-box-inner">
<span><p>Comparing research papers is a conventional method to demonstrate progress in
experimental research. We present COMPARE, a taxonomy and a dataset of
comparison discussions in peer reviews of research papers in the domain of
experimental deep learning. From a thorough observation of a large set of
review sentences, we build a taxonomy of categories in comparison discussions
and present a detailed annotation scheme to analyze this. Overall, we annotate
117 reviews covering 1,800 sentences. We experiment with various methods to
identify comparison sentences in peer reviews and report a maximum F1 Score of
0.49. We also pretrain two language models specifically on ML, NLP, and CV
paper abstracts and reviews to learn informative representations of peer
reviews. The annotated dataset and the pretrained models are available at
https://github.com/shruti-singh/COMPARE .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Making <span class="highlight-title">Transformer</span>s Solve Compositional Tasks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Onta%7Bn%7D%7Bo%7Dn_S/0/1/0/all/0/1">Santiago Onta&#xf1;&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1">Joshua Ainslie</a>, <a href="http://arxiv.org/find/cs/1/au:+Cvicek_V/0/1/0/all/0/1">Vaclav Cvicek</a>, <a href="http://arxiv.org/find/cs/1/au:+Fisher_Z/0/1/0/all/0/1">Zachary Fisher</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04378">
<div class="article-summary-box-inner">
<span><p>Several studies have reported the inability of Transformer models to
generalize compositionally, a key type of generalization in many NLP tasks such
as semantic parsing. In this paper we explore the design space of Transformer
models showing that the inductive biases given to the model by several design
decisions significantly impact compositional generalization. Through this
exploration, we identified Transformer configurations that generalize
compositionally significantly better than previously reported in the literature
in a diverse set of compositional tasks, and that achieve state-of-the-art
results in a semantic parsing compositional generalization benchmark (COGS),
and a string edit operation composition benchmark (PCFG).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> Lifelong Intent Detection via Multi-Strategy Rebalancing.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xiaoyan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shizhu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"><span class="highlight-author">Kang Liu</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jun Zhao</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04445">
<div class="article-summary-box-inner">
<span><p>Conventional Intent Detection (ID) models are usually trained offline, which
relies on a fixed dataset and a predefined set of intent classes. However, in
real-world applications, online systems usually involve continually emerging
new user intents, which pose a great challenge to the offline training
paradigm. Recently, lifelong learning has received increasing attention and is
considered to be the most promising solution to this challenge. In this paper,
we propose Lifelong Intent Detection (LID), which continually trains an ID
model on new data to learn newly emerging intents while avoiding
catastrophically forgetting old data. Nevertheless, we find that existing
lifelong learning methods usually suffer from a serious imbalance between old
and new data in the LID task. Therefore, we propose a novel lifelong learning
method, Multi-Strategy Rebalancing (MSR), which consists of cosine
normalization, hierarchical knowledge distillation, and inter-class margin loss
to alleviate the multiple negative effects of the imbalance problem.
Experimental results demonstrate the effectiveness of our method, which
significantly outperforms previous state-of-the-art lifelong learning methods
on the ATIS, SNIPS, HWU64, and CLINC150 benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ BROS: A Layout-Aware <span class="highlight-title">Pre-train</span>ed Language Model for Understanding Documents.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1">Teakgyu Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1">Donghyun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1">Mingi Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1">Wonseok Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1">Daehyun Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Sungrae Park</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04539">
<div class="article-summary-box-inner">
<span><p>Understanding documents from their visual snapshots is an emerging problem
that requires both advanced computer vision and NLP methods. The recent advance
in OCR enables the accurate recognition of text blocks, yet it is still
challenging to extract key information from documents due to the diversity of
their layouts. Although recent studies on pre-trained language models show the
importance of incorporating layout information on this task, the conjugation of
texts and their layouts still follows the style of BERT optimized for
understanding the 1D text. This implies there is room for further improvement
considering the 2D nature of text layouts. This paper introduces a pre-trained
language model, BERT Relying On Spatiality (BROS), which effectively utilizes
the information included in individual text blocks and their layouts.
Specifically, BROS encodes spatial information by utilizing relative positions
and learns spatial dependencies between OCR blocks with a novel area-masking
strategy. These two novel approaches lead to an efficient encoding of spatial
layout information highlighted by the robust performance of BROS under
low-resource environments. We also introduce a general-purpose parser that can
be combined with BROS to extract key information even when there is no order
information between text blocks. BROS shows its superiority on four public
benchmarks---FUNSD, SROIE*, CORD, and SciTSR---and its robustness in practical
cases where order information of text blocks is not available. Further
experiments with a varying number of training examples demonstrate the high
training efficiency of our approach. Our code will be open to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ CLSE<span class="highlight-title">BERT</span>: <span class="highlight-title">Contrastive Learning</span> for Syntax Enhanced Code Pre-Trained Model.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yasheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pingyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1">Meng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yadao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04556">
<div class="article-summary-box-inner">
<span><p>Pre-trained models for programming languages have proven their significant
values in various code-related tasks, such as code search, code clone
detection, and code translation. Currently, most pre-trained models treat a
code snippet as a sequence of tokens or only focus on the data flow between
code identifiers. However, rich code syntax and hierarchy are ignored which can
provide important structure information and semantic rules of codes to help
enhance code representations. In addition, although the BERT-based code
pre-trained models achieve high performance on many downstream tasks, the
native derived sequence representations of BERT are proven to be of
low-quality, it performs poorly on code matching and similarity tasks. To
address these problems, we propose CLSEBERT, a Constrastive Learning Framework
for Syntax Enhanced Code Pre-Trained Model, to deal with various code
intelligence tasks. In the pre-training stage, we consider the code syntax and
hierarchy contained in the Abstract Syntax Tree (AST) and leverage the
constrastive learning to learn noise-invariant code representations. Besides
the masked language modeling (MLM), we also introduce two novel pre-training
objectives. One is to predict the edges between nodes in the abstract syntax
tree, and the other is to predict the types of code tokens. Through extensive
experiments on four code intelligence tasks, we successfully show the
effectiveness of our proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Hope Speech detection in under-resourced Kannada language.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1">Adeep Hande</a>, <a href="http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1">Ruba Priyadharshini</a>, <a href="http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1">Anbukkarasi Sampath</a>, <a href="http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1">Kingston Pal Thamburaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1">Prabakaran Chandran</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1">Bharathi Raja Chakravarthi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04616">
<div class="article-summary-box-inner">
<span><p>Numerous methods have been developed to monitor the spread of negativity in
modern years by eliminating vulgar, offensive, and fierce comments from social
media platforms. However, there are relatively lesser amounts of study that
converges on embracing positivity, reinforcing supportive and reassuring
content in online forums. Consequently, we propose creating an English-Kannada
Hope speech dataset, KanHope and comparing several experiments to benchmark the
dataset. The dataset consists of 6,176 user-generated comments in code mixed
Kannada scraped from YouTube and manually annotated as bearing hope speech or
Not-hope speech. In addition, we introduce DC-BERT4HOPE, a dual-channel model
that uses the English translation of KanHope for additional training to promote
hope speech detection. The approach achieves a weighted F1-score of 0.756,
bettering other models. Henceforth, KanHope aims to instigate research in
Kannada while broadly promoting researchers to take a pragmatic approach
towards online content that encourages, positive, and supportive.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Differentiable Subset Pruning of <span class="highlight-title">Transformer</span> Heads.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaoda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a>, <a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1">Mrinmaya Sachan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04657">
<div class="article-summary-box-inner">
<span><p>Multi-head attention, a collection of several attention mechanisms that
independently attend to different parts of the input, is the key ingredient in
the Transformer (Vaswaniet al., 2017). Recent work has shown, however, that a
large proportion of the heads in a Transformer's multi-head attention mechanism
can be safely pruned away without significantly harming the performance of the
model; such pruning leads to models that are noticeably smaller and faster in
practice. Our work introduces a new head pruning technique that we term
differentiable subset pruning. Intuitively, our method learns per-head
importance variables and then enforces a user-specified hard constraint on the
number of unpruned heads. The importance variables are learned via stochastic
gradient descent. We conduct experiments on natural language inference and
machine translation; we show that differentiable subset pruning performs
comparably or better than Voita et al. (2019) while offering the same exact
control over the number of heads as Michel et al. (2019).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yubo Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1">Pearl Pu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04674">
<div class="article-summary-box-inner">
<span><p>In this paper, we give an overview of commonsense reasoning in natural
language processing, which requires a deeper understanding of the contexts and
usually involves inference over implicit external knowledge. We first review
some popular commonsense knowledge bases and commonsense reasoning benchmarks,
but give more emphasis on the methodologies, including recent approaches that
aim at solving some general natural language problems that take advantage of
external knowledge bases. Finally, we discuss some future directions in pushing
the boundary of commonsense reasoning in natural language processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Automated Audio Captioning using Transfer Learning and Reconstruction Latent Space Similarity Regularization.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Koh_A/0/1/0/all/0/1">Andrew Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1">Fuzhao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1">Eng Siong Chng</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04692">
<div class="article-summary-box-inner">
<span><p>In this paper, we examine the use of Transfer Learning using Pretrained Audio
Neural Networks (PANNs), and propose an architecture that is able to better
leverage the acoustic features provided by PANNs for the Automated Audio
Captioning Task. We also introduce a novel self-supervised objective,
Reconstruction Latent Space Similarity Regularization (RLSSR). The RLSSR module
supplements the training of the model by minimizing the similarity between the
encoder and decoder embedding. The combination of both methods allows us to
surpass state of the art results by a significant margin on the Clotho dataset
across several metrics and benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Eikema_B/0/1/0/all/0/1">Bryan Eikema</a>, <a href="http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1">Wilker Aziz</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04718">
<div class="article-summary-box-inner">
<span><p>In neural machine translation (NMT), we search for the mode of the model
distribution to form predictions. The mode as well as other high probability
translations found by beam search have been shown to often be inadequate in a
number of ways. This prevents practitioners from improving translation quality
through better search, as these idiosyncratic translations end up being
selected by the decoding algorithm, a problem known as the beam search curse.
Recently, a sampling-based approximation to minimum Bayes risk (MBR) decoding
has been proposed as an alternative decision rule for NMT that would likely not
suffer from the same problems. We analyse this approximation and establish that
it has no equivalent to the beam search curse, i.e. better search always leads
to better translations. We also design different approximations aimed at
decoupling the cost of exploration from the cost of robust estimation of
expected utility. This allows for exploration of much larger hypothesis spaces,
which we show to be beneficial. We also show that it can be beneficial to make
use of strategies like beam search and nucleus sampling to construct hypothesis
spaces efficiently. We show on three language pairs (English into and from
German, Romanian, and Nepali) that MBR can improve upon beam search with
moderate computation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Headed Span-Based Projective Dependency Parsing.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Songlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1">Kewei Tu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04750">
<div class="article-summary-box-inner">
<span><p>We propose a headed span-based method for projective dependency parsing. In a
projective tree, the subtree rooted at each word occurs in a contiguous
sequence (i.e., span) in the surface order, we call the span-headword pair
\textit{headed span}. In this view, a projective tree can be regarded as a
collection of headed spans. It is similar to the case in constituency parsing
since a constituency tree can be regarded as a collection of constituent spans.
Span-based methods decompose the score of a constituency tree sorely into the
score of constituent spans and use the CYK algorithm for global training and
exact inference, obtaining state-of-the-art results in constituency parsing.
Inspired by them, we decompose the score of a dependency tree into the score of
headed spans. We use neural networks to score headed spans and design a novel
$O(n^3)$ dynamic programming algorithm to enable global training and exact
inference. We evaluate our method on PTB, CTB, and UD, achieving
state-of-the-art or comparable results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1">Noriyuki Kojima</a>, <a href="http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1">Alane Suhr</a>, <a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1">Yoav Artzi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04812">
<div class="article-summary-box-inner">
<span><p>We study continual learning for natural language instruction generation, by
observing human users' instruction execution. We focus on a collaborative
scenario, where the system both acts and delegates tasks to human users using
natural language. We compare user execution of generated instructions to the
original system intent as an indication to the system's success communicating
its intent. We show how to use this signal to improve the system's ability to
generate instructions via contextual bandit learning. In interaction with real
users, our system demonstrates dramatic improvements in its ability to generate
language over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1">Amir Karami</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Michael Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1">Bailey Goldschmidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1">Hannah R. Boyajieff</a>, <a href="http://arxiv.org/find/cs/1/au:+Najafabadi_M/0/1/0/all/0/1">Mahdi M. Najafabadi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04816">
<div class="article-summary-box-inner">
<span><p>Public response to COVID-19 vaccines is the key success factor to control the
COVID-19 pandemic. To understand the public response, there is a need to
explore public opinion. Traditional surveys are expensive and time-consuming,
address limited health topics, and obtain small-scale data. Twitter can provide
a great opportunity to understand public opinion regarding COVID-19 vaccines.
The current study proposes an approach using computational and human coding
methods to collect and analyze a large number of tweets to provide a wider
perspective on the COVID-19 vaccine. This study identifies the sentiment of
tweets and their temporal trend, discovers major topics, compares topics of
negative and non-negative tweets, and discloses top topics of negative and
non-negative tweets. Our findings show that the negative sentiment regarding
the COVID-19 vaccine had a decreasing trend between November 2020 and February
2021. We found Twitter users have discussed a wide range of topics from
vaccination sites to the 2020 U.S. election between November 2020 and February
2021. The findings show that there was a significant difference between
negative and non-negative tweets regarding the weight of most topics. Our
results also indicate that the negative and non-negative tweets had different
topic priorities and focuses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Noise Robust Named Entity Understanding for Voice Assistants.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1">Deepak Muralidharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1">Joel Ruben Antony Moniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Sida Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1">Justine Kao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1">Stephen Pulman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1">Atish Kothari</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1">Ray Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yinying Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1">Vivek Kaul</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Mubarak Seyed Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1">Gang Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1">Nan Dun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yidan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1">Andy O</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1">Pooja Chitkara</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Alkesh Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1">Kushal Tayal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Roger Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1">Peter Grasch</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1">Jason D. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lin Li</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.14408">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) and Entity Linking (EL) play an essential role
in voice assistant interaction, but are challenging due to the special
difficulties associated with spoken user queries. In this paper, we propose a
novel architecture that jointly solves the NER and EL tasks by combining them
in a joint reranking module. We show that our proposed framework improves NER
accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features
used also lead to better accuracies in other natural language understanding
tasks, such as domain classification and semantic parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ AMUSED: An Annotation Framework of Multi-modal Social Media Data.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1">Gautam Kishore Shahi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00502">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a semi-automated framework called AMUSED for
gathering multi-modal annotated data from the multiple social media platforms.
The framework is designed to mitigate the issues of collecting and annotating
social media data by cohesively combining machine and human in the data
collection process. From a given list of the articles from professional news
media or blog, AMUSED detects links to the social media posts from news
articles and then downloads contents of the same post from the respective
social media platform to gather details about that specific post. The framework
is capable of fetching the annotated data from multiple platforms like Twitter,
YouTube, Reddit. The framework aims to reduce the workload and problems behind
the data annotation from the social media platforms. AMUSED can be applied in
multiple application domains, as a use case, we have implemented the framework
for collecting COVID-19 misinformation data from different social media
platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yubo Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Junze Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1">Pearl Pu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12007">
<div class="article-summary-box-inner">
<span><p>Humor recognition has been widely studied as a text classification problem
using data-driven approaches. However, most existing work does not examine the
actual joke mechanism to understand humor. We break down any joke into two
distinct components: the set-up and the punchline, and further explore the
special relationship between them. Inspired by the incongruity theory of humor,
we model the set-up as the part developing semantic uncertainty, and the
punchline disrupting audience expectations. With increasingly powerful language
models, we were able to feed the set-up along with the punchline into the GPT-2
language model, and calculate the uncertainty and surprisal values of the
jokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found
that these two features have better capabilities of telling jokes from
non-jokes, compared with existing baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ The Zero Resource Speech Challenge 2021: Spoken language modelling.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1">Ewan Dunbar</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernard_M/0/1/0/all/0/1">Mathieu Bernard</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamilakis_N/0/1/0/all/0/1">Nicolas Hamilakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tu Anh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Seyssel_M/0/1/0/all/0/1">Maureen de Seyssel</a>, <a href="http://arxiv.org/find/cs/1/au:+Roze_P/0/1/0/all/0/1">Patricia Roz&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1">Morgane Rivi&#xe8;re</a>, <a href="http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1">Eugene Kharitonov</a>, <a href="http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1">Emmanuel Dupoux</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14700">
<div class="article-summary-box-inner">
<span><p>We present the Zero Resource Speech Challenge 2021, which asks participants
to learn a language model directly from audio, without any text or labels. The
challenge is based on the Libri-light dataset, which provides up to 60k hours
of audio from English audio books without any associated text. We provide a
pipeline baseline system consisting on an encoder based on contrastive
predictive coding (CPC), a quantizer ($k$-means) and a standard language model
(BERT or LSTM). The metrics evaluate the learned representations at the
acoustic (ABX discrimination), lexical (spot-the-word), syntactic
(acceptability judgment) and semantic levels (similarity judgment). We present
an overview of the eight submitted systems from four groups and discuss the
main results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ It's not what you said, it's how you said it: discriminative perception of speech as a multichannel communication system.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wallbridge_S/0/1/0/all/0/1">Sarenne Wallbridge</a>, <a href="http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1">Peter Bell</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1">Catherine Lai</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00260">
<div class="article-summary-box-inner">
<span><p>People convey information extremely effectively through spoken interaction
using multiple channels of information transmission: the lexical channel of
what is said, and the non-lexical channel of how it is said. We propose
studying human perception of spoken communication as a means to better
understand how information is encoded across these channels, focusing on the
question 'What characteristics of communicative context affect listener's
expectations of speech?'. To investigate this, we present a novel behavioural
task testing whether listeners can discriminate between the true utterance in a
dialogue and utterances sampled from other contexts with the same lexical
content. We characterize how perception - and subsequent discriminative
capability - is affected by different degrees of additional contextual
information across both the lexical and non-lexical channel of speech. Results
demonstrate that people can effectively discriminate between different prosodic
realisations, that non-lexical context is informative, and that this channel
provides more salient information than the lexical channel, highlighting the
importance of the non-lexical channel in spoken interaction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1">Andrei-Marius Avram</a>, <a href="http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1">Vasile Pais</a>, <a href="http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1">Dan Tufis</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01139">
<div class="article-summary-box-inner">
<span><p>EuroVoc is a multilingual thesaurus that was built for organizing the
legislative documentary of the European Union institutions. It contains
thousands of categories at different levels of specificity and its descriptors
are targeted by legal texts in almost thirty languages. In this work we propose
a unified framework for EuroVoc classification on 22 languages by fine-tuning
modern Transformer-based pretrained language models. We study extensively the
performance of our trained models and show that they significantly improve the
results obtained by a similar tool - JEX - on the same dataset. The code and
the fine-tuned models were open sourced, together with a programmatic interface
that eases the process of loading the weights of a trained model and of
classifying a new document.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ StrucTexT: Structured Text Understanding with <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Transformer</span>s.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yulin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yuxi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yuchen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1">Xiameng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengquan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1">Kun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Junyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingtuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1">Errui Ding</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02923">
<div class="article-summary-box-inner">
<span><p>Structured text understanding on Visually Rich Documents (VRDs) is a crucial
part of Document Intelligence. Due to the complexity of content and layout in
VRDs, structured text understanding has been a challenging task. Most existing
studies decoupled this problem into two sub-tasks: entity labeling and entity
linking, which require an entire understanding of the context of documents at
both token and segment levels. However, little work has been concerned with the
solutions that efficiently extract the structured data from different levels.
This paper proposes a unified framework named StrucTexT, which is flexible and
effective for handling both sub-tasks. Specifically, based on the transformer,
we introduce a segment-token aligned encoder to deal with the entity labeling
and entity linking tasks at different levels of granularity. Moreover, we
design a novel pre-training strategy with three self-supervised tasks to learn
a richer representation. StrucTexT uses the existing Masked Visual Language
Modeling task and the new Sentence Length Prediction and Paired Boxes Direction
tasks to incorporate the multi-modal information across text, image, and
layout. We evaluate our method for structured text understanding at
segment-level and token-level and show it outperforms the state-of-the-art
counterparts with significantly superior performance on the FUNSD, SROIE, and
EPHOIE datasets.
</p></span>
</div>
</a>
</details>
</article>
</div>
</details>
</article>
<article>
<details>
<Summary>cs.CV updates on arXiv.org</Summary>
<div class="details-content">
<p style="text-align:center;">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</p>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ GANmapper: geographical content filling.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1">Abraham Noah Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1">Filip Biljecki</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04232">
<div class="article-summary-box-inner">
<span><p>We present a new method to create spatial data using a generative adversarial
network (GAN). Our contribution uses coarse and widely available geospatial
data to create maps of less available features at the finer scale in the built
environment, bypassing their traditional acquisition techniques (e.g. satellite
imagery or land surveying). In the work, we employ land use data and road
networks as input to generate building footprints, and conduct experiments in 9
cities around the world. The method, which we implement in a tool we release
openly, enables generating approximate maps of the urban form, and it is
generalisable to augment other types of geoinformation, enhancing the
completeness and quality of spatial data infrastructure. It may be especially
useful in locations missing detailed and high-resolution data and those that
are mapped with uncertain or heterogeneous quality, such as much of
OpenStreetMap. The quality of the results is influenced by the urban form and
scale. In most cases, experiments suggest promising performance as the method
tends to truthfully indicate the locations, amount, and shape of buildings. The
work has the potential to support several applications, such as energy,
climate, and urban morphology studies in areas previously lacking required
data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Deep Transfer Learning for Identifications of Slope Surface Cracks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuting Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1">Gang Mei</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04235">
<div class="article-summary-box-inner">
<span><p>Geohazards such as landslides have caused great losses to the safety of
people's lives and property, which is often accompanied with surface cracks. If
such surface cracks could be identified in time, it is of great significance
for the monitoring and early warning of geohazards. Currently, the most common
method for crack identification is manual detection, which is with low
efficiency and accuracy. In this paper, a deep transfer learning framework is
proposed to effectively and efficiently identify slope surface cracks for the
sake of fast monitoring and early warning of geohazards such as landslides. The
essential idea is to employ transfer learning by training (a) the large sample
dataset of concrete cracks and (b) the small sample dataset of soil and rock
masses cracks. In the proposed framework, (1) pretrained cracks identification
models are constructed based on the large sample dataset of concrete cracks;
(2) refined cracks identification models are further constructed based on the
small sample dataset of soil and rock masses cracks. The proposed framework
could be applied to conduct UAV surveys on high-steep slopes to realize the
monitoring and early warning of landslides to ensure the safety of people's
lives and property.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ An optical biomimetic eyes with interested object imaging.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shimei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shangyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1">Miao Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xiaofang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1">Chuangxue Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kunyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1">Shuxin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuhui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yuer Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1">Ting Zhong</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04236">
<div class="article-summary-box-inner">
<span><p>We presented an optical system to perform imaging interested objects in
complex scenes, like the creature easy see the interested prey in the hunt for
complex environments. It utilized Deep-learning network to learn the interested
objects's vision features and designed the corresponding "imaging matrices",
furthermore the learned matrixes act as the measurement matrix to complete
compressive imaging with a single-pixel camera, finally we can using the
compressed image data to only image the interested objects without the rest
objects and backgrounds of the scenes with the previous Deep-learning network.
Our results demonstrate that no matter interested object is single feature or
rich details, the interference can be successfully filtered out and this idea
can be applied in some common applications that effectively improve the
performance. This bio-inspired optical system can act as the creature eye to
achieve success on interested-based object imaging, object detection, object
recognition and object tracking, etc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1">Haocheng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Caleb Chen Cao</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04238">
<div class="article-summary-box-inner">
<span><p>Explanation of AI, as well as fairness of algorithms' decisions and the
transparency of the decision model, are becoming more and more important. And
it is crucial to design effective and human-friendly techniques when opening
the black-box model. Counterfactual conforms to the human way of thinking and
provides a human-friendly explanation, and its corresponding explanation
algorithm refers to a strategic alternation of a given data point so that its
model output is "counter-facted", i.e. the prediction is reverted. In this
paper, we adapt counterfactual explanation over fine-grained image
classification problem. We demonstrated an adaptive method that could give a
counterfactual explanation by showing the composed counterfactual feature map
using top-down layer searching algorithm (TDLS). We have proved that our TDLS
algorithm could provide more flexible counterfactual visual explanation in an
efficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,
we discussed several applicable scenarios of counterfactual visual
explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1">Santiago Estrada</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1">Ran Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1">Kersten Diers</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1">Weiyi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1">Philipp Ehses</a>, <a href="http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1">Tony St&#xf6;cker</a>, <a href="http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1">Monique M.B Breteler</a>, <a href="http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1">Martin Reuter</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04267">
<div class="article-summary-box-inner">
<span><p>The neuroimage analysis community has neglected the automated segmentation of
the olfactory bulb (OB) despite its crucial role in olfactory function. The
lack of an automatic processing method for the OB can be explained by its
challenging properties. Nonetheless, recent advances in MRI acquisition
techniques and resolution have allowed raters to generate more reliable manual
annotations. Furthermore, the high accuracy of deep learning methods for
solving semantic segmentation problems provides us with an option to reliably
assess even small structures. In this work, we introduce a novel, fast, and
fully automated deep learning pipeline to accurately segment OB tissue on
sub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we
designed a three-stage pipeline: (1) Localization of a region containing both
OBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized
region through four independent AttFastSurferCNN - a novel deep learning
architecture with a self-attention mechanism to improve modeling of contextual
information, and (3) Ensemble of the predicted label maps. The OB pipeline
exhibits high performance in terms of boundary delineation, OB localization,
and volume estimation across a wide range of ages in 203 participants of the
Rhineland Study. Moreover, it also generalizes to scans of an independent
dataset never encountered during training, the Human Connectome Project (HCP),
with different acquisition parameters and demographics, evaluated in 30 cases
at the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.
We extensively validated our pipeline not only with respect to segmentation
accuracy but also to known OB volume effects, where it can sensitively
replicate age effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Visual SLAM with Graph-Cut Optimized Multi-Plane Reconstruction.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1">Fangwen Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yaxu Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1">Jason Rambach</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1">Alain Pagani</a>, <a href="http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1">Didier Stricker</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04281">
<div class="article-summary-box-inner">
<span><p>This paper presents a semantic planar SLAM system that improves pose
estimation and mapping using cues from an instance planar segmentation network.
While the mainstream approaches are using RGB-D sensors, employing a monocular
camera with such a system still faces challenges such as robust data
association and precise geometric model fitting. In the majority of existing
work, geometric model estimation problems such as homography estimation and
piece-wise planar reconstruction (PPR) are usually solved by standard (greedy)
RANSAC separately and sequentially. However, setting the inlier-outlier
threshold is difficult in absence of information about the scene (i.e. the
scale). In this work, we revisit these problems and argue that two mentioned
geometric models (homographies/3D planes) can be solved by minimizing an energy
function that exploits the spatial coherence, i.e. with graph-cut optimization,
which also tackles the practical issue when the output of a trained CNN is
inaccurate. Moreover, we propose an adaptive parameter setting strategy based
on our experiments, and report a comprehensive evaluation on various
open-source datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Learning to Cut by Watching Movies.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1">Alejandro Pardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1">Fabian Caba Heilbron</a>, <a href="http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href="http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1">Ali Thabet</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04294">
<div class="article-summary-box-inner">
<span><p>Video content creation keeps growing at an incredible pace; yet, creating
engaging stories remains challenging and requires non-trivial video editing
expertise. Many video editing components are astonishingly hard to automate
primarily due to the lack of raw video materials. This paper focuses on a new
task for computational video editing, namely the task of raking cut
plausibility. Our key idea is to leverage content that has already been edited
to learn fine-grained audiovisual patterns that trigger cuts. To do this, we
first collected a data source of more than 10K videos, from which we extract
more than 255K cuts. We devise a model that learns to discriminate between real
and artificial cuts via contrastive learning. We set up a new task and a set of
baselines to benchmark video cut generation. We observe that our proposed model
outperforms the baselines by large margins. To demonstrate our model in
real-world applications, we conduct human studies in a collection of unedited
videos. The results show that our model does a better job at cutting than
random and alternative baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Scheuerman_M/0/1/0/all/0/1">Morgan Klaus Scheuerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1">Emily Denton</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanna_A/0/1/0/all/0/1">Alex Hanna</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04308">
<div class="article-summary-box-inner">
<span><p>Data is a crucial component of machine learning. The field is reliant on data
to train, validate, and test models. With increased technical capabilities,
machine learning research has boomed in both academic and industry settings,
and one major focus has been on computer vision. Computer vision is a popular
domain of machine learning increasingly pertinent to real-world applications,
from facial recognition in policing to object detection for autonomous
vehicles. Given computer vision's propensity to shape machine learning research
and impact human life, we seek to understand disciplinary practices around
dataset documentation - how data is collected, curated, annotated, and packaged
into datasets for computer vision researchers and practitioners to use for
model tuning and development. Specifically, we examine what dataset
documentation communicates about the underlying values of vision data and the
larger practices and goals of computer vision as a field. To conduct this
study, we collected a corpus of about 500 computer vision datasets, from which
we sampled 114 dataset publications across different vision tasks. Through both
a structured and thematic content analysis, we document a number of values
around accepted data practices, what makes desirable data, and the treatment of
humans in the dataset construction process. We discuss how computer vision
datasets authors value efficiency at the expense of care; universality at the
expense of contextuality; impartiality at the expense of positionality; and
model work at the expense of data work. Many of the silenced values we identify
sit in opposition with social computing practices. We conclude with suggestions
on how to better incorporate silenced values into the dataset creation and
curation process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ FairyTailor: A Multimodal Generative Framework for Storytelling.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1">Eden Bensaid</a>, <a href="http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1">Mauro Martino</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1">Benjamin Hoover</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1">Jacob Andreas</a>, <a href="http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1">Hendrik Strobelt</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04324">
<div class="article-summary-box-inner">
<span><p>Storytelling is an open-ended task that entails creative thinking and
requires a constant flow of ideas. Natural language generation (NLG) for
storytelling is especially challenging because it requires the generated text
to follow an overall theme while remaining creative and diverse to engage the
reader. In this work, we introduce a system and a web-based demo, FairyTailor,
for human-in-the-loop visual story co-creation. Users can create a cohesive
children's fairytale by weaving generated texts and retrieved images with their
input. FairyTailor adds another modality and modifies the text generation
process to produce a coherent and creative sequence of text and images. To our
knowledge, this is the first dynamic tool for multimodal story generation that
allows interactive co-formation of both texts and images. It allows users to
give feedback on co-created stories and share their results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ AnyoneNet: Synchronized Speech and Talking Head Generation for arbitrary person.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xinsheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qicong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jihua Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lei Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Scharenborg/0/1/0/all/0/1">Scharenborg</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04325">
<div class="article-summary-box-inner">
<span><p>Automatically generating videos in which synthesized speech is synchronized
with lip movements in a talking head has great potential in many human-computer
interaction scenarios. In this paper, we present an automatic method to
generate synchronized speech and talking-head videos on the basis of text and a
single face image of an arbitrary person as input. In contrast to previous
text-driven talking head generation methods, which can only synthesize the
voice of a specific person, the proposed method is capable of synthesizing
speech for any person that is inaccessible in the training stage. Specifically,
the proposed method decomposes the generation of synchronized speech and
talking head videos into two stages, i.e., a text-to-speech (TTS) stage and a
speech-driven talking head generation stage. The proposed TTS module is a
face-conditioned multi-speaker TTS model that gets the speaker identity
information from face images instead of speech, which allows us to synthesize a
personalized voice on the basis of the input face image. To generate the
talking head videos from the face images, a facial landmark-based method that
can predict both lip movements and head rotations is proposed. Extensive
experiments demonstrate that the proposed method is able to generate
synchronized speech and talking head videos for arbitrary persons and
non-persons. Synthesized speech shows consistency with the given face regarding
to the synthesized voice's timbre and one's appearance in the image, and the
proposed landmark-based talking head method outperforms the state-of-the-art
landmark-based method on generating natural talking head videos.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Natural Numerical Networks for Natura 2000 habitats classification by satellite images.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1">Karol Mikula</a>, <a href="http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1">Michal Kollar</a>, <a href="http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1">Aneta A. Ozvat</a>, <a href="http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1">Martin Ambroz</a>, <a href="http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1">Lucia Cahojova</a>, <a href="http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1">Ivan Jarolimek</a>, <a href="http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1">Jozef Sibik</a>, <a href="http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1">Maria Sibikova</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04327">
<div class="article-summary-box-inner">
<span><p>Natural numerical networks are introduced as a new classification algorithm
based on the numerical solution of nonlinear partial differential equations of
forward-backward diffusion type on complete graphs. The proposed natural
numerical network is applied to open important environmental and nature
conservation task, the automated identification of protected habitats by using
satellite images. In the natural numerical network, the forward diffusion
causes the movement of points in a feature space toward each other. The
opposite effect, keeping the points away from each other, is caused by backward
diffusion. This yields the desired classification. The natural numerical
network contains a few parameters that are optimized in the learning phase of
the method. After learning parameters and optimizing the topology of the
network graph, classification necessary for habitat identification is
performed. A relevancy map for each habitat is introduced as a tool for
validating the classification and finding new Natura 2000 habitat appearances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Generative Adversarial Neural Cellular Automata.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Otte_M/0/1/0/all/0/1">Maximilian Otte</a>, <a href="http://arxiv.org/find/cs/1/au:+Delfosse_Q/0/1/0/all/0/1">Quentin Delfosse</a>, <a href="http://arxiv.org/find/cs/1/au:+Czech_J/0/1/0/all/0/1">Johannes Czech</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04328">
<div class="article-summary-box-inner">
<span><p>Motivated by the interaction between cells, the recently introduced concept
of Neural Cellular Automata shows promising results in a variety of tasks. So
far, this concept was mostly used to generate images for a single scenario. As
each scenario requires a new model, this type of generation seems contradictory
to the adaptability of cells in nature. To address this contradiction, we
introduce a concept using different initial environments as input while using a
single Neural Cellular Automata to produce several outputs. Additionally, we
introduce GANCA, a novel algorithm that combines Neural Cellular Automata with
Generative Adversarial Networks, allowing for more generalization through
adversarial training. The experiments show that a single model is capable of
learning several images when presented with different inputs, and that the
adversarially trained model improves drastically on out-of-distribution data
compared to a supervised trained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Class dependency based learning using Bi-LSTM coupled with the transfer learning of VGG16 for the diagnosis of Tuberculosis from chest x-rays.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1">G Jignesh Chowdary</a>, <a href="http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1">Suganya G</a>, <a href="http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1">Premalatha M</a>, <a href="http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1">Karunamurthy K</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04329">
<div class="article-summary-box-inner">
<span><p>Tuberculosis is an infectious disease that is leading to the death of
millions of people across the world. The mortality rate of this disease is high
in patients suffering from immuno-compromised disorders. The early diagnosis of
this disease can save lives and can avoid further complications. But the
diagnosis of TB is a very complex task. The standard diagnostic tests still
rely on traditional procedures developed in the last century. These procedures
are slow and expensive. So this paper presents an automatic approach for the
diagnosis of TB from posteroanterior chest x-rays. This is a two-step approach,
where in the first step the lung regions are segmented from the chest x-rays
using the graph cut method, and then in the second step the transfer learning
of VGG16 combined with Bi-directional LSTM is used for extracting high-level
discriminative features from the segmented lung regions and then classification
is performed using a fully connected layer. The proposed model is evaluated
using data from two publicly available databases namely Montgomery Country set
and Schezien set. The proposed model achieved accuracy and sensitivity of
97.76%, 97.01% and 96.42%, 94.11% on Schezien and Montgomery county datasets.
This model enhanced the diagnostic accuracy of TB by 0.7% and 11.68% on
Schezien and Montgomery county datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Creating synthetic meteorology satellite visible light images during night based on GAN method.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wencong_C/0/1/0/all/0/1">CHENG Wencong</a> (1) ((1) Beijing Aviation Meteorological Institute)
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04330">
<div class="article-summary-box-inner">
<span><p>Meteorology satellite visible light images is critical for meteorology
support and forecast. However, there is no such kind of data during night time.
To overcome this, we propose a method based on deep learning to create
synthetic satellite visible light images during night. Specifically, to produce
more realistic products, we train a Generative Adversarial Networks (GAN) model
to generate visible light images given the corresponding satellite infrared
images and numerical weather prediction(NWP) products. To better model the
nonlinear relationship from infrared data and NWP products to visible light
images, we propose to use the channel-wise attention mechanics, e.g., SEBlock
to quantitative weight the input channels. The experiments based on the ECMWF
NWP products and FY-4A meteorology satellite visible light and infrared
channels date show that the proposed methods can be effective to create
realistic synthetic satellite visible light images during night.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1">Aishwarza Panday</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1">Muhammad Ashad Kabir</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1">Nihad Karim Chowdhury</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04344">
<div class="article-summary-box-inner">
<span><p>Due to the limited availability and high cost of the reverse
transcription-polymerase chain reaction (RT-PCR) test, many studies have
proposed machine learning techniques for detecting COVID-19 from medical
imaging. The purpose of this study is to systematically review, assess, and
synthesize research articles that have used different machine learning
techniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.
A structured literature search was conducted in the relevant bibliographic
databases to ensure that the survey solely centered on reproducible and
high-quality research. We selected papers based on our inclusion criteria. In
this survey, we reviewed $98$ articles that fulfilled our inclusion criteria.
We have surveyed a complete pipeline of chest imaging analysis techniques
related to COVID-19, including data collection, pre-processing, feature
extraction, classification, and visualization. We have considered CT scans and
X-rays as both are widely used to describe the latest developments in medical
imaging to detect COVID-19. This survey provides researchers with valuable
insights into different machine learning techniques and their performance in
the detection and diagnosis of COVID-19 from chest imaging. At the end, the
challenges and limitations in detecting COVID-19 using machine learning
techniques and the future direction of research are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1">Hamza Rasaee</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1">Hassan Rivaz</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04345">
<div class="article-summary-box-inner">
<span><p>Ultrasound is a non-invasive imaging modality that can be conveniently used
to classify suspicious breast nodules and potentially detect the onset of
breast cancer. Recently, Convolutional Neural Networks (CNN) techniques have
shown promising results in classifying ultrasound images of the breast into
benign or malignant. However, CNN inference acts as a black-box model, and as
such, its decision-making is not interpretable. Therefore, increasing effort
has been dedicated to explaining this process, most notably through GRAD-CAM
and other techniques that provide visual explanations into inner workings of
CNNs. In addition to interpretation, these methods provide clinically important
information, such as identifying the location for biopsy or treatment. In this
work, we analyze how adversarial assaults that are practically undetectable may
be devised to alter these importance maps dramatically. Furthermore, we will
show that this change in the importance maps can come with or without altering
the classification result, rendering them even harder to detect. As such, care
must be taken when using these importance maps to shed light on the inner
workings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and
propose a new network based on ResNet-50 to improve the classification
accuracies. Our sensitivity and specificity is comparable to the state of the
art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ AASeg: Attention Aware Network for Real Time Semantic Segmentation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1">Abhinav Sagar</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04349">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a new network named Attention Aware Network (AASeg)
for real time semantic image segmentation. Our network incorporates spatial and
channel information using Spatial Attention (SA) and Channel Attention (CA)
modules respectively. It also uses dense local multi-scale context information
using Multi Scale Context (MSC) module. The feature maps are concatenated
individually to produce the final segmentation map. We demonstrate the
effectiveness of our method using a comprehensive analysis, quantitative
experimental results and ablation study using Cityscapes, ADE20K and Camvid
datasets. Our network performs better than most previous architectures with a
74.4\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ VirtualConductor: Music-driven Conducting Video Generation System.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Delong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Fan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zewen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1">Feng Xu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04350">
<div class="article-summary-box-inner">
<span><p>In this demo, we present VirtualConductor, a system that can generate
conducting video from any given music and a single user's image. First, a
large-scale conductor motion dataset is collected and constructed. Then, we
propose Audio Motion Correspondence Network (AMCNet) and adversarial-perceptual
learning to learn the cross-modal relationship and generate diverse, plausible,
music-synchronized motion. Finally, we combine 3D animation rendering and a
pose transfer model to synthesize conducting video from a single given user's
image. Therefore, any user can become a virtual conductor through the system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1">Amey Thakur</a>, <a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1">Mega Satish</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04351">
<div class="article-summary-box-inner">
<span><p>This paper aims to demonstrate the efficiency of the Adversarial Open Domain
Adaption framework for sketch-to-photo synthesis. The unsupervised open domain
adaption for generating realistic photos from a hand-drawn sketch is
challenging as there is no such sketch of that class for training data. The
absence of learning supervision and the huge domain gap between both the
freehand drawing and picture domains make it hard. We present an approach that
learns both sketch-to-photo and photo-to-sketch generation to synthesise the
missing freehand drawings from pictures. Due to the domain gap between
synthetic sketches and genuine ones, the generator trained on false drawings
may produce unsatisfactory results when dealing with drawings of lacking
classes. To address this problem, we offer a simple but effective open-domain
sampling and optimization method that tricks the generator into considering
false drawings as genuine. Our approach generalises the learnt sketch-to-photo
and photo-to-sketch mappings from in-domain input to open-domain categories. On
the Scribble and SketchyCOCO datasets, we compared our technique to the most
current competing methods. For many types of open-domain drawings, our model
outperforms impressive results in synthesising accurate colour, substance, and
retaining the structural layout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Attribute Guided Sparse Tensor-Based Model for Person Re-Identification.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1">Fariborz Taherkhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1">Ali Dabouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1">Sobhan Soleymani</a>, <a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1">Jeremy Dawson</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1">Nasser M. Nasrabadi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04352">
<div class="article-summary-box-inner">
<span><p>Visual perception of a person is easily influenced by many factors such as
camera parameters, pose and viewpoint variations. These variations make person
Re-Identification (ReID) a challenging problem. Nevertheless, human attributes
usually stand as robust visual properties to such variations. In this paper, we
propose a new method to leverage features from human attributes for person
ReID. Our model uses a tensor to non-linearly fuse identity and attribute
features, and then forces the parameters of the tensor in the loss function to
generate discriminative fused features for ReID. Since tensor-based methods
usually contain a large number of parameters, training all of these parameters
becomes very slow, and the chance of overfitting increases as well. To address
this issue, we propose two new techniques based on Structural Sparsity Learning
(SSL) and Tensor Decomposition (TD) methods to create an accurate and stable
learning problem. We conducted experiments on several standard pedestrian
datasets, and experimental results indicate that our tensor-based approach
significantly improves person ReID baselines and also outperforms state of the
art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Tasks Structure Regularization in Multi-Task Learning for Improving Facial Attribute Prediction.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1">Fariborz Taherkhani</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1">Ali Dabouei</a>, <a href="http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1">Sobhan Soleymani</a>, <a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1">Jeremy Dawson</a>, <a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1">Nasser M. Nasrabadi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04353">
<div class="article-summary-box-inner">
<span><p>The great success of Convolutional Neural Networks (CNN) for facial attribute
prediction relies on a large amount of labeled images. Facial image datasets
are usually annotated by some commonly used attributes (e.g., gender), while
labels for the other attributes (e.g., big nose) are limited which causes their
prediction challenging. To address this problem, we use a new Multi-Task
Learning (MTL) paradigm in which a facial attribute predictor uses the
knowledge of other related attributes to obtain a better generalization
performance. Here, we leverage MLT paradigm in two problem settings. First, it
is assumed that the structure of the tasks (e.g., grouping pattern of facial
attributes) is known as a prior knowledge, and parameters of the tasks (i.e.,
predictors) within the same group are represented by a linear combination of a
limited number of underlying basis tasks. Here, a sparsity constraint on the
coefficients of this linear combination is also considered such that each task
is represented in a more structured and simpler manner. Second, it is assumed
that the structure of the tasks is unknown, and then structure and parameters
of the tasks are learned jointly by using a Laplacian regularization framework.
Our MTL methods are compared with competing methods for facial attribute
prediction to show its effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Local Morphometry of Closed, Implicit Surfaces.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Besler_B/0/1/0/all/0/1">Bryce A Besler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1">Tannis D. Kemp</a>, <a href="http://arxiv.org/find/cs/1/au:+Michalski_A/0/1/0/all/0/1">Andrew S. Michalski</a>, <a href="http://arxiv.org/find/cs/1/au:+Forkert_N/0/1/0/all/0/1">Nils D. Forkert</a>, <a href="http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1">Steven K. Boyd</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04354">
<div class="article-summary-box-inner">
<span><p>Anatomical structures such as the hippocampus, liver, and bones can be
analyzed as orientable, closed surfaces. This permits the computation of
volume, surface area, mean curvature, Gaussian curvature, and the
Euler-Poincar\'e characteristic as well as comparison of these morphometrics
between structures of different topology. The structures are commonly
represented implicitly in curve evolution problems as the zero level set of an
embedding. Practically, binary images of anatomical structures are embedded
using a signed distance transform. However, quantization prevents the accurate
computation of curvatures, leading to considerable errors in morphometry. This
paper presents a fast, simple embedding procedure for accurate local
morphometry as the zero crossing of the Gaussian blurred binary image. The
proposed method was validated based on the femur and fourth lumbar vertebrae of
50 clinical computed tomography datasets. The results show that the signed
distance transform leads to large quantization errors in the computed local
curvature. Global validation of morphometry using regression and Bland-Altman
analysis revealed that the coefficient of determination for the average mean
curvature is improved from 93.8% with the signed distance transform to 100%
with the proposed method. For the surface area, the proportional bias is
improved from -5.0% for the signed distance transform to +0.6% for the proposed
method. The Euler-Poincar\'e characteristic is improved from unusable in the
signed distance transform to 98% accuracy for the proposed method. The proposed
method enables an improved local and global evaluation of curvature for
purposes of morphometry on closed, implicit surfaces.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Hyperparameter Analysis for Derivative Compressive Sampling.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Rabbi_M/0/1/0/all/0/1">Md Fazle Rabbi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04355">
<div class="article-summary-box-inner">
<span><p>Derivative compressive sampling (DCS) is a signal reconstruction method from
measurements of the spatial gradient with sub-Nyquist sampling rate.
Applications of DCS include optical image reconstruction, photometric stereo,
and shape-from-shading. In this work, we study the sensitivity of DCS with
respect to algorithmic hyperparameters using a brute-force search algorithm. We
perform experiments on a dataset of surface images and deduce guidelines for
the user to setup values for the hyperparameters for improved signal recovery
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ A Robust Lane Detection Associated with Quaternion Hardy Filter.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1">Wenshan Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">Dong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1">Kit Ian Kou</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04356">
<div class="article-summary-box-inner">
<span><p>In this article, a robust color-edge feature extraction method based on the
Quaternion Hardy filter is proposed. The Quaternion Hardy filter is an emerging
edge detection theory. It is along with the Poisson and conjugate Poisson
smoothing kernels to handle various types of noise. Combining with the
Quaternion Hardy filter, Jin's color gradient operator and Hough transform, the
color-edge feature detection algorithm is proposed and applied to the lane
marking detection. Experiments are presented to demonstrate the validity of the
proposed algorithm. The results are accurate and robust with respect to the
complex environment lane markings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1">Ashild Kummen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1">Ali Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1">Teodora Ganeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qianying Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1">Robert Shaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1">Chenuka Ratwatte</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yang Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1">Emil Almazov</a>, <a href="http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1">Sheena Visram</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1">Andrew Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1">Neil J Sebire</a>, <a href="http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1">Lee Stott</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1">Yvonne Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1">Graham Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1">Dean Mohamedally</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04357">
<div class="article-summary-box-inner">
<span><p>Touchless computer interaction has become an important consideration during
the COVID-19 pandemic period. Despite progress in machine learning and computer
vision that allows for advanced gesture recognition, an integrated collection
of such open-source methods and a user-customisable approach to utilising them
in a low-cost solution for touchless interaction in existing software is still
missing. In this paper, we introduce the MotionInput v2.0 application. This
application utilises published open-source libraries and additional gesture
definitions developed to take the video stream from a standard RGB webcam as
input. It then maps human motion gestures to input operations for existing
applications and games. The user can choose their own preferred way of
interacting from a series of motion types, including single and bi-modal hand
gesturing, full-body repetitive or extremities-based exercises, head and facial
movements, eye tracking, and combinations of the above. We also introduce a
series of bespoke gesture recognition classifications as DirectInput triggers,
including gestures for idle states, auto calibration, depth capture from a 2D
RGB webcam stream and tracking of facial motions such as mouth motions,
winking, and head direction with rotation. Three use case areas assisted the
development of the modules: creativity software, office and clinical software,
and gaming software. A collection of open-source libraries has been integrated
and provide a layer of modular gesture mapping on top of existing mouse and
keyboard controls in Windows via DirectX. With ease of access to webcams
integrated into most laptops and desktop computers, touchless computing becomes
more available with MotionInput v2.0, in a federated and locally processed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1">Ayaan Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1">Ipsita Sutradhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Mahziba Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Mehedi Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1">Malabika Sarker</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04358">
<div class="article-summary-box-inner">
<span><p>Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as
a result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye
illness caused by diabetes, can lead to blindness if it is not identified and
treated in its early stages. Unfortunately, diagnosis of DR requires medically
trained professionals, but Bangladesh has limited specialists in comparison to
its population. Moreover, the screening process is often expensive, prohibiting
many from receiving timely and proper diagnosis. To address the problem, we
introduce a deep learning algorithm which screens for different stages of DR.
We use a state-of-the-art CNN architecture to diagnose patients based on
retinal fundus imagery. This paper is an experimental evaluation of the
algorithm we developed for DR diagnosis and screening specifically for
Bangladeshi patients. We perform this validation study using separate pools of
retinal image data of real patients from a hospital and field studies in
Bangladesh. Our results show that the algorithm is effective at screening
Bangladeshi eyes even when trained on a public dataset which is out of domain,
and can accurately determine the stage of DR as well, achieving an overall
accuracy of 92.27\% and 93.02\% on two validation sets of Bangladeshi eyes. The
results confirm the ability of the algorithm to be used in real clinical
settings and applications due to its high accuracy and classwise metrics. Our
algorithm is implemented in the application Drishti, which is used to screen
for DR in patients living in rural areas in Bangladesh, where access to
professional screening is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Adaptable image quality assessment using meta-reinforcement learning of task amenability.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1">Shaheer U. Saeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yunguan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1">Vasilis Stavrinides</a>, <a href="http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1">Zachary M. C. Baum</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qianye Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1">Mirabela Rusu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Richard E. Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1">Geoffrey A. Sonn</a>, <a href="http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1">J. Alison Noble</a>, <a href="http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1">Dean C. Barratt</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yipeng Hu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04359">
<div class="article-summary-box-inner">
<span><p>The performance of many medical image analysis tasks are strongly associated
with image data quality. When developing modern deep learning algorithms,
rather than relying on subjective (human-based) image quality assessment (IQA),
task amenability potentially provides an objective measure of task-specific
image quality. To predict task amenability, an IQA agent is trained using
reinforcement learning (RL) with a simultaneously optimised task predictor,
such as a classification or segmentation neural network. In this work, we
develop transfer learning or adaptation strategies to increase the adaptability
of both the IQA agent and the task predictor so that they are less dependent on
high-quality, expert-labelled training data. The proposed transfer learning
strategy re-formulates the original RL problem for task amenability in a
meta-reinforcement learning (meta-RL) framework. The resulting algorithm
facilitates efficient adaptation of the agent to different definitions of image
quality, each with its own Markov decision process environment including
different images, labels and an adaptable task predictor. Our work demonstrates
that the IQA agents pre-trained on non-expert task labels can be adapted to
predict task amenability as defined by expert task labels, using only a small
set of expert labels. Using 6644 clinical ultrasound images from 249 prostate
cancer patients, our results for image classification and segmentation tasks
show that the proposed IQA method can be adapted using data with as few as
respective 19.7% and 29.6% expert-reviewed consensus labels and still achieve
comparable IQA and task performance, which would otherwise require a training
dataset with 100% expert labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1">Yuki Tatsunami</a>, <a href="http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1">Masato Taki</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04384">
<div class="article-summary-box-inner">
<span><p>For the past ten years, CNN has reigned supreme in the world of computer
vision, but recently, Transformer is on the rise. However, the quadratic
computational cost of self-attention has become a severe problem of practice.
There has been much research on architectures without CNN and self-attention in
this context. In particular, MLP-Mixer is a simple idea designed using MLPs and
hit an accuracy comparable to the Vision Transformer. However, the only
inductive bias in this architecture is the embedding of tokens. Thus, there is
still a possibility to build a non-convolutional inductive bias into the
architecture itself, and we built in an inductive bias using two simple ideas.
A way is to divide the token-mixing block vertically and horizontally. Another
way is to make spatial correlations denser among some channels of token-mixing.
With this approach, we were able to improve the accuracy of the MLP-Mixer while
reducing its parameters and computational complexity. Compared to other
MLP-based models, the proposed model, named RaftMLP has a good balance of
computational complexity, the number of parameters, and actual memory usage. In
addition, our work indicates that MLP-based models have the potential to
replace CNNs by adopting inductive bias. The source code in PyTorch version is
available at \url{https://github.com/okojoalg/raft-mlp}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Rethinking Architecture Selection in Differentiable NAS.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruochen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Minhao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiangning Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiaocheng Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04392">
<div class="article-summary-box-inner">
<span><p>Differentiable Neural Architecture Search is one of the most popular Neural
Architecture Search (NAS) methods for its search efficiency and simplicity,
accomplished by jointly optimizing the model weight and architecture parameters
in a weight-sharing supernet via gradient-based algorithms. At the end of the
search phase, the operations with the largest architecture parameters will be
selected to form the final architecture, with the implicit assumption that the
values of architecture parameters reflect the operation strength. While much
has been discussed about the supernet's optimization, the architecture
selection process has received little attention. We provide empirical and
theoretical analysis to show that the magnitude of architecture parameters does
not necessarily indicate how much the operation contributes to the supernet's
performance. We propose an alternative perturbation-based architecture
selection that directly measures each operation's influence on the supernet. We
re-evaluate several differentiable NAS methods with the proposed architecture
selection and find that it is able to extract significantly improved
architectures from the underlying supernets consistently. Furthermore, we find
that several failure modes of DARTS can be greatly alleviated with the proposed
selection method, indicating that much of the poor generalization observed in
DARTS can be attributed to the failure of magnitude-based architecture
selection rather than entirely the optimization of its supernet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Stroke Correspondence by Labeling Closed Areas.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Miyauchi_R/0/1/0/all/0/1">Ryoma Miyauchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fukusato_T/0/1/0/all/0/1">Tsukasa Fukusato</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Haoran Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Miyata_K/0/1/0/all/0/1">Kazunori Miyata</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04393">
<div class="article-summary-box-inner">
<span><p>Constructing stroke correspondences between keyframes is one of the most
important processes in the production pipeline of hand-drawn inbetweening
frames. This process requires time-consuming manual work imposing a tremendous
burden on the animators. We propose a method to estimate stroke correspondences
between raster character images (keyframes) without vectorization processes.
First, the proposed system separates the closed areas in each keyframe and
estimates the correspondences between closed areas by using the characteristics
of shape, depth, and closed area connection. Second, the proposed system
estimates stroke correspondences from the estimated closed area
correspondences. We demonstrate the effectiveness of our method by performing a
user study and comparing the proposed system with conventional approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ On Procedural Adversarial Noise Attack And Defense.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xiaoyang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Huilin Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1">Wancheng Ge</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04409">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) are vulnerable to adversarial examples which
would inveigle neural networks to make prediction errors with small per-
turbations on the input images. Researchers have been devoted to promoting the
research on the universal adversarial perturbations (UAPs) which are
gradient-free and have little prior knowledge on data distributions. Procedural
adversarial noise at- tack is a data-free universal perturbation generation
method. In this paper, we propose two universal adversarial perturbation (UAP)
generation methods based on procedural noise functions: Simplex noise and
Worley noise. In our framework, the shading which disturbs visual
classification is generated with rendering technology. Without changing the
semantic representations, the adversarial examples generated via our methods
show superior performance on the attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1">Balagopal Unnikrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1">Shafa Balaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1">Chuan Sheng Foo</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1">Pavitra Krishnaswamy</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04423">
<div class="article-summary-box-inner">
<span><p>Deep learning models achieve strong performance for radiology image
classification, but their practical application is bottlenecked by the need for
large labeled training datasets. Semi-supervised learning (SSL) approaches
leverage small labeled datasets alongside larger unlabeled datasets and offer
potential for reducing labeling cost. In this work, we introduce NoTeacher, a
novel consistency-based SSL framework which incorporates probabilistic
graphical models. Unlike Mean Teacher which maintains a teacher network updated
via a temporal ensemble, NoTeacher employs two independent networks, thereby
eliminating the need for a teacher network. We demonstrate how NoTeacher can be
customized to handle a range of challenges in radiology image classification.
Specifically, we describe adaptations for scenarios with 2D and 3D inputs, uni
and multi-label classification, and class distribution mismatch between labeled
and unlabeled portions of the training data. In realistic empirical evaluations
on three public benchmark datasets spanning the workhorse modalities of
radiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the
fully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher
outperforms established SSL methods with minimal hyperparameter tuning, and has
implications as a principled and practical option for semisupervised learning
in radiology applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ FT-TDR: Frequency-guided <span class="highlight-title">Transformer</span> and Top-Down Refinement Network for Blind Face Inpainting.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junke Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shaoxiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zuxuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04424">
<div class="article-summary-box-inner">
<span><p>Blind face inpainting refers to the task of reconstructing visual contents
without explicitly indicating the corrupted regions in a face image.
Inherently, this task faces two challenges: (1) how to detect various mask
patterns of different shapes and contents; (2) how to restore visually
plausible and pleasing contents in the masked regions. In this paper, we
propose a novel two-stage blind face inpainting method named Frequency-guided
Transformer and Top-Down Refinement Network (FT-TDR) to tackle these
challenges. Specifically, we first use a transformer-based network to detect
the corrupted regions to be inpainted as masks by modeling the relation among
different patches. We also exploit the frequency modality as complementary
information for improved detection results and capture the local contextual
incoherence to enhance boundary consistency. Then a top-down refinement network
is proposed to hierarchically restore features at different levels and generate
contents that are semantically consistent with the unmasked face regions.
Extensive experiments demonstrate that our method outperforms current
state-of-the-art blind and non-blind face inpainting methods qualitatively and
quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Domain-Aware Universal Style Transfer.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1">Kibeom Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1">Seogkyu Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jianlong Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1">Hyeran Byun</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04441">
<div class="article-summary-box-inner">
<span><p>Style transfer aims to reproduce content images with the styles from
reference images. Existing universal style transfer methods successfully
deliver arbitrary styles to original images either in an artistic or a
photo-realistic way. However, the range of 'arbitrary style' defined by
existing works is bounded in the particular domain due to their structural
limitation. Specifically, the degrees of content preservation and stylization
are established according to a predefined target domain. As a result, both
photo-realistic and artistic models have difficulty in performing the desired
style transfer for the other domain. To overcome this limitation, we propose a
unified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer
not only the style but also the property of domain (i.e., domainness) from a
given reference image. To this end, we design a novel domainness indicator that
captures the domainness value from the texture and structural features of
reference images. Moreover, we introduce a unified framework with domain-aware
skip connection to adaptively transfer the stroke and palette to the input
contents guided by the domainness indicator. Our extensive experiments validate
that our model produces better qualitative results and outperforms previous
methods in terms of proxy metrics on both artistic and photo-realistic
stylizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-<span class="highlight-title">Transformer</span>.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1">Peng Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1">Xin Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu-Shen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yan-Pei Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1">Wen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1">Zhizhong Han</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04444">
<div class="article-summary-box-inner">
<span><p>Point cloud completion aims to predict a complete shape in high accuracy from
its partial observation. However, previous methods usually suffered from
discrete nature of point cloud and unstructured prediction of points in local
regions, which makes it hard to reveal fine local geometric details on the
complete shape. To resolve this issue, we propose SnowflakeNet with Snowflake
Point Deconvolution (SPD) to generate the complete point clouds. The
SnowflakeNet models the generation of complete point clouds as the
snowflake-like growth of points in 3D space, where the child points are
progressively generated by splitting their parent points after each SPD. Our
insight of revealing detailed geometry is to introduce skip-transformer in SPD
to learn point splitting patterns which can fit local regions the best.
Skip-transformer leverages attention mechanism to summarize the splitting
patterns used in the previous SPD layer to produce the splitting in the current
SPD layer. The locally compact and structured point cloud generated by SPD is
able to precisely capture the structure characteristic of 3D shape in local
patches, which enables the network to predict highly detailed geometries, such
as smooth regions, sharp edges and corners. Our experimental results outperform
the state-of-the-art point cloud completion methods under widely used
benchmarks. Code will be available at
https://github.com/AllenXiangX/SnowflakeNet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Method Towards CVPR 2021 Image Matching Challenge.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1">Xiaopeng Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xinyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dehao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Ran Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1">Zheng Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haotian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04453">
<div class="article-summary-box-inner">
<span><p>This report describes Megvii-3D team's approach towards CVPR 2021 Image
Matching Workshop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ CPNet: Cross-Parallel Network for Efficient Anomaly Detection.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Youngsaeng Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1">David Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1">Hanseok Ko</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04454">
<div class="article-summary-box-inner">
<span><p>Anomaly detection in video streams is a challengingproblem because of the
scarcity of abnormal events andthe difficulty of accurately annotating them.To
allevi-ate these issues, unsupervised learning-based predictionmethods have
been previously applied. These approachestrain the model with only normal
events and predict a fu-ture frame from a sequence of preceding frames by use
ofencoder-decoder architectures so that they result in smallprediction errors
on normal events but large errors on ab-normal events. The architecture,
however, comes with thecomputational burden as some anomaly detection tasks
re-quire low computational cost without sacrificing perfor-mance. In this
paper, Cross-Parallel Network (CPNet) forefficient anomaly detection is
proposed here to minimizecomputations without performance drops. It consists
ofNsmaller parallel U-Net, each of which is designed to handlea single input
frame, to make the calculations significantlymore efficient. Additionally, an
inter-network shift moduleis incorporated to capture temporal relationships
among se-quential frames to enable more accurate future predictions.The
quantitative results show that our model requires lesscomputational cost than
the baseline U-Net while deliver-ing equivalent performance in anomaly
detection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Reference-based Defect Detection Network.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1">Zhaoyang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jianlong Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1">Hongyang Chao</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04456">
<div class="article-summary-box-inner">
<span><p>The defect detection task can be regarded as a realistic scenario of object
detection in the computer vision field and it is widely used in the industrial
field. Directly applying vanilla object detector to defect detection task can
achieve promising results, while there still exists challenging issues that
have not been solved. The first issue is the texture shift which means a
trained defect detector model will be easily affected by unseen texture, and
the second issue is partial visual confusion which indicates that a partial
defect box is visually similar with a complete box. To tackle these two
problems, we propose a Reference-based Defect Detection Network (RDDN).
Specifically, we introduce template reference and context reference to against
those two problems, respectively. Template reference can reduce the texture
shift from image, feature or region levels, and encourage the detectors to
focus more on the defective area as a result. We can use either well-aligned
template images or the outputs of a pseudo template generator as template
references in this work, and they are jointly trained with detectors by the
supervision of normal samples. To solve the partial visual confusion issue, we
propose to leverage the carried context information of context reference, which
is the concentric bigger box of each region proposal, to perform more accurate
region classification and regression. Experiments on two defect detection
datasets demonstrate the effectiveness of our proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Method Towards CVPR 2021 SimLocMatch Challenge.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1">Xiaopeng Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1">Ran Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1">Zheng Chai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haotian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04466">
<div class="article-summary-box-inner">
<span><p>This report describes Megvii-3D team's approach to-wards SimLocMatch
Challenge @ CVPR 2021 Image Matching Workshop.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ SP-GAN: Sphere-Guided 3D Shape Generation and Manipulation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Ruihui Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1">Ka-Hei Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1">Chi-Wing Fu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04476">
<div class="article-summary-box-inner">
<span><p>We present SP-GAN, a new unsupervised sphere-guided generative model for
direct synthesis of 3D shapes in the form of point clouds. Compared with
existing models, SP-GAN is able to synthesize diverse and high-quality shapes
with fine details and promote controllability for part-aware shape generation
and manipulation, yet trainable without any parts annotations. In SP-GAN, we
incorporate a global prior (uniform points on a sphere) to spatially guide the
generative process and attach a local prior (a random latent code) to each
sphere point to provide local details. The key insight in our design is to
disentangle the complex 3D shape generation task into a global shape modeling
and a local structure adjustment, to ease the learning process and enhance the
shape generation quality. Also, our model forms an implicit dense
correspondence between the sphere points and points in every generated shape,
enabling various forms of structure-aware shape manipulations such as part
editing, part-wise shape interpolation, and multi-shape part composition, etc.,
beyond the existing generative models. Experimental results, which include both
visual and quantitative evaluations, demonstrate that our model is able to
synthesize diverse point clouds with fine details and less noise, as compared
with the state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Scalable Reverse Image Search Engine for NASAWorldview.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Sodani_A/0/1/0/all/0/1">Abhigya Sodani</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1">Michael Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1">Anirudh Koul</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1">Meher Anand Kasam</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1">Siddha Ganju</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04479">
<div class="article-summary-box-inner">
<span><p>Researchers often spend weeks sifting through decades of unlabeled satellite
imagery(on NASA Worldview) in order to develop datasets on which they can start
conducting research. We developed an interactive, scalable and fast image
similarity search engine (which can take one or more images as the query image)
that automatically sifts through the unlabeled dataset reducing dataset
generation time from weeks to minutes. In this work, we describe key components
of the end to end pipeline. Our similarity search system was created to be able
to identify similar images from a potentially petabyte scale database that are
similar to an input image, and for this we had to break down each query image
into its features, which were generated by a classification layer stripped CNN
trained in a supervised manner. To store and search these features efficiently,
we had to make several scalability improvements. To improve the speed, reduce
the storage, and shrink memory requirements for embedding search, we add a
fully connected layer to our CNN make all images into a 128 length vector
before entering the classification layers. This helped us compress the size of
our image features from 2048 (for ResNet, which was initially tried as our
featurizer) to 128 for our new custom model. Additionally, we utilize existing
approximate nearest neighbor search libraries to significantly speed up
embedding search. Our system currently searches over our entire database of
images at 5 seconds per query on a single virtual machine in the cloud. In the
future, we would like to incorporate a SimCLR based featurizing model which
could be trained without any labelling by a human (since the classification
aspect of the model is irrelevant to this use case).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Exploiting Featureswith Split-and-Share Module.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaemin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1">Minseok Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jongchan Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1">Dong-Geol Choi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04500">
<div class="article-summary-box-inner">
<span><p>Deep convolutional neural networks (CNNs) have shown state-of-the-art
performances in various computer vision tasks. Advances on CNN architectures
have focused mainly on designing convolutional blocks of the feature
extractors, but less on the classifiers that exploit extracted features. In
this work, we propose Split-and-Share Module (SSM),a classifier that splits a
given feature into parts, which are partially shared by multiple
sub-classifiers. Our intuition is that the more the features are shared, the
more common they will become, and SSM can encourage such structural
characteristics in the split features. SSM can be easily integrated into any
architecture without bells and whistles. We have extensively validated the
efficacy of SSM on ImageNet-1K classification task, andSSM has shown consistent
and significant improvements over baseline architectures. In addition, we
analyze the effect of SSM using the Grad-CAM visualization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ TBNet:Two-Stream Boundary-aware Network for Generic Image Manipulation Localization.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zhiyong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1">Weili Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1">Anan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04508">
<div class="article-summary-box-inner">
<span><p>Finding tampered regions in images is a hot research topic in machine
learning and computer vision. Although many image manipulation location
algorithms have been proposed, most of them only focus on the RGB images with
different color spaces, and the frequency information that contains the
potential tampering clues is often ignored. In this work, a novel end-to-end
two-stream boundary-aware network (abbreviated as TBNet) is proposed for
generic image manipulation localization in which the RGB stream, the frequency
stream, and the boundary artifact location are explored in a unified framework.
Specifically, we first design an adaptive frequency selection module (AFS) to
adaptively select the appropriate frequency to mine inconsistent statistics and
eliminate the interference of redundant statistics. Then, an adaptive
cross-attention fusion module (ACF) is proposed to adaptively fuse the RGB
feature and the frequency feature. Finally, the boundary artifact location
network (BAL) is designed to locate the boundary artifacts for which the
parameters are jointly updated by the outputs of the ACF, and its results are
further fed into the decoder. Thus, the parameters of the RGB stream, the
frequency stream, and the boundary artifact location network are jointly
optimized, and their latent complementary relationships are fully mined. The
results of extensive experiments performed on four public benchmarks of the
image manipulation localization task, namely, CASIA1.0, COVER, Carvalho, and
In-The-Wild, demonstrate that the proposed TBNet can significantly outperform
state-of-the-art generic image manipulation localization methods in terms of
both MCC and F1.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Iterative Self-consistent Parallel Magnetic Resonance Imaging Reconstruction based on Nonlocal Low-Rank Regularization.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1">Ting Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jizhong Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yu Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04517">
<div class="article-summary-box-inner">
<span><p>Iterative self-consistent parallel imaging reconstruction (SPIRiT) is an
effective self-calibrated reconstruction model for parallel magnetic resonance
imaging (PMRI). The joint L1 norm of wavelet coefficients and joint total
variation (TV) regularization terms are incorporated into the SPIRiT model to
improve the reconstruction performance. The simultaneous two-directional
low-rankness (STDLR) in k-space data is incorporated into SPIRiT to realize
improved reconstruction. Recent methods have exploited the nonlocal
self-similarity (NSS) of images by imposing nonlocal low-rankness of similar
patches to achieve a superior performance. To fully utilize both the NSS in
Magnetic resonance (MR) images and calibration consistency in the k-space
domain, we propose a nonlocal low-rank (NLR)-SPIRiT model by incorporating NLR
regularization into the SPIRiT model. We apply the weighted nuclear norm (WNN)
as a surrogate of the rank and employ the Nash equilibrium (NE) formulation and
alternating direction method of multipliers (ADMM) to efficiently solve the
NLR-SPIRiT model. The experimental results demonstrate the superior performance
of NLR-SPIRiT over the state-of-the-art methods via three objective metrics and
visual comparison.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiqing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1">Kai Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1">Bo Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yingkai Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Baocai Yin</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04521">
<div class="article-summary-box-inner">
<span><p>Jointly exploiting multiple different yet complementary domain information
has been proven to be an effective way to perform robust object tracking. This
paper focuses on effectively representing and utilizing complementary features
from the frame domain and event domain for boosting object tracking performance
in challenge scenarios. Specifically, we propose Common Features Extractor
(CFE) to learn potential common representations from the RGB domain and event
domain. For learning the unique features of the two domains, we utilize a
Unique Extractor for Event (UEE) based on Spiking Neural Networks to extract
edge cues in the event domain which may be missed in RGB in some challenging
conditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional
Neural Networks to extract texture and semantic information in RGB domain.
Extensive experiments on standard RGB benchmark and real event tracking dataset
demonstrate the effectiveness of the proposed approach. We show our approach
outperforms all compared state-of-the-art tracking algorithms and verify
event-based data is a powerful cue for tracking in challenging scenes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Multigranular Visual-Semantic Embedding for Cloth-Changing Person Re-identification.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Hongwei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1">Weili Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1">Weizhi Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Meng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Meng Wang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04527">
<div class="article-summary-box-inner">
<span><p>Person reidentification (ReID) is a very hot research topic in machine
learning and computer vision, and many person ReID approaches have been
proposed; however, most of these methods assume that the same person has the
same clothes within a short time interval, and thus their visual appearance
must be similar. However, in an actual surveillance environment, a given person
has a great probability of changing clothes after a long time span, and they
also often take different personal belongings with them. When the existing
person ReID methods are applied in this type of case, almost all of them fail.
To date, only a few works have focused on the cloth-changing person ReID task,
but since it is very difficult to extract generalized and robust features for
representing people with different clothes, their performances need to be
improved. Moreover, visual-semantic information is often ignored. To solve
these issues, in this work, a novel multigranular visual-semantic embedding
algorithm (MVSE) is proposed for cloth-changing person ReID, where visual
semantic information and human attributes are embedded into the network, and
the generalized features of human appearance can be well learned to effectively
solve the problem of clothing changes. Specifically, to fully represent a
person with clothing changes, a multigranular feature representation scheme
(MGR) is employed to focus on the unchanged part of the human, and then a cloth
desensitization network (CDN) is designed to improve the feature robustness of
the approach for the person with different clothing, where different high-level
human attributes are fully utilized. Moreover, to further solve the issue of
pose changes and occlusion under different camera perspectives, a partially
semantically aligned network (PSA) is proposed to obtain the visual-semantic
information that is used to align the human attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Hand Pose Classification Based on Neural Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1">Rashmi Bakshi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04529">
<div class="article-summary-box-inner">
<span><p>In this work, deep learning models are applied to a segment of a robust
hand-washing dataset that has been created with the help of 30 volunteers. This
work demonstrates the classification of presence of one hand, two hands and no
hand in the scene based on transfer learning. The pre-trained model; simplest
NN from Keras library is utilized to train the network with 704 images of hand
gestures and the predictions are carried out for the input image. Due to the
controlled and restricted dataset, 100% accuracy is achieved during the
training with correct predictions for the input image. Complete handwashing
dataset with dense models such as AlexNet for video classification for hand
hygiene stages will be used in the future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Jeong_B/0/1/0/all/0/1">Boseung Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jicheol Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1">Suha Kwak</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04533">
<div class="article-summary-box-inner">
<span><p>Attribute-based person search is the task of finding person images that are
best matched with a set of text attributes given as query. The main challenge
of this task is the large modality gap between attributes and images. To reduce
the gap, we present a new loss for learning cross-modal embeddings in the
context of attribute-based person search. We regard a set of attributes as a
category of people sharing the same traits. In a joint embedding space of the
two modalities, our loss pulls images close to their person categories for
modality alignment. More importantly, it pushes apart a pair of person
categories by a margin determined adaptively by their semantic distance, where
the distance metric is learned end-to-end so that the loss considers importance
of each attribute when relating person categories. Our loss guided by the
adaptive semantic margin leads to more discriminative and semantically
well-arranged distributions of person images. As a consequence, it enables a
simple embedding model to achieve state-of-the-art records on public benchmarks
without bells and whistles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tailin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Desen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuming He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1">Errui Ding</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04536">
<div class="article-summary-box-inner">
<span><p>The task of skeleton-based action recognition remains a core challenge in
human-centred scene understanding due to the multiple granularities and large
variation in human motion. Existing approaches typically employ a single neural
representation for different motion patterns, which has difficulty in capturing
fine-grained action classes given limited training data. To address the
aforementioned problems, we propose a novel multi-granular spatio-temporal
graph network for skeleton-based action classification that jointly models the
coarse- and fine-grained skeleton motion patterns. To this end, we develop a
dual-head graph network consisting of two interleaved branches, which enables
us to extract features at two spatio-temporal resolutions in an effective and
efficient manner. Moreover, our network utilises a cross-head communication
strategy to mutually enhance the representations of both heads. We conducted
extensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU
RGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance
on all the benchmarks, which validates the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ TrUMAn: Trope Understanding in Movies and Animations.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hung-Ting Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1">Po-Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_B/0/1/0/all/0/1">Bing-Chen Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wen-Feng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke-Jyun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1">Winston H. Hsu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04542">
<div class="article-summary-box-inner">
<span><p>Understanding and comprehending video content is crucial for many real-world
applications such as search and recommendation systems. While recent progress
of deep learning has boosted performance on various tasks using visual cues,
deep cognition to reason intentions, motivation, or causality remains
challenging. Existing datasets that aim to examine video reasoning capability
focus on visual signals such as actions, objects, relations, or could be
answered utilizing text bias. Observing this, we propose a novel task, along
with a new dataset: Trope Understanding in Movies and Animations (TrUMAn),
intending to evaluate and develop learning systems beyond visual signals.
Tropes are frequently used storytelling devices for creative works. By coping
with the trope understanding task and enabling the deep cognition skills of
machines, we are optimistic that data mining applications and algorithms could
be taken to the next level. To tackle the challenging TrUMAn dataset, we
present a Trope Understanding and Storytelling (TrUSt) with a new Conceptual
Storyteller module, which guides the video encoder by performing video
storytelling on a latent space. The generated story embedding is then fed into
the trope understanding model to provide further signals. Experimental results
demonstrate that state-of-the-art learning systems on existing tasks reach only
12.01% of accuracy with raw input signals. Also, even in the oracle case with
human-annotated descriptions, BERT contextual embedding achieves at most 28% of
accuracy. Our proposed TrUSt boosts the model performance and reaches 13.94%
performance. We also provide detailed analysis topave the way for future
research. TrUMAn is publicly available
at:https://www.cmlab.csie.ntu.edu.tw/project/trope
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1">Andreas Maier</a>, <a href="http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1">Harald K&#xf6;stler</a>, <a href="http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1">Marco Heisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1">Patrick Krauss</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Seung Hee Yang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04543">
<div class="article-summary-box-inner">
<span><p>In this article, we perform a review of the state-of-the-art of hybrid
machine learning in medical imaging. We start with a short summary of the
general developments of the past in machine learning and how general and
specialized approaches have been in competition in the past decades. A
particular focus will be the theoretical and experimental evidence pro and
contra hybrid modelling. Next, we inspect several new developments regarding
hybrid machine learning with a particular focus on so-called known operator
learning and how hybrid approaches gain more and more momentum across
essentially all applications in medical imaging and medical image analysis. As
we will point out by numerous examples, hybrid models are taking over in image
reconstruction and analysis. Even domains such as physical simulation and
scanner and acquisition design are being addressed using machine learning grey
box modelling approaches. Towards the end of the article, we will investigate a
few future directions and point out relevant areas in which hybrid modelling,
meta learning, and other domains will likely be able to drive the
state-of-the-art ahead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Instance-wise Hard Negative Example Generation for <span class="highlight-title">Contrastive Learning</span> in Unpaired Image-to-Image Translation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weilun Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Wengang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1">Jianmin Bao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Houqiang Li</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04547">
<div class="article-summary-box-inner">
<span><p>Contrastive learning shows great potential in unpaired image-to-image
translation, but sometimes the translated results are in poor quality and the
contents are not preserved consistently. In this paper, we uncover that the
negative examples play a critical role in the performance of contrastive
learning for image translation. The negative examples in previous methods are
randomly sampled from the patches of different positions in the source image,
which are not effective to push the positive examples close to the query
examples. To address this issue, we present instance-wise hard Negative Example
Generation for Contrastive learning in Unpaired image-to-image
Translation~(NEGCUT). Specifically, we train a generator to produce negative
examples online. The generator is novel from two perspectives: 1) it is
instance-wise which means that the generated examples are based on the input
image, and 2) it can generate hard negative examples since it is trained with
an adversarial loss. With the generator, the performance of unpaired
image-to-image translation is significantly improved. Experiments on three
benchmark datasets demonstrate that the proposed NEGCUT framework achieves
state-of-the-art performance compared to previous methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Ralekar_C/0/1/0/all/0/1">Chetan Ralekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1">Shubham Choudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1">Tapan Kumar Gandhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1">Santanu Chaudhury</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04558">
<div class="article-summary-box-inner">
<span><p>Human observers engage in selective information uptake when classifying
visual patterns. The same is true of deep neural networks, which currently
constitute the best performing artificial vision systems. Our goal is to
examine the congruence, or lack thereof, in the information-gathering
strategies of the two systems. We have operationalized our investigation as a
character recognition task. We have used eye-tracking to assay the spatial
distribution of information hotspots for humans via fixation maps and an
activation mapping technique for obtaining analogous distributions for deep
networks through visualization maps. Qualitative comparison between
visualization maps and fixation maps reveals an interesting correlate of
congruence. The deep learning model considered similar regions in character,
which humans have fixated in the case of correctly classified characters. On
the other hand, when the focused regions are different for humans and deep
nets, the characters are typically misclassified by the latter. Hence, we
propose to use the visual fixation maps obtained from the eye-tracking
experiment as a supervisory input to align the model's focus on relevant
character regions. We find that such supervision improves the model's
performance significantly and does not require any additional parameters. This
approach has the potential to find applications in diverse domains such as
medical analysis and surveillance in which explainability helps to determine
system fidelity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Deep Metric Learning for Open World Semantic Segmentation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1">Jun Cen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_P/0/1/0/all/0/1">Peng Yun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Junhao Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Michael Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Ming Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04562">
<div class="article-summary-box-inner">
<span><p>Classical close-set semantic segmentation networks have limited ability to
detect out-of-distribution (OOD) objects, which is important for
safety-critical applications such as autonomous driving. Incrementally learning
these OOD objects with few annotations is an ideal way to enlarge the knowledge
base of the deep learning models. In this paper, we propose an open world
semantic segmentation system that includes two modules: (1) an open-set
semantic segmentation module to detect both in-distribution and OOD objects.
(2) an incremental few-shot learning module to gradually incorporate those OOD
objects into its existing knowledge base. This open world semantic segmentation
system behaves like a human being, which is able to identify OOD objects and
gradually learn them with corresponding supervision. We adopt the Deep Metric
Learning Network (DMLNet) with contrastive clustering to implement open-set
semantic segmentation. Compared to other open-set semantic segmentation
methods, our DMLNet achieves state-of-the-art performance on three challenging
open-set semantic segmentation datasets without using additional data or
generative models. On this basis, two incremental few-shot learning methods are
further proposed to progressively improve the DMLNet with the annotations of
OOD objects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1">NareshKumar Gurulingan</a>, <a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1">Elahe Arani</a>, <a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1">Bahram Zonooz</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04584">
<div class="article-summary-box-inner">
<span><p>Scene understanding is crucial for autonomous systems which intend to operate
in the real world. Single task vision networks extract information only based
on some aspects of the scene. In multi-task learning (MTL), on the other hand,
these single tasks are jointly learned, thereby providing an opportunity for
tasks to share information and obtain a more comprehensive understanding. To
this end, we develop UniNet, a unified scene understanding network that
accurately and efficiently infers vital vision tasks including object
detection, semantic segmentation, instance segmentation, monocular depth
estimation, and monocular instance depth prediction. As these tasks look at
different semantic and geometric information, they can either complement or
conflict with each other. Therefore, understanding inter-task relationships can
provide useful cues to enable complementary information sharing. We evaluate
the task relationships in UniNet through the lens of adversarial attacks based
on the notion that they can exploit learned biases and task interactions in the
neural network. Extensive experiments on the Cityscapes dataset, using
untargeted and targeted attacks reveal that semantic tasks strongly interact
amongst themselves, and the same holds for geometric tasks. Additionally, we
show that the relationship between semantic and geometric tasks is asymmetric
and their interaction becomes weaker as we move towards higher-level
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for Autonomous Driving.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kemiao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1">Qi Hao</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04602">
<div class="article-summary-box-inner">
<span><p>Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results
of object detection, affinity computation and data association in real time.
This paper presents an efficient multi-modal MOT framework with online joint
detection and tracking schemes and robust data association for autonomous
driving applications. The novelty of this work includes: (1) development of an
end-to-end deep neural network for joint object detection and correlation using
2D and 3D measurements; (2) development of a robust affinity computation module
to compute occlusion-aware appearance and motion affinities in 3D space; (3)
development of a comprehensive data association module for joint optimization
among detection confidences, affinities and start-end probabilities. The
experiment results on the KITTI tracking benchmark demonstrate the superior
performance of the proposed method in terms of both tracking accuracy and
processing speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1">Yongkang Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1">Mohan Kankanhalli</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04603">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel model for recognizing images with composite
attribute-object concepts, notably for composite concepts that are unseen
during model training. We aim to explore the three key properties required by
the task --- relation-aware, consistent, and decoupled --- to learn rich and
robust features for primitive concepts that compose attribute-object pairs. To
this end, we propose the Blocked Message Passing Network (BMP-Net). The model
consists of two modules. The concept module generates semantically meaningful
features for primitive concepts, whereas the visual module extracts visual
features for attributes and objects from input images. A message passing
mechanism is used in the concept module to capture the relations between
primitive concepts. Furthermore, to prevent the model from being biased towards
seen composite concepts and reduce the entanglement between attributes and
objects, we propose a blocking mechanism that equalizes the information
available to the model for both seen and unseen concepts. Extensive experiments
and ablation studies on two benchmarks show the efficacy of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ White blood cell subtype detection and classification.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Praveen_N/0/1/0/all/0/1">Nalla Praveen</a>, <a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1">Narinder Singh Punn</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1">Sanjay Kumar Sonbhadra</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Sonali Agarwal</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04614">
<div class="article-summary-box-inner">
<span><p>Machine learning has endless applications in the health care industry. White
blood cell classification is one of the interesting and promising area of
research. The classification of the white blood cells plays an important part
in the medical diagnosis. In practise white blood cell classification is
performed by the haematologist by taking a small smear of blood and careful
examination under the microscope. The current procedures to identify the white
blood cell subtype is more time taking and error-prone. The computer aided
detection and diagnosis of the white blood cells tend to avoid the human error
and reduce the time taken to classify the white blood cells. In the recent
years several deep learning approaches have been developed in the context of
classification of the white blood cells that are able to identify but are
unable to localize the positions of white blood cells in the blood cell image.
Following this, the present research proposes to utilize YOLOv3 object
detection technique to localize and classify the white blood cells with
bounding boxes. With exhaustive experimental analysis, the proposed work is
found to detect the white blood cell with 99.2% accuracy and classify with 90%
accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Learning Canonical 3D Object Representation for Fine-Grained Recognition.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Joung_S/0/1/0/all/0/1">Sunghun Joung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seungryong Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1">Minsu Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1">Ig-Jae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kwanghoon Sohn</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04628">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework for fine-grained object recognition that learns
to recover object variation in 3D space from a single image, trained on an
image collection without using any ground-truth 3D annotation. We accomplish
this by representing an object as a composition of 3D shape and its appearance,
while eliminating the effect of camera viewpoint, in a canonical configuration.
Unlike conventional methods modeling spatial variation in 2D images only, our
method is capable of reconfiguring the appearance feature in a canonical 3D
space, thus enabling the subsequent object classifier to be invariant under 3D
geometric variation. Our representation also allows us to go beyond existing
methods, by incorporating 3D shape variation as an additional cue for object
recognition. To learn the model without ground-truth 3D annotation, we deploy a
differentiable renderer in an analysis-by-synthesis framework. By incorporating
3D shape and appearance jointly in a deep representation, our method learns the
discriminative representation of the object and achieves competitive
performance on fine-grained image recognition and vehicle re-identification. We
also demonstrate that the performance of 3D shape reconstruction is improved by
learning fine-grained shape deformation in a boosting manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via Multi-Scale Feature Decoupling Network.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1">Qiang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1">Weiqing Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1">Sujuan Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yuanjie Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shuqiang Jiang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04644">
<div class="article-summary-box-inner">
<span><p>Food logo detection plays an important role in the multimedia for its wide
real-world applications, such as food recommendation of the self-service shop
and infringement detection on e-commerce platforms. A large-scale food logo
dataset is urgently needed for developing advanced food logo detection
algorithms. However, there are no available food logo datasets with food brand
information. To support efforts towards food logo detection, we introduce the
dataset FoodLogoDet-1500, a new large-scale publicly available food logo
dataset, which has 1,500 categories, about 100,000 images and about 150,000
manually annotated food logo objects. We describe the collection and annotation
process of FoodLogoDet-1500, analyze its scale and diversity, and compare it
with other logo datasets. To the best of our knowledge, FoodLogoDet-1500 is the
first largest publicly available high-quality dataset for food logo detection.
The challenge of food logo detection lies in the large-scale categories and
similarities between food logo categories. For that, we propose a novel food
logo detection method Multi-scale Feature Decoupling Network (MFDNet), which
decouples classification and regression into two branches and focuses on the
classification branch to solve the problem of distinguishing multiple food logo
categories. Specifically, we introduce the feature offset module, which
utilizes the deformation-learning for optimal classification offset and can
effectively obtain the most representative features of classification in
detection. In addition, we adopt a balanced feature pyramid in MFDNet, which
pays attention to global information, balances the multi-scale feature maps,
and enhances feature extraction capability. Comprehensive experiments on
FoodLogoDet-1500 and other two benchmark logo datasets demonstrate the
effectiveness of the proposed method. The FoodLogoDet-1500 can be found at this
https URL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1">Jesper Kers</a>, <a href="http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1">Clarissa A. Cassol</a>, <a href="http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1">Joris J. Roelofs</a>, <a href="http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1">Najia Idrees</a>, <a href="http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1">Alik Farber</a>, <a href="http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1">Samir Haroon</a>, <a href="http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1">Kevin P. Daly</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1">Suvranu Ganguli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1">Vipul C. Chitalia</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1">Vijaya B. Kolachalama</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04658">
<div class="article-summary-box-inner">
<span><p>Development of deep learning systems for biomedical segmentation often
requires access to expert-driven, manually annotated datasets. If more than a
single expert is involved in the annotation of the same images, then the
inter-expert agreement is not necessarily perfect, and no single expert
annotation can precisely capture the so-called ground truth of the regions of
interest on all images. Also, it is not trivial to generate a reference
estimate using annotations from multiple experts. Here we present a deep neural
network, defined as U-Net-and-a-half, which can simultaneously learn from
annotations performed by multiple experts on the same set of images.
U-Net-and-a-half contains a convolutional encoder to generate features from the
input images, multiple decoders that allow simultaneous learning from image
masks obtained from annotations that were independently generated by multiple
experts, and a shared low-dimensional feature space. To demonstrate the
applicability of our framework, we used two distinct datasets from digital
pathology and radiology, respectively. Specifically, we trained two separate
models using pathologist-driven annotations of glomeruli on whole slide images
of human kidney biopsies (10 patients), and radiologist-driven annotations of
lumen cross-sections of human arteriovenous fistulae obtained from
intravascular ultrasound images (10 patients), respectively. The models based
on U-Net-and-a-half exceeded the performance of the traditional U-Net models
trained on single expert annotations alone, thus expanding the scope of
multitask learning in the context of biomedical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Deep Learning for Breast Cancer Classification: Enhanced Tangent Function.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/eess/1/au:+Thapa_A/0/1/0/all/0/1">Ashu Thapa</a>, <a href="http://arxiv.org/find/eess/1/au:+Alsadoon_A/0/1/0/all/0/1">Abeer Alsadoon</a>, <a href="http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1">P.W.C. Prasad</a>, <a href="http://arxiv.org/find/eess/1/au:+Bajaj_S/0/1/0/all/0/1">Simi Bajaj</a>, <a href="http://arxiv.org/find/eess/1/au:+Alsadoon_O/0/1/0/all/0/1">Omar Hisham Alsadoon</a>, <a href="http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1">Tarik A. Rashid</a>, <a href="http://arxiv.org/find/eess/1/au:+Ali_R/0/1/0/all/0/1">Rasha S. Ali</a>, <a href="http://arxiv.org/find/eess/1/au:+Jerew_O/0/1/0/all/0/1">Oday D. Jerew</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04663">
<div class="article-summary-box-inner">
<span><p>Background and Aim: Recently, deep learning using convolutional neural
network has been used successfully to classify the images of breast cells
accurately. However, the accuracy of manual classification of those
histopathological images is comparatively low. This research aims to increase
the accuracy of the classification of breast cancer images by utilizing a
Patch-Based Classifier (PBC) along with deep learning architecture.
Methodology: The proposed system consists of a Deep Convolutional Neural
Network (DCNN) that helps in enhancing and increasing the accuracy of the
classification process. This is done by the use of the Patch-based Classifier
(PBC). CNN has completely different layers where images are first fed through
convolutional layers using hyperbolic tangent function together with the
max-pooling layer, drop out layers, and SoftMax function for classification.
Further, the output obtained is fed to a patch-based classifier that consists
of patch-wise classification output followed by majority voting. Results: The
results are obtained throughout the classification stage for breast cancer
images that are collected from breast-histology datasets. The proposed solution
improves the accuracy of classification whether or not the images had normal,
benign, in-situ, or invasive carcinoma from 87% to 94% with a decrease in
processing time from 0.45 s to 0.2s on average. Conclusion: The proposed
solution focused on increasing the accuracy of classifying cancer in the breast
by enhancing the image contrast and reducing the vanishing gradient. Finally,
this solution for the implementation of the Contrast Limited Adaptive Histogram
Equalization (CLAHE) technique and modified tangent function helps in
increasing the accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Multi-Camera Trajectory Forecasting with Trajectory Tensors.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Styles_O/0/1/0/all/0/1">Olly Styles</a>, <a href="http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1">Tanaya Guha</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1">Victor Sanchez</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04694">
<div class="article-summary-box-inner">
<span><p>We introduce the problem of multi-camera trajectory forecasting (MCTF), which
involves predicting the trajectory of a moving object across a network of
cameras. While multi-camera setups are widespread for applications such as
surveillance and traffic monitoring, existing trajectory forecasting methods
typically focus on single-camera trajectory forecasting (SCTF), limiting their
use for such applications. Furthermore, using a single camera limits the
field-of-view available, making long-term trajectory forecasting impossible. We
address these shortcomings of SCTF by developing an MCTF framework that
simultaneously uses all estimated relative object locations from several
viewpoints and predicts the object's future location in all possible
viewpoints. Our framework follows a Which-When-Where approach that predicts in
which camera(s) the objects appear and when and where within the camera views
they appear. To this end, we propose the concept of trajectory tensors: a new
technique to encode trajectories across multiple camera views and the
associated uncertainties. We develop several encoder-decoder MCTF models for
trajectory tensors and present extensive experiments on our own database
(comprising 600 hours of video data from 15 camera views) created particularly
for the MCTF task. Results show that our trajectory tensor models outperform
coordinate trajectory-based MCTF models and existing SCTF methods adapted for
MCTF. Code is available from: https://github.com/olly-styles/Trajectory-Tensors
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ BIDCD - Bosch Industrial Depth Completion Dataset.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Botach_A/0/1/0/all/0/1">Adam Botach</a>, <a href="http://arxiv.org/find/cs/1/au:+Feldman_Y/0/1/0/all/0/1">Yuri Feldman</a>, <a href="http://arxiv.org/find/cs/1/au:+Miron_Y/0/1/0/all/0/1">Yakov Miron</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_Y/0/1/0/all/0/1">Yoel Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1">Dotan Di Castro</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04706">
<div class="article-summary-box-inner">
<span><p>We introduce BIDCD - the Bosch Industrial Depth Completion Dataset. BIDCD is
a new RGBD dataset of metallic industrial objects, collected with a depth
camera mounted on a robotic manipulator. The main purpose of this dataset is to
facilitate the training of domain-specific depth completion models, to be used
in logistics and manufacturing tasks. We trained a State-of-the-Art depth
completion model on this dataset, and report the results, setting an initial
benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chaoda Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiantao Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weibing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04728">
<div class="article-summary-box-inner">
<span><p>Current 3D single object tracking approaches track the target based on a
feature comparison between the target template and the search area. However,
due to the common occlusion in LiDAR scans, it is non-trivial to conduct
accurate feature comparisons on severe sparse and incomplete shapes. In this
work, we exploit the ground truth bounding box given in the first frame as a
strong cue to enhance the feature description of the target object, enabling a
more accurate feature comparison in a simple yet effective way. In particular,
we first propose the BoxCloud, an informative and robust representation, to
depict an object using the point-to-box relation. We further design an
efficient box-aware feature fusion module, which leverages the aforementioned
BoxCloud for reliable feature matching and embedding. Integrating the proposed
general components into an existing model P2B, we construct a superior
box-aware tracker (BAT). Experiments confirm that our proposed BAT outperforms
the previous state-of-the-art by a large margin on both KITTI and NuScenes
benchmarks, achieving a 12.8% improvement in terms of precision while running
~20% faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Semantics-STGCNN: A Semantics-guided Spatial-Temporal Graph Convolutional Network for Multi-class Trajectory Prediction.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Rainbow_B/0/1/0/all/0/1">Ben A. Rainbow</a>, <a href="http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1">Qianhui Men</a>, <a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1">Hubert P. H. Shum</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04740">
<div class="article-summary-box-inner">
<span><p>Predicting the movement trajectories of multiple classes of road users in
real-world scenarios is a challenging task due to the diverse trajectory
patterns. While recent works of pedestrian trajectory prediction successfully
modelled the influence of surrounding neighbours based on the relative
distances, they are ineffective on multi-class trajectory prediction. This is
because they ignore the impact of the implicit correlations between different
types of road users on the trajectory to be predicted - for example, a nearby
pedestrian has a different level of influence from a nearby car. In this paper,
we propose to introduce class information into a graph convolutional neural
network to better predict the trajectory of an individual. We embed the class
labels of the surrounding objects into the label adjacency matrix (LAM), which
is combined with the velocity-based adjacency matrix (VAM) comprised of the
objects' velocity, thereby generating a semantics-guided graph adjacency (SAM).
SAM effectively models semantic information with trainable parameters to
automatically learn the embedded label features that will contribute to the
fixed velocity-based trajectory. Such information of spatial and temporal
dependencies is passed to a graph convolutional and temporal convolutional
network to estimate the predicted trajectory distributions. We further propose
new metrics, known as Average2 Displacement Error (aADE) and Average Final
Displacement Error (aFDE), that assess network accuracy more accurately. We
call our framework Semantics-STGCNN. It consistently shows superior performance
to the state-of-the-arts in existing and the newly proposed metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ SUNet: Symmetric Undistortion Network for Rolling Shutter Correction.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1">Bin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yuchao Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1">Mingyi He</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04775">
<div class="article-summary-box-inner">
<span><p>The vast majority of modern consumer-grade cameras employ a rolling shutter
mechanism, leading to image distortions if the camera moves during image
acquisition. In this paper, we present a novel deep network to solve the
generic rolling shutter correction problem with two consecutive frames. Our
pipeline is symmetrically designed to predict the global shutter image
corresponding to the intermediate time of these two frames, which is difficult
for existing methods because it corresponds to a camera pose that differs most
from the two frames. First, two time-symmetric dense undistortion flows are
estimated by using well-established principles: pyramidal construction,
warping, and cost volume processing. Then, both rolling shutter images are
warped into a common global shutter one in the feature space, respectively.
Finally, a symmetric consistency constraint is constructed in the image decoder
to effectively aggregate the contextual cues of two rolling shutter images,
thereby recovering the high-quality global shutter image. Extensive experiments
with both synthetic and real data from public benchmarks demonstrate the
superiority of our proposed approach over the state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> Meta-repository of screening mammography classifiers.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1">Benjamin Stadnick</a>, <a href="http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1">Jan Witowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1">Vishwaesh Rajiv</a>, <a href="http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1">Jakub Ch&#x142;&#x119;dowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1">Farah E. Shamout</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"><span class="highlight-author">Kyunghyun Cho</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1">Krzysztof J. Geras</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04800">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) is transforming medicine and showing promise in
improving clinical diagnosis. In breast cancer screening, several recent
studies show that AI has the potential to improve radiologists' accuracy,
subsequently helping in early cancer diagnosis and reducing unnecessary workup.
As the number of proposed models and their complexity grows, it is becoming
increasingly difficult to re-implement them in order to reproduce the results
and to compare different approaches. To enable reproducibility of research in
this application area and to enable comparison between different methods, we
release a meta-repository containing deep learning models for classification of
screening mammograms. This meta-repository creates a framework that enables the
evaluation of machine learning models on any private or public screening
mammography data set. At its inception, our meta-repository contains five
state-of-the-art models with open-source implementations and cross-platform
compatibility. We compare their performance on five international data sets:
two private New York University breast cancer screening data sets as well as
three public (DDSM, INbreast and Chinese Mammography Database) data sets. Our
framework has a flexible design that can be generalized to other medical image
analysis tasks. The meta-repository is available at
https://www.github.com/nyukat/mammography_metarepository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ R4Dyn: Exploring Radar for <span class="highlight-title">Self-Supervised</span> Monocular Depth Estimation of Dynamic Scenes.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1">Stefano Gasperini</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1">Patrick Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1">Vinzenz Dallabetta</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1">Benjamin Busam</a>, <a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1">Federico Tombari</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04814">
<div class="article-summary-box-inner">
<span><p>While self-supervised monocular depth estimation in driving scenarios has
achieved comparable performance to supervised approaches, violations of the
static world assumption can still lead to erroneous depth predictions of
traffic participants, posing a potential safety issue. In this paper, we
present R4Dyn, a novel set of techniques to use cost-efficient radar data on
top of a self-supervised depth estimation framework. In particular, we show how
radar can be used during training as weak supervision signal, as well as an
extra input to enhance the estimation robustness at inference time. Since
automotive radars are readily available, this allows to collect training data
from a variety of existing vehicles. Moreover, by filtering and expanding the
signal to make it compatible with learning-based approaches, we address radar
inherent issues, such as noise and sparsity. With R4Dyn we are able to overcome
a major limitation of self-supervised depth estimation, i.e. the prediction of
traffic participants. We substantially improve the estimation on dynamic
objects, such as cars by 37% on the challenging nuScenes dataset, hence
demonstrating that radar is a valuable additional sensor for monocular depth
estimation in autonomous vehicles. Additionally, we plan on making the code
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ The Effect of the Loss on Generalization: Empirical Study on Synthetic Lung Nodule Data.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1">Vasileios Baltatzis</a>, <a href="http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1">Loic Le Folgoc</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1">Sam Ellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1">Octavio E. Martinez Manzanera</a>, <a href="http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1">Kyriaki-Margarita Bintsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1">Arjun Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1">Sujal Desai</a>, <a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1">Ben Glocker</a>, <a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1">Julia A. Schnabel</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04815">
<div class="article-summary-box-inner">
<span><p>Convolutional Neural Networks (CNNs) are widely used for image classification
in a variety of fields, including medical imaging. While most studies deploy
cross-entropy as the loss function in such tasks, a growing number of
approaches have turned to a family of contrastive learning-based losses. Even
though performance metrics such as accuracy, sensitivity and specificity are
regularly used for the evaluation of CNN classifiers, the features that these
classifiers actually learn are rarely identified and their effect on the
classification performance on out-of-distribution test samples is
insufficiently explored. In this paper, motivated by the real-world task of
lung nodule classification, we investigate the features that a CNN learns when
trained and tested on different distributions of a synthetic dataset with
controlled modes of variation. We show that different loss functions lead to
different features being learned and consequently affect the generalization
ability of the classifier on unseen data. This study provides some important
insights into the design of deep learning solutions for medical imaging tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Facial Behavior Analysis using 4D Curvature Statistics for Presentation Attack Detection.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Thummel_M/0/1/0/all/0/1">Martin Th&#xfc;mmel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sickert_S/0/1/0/all/0/1">Sven Sickert</a>, <a href="http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1">Joachim Denzler</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.06056">
<div class="article-summary-box-inner">
<span><p>The human face has a high potential for biometric identification due to its
many individual traits. At the same time, such identification is vulnerable to
biometric copies. These presentation attacks pose a great challenge in
unsupervised authentication settings. As a countermeasure, we propose a method
that automatically analyzes the plausibility of facial behavior based on a
sequence of 3D face scans. A compact feature representation measures facial
behavior using the temporal curvature change. Finally, we train our method only
on genuine faces in an anomaly detection scenario. Our method can detect
presentation attacks using elastic 3D masks, bent photographs with eye holes,
and monitor replay-attacks. For evaluation, we recorded a challenging database
containing such cases using a high-quality 3D sensor. It features 109 4D face
scans including eleven different types of presentation attacks. We achieve
error rates of 11% and 6% for APCER and BPCER, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> ♻ Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">William B. Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengshu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1">Priya Kasimbeg</a>, <a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1">Micael Tchapmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1">Alexander Toshev</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"><span class="highlight-author">Li Fei-Fei</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.14442">
<div class="article-summary-box-inner">
<span><p>We present Interactive Gibson Benchmark, the first comprehensive benchmark
for training and evaluating Interactive Navigation: robot navigation strategies
where physical interaction with objects is allowed and even encouraged to
accomplish a task. For example, the robot can move objects if needed in order
to clear a path leading to the goal location. Our benchmark comprises two novel
elements: 1) a new experimental setup, the Interactive Gibson Environment
(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high
fidelity physical dynamics of the robot and common objects found in these
scenes; 2) a set of Interactive Navigation metrics which allows one to study
the interplay between navigation and physical interaction. We present and
evaluate multiple learning-based baselines in Interactive Gibson, and provide
insights into regimes of navigation with different trade-offs between
navigation path efficiency and disturbance of surrounding objects. We make our
benchmark publicly
available(https://sites.google.com/view/interactivegibsonenv) and encourage
researchers from all disciplines in robotics (e.g. planning, learning, control)
to propose, evaluate, and compare their Interactive Navigation solutions in
Interactive Gibson.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1">Dongliang Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Yifeng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jiyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1">Ayan Kumar Bhunia</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaoxu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhanyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Ming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jun Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yi-Zhe Song</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.04264">
<div class="article-summary-box-inner">
<span><p>Key for solving fine-grained image categorization is finding discriminate and
local regions that correspond to subtle visual traits. Great strides have been
made, with complex networks designed specifically to learn part-level
discriminate feature representations. In this paper, we show it is possible to
cultivate subtle details without the need for overly complicated network
designs or training mechanisms -- a single loss is all it takes. The main trick
lies with how we delve into individual feature channels early on, as opposed to
the convention of starting from a consolidated feature map. The proposed loss
function, termed as mutual-channel loss (MC-Loss), consists of two
channel-specific components: a discriminality component and a diversity
component. The discriminality component forces all feature channels belonging
to the same class to be discriminative, through a novel channel-wise attention
mechanism. The diversity component additionally constraints channels so that
they become mutually exclusive on spatial-wise. The end result is therefore a
set of feature channels that each reflects different locally discriminative
regions for a specific class. The MC-Loss can be trained end-to-end, without
the need for any bounding-box/part annotations, and yields highly
discriminative regions during inference. Experimental results show our MC-Loss
when implemented on top of common base networks can achieve state-of-the-art
performance on all four fine-grained categorization datasets (CUB-Birds,
FGVC-Aircraft, Flowers-102, and Stanford-Cars). Ablative studies further
demonstrate the superiority of MC-Loss when compared with other recently
proposed general-purpose losses for visual classification, on two different
base networks. Code available at
https://github.com/dongliangchang/Mutual-Channel-Loss
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Ziwen He</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1">Jing Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tieniu Tan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.09674">
<div class="article-summary-box-inner">
<span><p>Gait recognition is widely used in social security applications due to its
advantages in long-distance human identification. Recently, sequence-based
methods have achieved high accuracy by learning abundant temporal and spatial
information. However, their robustness under adversarial attacks has not been
clearly explored. In this paper, we demonstrate that the state-of-the-art gait
recognition model is vulnerable to such attacks. To this end, we propose a
novel temporal sparse adversarial attack method. Different from previous
additive noise models which add perturbations on original samples, we employ a
generative adversarial network based architecture to semantically generate
adversarial high-quality gait silhouettes or video frames. Moreover, by
sparsely substituting or inserting a few adversarial gait silhouettes, the
proposed method ensures its imperceptibility and achieves a high attack success
rate. The experimental results show that if only one-fortieth of the frames are
attacked, the accuracy of the target model drops dramatically.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ A Solution to Product detection in Densely Packed Scenes.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1">Tianze Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yanjia Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hongxiang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yichao Xiong</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.11946">
<div class="article-summary-box-inner">
<span><p>This work is a solution to densely packed scenes dataset SKU-110k. Our work
is modified from Cascade R-CNN. To solve the problem, we proposed a random crop
strategy to ensure both the sampling rate and input scale is relatively
sufficient as a contrast to the regular random crop. And we adopted some of
trick and optimized the hyper-parameters. To grasp the essential feature of the
densely packed scenes, we analysis the stages of a detector and investigate the
bottleneck which limits the performance. As a result, our method obtains 58.7
mAP on test set of SKU-110k.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1">Abhinav Sagar</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.07588">
<div class="article-summary-box-inner">
<span><p>Deep learning motivated by convolutional neural networks has been highly
successful in a range of medical imaging problems like image classification,
image segmentation, image synthesis etc. However for validation and
interpretability, not only do we need the predictions made by the model but
also how confident it is while making those predictions. This is important in
safety critical applications for the people to accept it. In this work, we used
an encoder decoder architecture based on variational inference techniques for
segmenting brain tumour images. We evaluate our work on the publicly available
BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over
Union (IOU) as the evaluation metrics. Our model is able to segment brain
tumours while taking into account both aleatoric uncertainty and epistemic
uncertainty in a principled bayesian manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Fast Search on Binary Codes by Weighted Hamming Distance.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1">Zhenyu Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuesheng Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruixin Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08591">
<div class="article-summary-box-inner">
<span><p>Weighted Hamming distance, as a similarity measure between binary codes and
binary queries, provides superior accuracy in search tasks than Hamming
distance. However, how to efficiently and accurately find $K$ binary codes that
have the smallest weighted Hamming distance to the query remains an open issue.
In this paper, a fast search algorithm is proposed to perform the
non-exhaustive search for $K$ nearest binary codes by weighted Hamming
distance. By using binary codes as direct bucket indices in a hash table, the
search algorithm generates a sequence to probe the buckets based on the
independence characteristic of the weights for each bit. Furthermore, a fast
search framework based on the proposed search algorithm is designed to solve
the problem of long binary codes. Specifically, long binary codes are split
into substrings and multiple hash tables are built on them. Then, the search
algorithm probes the buckets to obtain candidates according to the generated
substring indices, and a merging algorithm is proposed to find the nearest
binary codes by merging the candidates. Theoretical analysis and experimental
results demonstrate that the search algorithm improves the search accuracy
compared to other non-exhaustive algorithms and provides orders-of-magnitude
faster search than the linear scan baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Robust Data Hiding Using Inverse Gradient Attention.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Honglei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuanzhouhan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yidong Li</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10850">
<div class="article-summary-box-inner">
<span><p>Data hiding is the procedure of encoding desired information into an image to
resist potential noises while ensuring the embedded image has little perceptual
perturbations from the original image. Recently, with the tremendous successes
gained by deep neural networks in various fields, data hiding areas have
attracted increasing number of attentions. The neglect of considering the pixel
sensitivity within the cover image of deep neural methods will inevitably
affect the model robustness for information hiding. Targeting at the problem,
in this paper, we propose a novel deep data hiding scheme with Inverse Gradient
Attention (IGA), combing the ideas of adversarial learning and attention
mechanism to endow different sensitivity to different pixels. With the proposed
component, the model can spotlight pixels with more robustness for embedding
data. Empirically, extensive experiments show that the proposed model
outperforms the state-of-the-art methods on two prevalent datasets under
multiple settings. Besides, we further identify and discuss the connections
between the proposed inverse gradient attention and high-frequency regions
within images.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Cross-Camera Convolutional Color Constancy.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1">Mahmoud Afifi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1">Jonathan T. Barron</a>, <a href="http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1">Chloe LeGendre</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yun-Ta Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bleibel_F/0/1/0/all/0/1">Francois Bleibel</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11890">
<div class="article-summary-box-inner">
<span><p>We present "Cross-Camera Convolutional Color Constancy" (C5), a
learning-based method, trained on images from multiple cameras, that accurately
estimates a scene's illuminant color from raw images captured by a new camera
previously unseen during training. C5 is a hypernetwork-like extension of the
convolutional color constancy (CCC) approach: C5 learns to generate the weights
of a CCC model that is then evaluated on the input image, with the CCC weights
dynamically adapted to different input content. Unlike prior cross-camera color
constancy models, which are usually designed to be agnostic to the spectral
properties of test-set images from unobserved cameras, C5 approaches this
problem through the lens of transductive inference: additional unlabeled images
are provided as input to the model at test time, which allows the model to
calibrate itself to the spectral properties of the test-set camera during
inference. C5 achieves state-of-the-art accuracy for cross-camera color
constancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on
a GPU or CPU, respectively), and requires little memory (~2 MB), and, thus, is
a practical solution to the problem of calibration-free automatic white balance
for mobile photography.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Good and Bad Boundaries in Ultrasound Compounding: Preserving Anatomic Boundaries While Suppressing Artifacts.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1">Alex Ling Yu Hung</a>, <a href="http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1">John Galeotti</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11962">
<div class="article-summary-box-inner">
<span><p>Ultrasound 3D compounding is important for volumetric reconstruction, but as
of yet there is no consensus on best practices for compounding. Ultrasound
images depend on probe direction and the path sound waves pass through, so when
multiple intersecting B-scans of the same spot from different perspectives
yield different pixel values, there is not a single, ideal representation for
compounding (i.e. combining) the overlapping pixel values. Current popular
methods inevitably suppress or altogether leave out bright or dark regions that
are useful, and potentially introduce new artifacts. In this work, we establish
a new algorithm to compound the overlapping pixels from different view points
in ultrasound. We uniquely leverage Laplacian and Gaussian Pyramids to preserve
the maximum boundary contrast without overemphasizing noise and speckle. We
evaluate our algorithm by comparing ours with previous algorithms, and we show
that our approach not only preserves both light and dark details, but also
somewhat suppresses artifacts, rather than amplifying them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> ♻ iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Bokui Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengshu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Linxi Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guanzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href="http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1">Shyamal Buch</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1">Sanjana Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1">Lyne P. Tchapmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1">Micael E. Tchapmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1">Kent Vainio</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1">Josiah Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"><span class="highlight-author">Li Fei-Fei</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02924">
<div class="article-summary-box-inner">
<span><p>We present iGibson 1.0, a novel simulation environment to develop robotic
solutions for interactive tasks in large-scale realistic scenes. Our
environment contains 15 fully interactive home-sized scenes with 108 rooms
populated with rigid and articulated objects. The scenes are replicas of
real-world homes, with distribution and the layout of objects aligned to those
of the real world. iGibson 1.0 integrates several key features to facilitate
the study of interactive tasks: i) generation of high-quality virtual sensor
signals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain
randomization to change the materials of the objects (both visual and physical)
and/or their shapes, iii) integrated sampling-based motion planners to generate
collision-free trajectories for robot bases and arms, and iv) intuitive
human-iGibson interface that enables efficient collection of human
demonstrations. Through experiments, we show that the full interactivity of the
scenes enables agents to learn useful visual representations that accelerate
the training of downstream manipulation tasks. We also show that iGibson 1.0
features enable the generalization of navigation agents, and that the
human-iGibson interface and integrated motion planners facilitate efficient
imitation learning of human demonstrated (mobile) manipulation behaviors.
iGibson 1.0 is open-source, equipped with comprehensive examples and
documentation. For more information, visit our project website:
<a href="http://svl.stanford.edu/igibson/">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> ♻ INeRF: Inverting Neural Radiance Fields for Pose Estimation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1">Lin Yen-Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1">Pete Florence</a>, <a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1">Jonathan T. Barron</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1">Alberto Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"><span class="highlight-author">Phillip Isola</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Tsung-Yi Lin</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05877">
<div class="article-summary-box-inner">
<span><p>We present iNeRF, a framework that performs mesh-free pose estimation by
"inverting" a Neural RadianceField (NeRF). NeRFs have been shown to be
remarkably effective for the task of view synthesis - synthesizing
photorealistic novel views of real-world scenes or objects. In this work, we
investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,
RGB-only 6DoF pose estimation - given an image, find the translation and
rotation of a camera relative to a 3D object or scene. Our method assumes that
no object mesh models are available during either training or test time.
Starting from an initial pose estimate, we use gradient descent to minimize the
residual between pixels rendered from a NeRF and pixels in an observed image.
In our experiments, we first study 1) how to sample rays during pose refinement
for iNeRF to collect informative gradients and 2) how different batch sizes of
rays affect iNeRF on a synthetic dataset. We then show that for complex
real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating
the camera poses of novel images and using these images as additional training
data for NeRF. Finally, we show iNeRF can perform category-level object pose
estimation, including object instances not seen during training, with RGB
images by inverting a NeRF model inferred from a single view.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> ♻ MiniVLM: A Smaller and Faster <span class="highlight-title">Vision-Language</span> Model.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaowei Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pengchuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiujun Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"><span class="highlight-author">Jianfeng Gao</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.06946">
<div class="article-summary-box-inner">
<span><p>Recent vision-language (VL) studies have shown remarkable progress by
learning generic representations from massive image-text pairs with transformer
models and then fine-tuning on downstream VL tasks. While existing research has
been focused on achieving high accuracy with large pre-trained models, building
a lightweight model is of great value in practice but is less explored. In this
paper, we propose a smaller and faster VL model, MiniVLM, which can be
finetuned with good performance on various downstream tasks like its larger
counterpart. MiniVLM consists of two modules, a vision feature extractor and a
transformer-based vision-language fusion module. We design a Two-stage
Efficient feature Extractor (TEE), inspired by the one-stage EfficientDet
network, to significantly reduce the time cost of visual feature extraction by
$95\%$, compared to a baseline model. We adopt the MiniLM structure to reduce
the computation cost of the transformer module after comparing different
compact BERT models. In addition, we improve the MiniVLM pre-training by adding
$7M$ Open Images data, which are pseudo-labeled by a state-of-the-art
captioning model. We also pre-train with high-quality image tags obtained from
a strong tagging model to enhance cross-modality alignment. The large models
are used offline without adding any overhead in fine-tuning and inference. With
the above design choices, our MiniVLM reduces the model size by $73\%$ and the
inference time cost by $94\%$ while being able to retain $94-97\%$ of the
accuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the
state-of-the-art VL research for on-the-edge applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ An Enhanced Prohibited Items Recognition Model.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1">Tianze Rong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Hongxiang Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yichao Xiong</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12256">
<div class="article-summary-box-inner">
<span><p>We proposed a new modeling method to promote the performance of prohibited
items recognition via X-ray image. We analyzed the characteristics of
prohibited items and X-ray images. We found the fact that the scales of some
items are too small to be recognized which encumber the model performance. Then
we adopted a set of data augmentation and modified the model to adapt the field
of prohibited items recognition. The Convolutional Block Attention Module(CBAM)
and rescoring mechanism has been assembled into the model. By the modification,
our model achieved a mAP of 89.9% on SIXray10, mAP of 74.8%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Embedded Knowledge <span class="highlight-title">Distillation</span> in Depth-Level Dynamic Neural Network.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Shuchang Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Ting-Bing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1">Guangliang Cheng</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00793">
<div class="article-summary-box-inner">
<span><p>In real applications, different computation-resource devices need
different-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,
existing methods either design multiple networks and train them independently,
or construct depth-level/width-level dynamic neural networks which is hard to
prove the accuracy of each sub-net. In this article, we propose an elegant
Depth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets
of similar architectures. To improve the generalization of sub-nets, we design
the Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to
implement knowledge transfer from the teacher (full-net) to multiple students
(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to
constrain the posterior class probability consistency between full-net and
sub-nets, and self-attention distillation on the same resolution feature of
different depth is addressed to drive more abundant feature representations of
sub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in
a DDNN via the online knowledge distillation in each training iteration without
extra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet
datasets demonstrate that sub-nets in DDNN with EKD training achieve better
performance than individually training networks while preserving the original
performance of full-nets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Aman Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1">Mayank Kothyari</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vishwajeet Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1">Preethi Jyothi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1">Ganesh Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1">Soumen Chakrabarti</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05568">
<div class="article-summary-box-inner">
<span><p>Multimodal IR, spanning text corpus, knowledge graph and images, called
outside knowledge visual question answering (OKVQA), is of much recent
interest. However, the popular data set has serious limitations. A surprisingly
large fraction of queries do not assess the ability to integrate cross-modal
information. Instead, some are independent of the image, some depend on
speculation, some require OCR or are otherwise answerable from the image alone.
To add to the above limitations, frequency-based guessing is very effective
because of (unintended) widespread answer overlaps between the train and test
folds. Overall, it is hard to determine when state-of-the-art systems exploit
these weaknesses rather than really infer the answers, because they are opaque
and their 'reasoning' process is uninterpretable. An equally important
limitation is that the dataset is designed for the quantitative assessment only
of the end-to-end answer retrieval task, with no provision for assessing the
correct(semantic) interpretation of the input query. In response, we identify a
key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and
build a new data set and challenge around it. Specifically, the questioner
identifies an entity in the image and asks a question involving that entity
which can be answered only by consulting a knowledge graph or corpus passage
mentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA
annotated based on the structural idiom and (ii)S3VQA, a new dataset built from
scratch. We also present a neural but structurally transparent OKVQA system,
S3, that explicitly addresses our challenge dataset, and outperforms recent
competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ VMAF And Variants: Towards A Unified VQA.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1">Pankaj Topiwala</a>, <a href="http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1">Wei Dai</a>, <a href="http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1">Jiangfeng Pian</a>, <a href="http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1">Katalina Biondi</a>, <a href="http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1">Arvind Krovvidi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07770">
<div class="article-summary-box-inner">
<span><p>Video quality assessment (VQA) is now a fastgrowing subject, beginning to
mature in the full reference (FR) case, while the burgeoning no reference (NR)
case remains challenging. We investigate variants of the popular VMAF video
quality assessment algorithm for the FR case, using support vector regression
and feedforward neural networks, and extend it to the NR case, using the same
learning architectures, to develop a partially unified framework for VQA. When
heavily trained, algorithms such as VMAF perform well on test datasets, with
90%+ match; but predicting performance in the wild is better done by
training/testing from scratch, as we do. Even from scratch, we achieve 90%+
performance in FR, with gains over VMAF. And we greatly reduce complexity vs.
leading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our
preliminary testing, we find the improvements in trainability, while also
constraining computational complexity, as quite encouraging, suggesting further
study and analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1">Xianbo Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1">Shunquan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiwu Huang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13689">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that non-additive image steganographic frameworks
effectively improve security performance through adjusting distortion
distribution. However, as far as we know, all of the existing non-additive
proposals are based on handcrafted policies, and can only be applied to a
specific image domain, which heavily prevent non-additive steganography from
releasing its full potentiality. In this paper, we propose an automatic
non-additive steganographic distortion learning framework called MCTSteg to
remove the above restrictions. Guided by the reinforcement learning paradigm,
we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental
model to build MCTSteg. MCTS makes sequential decisions to adjust distortion
distribution without human intervention. Our proposed environmental model is
used to obtain feedbacks from each decision. Due to its self-learning
characteristic and domain-independent reward function, MCTSteg has become the
first reported universal non-additive steganographic framework which can work
in both spatial and JPEG domains. Extensive experimental results show that
MCTSteg can effectively withstand the detection of both hand-crafted
feature-based and deep-learning-based steganalyzers. In both spatial and JPEG
domains, the security performance of MCTSteg steadily outperforms the state of
the art by a clear margin under different scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ RetrievalFuse: Neural 3D Scene Reconstruction with a Database.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Siddiqui_Y/0/1/0/all/0/1">Yawar Siddiqui</a>, <a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1">Justus Thies</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1">Fangchang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1">Qi Shan</a>, <a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1">Matthias Nie&#xdf;ner</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1">Angela Dai</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00024">
<div class="article-summary-box-inner">
<span><p>3D reconstruction of large scenes is a challenging problem due to the
high-complexity nature of the solution space, in particular for generative
neural networks. In contrast to traditional generative learned models which
encode the full generative process into a neural network and can struggle with
maintaining local details at the scene level, we introduce a new method that
directly leverages scene geometry from the training database. First, we learn
to synthesize an initial estimate for a 3D scene, constructed by retrieving a
top-k set of volumetric chunks from the scene database. These candidates are
then refined to a final scene generation with an attention-based refinement
that can effectively select the most consistent set of geometry from the
candidates and combine them together to create an output scene, facilitating
transfer of coherent structures and local detail from train scene geometry. We
demonstrate our neural scene reconstruction with a database for the tasks of 3D
super resolution and surface reconstruction from sparse point clouds, showing
that our approach enables generation of more coherent, accurate 3D scenes,
improving on average by over 8% in IoU over state-of-the-art scene
reconstruction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Poisoning the Unlabeled Dataset of Semi-Supervised Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1">Nicholas Carlini</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01622">
<div class="article-summary-box-inner">
<span><p>Semi-supervised machine learning models learn from a (small) set of labeled
training examples, and a (large) set of unlabeled training examples.
State-of-the-art models can reach within a few percentage points of
fully-supervised training, while requiring 100x less labeled data.
</p>
<p>We study a new class of vulnerabilities: poisoning attacks that modify the
unlabeled dataset. In order to be useful, unlabeled datasets are given strictly
less review than labeled datasets, and adversaries can therefore poison them
easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%
of the dataset size, we can manipulate a model trained on this poisoned dataset
to misclassify arbitrary examples at test time (as any desired label). Our
attacks are highly effective across datasets and semi-supervised learning
methods.
</p>
<p>We find that more accurate methods (thus more likely to be used) are
significantly more vulnerable to poisoning attacks, and as such better training
methods are unlikely to prevent this attack. To counter this we explore the
space of defenses, and propose two methods that mitigate our attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shaoqing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dingfu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jin Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1">Junbo Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1">Zhou Bin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liangjun Zhang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12449">
<div class="article-summary-box-inner">
<span><p>Accurate detection of obstacles in 3D is an essential task for autonomous
driving and intelligent transportation. In this work, we propose a general
multimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D
point clouds at a semantic level for boosting the 3D object detection task.
Especially, the FusionPainting framework consists of three main modules: a
multi-modal semantic segmentation module, an adaptive attention-based semantic
fusion module, and a 3D object detector. First, semantic information is
obtained for 2D images and 3D Lidar point clouds based on 2D and 3D
segmentation approaches. Then the segmentation results from different sensors
are adaptively fused based on the proposed attention-based semantic fusion
module. Finally, the point clouds painted with the fused semantic label are
sent to the 3D detector for obtaining the 3D objection results. The
effectiveness of the proposed framework has been verified on the large-scale
nuScenes detection benchmark by comparing it with three different baselines.
The experimental results show that the fusion strategy can significantly
improve the detection performance compared to the methods using only point
clouds, and the methods using point clouds only painted with 2D segmentation
information. Furthermore, the proposed approach outperforms other
state-of-the-art methods on the nuScenes testing benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Learning point embedding for 3D data processing.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenpeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+li_Y/0/1/0/all/0/1">Yuan li</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08565">
<div class="article-summary-box-inner">
<span><p>Among 2D convolutional networks on point clouds, point-based approaches
consume point clouds of fixed size directly. By analysis of PointNet, a pioneer
in introducing deep learning into point sets, we reveal that current
point-based methods are essentially spatial relationship processing networks.
In this paper, we take a different approach. Our architecture, named PE-Net,
learns the representation of point clouds in high-dimensional space, and
encodes the unordered input points to feature vectors, which standard 2D CNNs
can be applied to. The recommended network can adapt to changes in the number
of input points which is the limit of current methods. Experiments show that in
the tasks of classification and part segmentation, PE-Net achieves the
state-of-the-art performance in multiple challenging datasets, such as ModelNet
and ShapeNetPart.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Bias Loss for Mobile Neural Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1">Lusine Abrahamyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1">Valentin Ziatchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1">Nikos Deligiannis</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11170">
<div class="article-summary-box-inner">
<span><p>Compact convolutional neural networks (CNNs) have witnessed exceptional
improvements in performance in recent years. However, they still fail to
provide the same predictive power as CNNs with a large number of parameters.
The diverse and even abundant features captured by the layers is an important
characteristic of these successful CNNs. However, differences in this
characteristic between large CNNs and their compact counterparts have rarely
been investigated. In compact CNNs, due to the limited number of parameters,
abundant features are unlikely to be obtained, and feature diversity becomes an
essential characteristic. Diverse features present in the activation maps
derived from a data point during model inference may indicate the presence of a
set of unique descriptors necessary to distinguish between objects of different
classes. In contrast, data points with low feature diversity may not provide a
sufficient amount of unique descriptors to make a valid prediction; we refer to
them as random predictions. Random predictions can negatively impact the
optimization process and harm the final performance. This paper proposes
addressing the problem raised by random predictions by reshaping the standard
cross-entropy to make it biased toward data points with a limited number of
unique descriptive features. Our novel Bias Loss focuses the training on a set
of valuable data points and prevents the vast number of samples with poor
learning features from misleading the optimization process. Furthermore, to
show the importance of diversity, we present a family of SkipNet models whose
architectures are brought to boost the number of unique descriptors in the last
layers. Our Skipnet-M can achieve 1% higher classification accuracy than
MobileNetV3 Large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shi-Xue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaobin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongfa Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xu-Cheng Yin</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12664">
<div class="article-summary-box-inner">
<span><p>Arbitrary shape text detection is a challenging task due to the high
complexity and variety of scene texts. In this work, we propose a novel
adaptive boundary proposal network for arbitrary shape text detection, which
can learn to directly produce accurate boundary for arbitrary shape text
without any post-processing. Our method mainly consists of a boundary proposal
model and an innovative adaptive boundary deformation model. The boundary
proposal model constructed by multi-layer dilated convolutions is adopted to
produce prior information (including classification map, distance field, and
direction field) and coarse boundary proposals. The adaptive boundary
deformation model is an encoder-decoder network, in which the encoder mainly
consists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network
(RNN). It aims to perform boundary deformation in an iterative way for
obtaining text instance shape guided by prior information from the boundary
proposal model. In this way, our method can directly and efficiently generate
accurate text boundaries without complex post-processing. Extensive experiments
on publicly available datasets demonstrate the state-of-the-art performance of
our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ UIBert: Learning Generic Multimodal Representations for UI Understanding.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1">Chongyang Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1">Xiaoxue Zang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Ying Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1">Srinivas Sunkara</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1">Abhinav Rastogi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jindong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1">Blaise Aguera y Arcas</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13731">
<div class="article-summary-box-inner">
<span><p>To improve the accessibility of smart devices and to simplify their usage,
building models which understand user interfaces (UIs) and assist users to
complete their tasks is critical. However, unique challenges are proposed by
UI-specific characteristics, such as how to effectively leverage multimodal UI
features that involve image, text, and structural metadata and how to achieve
good performance when high-quality labeled data is unavailable. To address such
challenges we introduce UIBert, a transformer-based joint image-text model
trained through novel pre-training tasks on large-scale unlabeled UI data to
learn generic feature representations for a UI and its components. Our key
intuition is that the heterogeneous features in a UI are self-aligned, i.e.,
the image and text features of UI components, are predictive of each other. We
propose five pretraining tasks utilizing this self-alignment among different
features of a UI component and across various components in the same UI. We
evaluate our method on nine real-world downstream UI tasks where UIBert
outperforms strong multimodal baselines by up to 9.26% accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1">Ron Shmelkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1">Tomer Friedlander</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lior Wolf</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01077">
<div class="article-summary-box-inner">
<span><p>A master face is a face image that passes face-based identity-authentication
for a large portion of the population. These faces can be used to impersonate,
with a high probability of success, any user, without having access to any
user-information. We optimize these faces, by using an evolutionary algorithm
in the latent embedding space of the StyleGAN face generator. Multiple
evolutionary strategies are compared, and we propose a novel approach that
employs a neural network in order to direct the search in the direction of
promising samples, without adding fitness evaluations. The results we present
demonstrate that it is possible to obtain a high coverage of the LFW identities
(over 40%) with less than 10 master faces, for three leading deep face
recognition systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Internal Video Inpainting by Implicit Long-range Propagation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1">Hao Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tengfei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qifeng Chen</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01912">
<div class="article-summary-box-inner">
<span><p>We propose a novel framework for video inpainting by adopting an internal
learning strategy. Unlike previous methods that use optical flow for
cross-frame context propagation to inpaint unknown regions, we show that this
can be achieved implicitly by fitting a convolutional neural network to known
regions. Moreover, to handle challenging sequences with ambiguous backgrounds
or long-term occlusion, we design two regularization terms to preserve
high-frequency details and long-term temporal consistency. Extensive
experiments on the DAVIS dataset demonstrate that the proposed method achieves
state-of-the-art inpainting quality quantitatively and qualitatively. We
further extend the proposed method to another challenging task: learning to
remove an object from a video giving a single object mask in only one frame in
a 4K video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ StrucTexT: Structured Text Understanding with <span class="highlight-title">Multi-Modal</span> <span class="highlight-title">Transformer</span>s.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yulin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yuxi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yuchen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1">Xiameng Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengquan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1">Kun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Junyu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jingtuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1">Errui Ding</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02923">
<div class="article-summary-box-inner">
<span><p>Structured text understanding on Visually Rich Documents (VRDs) is a crucial
part of Document Intelligence. Due to the complexity of content and layout in
VRDs, structured text understanding has been a challenging task. Most existing
studies decoupled this problem into two sub-tasks: entity labeling and entity
linking, which require an entire understanding of the context of documents at
both token and segment levels. However, little work has been concerned with the
solutions that efficiently extract the structured data from different levels.
This paper proposes a unified framework named StrucTexT, which is flexible and
effective for handling both sub-tasks. Specifically, based on the transformer,
we introduce a segment-token aligned encoder to deal with the entity labeling
and entity linking tasks at different levels of granularity. Moreover, we
design a novel pre-training strategy with three self-supervised tasks to learn
a richer representation. StrucTexT uses the existing Masked Visual Language
Modeling task and the new Sentence Length Prediction and Paired Boxes Direction
tasks to incorporate the multi-modal information across text, image, and
layout. We evaluate our method for structured text understanding at
segment-level and token-level and show it outperforms the state-of-the-art
counterparts with significantly superior performance on the FUNSD, SROIE, and
EPHOIE datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Learning Meta-class Memory for Few-Shot Semantic Segmentation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhonghua Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiangxi Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1">Guosheng lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1">Jianfei Cai</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02958">
<div class="article-summary-box-inner">
<span><p>Currently, the state-of-the-art methods treat few-shot semantic segmentation
task as a conditional foreground-background segmentation problem, assuming each
class is independent. In this paper, we introduce the concept of meta-class,
which is the meta information (e.g. certain middle-level features) shareable
among all classes. To explicitly learn meta-class representations in few-shot
segmentation task, we propose a novel Meta-class Memory based few-shot
segmentation method (MM-Net), where we introduce a set of learnable memory
embeddings to memorize the meta-class information during the base class
training and transfer to novel classes during the inference stage. Moreover,
for the $k$-shot scenario, we propose a novel image quality measurement module
to select images from the set of support images. A high-quality class prototype
could be obtained with the weighted sum of support image features based on the
quality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that
our proposed method is able to achieve state-of-the-art results in both 1-shot
and 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\% mIoU on
the COCO dataset in 1-shot setting, which is 5.1\% higher than the previous
state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Adapting Segmentation Networks to New Domains by Disentangling Latent Representations.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1">Francesco Barbato</a>, <a href="http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1">Umberto Michieli</a>, <a href="http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1">Marco Toldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1">Pietro Zanuttigh</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03021">
<div class="article-summary-box-inner">
<span><p>Deep learning models achieve outstanding accuracy in semantic segmentation,
however they require a huge amount of labeled data for their optimization.
Hence, domain adaptation approaches have come into play to transfer knowledge
acquired on a label-abundant source domain to a related label-scarce target
domain. However, such models do not generalize well to data with statistical
properties not perfectly matching the ones of the training samples. In this
work, we design and carefully analyze multiple latent space-shaping
regularization strategies that work in conjunction to reduce the domain
discrepancy in semantic segmentation. In particular, we devise a feature
clustering strategy to increase domain alignment, a feature perpendicularity
constraint to space apart feature belonging to different semantic classes,
including those not present in the current batch, and a feature norm alignment
strategy to separate active and inactive channels. Additionally, we propose a
novel performance metric to capture the relative efficacy of an adaptation
strategy compared to supervised training. We verify the effectiveness of our
framework in synthetic-to-real and real-to-real adaptation scenarios,
outperforming previous state-of-the-art methods on multiple road scenes
benchmarks and using different backbones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> ♻ iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengshu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1">Michael Lingelbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1">Sanjana Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Bokui Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1">Kent Vainio</a>, <a href="http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1">Cem Gokmen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1">Gokul Dharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1">Tanish Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1">Andrey Kurenkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">C. Karen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1">Hyowon Gweon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"><span class="highlight-author">Jiajun Wu</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"><span class="highlight-author">Li Fei-Fei</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03272">
<div class="article-summary-box-inner">
<span><p>Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset will be publicly available at
<a href="http://svl.stanford.edu/igibson/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Expressive Power and Loss Surfaces of Deep Learning Models.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1">Simant Dube</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03579">
<div class="article-summary-box-inner">
<span><p>The goals of this paper are two-fold. The first goal is to serve as an
expository tutorial on the working of deep learning models which emphasizes
geometrical intuition about the reasons for success of deep learning. The
second goal is to complement the current results on the expressive power of
deep learning models and their loss surfaces with novel insights and results.
In particular, we describe how deep neural networks carve out manifolds
especially when the multiplication neurons are introduced. Multiplication is
used in dot products and the attention mechanism and it is employed in capsule
networks and self-attention based transformers. We also describe how random
polynomial, random matrix, spin glass and computational complexity perspectives
on the loss surfaces are interconnected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Discriminative Latent Semantic Graph for Video Captioning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yang Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1">Yang Long</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1">Bingzhang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1">Maurice Pagnucco</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yu Guan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03662">
<div class="article-summary-box-inner">
<span><p>Video captioning aims to automatically generate natural language sentences
that can describe the visual contents of a given video. Existing generative
models like encoder-decoder frameworks cannot explicitly explore the
object-level interactions and frame-level information from complex
spatio-temporal data to generate semantic-rich captions. Our main contribution
is to identify three key problems in a joint framework for future video
summarization tasks. 1) Enhanced Object Proposal: we propose a novel
Conditional Graph that can fuse spatio-temporal information into latent object
proposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to
dynamically extract visual words with higher semantic levels. 3) Sentence
Validation: A novel Discriminative Language Validator is proposed to verify
generated captions so that key semantic concepts can be effectively preserved.
Our experiments on two public datasets (MVSD and MSR-VTT) manifest significant
improvements over state-of-the-art approaches on all metrics, especially for
BLEU-4 and CIDEr. Our code is available at
https://github.com/baiyang4/D-LSG-Video-Caption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Self-supervised Learning of Occlusion Aware Flow Guided 3D Geometry Perception with Adaptive Cross Weighted Loss from Monocular Videos.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1">Jiaojiao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guizhong Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03893">
<div class="article-summary-box-inner">
<span><p>Self-supervised deep learning-based 3D scene understanding methods can
overcome the difficulty of acquiring the densely labeled ground-truth and have
made a lot of advances. However, occlusions and moving objects are still some
of the major limitations. In this paper, we explore the learnable occlusion
aware optical flow guided self-supervised depth and camera pose estimation by
an adaptive cross weighted loss to address the above limitations. Firstly, we
explore to train the learnable occlusion mask fused optical flow network by an
occlusion-aware photometric loss with the temporally supplemental information
and backward-forward consistency of adjacent views. And then, we design an
adaptive cross-weighted loss between the depth-pose and optical flow loss of
the geometric and photometric error to distinguish the moving objects which
violate the static scene assumption. Our method shows promising results on
KITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good
generalization ability under a variety of challenging scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Deep Learning methods for automatic evaluation of delayed enhancement-MRI. The results of the EMIDEC challenge.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Lalande_A/0/1/0/all/0/1">Alain Lalande</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhihao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pommier_T/0/1/0/all/0/1">Thibaut Pommier</a>, <a href="http://arxiv.org/find/cs/1/au:+Decourselle_T/0/1/0/all/0/1">Thomas Decourselle</a>, <a href="http://arxiv.org/find/cs/1/au:+Qayyum_A/0/1/0/all/0/1">Abdul Qayyum</a>, <a href="http://arxiv.org/find/cs/1/au:+Salomon_M/0/1/0/all/0/1">Michel Salomon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ginhac_D/0/1/0/all/0/1">Dominique Ginhac</a>, <a href="http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1">Youssef Skandarani</a>, <a href="http://arxiv.org/find/cs/1/au:+Boucher_A/0/1/0/all/0/1">Arnaud Boucher</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahim_K/0/1/0/all/0/1">Khawla Brahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1">Marleen de Bruijne</a>, <a href="http://arxiv.org/find/cs/1/au:+Camarasa_R/0/1/0/all/0/1">Robin Camarasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Correia_T/0/1/0/all/0/1">Teresa M. Correia</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xue Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Girum_K/0/1/0/all/0/1">Kibrom B. Girum</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennemuth_A/0/1/0/all/0/1">Anja Hennemuth</a>, <a href="http://arxiv.org/find/cs/1/au:+Huellebrand_M/0/1/0/all/0/1">Markus Huellebrand</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1">Raabid Hussain</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivantsits_M/0/1/0/all/0/1">Matthias Ivantsits</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1">Jun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1">Craig Meyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1">Rishabh Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1">Jixi Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsekos_N/0/1/0/all/0/1">Nikolaos V. Tsekos</a>, <a href="http://arxiv.org/find/cs/1/au:+Varela_M/0/1/0/all/0/1">Marta Varela</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hannu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuncheng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1">Xiahai Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1">Raphael Couturier</a>, <a href="http://arxiv.org/find/cs/1/au:+Meriaudeau_F/0/1/0/all/0/1">Fabrice Meriaudeau</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04016">
<div class="article-summary-box-inner">
<span><p>A key factor for assessing the state of the heart after myocardial infarction
(MI) is to measure whether the myocardium segment is viable after reperfusion
or revascularization therapy. Delayed enhancement-MRI or DE-MRI, which is
performed several minutes after injection of the contrast agent, provides high
contrast between viable and nonviable myocardium and is therefore a method of
choice to evaluate the extent of MI. To automatically assess myocardial status,
the results of the EMIDEC challenge that focused on this task are presented in
this paper. The challenge's main objectives were twofold. First, to evaluate if
deep learning methods can distinguish between normal and pathological cases.
Second, to automatically calculate the extent of myocardial infarction. The
publicly available database consists of 150 exams divided into 50 cases with
normal MRI after injection of a contrast agent and 100 cases with myocardial
infarction (and then with a hyperenhanced area on DE-MRI), whatever their
inclusion in the cardiac emergency department. Along with MRI, clinical
characteristics are also provided. The obtained results issued from several
works show that the automatic classification of an exam is a reachable task
(the best method providing an accuracy of 0.92), and the automatic segmentation
of the myocardium is possible. However, the segmentation of the diseased area
needs to be improved, mainly due to the small size of these areas and the lack
of contrast with the surrounding structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ No-Reference Image Quality Assessment by Hallucinating Pristine Features.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1">Baoliang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1">Lingyu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1">Chenqi Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hanwei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhu Li</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04165">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a no-reference (NR) image quality assessment (IQA)
method via feature level pseudo-reference (PR) hallucination. The proposed
quality assessment framework is grounded on the prior models of natural image
statistical behaviors and rooted in the view that the perceptually meaningful
features could be well exploited to characterize the visual quality. Herein,
the PR features from the distorted images are learned by a mutual learning
scheme with the pristine reference as the supervision, and the discriminative
characteristics of PR features are further ensured with the triplet
constraints. Given a distorted image for quality inference, the feature level
disentanglement is performed with an invertible neural layer for final quality
prediction, leading to the PR and the corresponding distortion features for
comparison. The effectiveness of our proposed method is demonstrated on four
popular IQA databases, and superior performance on cross-database evaluation
also reveals the high generalization capability of our method. The
implementation of our method is publicly available on
https://github.com/Baoliang93/FPR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Meta Gradient Adversarial Attack.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zheng Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yunpei Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1">Chuanqi Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1">Tao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1">Shiguang Shan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04204">
<div class="article-summary-box-inner">
<span><p>In recent years, research on adversarial attacks has become a hot spot.
Although current literature on the transfer-based adversarial attack has
achieved promising results for improving the transferability to unseen
black-box models, it still leaves a long way to go. Inspired by the idea of
meta-learning, this paper proposes a novel architecture called Meta Gradient
Adversarial Attack (MGAA), which is plug-and-play and can be integrated with
any existing gradient-based attack method for improving the cross-model
transferability. Specifically, we randomly sample multiple models from a model
zoo to compose different tasks and iteratively simulate a white-box attack and
a black-box attack in each task. By narrowing the gap between the gradient
directions in white-box and black-box attacks, the transferability of
adversarial examples on the black-box setting can be improved. Extensive
experiments on the CIFAR10 and ImageNet datasets show that our architecture
outperforms the state-of-the-art methods for both black-box and white-box
attack settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ AutoVideo: An Automated Video Action Recognition System.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1">Zaid Pervaiz Bhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yicheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Sirui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Anmoll Kumar Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1">Mohammad Qazim Bhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1">Kwei-Herng Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaben Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1">Na Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xia Hu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04212">
<div class="article-summary-box-inner">
<span><p>Action recognition is a crucial task for video understanding. In this paper,
we present AutoVideo, a Python system for automated video action recognition.
It currently supports seven action recognition algorithms and various
pre-processing modules. Unlike the existing libraries that only provide model
zoos, AutoVideo is built with the standard pipeline language. The basic
building block is primitive, which wraps a pre-processing module or an
algorithm with some hyperparameters. AutoVideo is highly modular and
extendable. It can be easily combined with AutoML searchers. The pipeline
language is quite general so that we can easily enrich AutoVideo with
algorithms for various other video-related tasks in the future. AutoVideo is
released under MIT license at https://github.com/datamllab/autovideo
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Segmentation of VHR EO Images using Unsupervised Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Sudipan Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1">Lichao Mou</a>, <a href="http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1">Muhammad Shahzad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiao Xiang Zhu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04222">
<div class="article-summary-box-inner">
<span><p>Semantic segmentation is a crucial step in many Earth observation tasks.
Large quantity of pixel-level annotation is required to train deep networks for
semantic segmentation. Earth observation techniques are applied to varieties of
applications and since classes vary widely depending on the applications,
therefore, domain knowledge is often required to label Earth observation
images, impeding availability of labeled training data in many Earth
observation applications. To tackle these challenges, in this paper we propose
an unsupervised semantic segmentation method that can be trained using just a
single unlabeled scene. Remote sensing scenes are generally large. The proposed
method exploits this property to sample smaller patches from the larger scene
and uses deep clustering and contrastive learning to refine the weights of a
lightweight deep model composed of a series of the convolution layers along
with an embedded channel attention. After unsupervised training on the target
image/scene, the model automatically segregates the major classes present in
the scene and produces the segmentation map. Experimental results on the
Vaihingen dataset demonstrate the efficacy of the proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1">Florian Kromp</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1">Lukas Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1">Eva Bozsaky</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1">Inge Ambros</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1">Wolfgang Doerr</a>, <a href="http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1">Sabine Taschner-Mandl</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1">Peter Ambros</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1">Allan Hanbury</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.12975">
<div class="article-summary-box-inner">
<span><p>Separating and labeling each instance of a nucleus (instance-aware
segmentation) is the key challenge in segmenting single cell nuclei on
fluorescence microscopy images. Deep Neural Networks can learn the implicit
transformation of a nuclear image into a probability map indicating the class
membership of each pixel (nucleus or background), but the use of
post-processing steps to turn the probability map into a labeled object mask is
error-prone. This especially accounts for nuclear images of tissue sections and
nuclear images across varying tissue preparations. In this work, we aim to
evaluate the performance of state-of-the-art deep learning architectures to
segment nuclei in fluorescence images of various tissue origins and sample
preparation types without post-processing. We compare architectures that
operate on pixel to pixel translation and an architecture that operates on
object detection and subsequent locally applied segmentation. In addition, we
propose a novel strategy to create artificial images to extend the training
set. We evaluate the influence of ground truth annotation quality, image scale
and segmentation complexity on segmentation performance. Results show that
three out of four deep learning architectures (U-Net, U-Net with ResNet34
backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the
sample preparation types and tissue origins with satisfactory segmentation
performance. Mask R-CNN, an architecture designed to address instance aware
segmentation tasks, outperforms other architectures. Equal nuclear mean size,
consistent nuclear annotations and the use of artificially generated images
result in overall acceptable precision and recall across different tissues and
sample preparation types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jindi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1">Yang Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Kejie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xiaohua Jia</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02940">
<div class="article-summary-box-inner">
<span><p>In recent years, many deep learning models have been adopted in autonomous
driving. At the same time, these models introduce new vulnerabilities that may
compromise the safety of autonomous vehicles. Specifically, recent studies have
demonstrated that adversarial attacks can cause a significant decline in
detection precision of deep learning-based 3D object detection models. Although
driving safety is the ultimate concern for autonomous driving, there is no
comprehensive study on the linkage between the performance of deep learning
models and the driving safety of autonomous vehicles under adversarial attacks.
In this paper, we investigate the impact of two primary types of adversarial
attacks, perturbation attacks and patch attacks, on the driving safety of
vision-based autonomous vehicles rather than the detection precision of deep
learning models. In particular, we consider two state-of-the-art models in
vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving
safety, we propose an end-to-end evaluation framework with a set of driving
safety performance metrics. By analyzing the results of our extensive
evaluation experiments, we find that (1) the attack's impact on the driving
safety of autonomous vehicles and the attack's impact on the precision of 3D
object detectors are decoupled, and (2) the DSGN model demonstrates stronger
robustness to adversarial attacks than the Stereo R-CNN model. In addition, we
further investigate the causes behind the two findings with an ablation study.
The findings of this paper provide a new perspective to evaluate adversarial
attacks and guide the selection of deep learning models in autonomous driving.
</p></span>
</div>
</a>
</details>
</article>
</div>
</details>
</article>
<article>
<details>
<Summary>cs.IR updates on arXiv.org</Summary>
<div class="details-content">
<p style="text-align:center;">Computer Science -- Information Retrieval (cs.IR) updates on the arXiv.org e-print archive</p>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ High Quality Related Search Query Suggestions using Deep Reinforcement Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1">Praveen Kumar Bodigutla</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04452">
<div class="article-summary-box-inner">
<span><p>"High Quality Related Search Query Suggestions" task aims at recommending
search queries which are real, accurate, diverse, relevant and engaging.
Obtaining large amounts of query-quality human annotations is expensive. Prior
work on supervised query suggestion models suffered from selection and exposure
bias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),
leading to low quality suggestions. Reinforcement Learning techniques employed
to reformulate a query using terms from search results, have limited
scalability to large-scale industry applications. To recommend high quality
related search queries, we train a Deep Reinforcement Learning model to predict
the query a user would enter next. The reward signal is composed of long-term
session-based user feedback, syntactic relatedness and estimated naturalness of
generated query. Over the baseline supervised model, our proposed approach
achieves a significant relative improvement in terms of recommendation
diversity (3%), down-stream user-engagement (4.2%) and per-sentence word
repetitions (82%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ End-to-End User Behavior Retrieval in Click-Through RatePrediction Model.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qiwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_C/0/1/0/all/0/1">Changhua Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1">Shanshan Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Junfeng Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1">Wenwu Ou</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04468">
<div class="article-summary-box-inner">
<span><p>Click-Through Rate (CTR) prediction is one of the core tasks in recommender
systems (RS). It predicts a personalized click probability for each user-item
pair. Recently, researchers have found that the performance of CTR model can be
improved greatly by taking user behavior sequence into consideration,
especially long-term user behavior sequence. The report on an e-commerce
website shows that 23\% of users have more than 1000 clicks during the past 5
months. Though there are numerous works focus on modeling sequential user
behaviors, few works can handle long-term user behavior sequence due to the
strict inference time constraint in real world system. Two-stage methods are
proposed to push the limit for better performance. At the first stage, an
auxiliary task is designed to retrieve the top-$k$ similar items from long-term
user behavior sequence. At the second stage, the classical attention mechanism
is conducted between the candidate item and $k$ items selected in the first
stage. However, information gap happens between retrieval stage and the main
CTR task. This goal divergence can greatly diminishing the performance gain of
long-term user sequence. In this paper, inspired by Reformer, we propose a
locality-sensitive hashing (LSH) method called ETA (End-to-end Target
Attention) which can greatly reduce the training and inference cost and make
the end-to-end training with long-term user behavior sequence possible. Both
offline and online experiments confirm the effectiveness of our model. We
deploy ETA into a large-scale real world E-commerce system and achieve extra
3.1\% improvements on GMV (Gross Merchandise Value) compared to a two-stage
long user sequence CTR model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Localized Graph Collaborative Filtering.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yiqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chaozhuo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingzheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Wei Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Hao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04475">
<div class="article-summary-box-inner">
<span><p>User-item interactions in recommendations can be naturally de-noted as a
user-item bipartite graph. Given the success of graph neural networks (GNNs) in
graph representation learning, GNN-based C methods have been proposed to
advance recommender systems. These methods often make recommendations based on
the learned user and item embeddings. However, we found that they do not
perform well wit sparse user-item graphs which are quite common in real-world
recommendations. Therefore, in this work, we introduce a novel perspective to
build GNN-based CF methods for recommendations which leads to the proposed
framework Localized Graph Collaborative Filtering (LGCF). One key advantage of
LGCF is that it does not need to learn embeddings for each user and item, which
is challenging in sparse scenarios.
</p>
<p>Alternatively, LGCF aims at encoding useful CF information into a localized
graph and making recommendations based on such graph. Extensive experiments on
various datasets validate the effectiveness of LGCF especially in sparse
scenarios. Furthermore, empirical results demonstrate that LGCF provides
complementary information to the embedding-based CF model which can be utilized
to boost recommendation performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Fully Hyperbolic Graph Convolution Network for Recommendation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1">Fenyu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04607">
<div class="article-summary-box-inner">
<span><p>Recently, Graph Convolution Network (GCN) based methods have achieved
outstanding performance for recommendation. These methods embed users and items
in Euclidean space, and perform graph convolution on user-item interaction
graphs. However, real-world datasets usually exhibit tree-like hierarchical
structures, which make Euclidean space less effective in capturing user-item
relationship. In contrast, hyperbolic space, as a continuous analogue of a
tree-graph, provides a promising alternative. In this paper, we propose a fully
hyperbolic GCN model for recommendation, where all operations are performed in
hyperbolic space. Utilizing the advantage of hyperbolic space, our method is
able to embed users/items with less distortion and capture user-item
interaction relationship more accurately. Extensive experiments on public
benchmark datasets show that our method outperforms both Euclidean and
hyperbolic counterparts and requires far lower embedding dimensionality to
achieve comparable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Hierarchical Latent Relation Modeling for Collaborative Metric Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1">Viet-Anh Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1">Guillaume Salha-Galvan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1">Romain Hennequin</a>, <a href="http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1">Manuel Moussallam</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04655">
<div class="article-summary-box-inner">
<span><p>Collaborative Metric Learning (CML) recently emerged as a powerful paradigm
for recommendation based on implicit feedback collaborative filtering. However,
standard CML methods learn fixed user and item representations, which fails to
capture the complex interests of users. Existing extensions of CML also either
ignore the heterogeneity of user-item relations, i.e. that a user can
simultaneously like very different items, or the latent item-item relations,
i.e. that a user's preference for an item depends, not only on its intrinsic
characteristics, but also on items they previously interacted with. In this
paper, we present a hierarchical CML model that jointly captures latent
user-item and item-item relations from implicit data. Our approach is inspired
by translation mechanisms from knowledge graph embedding and leverages
memory-based attention networks. We empirically show the relevance of this
joint relational modeling, by outperforming existing CML models on
recommendation tasks on several real-world datasets. Our experiments also
emphasize the limits of current CML relational models on very sparse datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ POSO: Personalized Cold Start Modules for Large-scale Recommender Systems.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1">Shangfeng Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Haobin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhichen Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jianying Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Honghuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+PinghuaGong/0/1/0/all/0/1">PinghuaGong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Sen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Ji Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04690">
<div class="article-summary-box-inner">
<span><p>Recommendation for new users, also called user cold start, has been a
well-recognized challenge for online recommender systems. Most existing methods
view the crux as the lack of initial data. However, in this paper, we argue
that there are neglected problems: 1) New users' behaviour follows much
different distributions from regular users. 2) Although personalized features
are involved, heavily imbalanced samples prevent the model from balancing
new/regular user distributions, as if the personalized features are
overwhelmed. We name the problem as the ``submergence" of personalization. To
tackle this problem, we propose a novel module: Personalized COld Start MOdules
(POSO). Considering from a model architecture perspective, POSO personalizes
existing modules by introducing multiple user-group-specialized sub-modules.
Then, it fuses their outputs by personalized gates, resulting in comprehensive
representations. In such way, POSO projects imbalanced features to even
modules. POSO can be flexibly integrated into many existing modules and
effectively improves their performance with negligible computational overheads.
The proposed method shows remarkable advantage in industrial scenario. It has
been deployed on the large-scale recommender system of Kwai, and improves new
user Watch Time by a large margin (+7.75%). Moreover, POSO can be further
generalized to regular users, inactive users and returning users (+2%-3% on
Watch Time), as well as item cold start (+3.8% on Watch Time). Its
effectiveness has also been verified on public dataset (MovieLens 20M). We
believe such practical experience can be well generalized to other scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ AMUSED: An Annotation Framework of Multi-modal Social Media Data.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1">Gautam Kishore Shahi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.00502">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a semi-automated framework called AMUSED for
gathering multi-modal annotated data from the multiple social media platforms.
The framework is designed to mitigate the issues of collecting and annotating
social media data by cohesively combining machine and human in the data
collection process. From a given list of the articles from professional news
media or blog, AMUSED detects links to the social media posts from news
articles and then downloads contents of the same post from the respective
social media platform to gather details about that specific post. The framework
is capable of fetching the annotated data from multiple platforms like Twitter,
YouTube, Reddit. The framework aims to reduce the workload and problems behind
the data annotation from the social media platforms. AMUSED can be applied in
multiple application domains, as a use case, we have implemented the framework
for collecting COVID-19 misinformation data from different social media
platforms.
</p></span>
</div>
</a>
</details>
</article>
</div>
</details>
</article>
<article>
<details>
<Summary>cs.LG updates on arXiv.org</Summary>
<div class="details-content">
<p style="text-align:center;">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</p>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Deep Transfer Learning for Identifications of Slope Surface Cracks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuting Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1">Gang Mei</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04235">
<div class="article-summary-box-inner">
<span><p>Geohazards such as landslides have caused great losses to the safety of
people's lives and property, which is often accompanied with surface cracks. If
such surface cracks could be identified in time, it is of great significance
for the monitoring and early warning of geohazards. Currently, the most common
method for crack identification is manual detection, which is with low
efficiency and accuracy. In this paper, a deep transfer learning framework is
proposed to effectively and efficiently identify slope surface cracks for the
sake of fast monitoring and early warning of geohazards such as landslides. The
essential idea is to employ transfer learning by training (a) the large sample
dataset of concrete cracks and (b) the small sample dataset of soil and rock
masses cracks. In the proposed framework, (1) pretrained cracks identification
models are constructed based on the large sample dataset of concrete cracks;
(2) refined cracks identification models are further constructed based on the
small sample dataset of soil and rock masses cracks. The proposed framework
could be applied to conduct UAV surveys on high-steep slopes to realize the
monitoring and early warning of landslides to ensure the safety of people's
lives and property.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Cong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1">Haocheng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Caleb Chen Cao</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04238">
<div class="article-summary-box-inner">
<span><p>Explanation of AI, as well as fairness of algorithms' decisions and the
transparency of the decision model, are becoming more and more important. And
it is crucial to design effective and human-friendly techniques when opening
the black-box model. Counterfactual conforms to the human way of thinking and
provides a human-friendly explanation, and its corresponding explanation
algorithm refers to a strategic alternation of a given data point so that its
model output is "counter-facted", i.e. the prediction is reverted. In this
paper, we adapt counterfactual explanation over fine-grained image
classification problem. We demonstrated an adaptive method that could give a
counterfactual explanation by showing the composed counterfactual feature map
using top-down layer searching algorithm (TDLS). We have proved that our TDLS
algorithm could provide more flexible counterfactual visual explanation in an
efficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,
we discussed several applicable scenarios of counterfactual visual
explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Classification of Influenza Hemagglutinin Protein Sequences using Convolutional Neural Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/q-bio/1/au:+Chrysostomou_C/0/1/0/all/0/1">Charalambos Chrysostomou</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Alexandrou_F/0/1/0/all/0/1">Floris Alexandrou</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Nicolaou_M/0/1/0/all/0/1">Mihalis A. Nicolaou</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Seker_H/0/1/0/all/0/1">Huseyin Seker</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04240">
<div class="article-summary-box-inner">
<span><p>The Influenza virus can be considered as one of the most severe viruses that
can infect multiple species with often fatal consequences to the hosts. The
Hemagglutinin (HA) gene of the virus can be a target for antiviral drug
development realised through accurate identification of its sub-types and
possible the targeted hosts. This paper focuses on accurately predicting if an
Influenza type A virus can infect specific hosts, and more specifically, Human,
Avian and Swine hosts, using only the protein sequence of the HA gene. In more
detail, we propose encoding the protein sequences into numerical signals using
the Hydrophobicity Index and subsequently utilising a Convolutional Neural
Network-based predictive model. The Influenza HA protein sequences used in the
proposed work are obtained from the Influenza Research Database (IRD).
Specifically, complete and unique HA protein sequences were used for avian,
human and swine hosts. The data obtained for this work was 17999 human-host
proteins, 17667 avian-host proteins and 9278 swine-host proteins. Given this
set of collected proteins, the proposed method yields as much as 10% higher
accuracy for an individual class (namely, Avian) and 5% higher overall accuracy
than in an earlier study. It is also observed that the accuracy for each class
in this work is more balanced than what was presented in this earlier study. As
the results show, the proposed model can distinguish HA protein sequences with
high accuracy whenever the virus under investigation can infect Human, Avian or
Swine hosts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1">Santiago Estrada</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1">Ran Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1">Kersten Diers</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1">Weiyi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1">Philipp Ehses</a>, <a href="http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1">Tony St&#xf6;cker</a>, <a href="http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1">Monique M.B Breteler</a>, <a href="http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1">Martin Reuter</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04267">
<div class="article-summary-box-inner">
<span><p>The neuroimage analysis community has neglected the automated segmentation of
the olfactory bulb (OB) despite its crucial role in olfactory function. The
lack of an automatic processing method for the OB can be explained by its
challenging properties. Nonetheless, recent advances in MRI acquisition
techniques and resolution have allowed raters to generate more reliable manual
annotations. Furthermore, the high accuracy of deep learning methods for
solving semantic segmentation problems provides us with an option to reliably
assess even small structures. In this work, we introduce a novel, fast, and
fully automated deep learning pipeline to accurately segment OB tissue on
sub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we
designed a three-stage pipeline: (1) Localization of a region containing both
OBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized
region through four independent AttFastSurferCNN - a novel deep learning
architecture with a self-attention mechanism to improve modeling of contextual
information, and (3) Ensemble of the predicted label maps. The OB pipeline
exhibits high performance in terms of boundary delineation, OB localization,
and volume estimation across a wide range of ages in 203 participants of the
Rhineland Study. Moreover, it also generalizes to scans of an independent
dataset never encountered during training, the Human Connectome Project (HCP),
with different acquisition parameters and demographics, evaluated in 30 cases
at the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.
We extensively validated our pipeline not only with respect to segmentation
accuracy but also to known OB volume effects, where it can sensitively
replicate age effects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ACE: A Novel Approach for the Statistical Analysis of Pairwise Connectivity.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/q-bio/1/au:+Krempl/0/1/0/all/0/1">Krempl</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Georg/0/1/0/all/0/1">Georg</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kottke/0/1/0/all/0/1">Kottke</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Daniel/0/1/0/all/0/1">Daniel</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Minh_P/0/1/0/all/0/1">Pham Minh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tuan/0/1/0/all/0/1">Tuan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04289">
<div class="article-summary-box-inner">
<span><p>Analysing correlations between streams of events is an important problem. It
arises for example in Neurosciences, when the connectivity of neurons should be
inferred from spike trains that record neurons' individual spiking activity.
While recently some approaches for inferring delayed synaptic connections have
been proposed, they are limited in the types of connectivities and delays they
are able to handle, or require computation-intensive procedures. This paper
proposes a faster and more flexible approach for analysing such delayed
correlated activity: a statistical approach for the Analysis of Connectivity in
spiking Events (ACE), based on the idea of hypothesis testing. It first
computes for any pair of a source and a target neuron the inter-spike delays
between subsequent source- and target-spikes. Then, it derives a null model for
the distribution of inter-spike delays for \emph{uncorrelated}~neurons.
Finally, it compares the observed distribution of inter-spike delays to this
null model and infers pairwise connectivity based on the Pearson's Chi-squared
test statistic. Thus, ACE is capable to detect connections with a priori
unknown, non-discrete (and potentially large) inter-spike delays, which might
vary between pairs of neurons. Since ACE works incrementally, it has potential
for being used in online processing. In our experiments, we visualise the
advantages of ACE in varying experimental scenarios (except for one special
case) and in a state-of-the-art dataset which has been generated for
neuro-scientific research under most realistic conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Natural Numerical Networks for Natura 2000 habitats classification by satellite images.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1">Karol Mikula</a>, <a href="http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1">Michal Kollar</a>, <a href="http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1">Aneta A. Ozvat</a>, <a href="http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1">Martin Ambroz</a>, <a href="http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1">Lucia Cahojova</a>, <a href="http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1">Ivan Jarolimek</a>, <a href="http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1">Jozef Sibik</a>, <a href="http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1">Maria Sibikova</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04327">
<div class="article-summary-box-inner">
<span><p>Natural numerical networks are introduced as a new classification algorithm
based on the numerical solution of nonlinear partial differential equations of
forward-backward diffusion type on complete graphs. The proposed natural
numerical network is applied to open important environmental and nature
conservation task, the automated identification of protected habitats by using
satellite images. In the natural numerical network, the forward diffusion
causes the movement of points in a feature space toward each other. The
opposite effect, keeping the points away from each other, is caused by backward
diffusion. This yields the desired classification. The natural numerical
network contains a few parameters that are optimized in the learning phase of
the method. After learning parameters and optimizing the topology of the
network graph, classification necessary for habitat identification is
performed. A relevancy map for each habitat is introduced as a tool for
validating the classification and finding new Natura 2000 habitat appearances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Towards a Generic Multimodal Architecture for Batch and Streaming Big Data Integration.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yousfi_S/0/1/0/all/0/1">Siham Yousfi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rhanoui_M/0/1/0/all/0/1">Maryem Rhanoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiadmi_D/0/1/0/all/0/1">Dalila Chiadmi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04343">
<div class="article-summary-box-inner">
<span><p>Big Data are rapidly produced from various heterogeneous data sources. They
are of different types (text, image, video or audio) and have different levels
of reliability and completeness. One of the most interesting architectures that
deal with the large amount of emerging data at high velocity is called the
lambda architecture. In fact, it combines two different processing layers
namely batch and speed layers, each providing specific views of data while
ensuring robustness, fast and scalable data processing. However, most papers
dealing with the lambda architecture are focusing one single type of data
generally produced by a single data source. Besides, the layers of the
architecture are implemented independently, or, at best, are combined to
perform basic processing without assessing either the data reliability or
completeness. Therefore, inspired by the lambda architecture, we propose in
this paper a generic multimodal architecture that combines both batch and
streaming processing in order to build a complete, global and accurate insight
in near-real-time based on the knowledge extracted from multiple heterogeneous
Big Data sources. Our architecture uses batch processing to analyze the data
structures and contents, build the learning models and calculate the
reliability index of the involved sources, while the streaming processing uses
the built-in models of the batch layer to immediately process incoming data and
rapidly provide results. We validate our architecture in the context of urban
traffic management systems in order to detect congestions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1">Aishwarza Panday</a>, <a href="http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1">Muhammad Ashad Kabir</a>, <a href="http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1">Nihad Karim Chowdhury</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04344">
<div class="article-summary-box-inner">
<span><p>Due to the limited availability and high cost of the reverse
transcription-polymerase chain reaction (RT-PCR) test, many studies have
proposed machine learning techniques for detecting COVID-19 from medical
imaging. The purpose of this study is to systematically review, assess, and
synthesize research articles that have used different machine learning
techniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.
A structured literature search was conducted in the relevant bibliographic
databases to ensure that the survey solely centered on reproducible and
high-quality research. We selected papers based on our inclusion criteria. In
this survey, we reviewed $98$ articles that fulfilled our inclusion criteria.
We have surveyed a complete pipeline of chest imaging analysis techniques
related to COVID-19, including data collection, pre-processing, feature
extraction, classification, and visualization. We have considered CT scans and
X-rays as both are widely used to describe the latest developments in medical
imaging to detect COVID-19. This survey provides researchers with valuable
insights into different machine learning techniques and their performance in
the detection and diagnosis of COVID-19 from chest imaging. At the end, the
challenges and limitations in detecting COVID-19 using machine learning
techniques and the future direction of research are discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1">Hamza Rasaee</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1">Hassan Rivaz</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04345">
<div class="article-summary-box-inner">
<span><p>Ultrasound is a non-invasive imaging modality that can be conveniently used
to classify suspicious breast nodules and potentially detect the onset of
breast cancer. Recently, Convolutional Neural Networks (CNN) techniques have
shown promising results in classifying ultrasound images of the breast into
benign or malignant. However, CNN inference acts as a black-box model, and as
such, its decision-making is not interpretable. Therefore, increasing effort
has been dedicated to explaining this process, most notably through GRAD-CAM
and other techniques that provide visual explanations into inner workings of
CNNs. In addition to interpretation, these methods provide clinically important
information, such as identifying the location for biopsy or treatment. In this
work, we analyze how adversarial assaults that are practically undetectable may
be devised to alter these importance maps dramatically. Furthermore, we will
show that this change in the importance maps can come with or without altering
the classification result, rendering them even harder to detect. As such, care
must be taken when using these importance maps to shed light on the inner
workings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and
propose a new network based on ResNet-50 to improve the classification
accuracies. Our sensitivity and specificity is comparable to the state of the
art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ AASeg: Attention Aware Network for Real Time Semantic Segmentation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1">Abhinav Sagar</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04349">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a new network named Attention Aware Network (AASeg)
for real time semantic image segmentation. Our network incorporates spatial and
channel information using Spatial Attention (SA) and Channel Attention (CA)
modules respectively. It also uses dense local multi-scale context information
using Multi Scale Context (MSC) module. The feature maps are concatenated
individually to produce the final segmentation map. We demonstrate the
effectiveness of our method using a comprehensive analysis, quantitative
experimental results and ablation study using Cityscapes, ADE20K and Camvid
datasets. Our network performs better than most previous architectures with a
74.4\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1">Amey Thakur</a>, <a href="http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1">Mega Satish</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04351">
<div class="article-summary-box-inner">
<span><p>This paper aims to demonstrate the efficiency of the Adversarial Open Domain
Adaption framework for sketch-to-photo synthesis. The unsupervised open domain
adaption for generating realistic photos from a hand-drawn sketch is
challenging as there is no such sketch of that class for training data. The
absence of learning supervision and the huge domain gap between both the
freehand drawing and picture domains make it hard. We present an approach that
learns both sketch-to-photo and photo-to-sketch generation to synthesise the
missing freehand drawings from pictures. Due to the domain gap between
synthetic sketches and genuine ones, the generator trained on false drawings
may produce unsatisfactory results when dealing with drawings of lacking
classes. To address this problem, we offer a simple but effective open-domain
sampling and optimization method that tricks the generator into considering
false drawings as genuine. Our approach generalises the learnt sketch-to-photo
and photo-to-sketch mappings from in-domain input to open-domain categories. On
the Scribble and SketchyCOCO datasets, we compared our technique to the most
current competing methods. For many types of open-domain drawings, our model
outperforms impressive results in synthesising accurate colour, substance, and
retaining the structural layout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1">Ashild Kummen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1">Ali Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1">Teodora Ganeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1">Qianying Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1">Robert Shaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1">Chenuka Ratwatte</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yang Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1">Emil Almazov</a>, <a href="http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1">Sheena Visram</a>, <a href="http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1">Andrew Taylor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1">Neil J Sebire</a>, <a href="http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1">Lee Stott</a>, <a href="http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1">Yvonne Rogers</a>, <a href="http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1">Graham Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1">Dean Mohamedally</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04357">
<div class="article-summary-box-inner">
<span><p>Touchless computer interaction has become an important consideration during
the COVID-19 pandemic period. Despite progress in machine learning and computer
vision that allows for advanced gesture recognition, an integrated collection
of such open-source methods and a user-customisable approach to utilising them
in a low-cost solution for touchless interaction in existing software is still
missing. In this paper, we introduce the MotionInput v2.0 application. This
application utilises published open-source libraries and additional gesture
definitions developed to take the video stream from a standard RGB webcam as
input. It then maps human motion gestures to input operations for existing
applications and games. The user can choose their own preferred way of
interacting from a series of motion types, including single and bi-modal hand
gesturing, full-body repetitive or extremities-based exercises, head and facial
movements, eye tracking, and combinations of the above. We also introduce a
series of bespoke gesture recognition classifications as DirectInput triggers,
including gestures for idle states, auto calibration, depth capture from a 2D
RGB webcam stream and tracking of facial motions such as mouth motions,
winking, and head direction with rotation. Three use case areas assisted the
development of the modules: creativity software, office and clinical software,
and gaming software. A collection of open-source libraries has been integrated
and provide a layer of modular gesture mapping on top of existing mouse and
keyboard controls in Windows via DirectX. With ease of access to webcams
integrated into most laptops and desktop computers, touchless computing becomes
more available with MotionInput v2.0, in a federated and locally processed
method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1">Ayaan Haque</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1">Ipsita Sutradhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1">Mahziba Rahman</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">Mehedi Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1">Malabika Sarker</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04358">
<div class="article-summary-box-inner">
<span><p>Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as
a result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye
illness caused by diabetes, can lead to blindness if it is not identified and
treated in its early stages. Unfortunately, diagnosis of DR requires medically
trained professionals, but Bangladesh has limited specialists in comparison to
its population. Moreover, the screening process is often expensive, prohibiting
many from receiving timely and proper diagnosis. To address the problem, we
introduce a deep learning algorithm which screens for different stages of DR.
We use a state-of-the-art CNN architecture to diagnose patients based on
retinal fundus imagery. This paper is an experimental evaluation of the
algorithm we developed for DR diagnosis and screening specifically for
Bangladeshi patients. We perform this validation study using separate pools of
retinal image data of real patients from a hospital and field studies in
Bangladesh. Our results show that the algorithm is effective at screening
Bangladeshi eyes even when trained on a public dataset which is out of domain,
and can accurately determine the stage of DR as well, achieving an overall
accuracy of 92.27\% and 93.02\% on two validation sets of Bangladeshi eyes. The
results confirm the ability of the algorithm to be used in real clinical
settings and applications due to its high accuracy and classwise metrics. Our
algorithm is implemented in the application Drishti, which is used to screen
for DR in patients living in rural areas in Bangladesh, where access to
professional screening is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Adaptable image quality assessment using meta-reinforcement learning of task amenability.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1">Shaheer U. Saeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yunguan Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1">Vasilis Stavrinides</a>, <a href="http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1">Zachary M. C. Baum</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qianye Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1">Mirabela Rusu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1">Richard E. Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1">Geoffrey A. Sonn</a>, <a href="http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1">J. Alison Noble</a>, <a href="http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1">Dean C. Barratt</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yipeng Hu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04359">
<div class="article-summary-box-inner">
<span><p>The performance of many medical image analysis tasks are strongly associated
with image data quality. When developing modern deep learning algorithms,
rather than relying on subjective (human-based) image quality assessment (IQA),
task amenability potentially provides an objective measure of task-specific
image quality. To predict task amenability, an IQA agent is trained using
reinforcement learning (RL) with a simultaneously optimised task predictor,
such as a classification or segmentation neural network. In this work, we
develop transfer learning or adaptation strategies to increase the adaptability
of both the IQA agent and the task predictor so that they are less dependent on
high-quality, expert-labelled training data. The proposed transfer learning
strategy re-formulates the original RL problem for task amenability in a
meta-reinforcement learning (meta-RL) framework. The resulting algorithm
facilitates efficient adaptation of the agent to different definitions of image
quality, each with its own Markov decision process environment including
different images, labels and an adaptable task predictor. Our work demonstrates
that the IQA agents pre-trained on non-expert task labels can be adapted to
predict task amenability as defined by expert task labels, using only a small
set of expert labels. Using 6644 clinical ultrasound images from 249 prostate
cancer patients, our results for image classification and segmentation tasks
show that the proposed IQA method can be adapted using data with as few as
respective 19.7% and 29.6% expert-reviewed consensus labels and still achieve
comparable IQA and task performance, which would otherwise require a training
dataset with 100% expert labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1">Yuki Tatsunami</a>, <a href="http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1">Masato Taki</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04384">
<div class="article-summary-box-inner">
<span><p>For the past ten years, CNN has reigned supreme in the world of computer
vision, but recently, Transformer is on the rise. However, the quadratic
computational cost of self-attention has become a severe problem of practice.
There has been much research on architectures without CNN and self-attention in
this context. In particular, MLP-Mixer is a simple idea designed using MLPs and
hit an accuracy comparable to the Vision Transformer. However, the only
inductive bias in this architecture is the embedding of tokens. Thus, there is
still a possibility to build a non-convolutional inductive bias into the
architecture itself, and we built in an inductive bias using two simple ideas.
A way is to divide the token-mixing block vertically and horizontally. Another
way is to make spatial correlations denser among some channels of token-mixing.
With this approach, we were able to improve the accuracy of the MLP-Mixer while
reducing its parameters and computational complexity. Compared to other
MLP-based models, the proposed model, named RaftMLP has a good balance of
computational complexity, the number of parameters, and actual memory usage. In
addition, our work indicates that MLP-based models have the potential to
replace CNNs by adopting inductive bias. The source code in PyTorch version is
available at \url{https://github.com/okojoalg/raft-mlp}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Diversity-aware Web APIs Recommendation with Compatibility Guarantee.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gonga_W/0/1/0/all/0/1">Wenwen Gonga</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yulan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1">Yucong Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yawei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chena_Y/0/1/0/all/0/1">Yifei Chena</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1">Lianyong Qi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04389">
<div class="article-summary-box-inner">
<span><p>With the ever-increasing prevalence of web APIs (Application Programming
Interfaces) in enabling smart software developments, finding and composing a
list of existing web APIs that can corporately fulfil the software developers'
functional needs have become a promising way to develop a successful mobile
app, economically and conveniently. However, the big volume and diversity of
candidate web APIs put additional burden on the app developers' web APIs
selection decision-makings, since it is often a challenging task to
simultaneously guarantee the diversity and compatibility of the finally
selected a set of web APIs. Considering this challenge, a Diversity-aware and
Compatibility-driven web APIs Recommendation approach, namely DivCAR, is put
forward in this paper. First, to achieve diversity, DivCAR employs random walk
sampling technique on a pre-built correlation graph to generate diverse
correlation subgraphs. Afterwards, with the diverse correlation subgraphs, we
model the compatible web APIs recommendation problem to be a minimum group
Steiner tree search problem. Through solving the minimum group Steiner tree
search problem, manifold sets of compatible and diverse web APIs ranked are
returned to the app developers. At last, we design and enact a set of
experiments on a real-world dataset crawled from www.programmableWeb.com.
Experimental results validate the effectiveness and efficiency of our proposed
DivCAR approach in balancing the web APIs recommendation diversity and
compatibility.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Rethinking Architecture Selection in Differentiable NAS.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruochen Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1">Minhao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiangning Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xiaocheng Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1">Cho-Jui Hsieh</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04392">
<div class="article-summary-box-inner">
<span><p>Differentiable Neural Architecture Search is one of the most popular Neural
Architecture Search (NAS) methods for its search efficiency and simplicity,
accomplished by jointly optimizing the model weight and architecture parameters
in a weight-sharing supernet via gradient-based algorithms. At the end of the
search phase, the operations with the largest architecture parameters will be
selected to form the final architecture, with the implicit assumption that the
values of architecture parameters reflect the operation strength. While much
has been discussed about the supernet's optimization, the architecture
selection process has received little attention. We provide empirical and
theoretical analysis to show that the magnitude of architecture parameters does
not necessarily indicate how much the operation contributes to the supernet's
performance. We propose an alternative perturbation-based architecture
selection that directly measures each operation's influence on the supernet. We
re-evaluate several differentiable NAS methods with the proposed architecture
selection and find that it is able to extract significantly improved
architectures from the underlying supernets consistently. Furthermore, we find
that several failure modes of DARTS can be greatly alleviated with the proposed
selection method, indicating that much of the poor generalization observed in
DARTS can be attributed to the failure of magnitude-based architecture
selection rather than entirely the optimization of its supernet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ On Procedural Adversarial Noise Attack And Defense.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Jun Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xiaoyang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Huilin Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1">Wancheng Ge</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04409">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks (DNNs) are vulnerable to adversarial examples which
would inveigle neural networks to make prediction errors with small per-
turbations on the input images. Researchers have been devoted to promoting the
research on the universal adversarial perturbations (UAPs) which are
gradient-free and have little prior knowledge on data distributions. Procedural
adversarial noise at- tack is a data-free universal perturbation generation
method. In this paper, we propose two universal adversarial perturbation (UAP)
generation methods based on procedural noise functions: Simplex noise and
Worley noise. In our framework, the shading which disturbs visual
classification is generated with rendering technology. Without changing the
semantic representations, the adversarial examples generated via our methods
show superior performance on the attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Privacy-Preserving Machine Learning: Methods, Challenges and Directions.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Runhua Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1">Nathalie Baracaldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_J/0/1/0/all/0/1">James Joshi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04417">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) is increasingly being adopted in a wide variety of
application domains. Usually, a well-performing ML model, especially, emerging
deep neural network model, relies on a large volume of training data and
high-powered computational resources. The need for a vast volume of available
data raises serious privacy concerns because of the risk of leakage of highly
privacy-sensitive information and the evolving regulatory environments that
increasingly restrict access to and use of privacy-sensitive data. Furthermore,
a trained ML model may also be vulnerable to adversarial attacks such as
membership/property inference attacks and model inversion attacks. Hence,
well-designed privacy-preserving ML (PPML) solutions are crucial and have
attracted increasing research interest from academia and industry. More and
more efforts of PPML are proposed via integrating privacy-preserving techniques
into ML algorithms, fusing privacy-preserving approaches into ML pipeline, or
designing various privacy-preserving architectures for existing ML systems. In
particular, existing PPML arts cross-cut ML, system, security, and privacy;
hence, there is a critical need to understand state-of-art studies, related
challenges, and a roadmap for future research. This paper systematically
reviews and summarizes existing privacy-preserving approaches and proposes a
PGU model to guide evaluation for various PPML solutions through elaborately
decomposing their privacy-preserving functionalities. The PGU model is designed
as the triad of Phase, Guarantee, and technical Utility. Furthermore, we also
discuss the unique characteristics and challenges of PPML and outline possible
directions of future work that benefit a wide range of research communities
among ML, distributed systems, security, and privacy areas.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1">Balagopal Unnikrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1">Cuong Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1">Shafa Balaram</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1">Chuan Sheng Foo</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1">Pavitra Krishnaswamy</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04423">
<div class="article-summary-box-inner">
<span><p>Deep learning models achieve strong performance for radiology image
classification, but their practical application is bottlenecked by the need for
large labeled training datasets. Semi-supervised learning (SSL) approaches
leverage small labeled datasets alongside larger unlabeled datasets and offer
potential for reducing labeling cost. In this work, we introduce NoTeacher, a
novel consistency-based SSL framework which incorporates probabilistic
graphical models. Unlike Mean Teacher which maintains a teacher network updated
via a temporal ensemble, NoTeacher employs two independent networks, thereby
eliminating the need for a teacher network. We demonstrate how NoTeacher can be
customized to handle a range of challenges in radiology image classification.
Specifically, we describe adaptations for scenarios with 2D and 3D inputs, uni
and multi-label classification, and class distribution mismatch between labeled
and unlabeled portions of the training data. In realistic empirical evaluations
on three public benchmark datasets spanning the workhorse modalities of
radiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the
fully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher
outperforms established SSL methods with minimal hyperparameter tuning, and has
implications as a principled and practical option for semisupervised learning
in radiology applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Tensor Principal Component Analysis in High Dimensional CP Models.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/stat/1/au:+Han_Y/0/1/0/all/0/1">Yuefeng Han</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1">Cun-Hui Zhang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04428">
<div class="article-summary-box-inner">
<span><p>The CP decomposition for high dimensional non-orthogonal spike tensors is an
important problem with broad applications across many disciplines. However,
previous works with theoretical guarantee typically assume restrictive
incoherence conditions on the basis vectors for the CP components. In this
paper, we propose new computationally efficient composite PCA and concurrent
orthogonalization algorithms for tensor CP decomposition with theoretical
guarantees under mild incoherence conditions. The composite PCA applies the
principal component or singular value decompositions twice, first to a matrix
unfolding of the tensor data to obtain singular vectors and then to the matrix
folding of the singular vectors obtained in the first step. It can be used as
an initialization for any iterative optimization schemes for the tensor CP
decomposition. The concurrent orthogonalization algorithm iteratively estimates
the basis vector in each mode of the tensor by simultaneously applying
projections to the orthogonal complements of the spaces generated by others CP
components in other modes. It is designed to improve the alternating least
squares estimator and other forms of the high order orthogonal iteration for
tensors with low or moderately high CP ranks. Our theoretical investigation
provides estimation accuracy and statistical convergence rates for the two
proposed algorithms. Our implementations on synthetic data demonstrate
significant practical superiority of our approach over existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Enhancing Knowledge Tracing via Adversarial Training.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaopeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhijie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jie Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1">Mingyu Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1">Maojing Shu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jun Sun</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04430">
<div class="article-summary-box-inner">
<span><p>We study the problem of knowledge tracing (KT) where the goal is to trace the
students' knowledge mastery over time so as to make predictions on their future
performance. Owing to the good representation capacity of deep neural networks
(DNNs), recent advances on KT have increasingly concentrated on exploring DNNs
to improve the performance of KT. However, we empirically reveal that the DNNs
based KT models may run the risk of overfitting, especially on small datasets,
leading to limited generalization. In this paper, by leveraging the current
advances in adversarial training (AT), we propose an efficient AT based KT
method (ATKT) to enhance KT model's generalization and thus push the limit of
KT. Specifically, we first construct adversarial perturbations and add them on
the original interaction embeddings as adversarial examples. The original and
adversarial examples are further used to jointly train the KT model, forcing it
is not only to be robust to the adversarial examples, but also to enhance the
generalization over the original ones. To better implement AT, we then present
an efficient attentive-LSTM model as KT backbone, where the key is a proposed
knowledge hidden state attention module that adaptively aggregates information
from previous knowledge hidden states while simultaneously highlighting the
importance of current knowledge hidden state to make a more accurate
prediction. Extensive experiments on four public benchmark datasets demonstrate
that our ATKT achieves new state-of-the-art performance. Code is available at:
\color{blue} {\url{https://github.com/xiaopengguo/ATKT}}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Revisit the Fundamental Theorem of Linear Algebra.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jun Lu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04432">
<div class="article-summary-box-inner">
<span><p>This survey is meant to provide an introduction to the fundamental theorem of
linear algebra and the theories behind them. Our goal is to give a rigorous
introduction to the readers with prior exposure to linear algebra.
Specifically, we provide some details and proofs of some results from (Strang,
1993). We then describe the fundamental theorem of linear algebra from
different views and find the properties and relationships behind the views. The
fundamental theorem of linear algebra is essential in many fields, such as
electrical engineering, computer science, machine learning, and deep learning.
This survey is primarily a summary of purpose, significance of important
theories behind it.
</p>
<p>The sole aim of this survey is to give a self-contained introduction to
concepts and mathematical tools in theory behind the fundamental theorem of
linear algebra and rigorous analysis in order to seamlessly introduce its
properties in four subspaces in subsequent sections. However, we clearly
realize our inability to cover all the useful and interesting results and given
the paucity of scope to present this discussion, e.g., the separated analysis
of the (orthogonal) projection matrices. We refer the reader to literature in
the field of linear algebra for a more detailed introduction to the related
fields. Some excellent examples include (Rose, 1982; Strang, 2009; Trefethen
and Bau III, 1997; Strang, 2019, 2021).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Deep Learning Enhanced Dynamic Mode Decomposition.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Curtis_C/0/1/0/all/0/1">Christopher W. Curtis</a>, <a href="http://arxiv.org/find/cs/1/au:+Alford_Lago_D/0/1/0/all/0/1">Daniel Jay Alford-Lago</a>, <a href="http://arxiv.org/find/cs/1/au:+Issan_O/0/1/0/all/0/1">Opal Issan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04433">
<div class="article-summary-box-inner">
<span><p>Koopman operator theory shows how nonlinear dynamical systems can be
represented as an infinite-dimensional, linear operator acting on a Hilbert
space of observables of the system. However, determining the relevant modes and
eigenvalues of this infinite-dimensional operator can be difficult. The
extended dynamic mode decomposition (EDMD) is one such method for generating
approximations to Koopman spectra and modes, but the EDMD method faces its own
set of challenges due to the need of user defined observables. To address this
issue, we explore the use of convolutional autoencoder networks to
simultaneously find optimal families of observables which also generate both
accurate embeddings of the flow into a space of observables and immersions of
the observables back into flow coordinates. This network results in a global
transformation of the flow and affords future state prediction via EDMD and the
decoder network. We call this method deep learning dynamic mode decomposition
(DLDMD). The method is tested on canonical nonlinear data sets and is shown to
produce results that outperform a standard DMD approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ A Generalizable Model-and-Data Driven Approach for Open-Set RFF Authentication.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1">Renjie Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yanzhi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jiabao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1">Aiqun Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1">Derrick Wing Kwan Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Swindlehurst_A/0/1/0/all/0/1">A. Lee Swindlehurst</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04436">
<div class="article-summary-box-inner">
<span><p>Radio-frequency fingerprints~(RFFs) are promising solutions for realizing
low-cost physical layer authentication. Machine learning-based methods have
been proposed for RFF extraction and discrimination. However, most existing
methods are designed for the closed-set scenario where the set of devices is
remains unchanged. These methods can not be generalized to the RFF
discrimination of unknown devices. To enable the discrimination of RFF from
both known and unknown devices, we propose a new end-to-end deep learning
framework for extracting RFFs from raw received signals. The proposed framework
comprises a novel preprocessing module, called neural synchronization~(NS),
which incorporates the data-driven learning with signal processing priors as an
inductive bias from communication-model based processing. Compared to
traditional carrier synchronization techniques, which are static, this module
estimates offsets by two learnable deep neural networks jointly trained by the
RFF extractor. Additionally, a hypersphere representation is proposed to
further improve the discrimination of RFF. Theoretical analysis shows that such
a data-and-model framework can better optimize the mutual information between
device identity and the RFF, which naturally leads to better performance.
Experimental results verify that the proposed RFF significantly outperforms
purely data-driven DNN-design and existing handcrafted RFF methods in terms of
both discrimination and network generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ AdaRNN: Adaptive Learning and Forecasting of Time Series.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yuntao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jindong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1">Wenjie Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Sinno Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1">Tao Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chongjun Wang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04443">
<div class="article-summary-box-inner">
<span><p>Time series has wide applications in the real world and is known to be
difficult to forecast. Since its statistical properties change over time, its
distribution also changes temporally, which will cause severe distribution
shift problem to existing methods. However, it remains unexplored to model the
time series in the distribution perspective. In this paper, we term this as
Temporal Covariate Shift (TCS). This paper proposes Adaptive RNNs (AdaRNN) to
tackle the TCS problem by building an adaptive model that generalizes well on
the unseen test data. AdaRNN is sequentially composed of two novel algorithms.
First, we propose Temporal Distribution Characterization to better characterize
the distribution information in the TS. Second, we propose Temporal
Distribution Matching to reduce the distribution mismatch in TS to learn the
adaptive TS model. AdaRNN is a general framework with flexible distribution
distances integrated. Experiments on human activity recognition, air quality
prediction, and financial analysis show that AdaRNN outperforms the latest
methods by a classification accuracy of 2.6% and significantly reduces the RMSE
by 9.0%. We also show that the temporal distribution matching algorithm can be
extended in Transformer structure to boost its performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Decentralized Composite Optimization with Compression.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaorui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jiliang Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1">Ming Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1">Kun Yuan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04448">
<div class="article-summary-box-inner">
<span><p>Decentralized optimization and communication compression have exhibited their
great potential in accelerating distributed machine learning by mitigating the
communication bottleneck in practice. While existing decentralized algorithms
with communication compression mostly focus on the problems with only smooth
components, we study the decentralized stochastic composite optimization
problem with a potentially non-smooth component. A \underline{Prox}imal
gradient \underline{L}in\underline{EA}r convergent \underline{D}ecentralized
algorithm with compression, Prox-LEAD, is proposed with rigorous theoretical
analyses in the general stochastic setting and the finite-sum setting. Our
theorems indicate that Prox-LEAD works with arbitrary compression precision,
and it tremendously reduces the communication cost almost for free. The
superiorities of the proposed algorithms are demonstrated through the
comparison with state-of-the-art algorithms in terms of convergence
complexities and numerical experiments. Our algorithmic framework also
generally enlightens the compressed communication on other primal-dual
algorithms by reducing the impact of inexact iterations, which might be of
independent interest.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ An empirical investigation into audio pipeline approaches for classifying bird species.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Behr_D/0/1/0/all/0/1">David Behr</a>, <a href="http://arxiv.org/find/cs/1/au:+Maina_C/0/1/0/all/0/1">Ciira wa Maina</a>, <a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1">Vukosi Marivate</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04449">
<div class="article-summary-box-inner">
<span><p>This paper is an investigation into aspects of an audio classification
pipeline that will be appropriate for the monitoring of bird species on edges
devices. These aspects include transfer learning, data augmentation and model
optimization. The hope is that the resulting models will be good candidates to
deploy on edge devices to monitor bird populations. Two classification
approaches will be taken into consideration, one which explores the
effectiveness of a traditional Deep Neural Network(DNN) and another that makes
use of Convolutional layers.This study aims to contribute empirical evidence of
the merits and demerits of each approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ High Quality Related Search Query Suggestions using Deep Reinforcement Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1">Praveen Kumar Bodigutla</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04452">
<div class="article-summary-box-inner">
<span><p>"High Quality Related Search Query Suggestions" task aims at recommending
search queries which are real, accurate, diverse, relevant and engaging.
Obtaining large amounts of query-quality human annotations is expensive. Prior
work on supervised query suggestion models suffered from selection and exposure
bias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),
leading to low quality suggestions. Reinforcement Learning techniques employed
to reformulate a query using terms from search results, have limited
scalability to large-scale industry applications. To recommend high quality
related search queries, we train a Deep Reinforcement Learning model to predict
the query a user would enter next. The reward signal is composed of long-term
session-based user feedback, syntactic relatedness and estimated naturalness of
generated query. Over the baseline supervised model, our proposed approach
achieves a significant relative improvement in terms of recommendation
diversity (3%), down-stream user-engagement (4.2%) and per-sentence word
repetitions (82%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation Systems: A Survey.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1">Zefang Zong</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1">Tao Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1">Tong Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Depeng/0/1/0/all/0/1">Depeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yong Li</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04462">
<div class="article-summary-box-inner">
<span><p>Recent technology development brings the booming of numerous new
Demand-Driven Services (DDS) into urban lives, including ridesharing, on-demand
delivery, express systems and warehousing. In DDS, a service loop is an
elemental structure, including its service worker, the service providers and
corresponding service targets. The service workers should transport either
humans or parcels from the providers to the target locations. Various planning
tasks within DDS can thus be classified into two individual stages: 1)
Dispatching, which is to form service loops from demand/supply distributions,
and 2)Routing, which is to decide specific serving orders within the
constructed loops. Generating high-quality strategies in both stages is
important to develop DDS but faces several challenging. Meanwhile, deep
reinforcement learning (DRL) has been developed rapidly in recent years. It is
a powerful tool to solve these problems since DRL can learn a parametric model
without relying on too many problem-based assumptions and optimize long-term
effect by learning sequential decisions. In this survey, we first define DDS,
then highlight common applications and important decision/control problems
within. For each problem, we comprehensively introduce the existing DRL
solutions, and further summarize them in
\textit{https://github.com/tsinghua-fib-lab/DDS\_Survey}. We also introduce
open simulation environments for development and evaluation of DDS
applications. Finally, we analyze remaining challenges and discuss further
research opportunities in DRL solutions for DDS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Computational complexity of Inexact Proximal Point Algorithm for Convex Optimization under Holderian Growth.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Patrascu_A/0/1/0/all/0/1">Andrei Patrascu</a>, <a href="http://arxiv.org/find/cs/1/au:+Irofti_P/0/1/0/all/0/1">Paul Irofti</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04482">
<div class="article-summary-box-inner">
<span><p>Several decades ago the Proximal Point Algorithm (PPA) started to gain much
attraction for both abstract operator theory and the numerical optimization
communities. Even in modern applications, researchers still use proximal
minimization theory to design scalable algorithms that overcome nonsmoothness
in high dimensional models. Several remarkable references as
\cite{Fer:91,Ber:82constrained,Ber:89parallel,Tom:11} analyzed the tight local
relations between the convergence rate of PPA and the regularity of the
objective function. However, without taking into account the concrete
computational effort paid for computing each PPA iteration, any iteration
complexity remains abstract and purely informative. In this manuscript we aim
to evaluate the computational complexity of practical PPA in terms of
(proximal) gradient/subgradient iterations, which might allow a fair
positioning of the famous PPA numerical performance in the class of first order
methods. First, we derive nonasymptotic iteration complexity estimates of exact
and inexact PPA to minimize convex functions under $\gamma-$Holderian growth:
$\BigO{\log(1/\epsilon)}$ (for $\gamma \in [1,2]$) and
$\BigO{1/\epsilon^{\gamma - 2}}$ (for $\gamma &gt; 2$). In particular, we recover
well-known results on exact PPA: finite convergence for sharp minima and linear
convergence for quadratic growth, even under presence of inexactness. Second,
assuming that an usual (proximal) gradient/subgradient method subroutine is
employed to compute inexact PPA iteration, we show novel computational
complexity bounds on a restarted variant of the inexact PPA, available when no
information on the growth of the objective function is known. In the numerical
experiments we confirm the practical performance and implementability of our
schemes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Regularized Sequential Latent Variable Models with Adversarial Neural Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1">Ming Xiao</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04496">
<div class="article-summary-box-inner">
<span><p>The recurrent neural networks (RNN) with richly distributed internal states
and flexible non-linear transition functions, have overtaken the dynamic
Bayesian networks such as the hidden Markov models (HMMs) in the task of
modeling highly structured sequential data. These data, such as from speech and
handwriting, often contain complex relationships between the underlaying
variational factors and the observed data. The standard RNN model has very
limited randomness or variability in its structure, coming from the output
conditional probability model. This paper will present different ways of using
high level latent random variables in RNN to model the variability in the
sequential data, and the training method of such RNN model under the VAE
(Variational Autoencoder) principle. We will explore possible ways of using
adversarial method to train a variational RNN model. Contrary to competing
approaches, our approach has theoretical optimum in the model training and
provides better model training stability. Our approach also improves the
posterior approximation in the variational inference network by a separated
adversarial training step. Numerical results simulated from TIMIT speech data
show that reconstruction loss and evidence lower bound converge to the same
level and adversarial training loss converges to 0.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ A Survey on Deep Reinforcement Learning for Data Processing and Analytics.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1">Qingpeng Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1">Can Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1">Yiyuan Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhongle Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Meihui Zhang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04526">
<div class="article-summary-box-inner">
<span><p>Data processing and analytics are fundamental and pervasive. Algorithms play
a vital role in data processing and analytics where many algorithm designs have
incorporated heuristics and general rules from human knowledge and experience
to improve their effectiveness. Recently, reinforcement learning, deep
reinforcement learning (DRL) in particular, is increasingly explored and
exploited in many areas because it can learn better strategies in complicated
environments it is interacting with than statically designed algorithms.
Motivated by this trend, we provide a comprehensive review of recent works
focusing on utilizing deep reinforcement learning to improve data processing
and analytics. First, we present an introduction to key concepts, theories, and
methods in deep reinforcement learning. Next, we discuss deep reinforcement
learning deployment on database systems, facilitating data processing and
analytics in various aspects, including data organization, scheduling, tuning,
and indexing. Then, we survey the application of deep reinforcement learning in
data processing and analytics, ranging from data preparation, natural language
interface to healthcare, fintech, etc. Finally, we discuss important open
challenges and future research directions of using deep reinforcement learning
in data processing and analytics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Tailin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Desen Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shidong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuming He</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1">Errui Ding</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04536">
<div class="article-summary-box-inner">
<span><p>The task of skeleton-based action recognition remains a core challenge in
human-centred scene understanding due to the multiple granularities and large
variation in human motion. Existing approaches typically employ a single neural
representation for different motion patterns, which has difficulty in capturing
fine-grained action classes given limited training data. To address the
aforementioned problems, we propose a novel multi-granular spatio-temporal
graph network for skeleton-based action classification that jointly models the
coarse- and fine-grained skeleton motion patterns. To this end, we develop a
dual-head graph network consisting of two interleaved branches, which enables
us to extract features at two spatio-temporal resolutions in an effective and
efficient manner. Moreover, our network utilises a cross-head communication
strategy to mutually enhance the representations of both heads. We conducted
extensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU
RGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance
on all the benchmarks, which validates the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1">Andreas Maier</a>, <a href="http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1">Harald K&#xf6;stler</a>, <a href="http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1">Marco Heisig</a>, <a href="http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1">Patrick Krauss</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Seung Hee Yang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04543">
<div class="article-summary-box-inner">
<span><p>In this article, we perform a review of the state-of-the-art of hybrid
machine learning in medical imaging. We start with a short summary of the
general developments of the past in machine learning and how general and
specialized approaches have been in competition in the past decades. A
particular focus will be the theoretical and experimental evidence pro and
contra hybrid modelling. Next, we inspect several new developments regarding
hybrid machine learning with a particular focus on so-called known operator
learning and how hybrid approaches gain more and more momentum across
essentially all applications in medical imaging and medical image analysis. As
we will point out by numerous examples, hybrid models are taking over in image
reconstruction and analysis. Even domains such as physical simulation and
scanner and acquisition design are being addressed using machine learning grey
box modelling approaches. Towards the end of the article, we will investigate a
few future directions and point out relevant areas in which hybrid modelling,
meta learning, and other domains will likely be able to drive the
state-of-the-art ahead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ABC-FL: Anomalous and Benign client Classification in Federated Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1">Hyejun Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1">Joonyong Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1">Tai Myung Chung</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04551">
<div class="article-summary-box-inner">
<span><p>Federated Learning is a distributed machine learning framework designed for
data privacy preservation i.e., local data remain private throughout the entire
training and testing procedure. Federated Learning is gaining popularity
because it allows one to use machine learning techniques while preserving
privacy. However, it inherits the vulnerabilities and susceptibilities raised
in deep learning techniques. For instance, Federated Learning is particularly
vulnerable to data poisoning attacks that may deteriorate its performance and
integrity due to its distributed nature and inaccessibility to the raw data. In
addition, it is extremely difficult to correctly identify malicious clients due
to the non-Independently and/or Identically Distributed (non-IID) data. The
real-world data can be complex and diverse, making them hardly distinguishable
from the malicious data without direct access to the raw data. Prior research
has focused on detecting malicious clients while treating only the clients
having IID data as benign. In this study, we propose a method that detects and
classifies anomalous clients from benign clients when benign ones have non-IID
data. Our proposed method leverages feature dimension reduction, dynamic
clustering, and cosine similarity-based clipping. The experimental results
validates that our proposed method not only classifies the malicious clients
but also alleviates their negative influences from the entire procedure. Our
findings may be used in future studies to effectively eliminate anomalous
clients when building a model with diverse data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ The Benefits of Implicit Regularization from SGD in Least Squares Problems.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1">Difan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jingfeng Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1">Vladimir Braverman</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1">Quanquan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1">Dean P. Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1">Sham M. Kakade</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04552">
<div class="article-summary-box-inner">
<span><p>Stochastic gradient descent (SGD) exhibits strong algorithmic regularization
effects in practice, which has been hypothesized to play an important role in
the generalization of modern machine learning approaches. In this work, we seek
to understand these issues in the simpler setting of linear regression
(including both underparameterized and overparameterized regimes), where our
goal is to make sharp instance-based comparisons of the implicit regularization
afforded by (unregularized) average SGD with the explicit regularization of
ridge regression. For a broad class of least squares problem instances (that
are natural in high-dimensional settings), we show: (1) for every problem
instance and for every ridge parameter, (unregularized) SGD, when provided with
logarithmically more samples than that provided to the ridge algorithm,
generalizes no worse than the ridge solution (provided SGD uses a tuned
constant stepsize); (2) conversely, there exist instances (in this wide problem
class) where optimally-tuned ridge regression requires quadratically more
samples than SGD in order to have the same generalization performance. Taken
together, our results show that, up to the logarithmic factors, the
generalization performance of SGD is always no worse than that of ridge
regression in a wide range of overparameterized problems, and, in fact, could
be much better for some problem instances. More generally, our results show how
algorithmic regularization has important consequences even in simpler
(overparameterized) convex settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Data Driven VRP: A Neural Network Model to Learn Hidden Preferences for VRP.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1">Jayanta Mandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Canoy_R/0/1/0/all/0/1">Rocsildes Canoy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bucarey_V/0/1/0/all/0/1">V&#xed;ctor Bucarey</a>, <a href="http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1">Tias Guns</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04578">
<div class="article-summary-box-inner">
<span><p>The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the
total distance of the routes under the capacity constraints of the vehicles.
But more often, the objective involves multiple criteria including not only the
total distance of the tour but also other factors such as travel costs, travel
time, and fuel consumption.Moreover, in reality, there are numerous implicit
preferences ingrained in the minds of the route planners and the drivers.
Drivers, for instance, have familiarity with certain neighborhoods and
knowledge of the state of roads, and often consider the best places for rest
and lunch breaks. This knowledge is difficult to formulate and balance when
operational routing decisions have to be made. This motivates us to learn the
implicit preferences from past solutions and to incorporate these learned
preferences in the optimization process. These preferences are in the form of
arc probabilities, i.e., the more preferred a route is, the higher is the joint
probability. The novelty of this work is the use of a neural network model to
estimate the arc probabilities, which allows for additional features and
automatic parameter estimation. This first requires identifying suitable
features, neural architectures and loss functions, taking into account that
there is typically few data available. We investigate the difference with a
prior weighted Markov counting approach, and study the applicability of neural
networks in this setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1">NareshKumar Gurulingan</a>, <a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1">Elahe Arani</a>, <a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1">Bahram Zonooz</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04584">
<div class="article-summary-box-inner">
<span><p>Scene understanding is crucial for autonomous systems which intend to operate
in the real world. Single task vision networks extract information only based
on some aspects of the scene. In multi-task learning (MTL), on the other hand,
these single tasks are jointly learned, thereby providing an opportunity for
tasks to share information and obtain a more comprehensive understanding. To
this end, we develop UniNet, a unified scene understanding network that
accurately and efficiently infers vital vision tasks including object
detection, semantic segmentation, instance segmentation, monocular depth
estimation, and monocular instance depth prediction. As these tasks look at
different semantic and geometric information, they can either complement or
conflict with each other. Therefore, understanding inter-task relationships can
provide useful cues to enable complementary information sharing. We evaluate
the task relationships in UniNet through the lens of adversarial attacks based
on the notion that they can exploit learned biases and task interactions in the
neural network. Extensive experiments on the Cityscapes dataset, using
untargeted and targeted attacks reveal that semantic tasks strongly interact
amongst themselves, and the same holds for geometric tasks. Additionally, we
show that the relationship between semantic and geometric tasks is asymmetric
and their interaction becomes weaker as we move towards higher-level
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Recurrent neural network-based Internal Model Control of unknown nonlinear stable systems.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bonassi_F/0/1/0/all/0/1">Fabio Bonassi</a>, <a href="http://arxiv.org/find/cs/1/au:+Scattolini_R/0/1/0/all/0/1">Riccardo Scattolini</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04585">
<div class="article-summary-box-inner">
<span><p>Owing to their superior modeling capabilities, gated Recurrent Neural
Networks (RNNs), such as Gated Recurrent Units (GRUs) and Long Short-Term
Memory networks (LSTMs), have become popular tools for learning dynamical
systems. This paper aims to discuss how these networks can be adopted for the
synthesis of Internal Model Control (IMC) architectures. To this end, a first
gated RNN is used to learn a model of the unknown input-output stable plant.
Then, another gated RNN approximating the model inverse is trained. The
proposed scheme is able to cope with the saturation of the control variables,
and it can be deployed on low-power embedded controllers since it does not
require any online computation. The approach is then tested on the Quadruple
Tank benchmark system, resulting in satisfactory closed-loop performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ On Learning and Testing Decision Tree.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bshouty_N/0/1/0/all/0/1">Nader H. Bshouty</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddad_Zaknoon_C/0/1/0/all/0/1">Catherine A. Haddad-Zaknoon</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04587">
<div class="article-summary-box-inner">
<span><p>In this paper, we study learning and testing decision tree of size and depth
that are significantly smaller than the number of attributes $n$.
</p>
<p>Our main result addresses the problem of poly$(n,1/\epsilon)$ time algorithms
with poly$(s,1/\epsilon)$ query complexity (independent of $n$) that
distinguish between functions that are decision trees of size $s$ from
functions that are $\epsilon$-far from any decision tree of size
$\phi(s,1/\epsilon)$, for some function $\phi &gt; s$. The best known result is
the recent one that follows from Blank, Lange and Tan,~\cite{BlancLT20}, that
gives $\phi(s,1/\epsilon)=2^{O((\log^3s)/\epsilon^3)}$. In this paper, we give
a new algorithm that achieves $\phi(s,1/\epsilon)=2^{O(\log^2 (s/\epsilon))}$.
</p>
<p>Moreover, we study the testability of depth-$d$ decision tree and give a {\it
distribution free} tester that distinguishes between depth-$d$ decision tree
and functions that are $\epsilon$-far from depth-$d^2$ decision tree. In
particular, for decision trees of size $s$, the above result holds in the
distribution-free model when the tree depth is $O(\log(s/\epsilon))$.
</p>
<p>We also give other new results in learning and testing of size-$s$ decision
trees and depth-$d$ decision trees that follow from results in the literature
and some results we prove in this paper.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Label-informed Graph Structure Learning for Node Classification.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1">Fenyu Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shu Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Liang Wang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04595">
<div class="article-summary-box-inner">
<span><p>Graph Neural Networks (GNNs) have achieved great success among various
domains. Nevertheless, most GNN methods are sensitive to the quality of graph
structures. To tackle this problem, some studies exploit different graph
structure learning strategies to refine the original graph structure. However,
these methods only consider feature information while ignoring available label
information. In this paper, we propose a novel label-informed graph structure
learning framework which incorporates label information explicitly through a
class transition matrix. We conduct extensive experiments on seven node
classification benchmark datasets and the results show that our method
outperforms or matches the state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1">Arnulf Jentzen</a>, <a href="http://arxiv.org/find/math/1/au:+Riekert_A/0/1/0/all/0/1">Adrian Riekert</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04620">
<div class="article-summary-box-inner">
<span><p>Gradient descent (GD) type optimization methods are the standard instrument
to train artificial neural networks (ANNs) with rectified linear unit (ReLU)
activation. Despite the great success of GD type optimization methods in
numerical simulations for the training of ANNs with ReLU activation, it remains
- even in the simplest situation of the plain vanilla GD optimization method
with random initializations and ANNs with one hidden layer - an open problem to
prove (or disprove) the conjecture that the risk of the GD optimization method
converges in the training of such ANNs to zero as the width of the ANNs, the
number of independent random initializations, and the number of GD steps
increase to infinity. In this article we prove this conjecture in the situation
where the probability distribution of the input data is equivalent to the
continuous uniform distribution on a compact interval, where the probability
distributions for the random initializations of the ANN parameters are standard
normal distributions, and where the target function under consideration is
continuous and piecewise affine linear. Roughly speaking, the key ingredients
in our mathematical convergence analysis are (i) to prove that suitable sets of
global minima of the risk functions are \emph{twice continuously differentiable
submanifolds of the ANN parameter spaces}, (ii) to prove that the Hessians of
the risk functions on these sets of global minima satisfy an appropriate
\emph{maximal rank condition}, and, thereafter, (iii) to apply the machinery in
[Fehrman, B., Gess, B., Jentzen, A., Convergence rates for the stochastic
gradient descent method for non-convex objective functions. J. Mach. Learn.
Res. 21(136): 1--48, 2020] to establish convergence of the GD optimization
method with random initializations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Learning to Maximize Influence.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Panagopoulos_G/0/1/0/all/0/1">George Panagopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Tziortziotis_N/0/1/0/all/0/1">Nikolaos Tziortziotis</a>, <a href="http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1">Fragkiskos D. Malliaros</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1">Michalis Vazirgiannis</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04623">
<div class="article-summary-box-inner">
<span><p>As the field of machine learning for combinatorial optimization advances,
traditional problems are resurfaced and readdressed through this new
perspective. The overwhelming majority of the literature focuses on small graph
problems, while several real-world problems are devoted to large graphs. Here,
we focus on two such problems that are related: influence estimation, a
\#P-hard counting problem, and influence maximization, an NP-hard problem. We
develop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an
upper bound of influence estimation and train it on small simulated graphs.
Experiments show that GLIE can provide accurate predictions faster than the
alternatives for graphs 10 times larger than the train set. More importantly,
it can be used on arbitrary large graphs for influence maximization, as the
predictions can rank effectively seed sets even when the accuracy deteriorates.
To showcase this, we propose a version of a standard Influence Maximization
(IM) algorithm where we substitute traditional influence estimation with the
predictions of GLIE.We also transfer GLIE into a reinforcement learning model
that learns how to choose seeds to maximize influence sequentially using GLIE's
hidden representations and predictions. The final results show that the
proposed methods surpasses a previous GNN-RL approach and perform on par with a
state-of-the-art IM algorithm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Hierarchical Latent Relation Modeling for Collaborative Metric Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1">Viet-Anh Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1">Guillaume Salha-Galvan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1">Romain Hennequin</a>, <a href="http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1">Manuel Moussallam</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04655">
<div class="article-summary-box-inner">
<span><p>Collaborative Metric Learning (CML) recently emerged as a powerful paradigm
for recommendation based on implicit feedback collaborative filtering. However,
standard CML methods learn fixed user and item representations, which fails to
capture the complex interests of users. Existing extensions of CML also either
ignore the heterogeneity of user-item relations, i.e. that a user can
simultaneously like very different items, or the latent item-item relations,
i.e. that a user's preference for an item depends, not only on its intrinsic
characteristics, but also on items they previously interacted with. In this
paper, we present a hierarchical CML model that jointly captures latent
user-item and item-item relations from implicit data. Our approach is inspired
by translation mechanisms from knowledge graph embedding and leverages
memory-based attention networks. We empirically show the relevance of this
joint relational modeling, by outperforming existing CML models on
recommendation tasks on several real-world datasets. Our experiments also
emphasize the limits of current CML relational models on very sparse datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Empirical Analysis on Effectiveness of NLP Methods for Predicting Code Smell.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Himanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Gulanikar_A/0/1/0/all/0/1">Abhiram Anand Gulanikar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1">Lov Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1">Lalita Bhanu Murthy Neti</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04656">
<div class="article-summary-box-inner">
<span><p>A code smell is a surface indicator of an inherent problem in the system,
most often due to deviation from standard coding practices on the developers
part during the development phase. Studies observe that code smells made the
code more susceptible to call for modifications and corrections than code that
did not contain code smells. Restructuring the code at the early stage of
development saves the exponentially increasing amount of effort it would
require to address the issues stemming from the presence of these code smells.
Instead of using traditional features to detect code smells, we use user
comments to manually construct features to predict code smells. We use three
Extreme learning machine kernels over 629 packages to identify eight code
smells by leveraging feature engineering aspects and using sampling techniques.
Our findings indicate that the radial basis functional kernel performs best out
of the three kernel methods with a mean accuracy of 98.52.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yichi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1">Jesper Kers</a>, <a href="http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1">Clarissa A. Cassol</a>, <a href="http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1">Joris J. Roelofs</a>, <a href="http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1">Najia Idrees</a>, <a href="http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1">Alik Farber</a>, <a href="http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1">Samir Haroon</a>, <a href="http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1">Kevin P. Daly</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1">Suvranu Ganguli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1">Vipul C. Chitalia</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1">Vijaya B. Kolachalama</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04658">
<div class="article-summary-box-inner">
<span><p>Development of deep learning systems for biomedical segmentation often
requires access to expert-driven, manually annotated datasets. If more than a
single expert is involved in the annotation of the same images, then the
inter-expert agreement is not necessarily perfect, and no single expert
annotation can precisely capture the so-called ground truth of the regions of
interest on all images. Also, it is not trivial to generate a reference
estimate using annotations from multiple experts. Here we present a deep neural
network, defined as U-Net-and-a-half, which can simultaneously learn from
annotations performed by multiple experts on the same set of images.
U-Net-and-a-half contains a convolutional encoder to generate features from the
input images, multiple decoders that allow simultaneous learning from image
masks obtained from annotations that were independently generated by multiple
experts, and a shared low-dimensional feature space. To demonstrate the
applicability of our framework, we used two distinct datasets from digital
pathology and radiology, respectively. Specifically, we trained two separate
models using pathologist-driven annotations of glomeruli on whole slide images
of human kidney biopsies (10 patients), and radiologist-driven annotations of
lumen cross-sections of human arteriovenous fistulae obtained from
intravascular ultrasound images (10 patients), respectively. The models based
on U-Net-and-a-half exceeded the performance of the traditional U-Net models
trained on single expert annotations alone, thus expanding the scope of
multitask learning in the context of biomedical image segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ An Empirical Study on Predictability of Software Code Smell Using Deep Learning Models.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Himanshu Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_T/0/1/0/all/0/1">Tanmay G. Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1">Lov Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1">Lalita Bhanu Murthy Neti</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1">Aneesh Krishna</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04659">
<div class="article-summary-box-inner">
<span><p>Code Smell, similar to a bad smell, is a surface indication of something
tainted but in terms of software writing practices. This metric is an
indication of a deeper problem lies within the code and is associated with an
issue which is prominent to experienced software developers with acceptable
coding practices. Recent studies have often observed that codes having code
smells are often prone to a higher probability of change in the software
development cycle. In this paper, we developed code smell prediction models
with the help of features extracted from source code to predict eight types of
code smell. Our work also presents the application of data sampling techniques
to handle class imbalance problem and feature selection techniques to find
relevant feature sets. Previous studies had made use of techniques such as
Naive - Bayes and Random forest but had not explored deep learning methods to
predict code smell. A total of 576 distinct Deep Learning models were trained
using the features and datasets mentioned above. The study concluded that the
deep learning models which used data from Synthetic Minority Oversampling
Technique gave better results in terms of accuracy, AUC with the accuracy of
some models improving from 88.47 to 96.84.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ChemiRise: a data-driven retrosynthesis engine.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/physics/1/au:+Sun_X/0/1/0/all/0/1">Xiangyan Sun</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1">Ke Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1">Yuquan Lin</a>, <a href="http://arxiv.org/find/physics/1/au:+Wu_L/0/1/0/all/0/1">Lingjie Wu</a>, <a href="http://arxiv.org/find/physics/1/au:+Xing_H/0/1/0/all/0/1">Haoming Xing</a>, <a href="http://arxiv.org/find/physics/1/au:+Gao_M/0/1/0/all/0/1">Minghong Gao</a>, <a href="http://arxiv.org/find/physics/1/au:+Liu_J/0/1/0/all/0/1">Ji Liu</a>, <a href="http://arxiv.org/find/physics/1/au:+Tan_S/0/1/0/all/0/1">Suocheng Tan</a>, <a href="http://arxiv.org/find/physics/1/au:+Ni_Z/0/1/0/all/0/1">Zekun Ni</a>, <a href="http://arxiv.org/find/physics/1/au:+Han_Q/0/1/0/all/0/1">Qi Han</a>, <a href="http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1">Junqiu Wu</a>, <a href="http://arxiv.org/find/physics/1/au:+Fan_J/0/1/0/all/0/1">Jie Fan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04682">
<div class="article-summary-box-inner">
<span><p>We have developed an end-to-end, retrosynthesis system, named ChemiRise, that
can propose complete retrosynthesis routes for organic compounds rapidly and
reliably. The system was trained on a processed patent database of over 3
million organic reactions. Experimental reactions were atom-mapped, clustered,
and extracted into reaction templates. We then trained a graph convolutional
neural network-based one-step reaction proposer using template embeddings and
developed a guiding algorithm on the directed acyclic graph (DAG) of chemical
compounds to find the best candidate to explore. The atom-mapping algorithm and
the one-step reaction proposer were benchmarked against previous studies and
showed better results. The final product was demonstrated by retrosynthesis
routes reviewed and rated by human experts, showing satisfying functionality
and a potential productivity boost in real-life use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Active Learning for Transition State Calculation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/stat/1/au:+Gu_S/0/1/0/all/0/1">Shuting Gu</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1">Hongqiao Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhou_X/0/1/0/all/0/1">Xiang Zhou</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04698">
<div class="article-summary-box-inner">
<span><p>The transition state (TS) calculation is a grand challenge for computational
intensive energy function. The traditional methods need to evaluate the
gradients of the energy function at a very large number of locations. To reduce
the number of expensive computations of the true gradients, we propose an
active learning framework consisting of a statistical surrogate model, Gaussian
process regression (GPR) for the energy function, and a single-walker dynamics
method, gentle accent dynamics (GAD), for the saddle-type transition states. TS
is detected by the GAD applied to the GPR surrogate for the gradient vector and
the Hessian matrix. Our key ingredient for efficiency improvements is an active
learning method which sequentially designs the most informative locations and
takes evaluations of the original model at these locations to train GPR. We
formulate this active learning task as the optimal experimental design problem
and propose a very efficient sample-based sub-optimal criterion to construct
the optimal locations. We show that the new method significantly decreases the
required number of energy or force evaluations of the original model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ PRECODE - A Generic Model Extension to Prevent Deep Gradient Leakage.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Scheliga_D/0/1/0/all/0/1">Daniel Scheliga</a>, <a href="http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1">Patrick M&#xe4;der</a>, <a href="http://arxiv.org/find/cs/1/au:+Seeland_M/0/1/0/all/0/1">Marco Seeland</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04725">
<div class="article-summary-box-inner">
<span><p>Collaborative training of neural networks leverages distributed data by
exchanging gradient information between different clients. Although training
data entirely resides with the clients, recent work shows that training data
can be reconstructed from such exchanged gradient information. To enhance
privacy, gradient perturbation techniques have been proposed. However, they
come at the cost of reduced model performance, increased convergence time, or
increased data demand. In this paper, we introduce PRECODE, a PRivacy EnhanCing
mODulE that can be used as generic extension for arbitrary model architectures.
We propose a simple yet effective realization of PRECODE using variational
modeling. The stochastic sampling induced by variational modeling effectively
prevents privacy leakage from gradients and in turn preserves privacy of data
owners. We evaluate PRECODE using state of the art gradient inversion attacks
on two different model architectures trained on three datasets. In contrast to
commonly used defense mechanisms, we find that our proposed modification
consistently reduces the attack success rate to 0% while having almost no
negative impact on model training and final performance. As a result, PRECODE
reveals a promising path towards privacy enhancing model extensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Crowdsourced Databases and Sui Generis Rights.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Almeida_G/0/1/0/all/0/1">Gon&#xe7;alo Sim&#xf5;es de Almeida</a>, <a href="http://arxiv.org/find/cs/1/au:+Abreu_G/0/1/0/all/0/1">Gon&#xe7;alo Faria Abreu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04727">
<div class="article-summary-box-inner">
<span><p>In this study we propose a new concept of databases (crowdsourced databases),
adding a new conceptual approach to the debate on legal protection of databases
in Europe. We also summarise the current legal framework and current indexing
and web scraping practices - it would not be prudent to suggest a new theory
without contextualising it in the legal and practical context in which it is
developed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Correlation Clustering Reconstruction in Semi-Adversarial Models.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chierichetti_F/0/1/0/all/0/1">Flavio Chierichetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Panconesi_A/0/1/0/all/0/1">Alessandro Panconesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Re_G/0/1/0/all/0/1">Giuseppe Re</a>, <a href="http://arxiv.org/find/cs/1/au:+Trevisan_L/0/1/0/all/0/1">Luca Trevisan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04729">
<div class="article-summary-box-inner">
<span><p>Correlation Clustering is an important clustering problem with many
applications. We study the reconstruction version of this problem in which one
is seeking to reconstruct a latent clustering that has been corrupted by random
noise and adversarial modifications.
</p>
<p>Concerning the latter, we study a standard "post-adversarial" model, in which
adversarial modifications come after the noise, and also introduce and analyze
a "pre-adversarial" model in which adversarial modifications come before the
noise. Given an input coming from such a semi-adversarial generative model, the
goal is to reconstruct almost perfectly and with high probability the latent
clustering.
</p>
<p>We focus on the case where the hidden clusters have equal size and show the
following. In the pre-adversarial setting, spectral algorithms are optimal, in
the sense that they reconstruct all the way to the information-theoretic
threshold beyond which no reconstruction is possible. In contrast, in the
post-adversarial setting their ability to restore the hidden clusters stops
before the threshold, but the gap is optimally filled by SDP-based algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Multi-Factors Aware Dual-Attentional Knowledge Tracing.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Moyu Zhang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xinning Zhu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chunhong Zhang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1">Yang Ji</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1">Feng Pan</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1">Changchuan Yin</a> (1) ((1) Beijing University of Posts and Telecommunications)
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04741">
<div class="article-summary-box-inner">
<span><p>With the increasing demands of personalized learning, knowledge tracing has
become important which traces students' knowledge states based on their
historical practices. Factor analysis methods mainly use two kinds of factors
which are separately related to students and questions to model students'
knowledge states. These methods use the total number of attempts of students to
model students' learning progress and hardly highlight the impact of the most
recent relevant practices. Besides, current factor analysis methods ignore rich
information contained in questions. In this paper, we propose Multi-Factors
Aware Dual-Attentional model (MF-DAKT) which enriches question representations
and utilizes multiple factors to model students' learning progress based on a
dual-attentional mechanism. More specifically, we propose a novel
student-related factor which records the most recent attempts on relevant
concepts of students to highlight the impact of recent exercises. To enrich
questions representations, we use a pre-training method to incorporate two
kinds of question information including questions' relation and difficulty
level. We also add a regularization term about questions' difficulty level to
restrict pre-trained question representations to fine-tuning during the process
of predicting students' performance. Moreover, we apply a dual-attentional
mechanism to differentiate contributions of factors and factor interactions to
final prediction in different practice records. At last, we conduct experiments
on several real-world datasets and results show that MF-DAKT can outperform
existing knowledge tracing methods. We also conduct several studies to validate
the effects of each component of MF-DAKT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ The information of attribute uncertainties: what convolutional neural networks can learn about errors in input data.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Rodrigues_N/0/1/0/all/0/1">Nat&#xe1;lia V. N. Rodrigues</a>, <a href="http://arxiv.org/find/cs/1/au:+Abramo_L/0/1/0/all/0/1">L. Raul Abramo</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirata_N/0/1/0/all/0/1">Nina S. Hirata</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04742">
<div class="article-summary-box-inner">
<span><p>Errors in measurements are key to weighting the value of data, but are often
neglected in Machine Learning (ML). We show how Convolutional Neural Networks
(CNNs) are able to learn about the context and patterns of signal and noise,
leading to improvements in the performance of classification methods. We
construct a model whereby two classes of objects follow an underlying Gaussian
distribution, and where the features (the input data) have varying, but known,
levels of noise. This model mimics the nature of scientific data sets, where
the noises arise as realizations of some random processes whose underlying
distributions are known. The classification of these objects can then be
performed using standard statistical techniques (e.g., least-squares
minimization or Markov-Chain Monte Carlo), as well as ML techniques. This
allows us to take advantage of a maximum likelihood approach to object
classification, and to measure the amount by which the ML methods are
incorporating the information in the input data uncertainties. We show that,
when each data point is subject to different levels of noise (i.e., noises with
different distribution functions), that information can be learned by the CNNs,
raising the ML performance to at least the same level of the least-squares
method -- and sometimes even surpassing it. Furthermore, we show that, with
varying noise levels, the confidence of the ML classifiers serves as a proxy
for the underlying cumulative distribution function, but only if the
information about specific input data uncertainties is provided to the CNNs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haoyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhize Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04755">
<div class="article-summary-box-inner">
<span><p>Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)
is a classical federated learning algorithm in which clients run multiple local
SGD steps before communicating their update to an orchestrating server. We
propose a new federated learning algorithm, FedPAGE, able to further reduce the
communication complexity by utilizing the recent optimal PAGE method (Li et
al., 2021) instead of plain SGD in FedAvg. We show that FedPAGE uses much fewer
communication rounds than previous local methods for both federated convex and
nonconvex optimization. Concretely, 1) in the convex setting, the number of
communication rounds of FedPAGE is $O(\frac{N^{3/4}}{S\epsilon})$, improving
the best-known result $O(\frac{N}{S\epsilon})$ of SCAFFOLD (Karimireddy et
al.,2020) by a factor of $N^{1/4}$, where $N$ is the total number of clients
(usually is very large in federated learning), $S$ is the sampled subset of
clients in each communication round, and $\epsilon$ is the target error; 2) in
the nonconvex setting, the number of communication rounds of FedPAGE is
$O(\frac{\sqrt{N}+S}{S\epsilon^2})$, improving the best-known result
$O(\frac{N^{2/3}}{S^{2/3}\epsilon^2})$ of SCAFFOLD (Karimireddy et al.,2020) by
a factor of $N^{1/6}S^{1/3}$, if the sampled clients $S\leq \sqrt{N}$. Note
that in both settings, the communication cost for each round is the same for
both FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new state-of-the-art
results in terms of communication complexity for both federated convex and
nonconvex optimization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Imitation Learning by Reinforcement Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/stat/1/au:+Ciosek_K/0/1/0/all/0/1">Kamil Ciosek</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04763">
<div class="article-summary-box-inner">
<span><p>Imitation Learning algorithms learn a policy from demonstrations of expert
behavior. Somewhat counterintuitively, we show that, for deterministic experts,
imitation learning can be done by reduction to reinforcement learning, which is
commonly considered more difficult. We conduct experiments which confirm that
our reduction works well in practice for a continuous control task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Bandit Algorithms for Precision Medicine.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1">Yangyi Lu</a>, <a href="http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1">Ziping Xu</a>, <a href="http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1">Ambuj Tewari</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04782">
<div class="article-summary-box-inner">
<span><p>The Oxford English Dictionary defines precision medicine as "medical care
designed to optimize efficiency or therapeutic benefit for particular groups of
patients, especially by using genetic or molecular profiling." It is not an
entirely new idea: physicians from ancient times have recognized that medical
treatment needs to consider individual variations in patient characteristics.
However, the modern precision medicine movement has been enabled by a
confluence of events: scientific advances in fields such as genetics and
pharmacology, technological advances in mobile devices and wearable sensors,
and methodological advances in computing and data sciences.
</p>
<p>This chapter is about bandit algorithms: an area of data science of special
relevance to precision medicine. With their roots in the seminal work of
Bellman, Robbins, Lai and others, bandit algorithms have come to occupy a
central place in modern data science ( Lattimore and Szepesvari, 2020). Bandit
algorithms can be used in any situation where treatment decisions need to be
made to optimize some health outcome. Since precision medicine focuses on the
use of patient characteristics to guide treatment, contextual bandit algorithms
are especially useful since they are designed to take such information into
account. The role of bandit algorithms in areas of precision medicine such as
mobile health and digital phenotyping has been reviewed before (Tewari and
Murphy, 2017; Rabbi et al., 2019). Since these reviews were published, bandit
algorithms have continued to find uses in mobile health and several new topics
have emerged in the research on bandit algorithms. This chapter is written for
quantitative researchers in fields such as statistics, machine learning, and
operations research who might be interested in knowing more about the
algorithmic and mathematical details of bandit algorithms that have been used
in mobile health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Analyzing Effects of The COVID-19 Pandemic on Road Traffic Safety: The Cases of New York City, Los Angeles, and Boston.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Karadla_L/0/1/0/all/0/1">Lahari Karadla</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weizi Li</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04787">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic has resulted in significant social and economic impacts
throughout the world. In addition to the health consequences, the impacts on
traffic behaviors have also been sudden and dramatic. We have analyzed how the
road traffic safety of New York City, Los Angeles, and Boston in the U.S. have
been impacted by the pandemic and corresponding local government orders and
restrictions. To be specific, we have studied the accident hotspots'
distributions before and after the outbreak of the pandemic and found that
traffic accidents have shifted in both location and time compared to previous
years. In addition, we have studied the road network characteristics in those
hotspot regions with the hope to understand the underlying cause of the hotspot
shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> Meta-repository of screening mammography classifiers.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1">Benjamin Stadnick</a>, <a href="http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1">Jan Witowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1">Vishwaesh Rajiv</a>, <a href="http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1">Jakub Ch&#x142;&#x119;dowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1">Farah E. Shamout</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"><span class="highlight-author">Kyunghyun Cho</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1">Krzysztof J. Geras</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04800">
<div class="article-summary-box-inner">
<span><p>Artificial intelligence (AI) is transforming medicine and showing promise in
improving clinical diagnosis. In breast cancer screening, several recent
studies show that AI has the potential to improve radiologists' accuracy,
subsequently helping in early cancer diagnosis and reducing unnecessary workup.
As the number of proposed models and their complexity grows, it is becoming
increasingly difficult to re-implement them in order to reproduce the results
and to compare different approaches. To enable reproducibility of research in
this application area and to enable comparison between different methods, we
release a meta-repository containing deep learning models for classification of
screening mammograms. This meta-repository creates a framework that enables the
evaluation of machine learning models on any private or public screening
mammography data set. At its inception, our meta-repository contains five
state-of-the-art models with open-source implementations and cross-platform
compatibility. We compare their performance on five international data sets:
two private New York University breast cancer screening data sets as well as
three public (DDSM, INbreast and Chinese Mammography Database) data sets. Our
framework has a flexible design that can be generalized to other medical image
analysis tasks. The meta-repository is available at
https://www.github.com/nyukat/mammography_metarepository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Spiderweb nanomechanical resonators via Bayesian optimization: inspired by nature and guided by machine learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cond-mat/1/au:+Shin_D/0/1/0/all/0/1">Dongil Shin</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Cupertino_A/0/1/0/all/0/1">Andrea Cupertino</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Jong_M/0/1/0/all/0/1">Matthijs H. J. de Jong</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Steeneken_P/0/1/0/all/0/1">Peter G. Steeneken</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Bessa_M/0/1/0/all/0/1">Miguel A. Bessa</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Norte_R/0/1/0/all/0/1">Richard A. Norte</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04809">
<div class="article-summary-box-inner">
<span><p>From ultra-sensitive detectors of fundamental forces to quantum networks and
sensors, mechanical resonators are enabling next-generation technologies to
operate in room temperature environments. Currently, silicon nitride
nanoresonators stand as a leading microchip platform in these advances by
allowing for mechanical resonators whose motion is remarkably isolated from
ambient thermal noise. However, to date, human intuition has remained the
driving force behind design processes. Here, inspired by nature and guided by
machine learning, a spiderweb nanomechanical resonator is developed that
exhibits vibration modes which are isolated from ambient thermal environments
via a novel "torsional soft-clamping" mechanism discovered by the data-driven
optimization algorithm. This bio-inspired resonator is then fabricated;
experimentally confirming a new paradigm in mechanics with quality factors
above 1 billion in room temperature environments. In contrast to other
state-of-the-art resonators, this milestone is achieved with a compact design
which does not require sub-micron lithographic features or complex phononic
bandgaps, making it significantly easier and cheaper to manufacture at large
scales. Here we demonstrate the ability of machine learning to work in tandem
with human intuition to augment creative possibilities and uncover new
strategies in computing and nanotechnology.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Binary Complex Neural Network Acceleration on FPGA.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hongwu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1">Shanglin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Weitze_S/0/1/0/all/0/1">Scott Weitze</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaxin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1">Sahidul Islam</a>, <a href="http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1">Tong Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Ang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1">Minghu Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1">Mimi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Caiwen Ding</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04811">
<div class="article-summary-box-inner">
<span><p>Being able to learn from complex data with phase information is imperative
for many signal processing applications. Today' s real-valued deep neural
networks (DNNs) have shown efficiency in latent information analysis but fall
short when applied to the complex domain. Deep complex networks (DCN), in
contrast, can learn from complex data, but have high computational costs;
therefore, they cannot satisfy the instant decision-making requirements of many
deployable systems dealing with short observations or short signal bursts.
Recent, Binarized Complex Neural Network (BCNN), which integrates DCNs with
binarized neural networks (BNN), shows great potential in classifying complex
data in real-time. In this paper, we propose a structural pruning based
accelerator of BCNN, which is able to provide more than 5000 frames/s inference
throughput on edge devices. The high performance comes from both the algorithm
and hardware sides. On the algorithm side, we conduct structural pruning to the
original BCNN models and obtain 20 $\times$ pruning rates with negligible
accuracy loss; on the hardware side, we propose a novel 2D convolution
operation accelerator for the binary complex neural network. Experimental
results show that the proposed design works with over 90% utilization and is
able to achieve the inference throughput of 5882 frames/s and 4938 frames/s for
complex NIN-Net and ResNet-18 using CIFAR-10 dataset and Alveo U280 Board.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1">Noriyuki Kojima</a>, <a href="http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1">Alane Suhr</a>, <a href="http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1">Yoav Artzi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04812">
<div class="article-summary-box-inner">
<span><p>We study continual learning for natural language instruction generation, by
observing human users' instruction execution. We focus on a collaborative
scenario, where the system both acts and delegates tasks to human users using
natural language. We compare user execution of generated instructions to the
original system intent as an indication to the system's success communicating
its intent. We show how to use this signal to improve the system's ability to
generate instructions via contextual bandit learning. In interaction with real
users, our system demonstrates dramatic improvements in its ability to generate
language over time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ R4Dyn: Exploring Radar for <span class="highlight-title">Self-Supervised</span> Monocular Depth Estimation of Dynamic Scenes.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1">Stefano Gasperini</a>, <a href="http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1">Patrick Koch</a>, <a href="http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1">Vinzenz Dallabetta</a>, <a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1">Nassir Navab</a>, <a href="http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1">Benjamin Busam</a>, <a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1">Federico Tombari</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04814">
<div class="article-summary-box-inner">
<span><p>While self-supervised monocular depth estimation in driving scenarios has
achieved comparable performance to supervised approaches, violations of the
static world assumption can still lead to erroneous depth predictions of
traffic participants, posing a potential safety issue. In this paper, we
present R4Dyn, a novel set of techniques to use cost-efficient radar data on
top of a self-supervised depth estimation framework. In particular, we show how
radar can be used during training as weak supervision signal, as well as an
extra input to enhance the estimation robustness at inference time. Since
automotive radars are readily available, this allows to collect training data
from a variety of existing vehicles. Moreover, by filtering and expanding the
signal to make it compatible with learning-based approaches, we address radar
inherent issues, such as noise and sparsity. With R4Dyn we are able to overcome
a major limitation of self-supervised depth estimation, i.e. the prediction of
traffic participants. We substantially improve the estimation on dynamic
objects, such as cars by 37% on the challenging nuScenes dataset, hence
demonstrating that radar is a valuable additional sensor for monocular depth
estimation in autonomous vehicles. Additionally, we plan on making the code
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Novel Compressible Adaptive Spectral Mixture Kernels for Gaussian Processes with Sparse Time and Phase Delay Structures.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yijue Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1">Feng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1">Elena Marchiori</a>, <a href="http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1">Sergios Theodoridis</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1808.00560">
<div class="article-summary-box-inner">
<span><p>Spectral mixture (SM) kernels comprise a powerful class of kernels for
Gaussian processes (GPs) capable of discovering structurally complex patterns
and modeling negative covariances. Being a linear superposition of
quasi-periodical kernel components, the state-of-the-art SM kernel does not
consider component compression and dependency structures between components. In
this paper, we investigate the benefits of component compression and modeling
of both time and phase delay structures between basis components in the SM
kernel. By verifying the presence of dependencies between function components
using Gaussian conditionals and posterior covariance, we first propose a new SM
kernel variant with a time and phase delay dependency structure (SMD) and then
provide a structure adaptation (SA) algorithm for the SMD. The SMD kernel is
constructed in two steps: first, time delay and phase delay are incorporated
into each basis component; next, cross-convolution between a basis component
and the reversed complex conjugate of another basis component is performed,
which yields a complex-valued and positive definite kernel incorporating
dependency structures between basis components. The model compression and
dependency sparsity of the SMD kernel can be obtained by using automatic
pruning in SA. We perform a thorough comparative experimental analysis of the
SMD on both synthetic and real-life datasets. The results corroborate the
efficacy of the dependency structure and SA in the SMD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Tensor-based computation of metastable and coherent sets.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/math/1/au:+Nuske_F/0/1/0/all/0/1">Feliks N&#xfc;ske</a>, <a href="http://arxiv.org/find/math/1/au:+Gelss_P/0/1/0/all/0/1">Patrick Gel&#xdf;</a>, <a href="http://arxiv.org/find/math/1/au:+Klus_S/0/1/0/all/0/1">Stefan Klus</a>, <a href="http://arxiv.org/find/math/1/au:+Clementi_C/0/1/0/all/0/1">Cecilia Clementi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.04741">
<div class="article-summary-box-inner">
<span><p>Recent years have seen rapid advances in the data-driven analysis of
dynamical systems based on Koopman operator theory and related approaches. On
the other hand, low-rank tensor product approximations -- in particular the
tensor train (TT) format -- have become a valuable tool for the solution of
large-scale problems in a number of fields. In this work, we combine
Koopman-based models and the TT format, enabling their application to
high-dimensional problems in conjunction with a rich set of basis functions or
features. We derive efficient algorithms to obtain a reduced matrix
representation of the system's evolution operator starting from an appropriate
low-rank representation of the data. These algorithms can be applied to both
stationary and non-stationary systems. We establish the infinite-data limit of
these matrix representations, and demonstrate our methods' capabilities using
several benchmark data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> ♻ Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">William B. Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengshu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1">Priya Kasimbeg</a>, <a href="http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1">Micael Tchapmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1">Alexander Toshev</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"><span class="highlight-author">Li Fei-Fei</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.14442">
<div class="article-summary-box-inner">
<span><p>We present Interactive Gibson Benchmark, the first comprehensive benchmark
for training and evaluating Interactive Navigation: robot navigation strategies
where physical interaction with objects is allowed and even encouraged to
accomplish a task. For example, the robot can move objects if needed in order
to clear a path leading to the goal location. Our benchmark comprises two novel
elements: 1) a new experimental setup, the Interactive Gibson Environment
(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high
fidelity physical dynamics of the robot and common objects found in these
scenes; 2) a set of Interactive Navigation metrics which allows one to study
the interplay between navigation and physical interaction. We present and
evaluate multiple learning-based baselines in Interactive Gibson, and provide
insights into regimes of navigation with different trade-offs between
navigation path efficiency and disturbance of surrounding objects. We make our
benchmark publicly
available(https://sites.google.com/view/interactivegibsonenv) and encourage
researchers from all disciplines in robotics (e.g. planning, learning, control)
to propose, evaluate, and compare their Interactive Navigation solutions in
Interactive Gibson.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Learning to Generate Levels From Nothing.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bontrager_P/0/1/0/all/0/1">Philip Bontrager</a>, <a href="http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1">Julian Togelius</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.05259">
<div class="article-summary-box-inner">
<span><p>Machine learning for procedural content generation has recently become an
active area of research. Levels vary in both form and function and are mostly
unrelated to each other across games. This has made it difficult to assemble
suitably large datasets to bring machine learning to level design in the same
way as it's been used for image generation. Here we propose Generative Playing
Networks which design levels for itself to play. The algorithm is built in two
parts; an agent that learns to play game levels, and a generator that learns
the distribution of playable levels. As the agent learns and improves its
ability, the space of playable levels, as defined by the agent, grows. The
generator targets the agent's playability estimates to then update its
understanding of what constitutes a playable level. We call this process of
learning the distribution of data found through self-discovery with an
environment, self-supervised inductive learning. Unlike previous approaches to
procedural content generation, Generative Playing Networks are end-to-end
differentiable and do not require human-designed examples or domain knowledge.
We demonstrate the capability of this framework by training an agent and level
generator for a 2D dungeon crawler game.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ GPCA: A Probabilistic Framework for Gaussian Process Embedded Channel Attention.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jiyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1">Dongliang Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhanyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guoqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jun Guo</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.04575">
<div class="article-summary-box-inner">
<span><p>Channel attention mechanisms have been commonly applied in many visual tasks
for effective performance improvement. It is able to reinforce the informative
channels as well as to suppress the useless channels. Recently, different
channel attention modules have been proposed and implemented in various ways.
Generally speaking, they are mainly based on convolution and pooling
operations. In this paper, we propose Gaussian process embedded channel
attention (GPCA) module and further interpret the channel attention schemes in
a probabilistic way. The GPCA module intends to model the correlations among
the channels, which are assumed to be captured by beta distributed variables.
As the beta distribution cannot be integrated into the end-to-end training of
convolutional neural networks (CNNs) with a mathematically tractable solution,
we utilize an approximation of the beta distribution to solve this problem. To
specify, we adapt a Sigmoid-Gaussian approximation, in which the Gaussian
distributed variables are transferred into the interval [0,1]. The Gaussian
process is then utilized to model the correlations among different channels. In
this case, a mathematically tractable solution is derived. The GPCA module can
be efficiently implemented and integrated into the end-to-end training of the
CNNs. Experimental results demonstrate the promising performance of the
proposed GPCA module. Codes are available at https://github.com/PRIS-CV/GPCA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Noise Robust Named Entity Understanding for Voice Assistants.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1">Deepak Muralidharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1">Joel Ruben Antony Moniz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Sida Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1">Justine Kao</a>, <a href="http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1">Stephen Pulman</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1">Atish Kothari</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1">Ray Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1">Yinying Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1">Vivek Kaul</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1">Mubarak Seyed Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1">Gang Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1">Nan Dun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yidan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1">Andy O</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1">Pooja Chitkara</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1">Alkesh Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1">Kushal Tayal</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Roger Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1">Peter Grasch</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1">Jason D. Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lin Li</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.14408">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) and Entity Linking (EL) play an essential role
in voice assistant interaction, but are challenging due to the special
difficulties associated with spoken user queries. In this paper, we propose a
novel architecture that jointly solves the NER and EL tasks by combining them
in a joint reranking module. We show that our proposed framework improves NER
accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features
used also lead to better accuracies in other natural language understanding
tasks, such as domain classification and semantic parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ A Baseline for Shapley Values in MLPs: from Missingness to Neutrality.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Izzo_C/0/1/0/all/0/1">Cosimo Izzo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1">Aldo Lipani</a>, <a href="http://arxiv.org/find/cs/1/au:+Okhrati_R/0/1/0/all/0/1">Ramin Okhrati</a>, <a href="http://arxiv.org/find/cs/1/au:+Medda_F/0/1/0/all/0/1">Francesca Medda</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04896">
<div class="article-summary-box-inner">
<span><p>Deep neural networks have gained momentum based on their accuracy, but their
interpretability is often criticised. As a result, they are labelled as black
boxes. In response, several methods have been proposed in the literature to
explain their predictions. Among the explanatory methods, Shapley values is a
feature attribution method favoured for its robust theoretical foundation.
However, the analysis of feature attributions using Shapley values requires
choosing a baseline that represents the concept of missingness. An arbitrary
choice of baseline could negatively impact the explanatory power of the method
and possibly lead to incorrect interpretations. In this paper, we present a
method for choosing a baseline according to a neutrality value: as a parameter
selected by decision-makers, the point at which their choices are determined by
the model predictions being either above or below it. Hence, the proposed
baseline is set based on a parameter that depends on the actual use of the
model. This procedure stands in contrast to how other baselines are set, i.e.
without accounting for how the model is used. We empirically validate our
choice of baseline in the context of binary classification tasks, using two
datasets: a synthetic dataset and a dataset derived from the financial domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Bandits with Partially Observable Confounded Data.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1">Guy Tennenholtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1">Uri Shalit</a>, <a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1">Shie Mannor</a>, <a href="http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1">Yonathan Efroni</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06731">
<div class="article-summary-box-inner">
<span><p>We study linear contextual bandits with access to a large, confounded,
offline dataset that was sampled from some fixed policy. We show that this
problem is closely related to a variant of the bandit problem with side
information. We construct a linear bandit algorithm that takes advantage of the
projected information, and prove regret bounds. Our results demonstrate the
ability to take advantage of confounded offline data. Particularly, we prove
regret bounds that improve current bounds by a factor related to the visible
dimensionality of the contexts in the data. Our results indicate that
confounded offline data can significantly improve online learning algorithms.
Finally, we demonstrate various characteristics of our approach through
synthetic simulations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Provable Training Set Debugging for Linear Regression.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaomin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaojin Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Loh_P/0/1/0/all/0/1">Po-Ling Loh</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.09009">
<div class="article-summary-box-inner">
<span><p>We investigate problems in penalized $M$-estimation, inspired by applications
in machine learning debugging. Data are collected from two pools, one
containing data with possibly contaminated labels, and the other which is known
to contain only cleanly labeled points. We first formulate a general
statistical algorithm for identifying buggy points and provide rigorous
theoretical guarantees under the assumption that the data follow a linear
model. We then present two case studies to illustrate the results of our
general theory and the dependence of our estimator on clean versus buggy
points. We further propose an algorithm for tuning parameter selection of our
Lasso-based algorithm and provide corresponding theoretical guarantees.
Finally, we consider a two-person "game" played between a bug generator and a
debugger, where the debugger can augment the contaminated data set with cleanly
labeled versions of points in the original data pool. We establish a
theoretical result showing a sufficient condition under which the bug generator
can always fool the debugger. Nonetheless, we provide empirical results showing
that such a situation may not occur in practice, making it possible for natural
augmentation strategies combined with our Lasso debugging algorithm to succeed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Graph Backdoor.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1">Zhaohan Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1">Ren Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1">Shouling Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Ting Wang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.11890">
<div class="article-summary-box-inner">
<span><p>One intriguing property of deep neural networks (DNNs) is their inherent
vulnerability to backdoor attacks -- a trojan model responds to
trigger-embedded inputs in a highly predictable manner while functioning
normally otherwise. Despite the plethora of prior work on DNNs for continuous
data (e.g., images), the vulnerability of graph neural networks (GNNs) for
discrete-structured data (e.g., graphs) is largely unexplored, which is highly
concerning given their increasing use in security-sensitive domains. To bridge
this gap, we present GTA, the first backdoor attack on GNNs. Compared with
prior work, GTA departs in significant ways: graph-oriented -- it defines
triggers as specific subgraphs, including both topological structures and
descriptive features, entailing a large design spectrum for the adversary;
input-tailored -- it dynamically adapts triggers to individual graphs, thereby
optimizing both attack effectiveness and evasiveness; downstream model-agnostic
-- it can be readily launched without knowledge regarding downstream models or
fine-tuning strategies; and attack-extensible -- it can be instantiated for
both transductive (e.g., node classification) and inductive (e.g., graph
classification) tasks, constituting severe threats for a range of
security-critical applications. Through extensive evaluation using benchmark
datasets and state-of-the-art models, we demonstrate the effectiveness of GTA.
We further provide analytical justification for its effectiveness and discuss
potential countermeasures, pointing to several promising research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1">Abhinav Sagar</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.07588">
<div class="article-summary-box-inner">
<span><p>Deep learning motivated by convolutional neural networks has been highly
successful in a range of medical imaging problems like image classification,
image segmentation, image synthesis etc. However for validation and
interpretability, not only do we need the predictions made by the model but
also how confident it is while making those predictions. This is important in
safety critical applications for the people to accept it. In this work, we used
an encoder decoder architecture based on variational inference techniques for
segmenting brain tumour images. We evaluate our work on the publicly available
BRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over
Union (IOU) as the evaluation metrics. Our model is able to segment brain
tumours while taking into account both aleatoric uncertainty and epistemic
uncertainty in a principled bayesian manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Applications of Deep Neural Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Heaton_J/0/1/0/all/0/1">Jeff Heaton</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05673">
<div class="article-summary-box-inner">
<span><p>Deep learning is a group of exciting new technologies for neural networks.
Through a combination of advanced training techniques and neural network
architectural components, it is now possible to create neural networks that can
handle tabular data, images, text, and audio as both input and output. Deep
learning allows a neural network to learn hierarchies of information in a way
that is like the function of the human brain. This course will introduce the
student to classic neural network structures, Convolution Neural Networks
(CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU),
General Adversarial Networks (GAN), and reinforcement learning. Application of
these architectures to computer vision, time series, security, natural language
processing (NLP), and data generation will be covered. High-Performance
Computing (HPC) aspects will demonstrate how deep learning can be leveraged
both on graphical processing units (GPUs), as well as grids. Focus is primarily
upon the application of deep learning to problems, with some introduction to
mathematical foundations. Readers will use the Python programming language to
implement deep learning using Google TensorFlow and Keras. It is not necessary
to know Python prior to this book; however, familiarity with at least one
programming language is assumed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Into the Unknown: Active Monitoring of Neural Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Lukina_A/0/1/0/all/0/1">Anna Lukina</a>, <a href="http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1">Christian Schilling</a>, <a href="http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1">Thomas A. Henzinger</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06429">
<div class="article-summary-box-inner">
<span><p>Neural-network classifiers achieve high accuracy when predicting the class of
an input that they were trained to identify. Maintaining this accuracy in
dynamic environments, where inputs frequently fall outside the fixed set of
initially known classes, remains a challenge. The typical approach is to detect
inputs from novel classes and retrain the classifier on an augmented dataset.
However, not only the classifier but also the detection mechanism needs to
adapt in order to distinguish between newly learned and yet unknown input
classes. To address this challenge, we introduce an algorithmic framework for
active monitoring of a neural network. A monitor wrapped in our framework
operates in parallel with the neural network and interacts with a human user
via a series of interpretable labeling queries for incremental adaptation. In
addition, we propose an adaptive quantitative monitor to improve precision. An
experimental evaluation on a diverse set of benchmarks with varying numbers of
classes confirms the benefits of our active monitoring framework in dynamic
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1">Felipe Maia Polo</a>, <a href="http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1">Renato Vicente</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01184">
<div class="article-summary-box-inner">
<span><p>In supervised learning, training and test datasets are often sampled from
distinct distributions. Domain adaptation techniques are thus required.
Covariate shift adaptation yields good generalization performance when domains
differ only by the marginal distribution of features. Covariate shift
adaptation is usually implemented using importance weighting, which may fail,
according to common wisdom, due to small effective sample sizes (ESS). Previous
research argues this scenario is more common in high-dimensional settings.
However, how effective sample size, dimensionality, and model
performance/generalization are formally related in supervised learning,
considering the context of covariate shift adaptation, is still somewhat
obscure in the literature. Thus, a main challenge is presenting a unified
theory connecting those points. Hence, in this paper, we focus on building a
unified view connecting the ESS, data dimensionality, and generalization in the
context of covariate shift adaptation. Moreover, we also demonstrate how
dimensionality reduction or feature selection can increase the ESS, and argue
that our results support dimensionality reduction before covariate shift
adaptation as a good practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Advanced Dropout: A Model-free Methodology for Bayesian Dropout Optimization.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1">Jiyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhanyu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_a/0/1/0/all/0/1">and Jianjun Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guoqiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1">Jing-Hao Xue</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zheng-Hua Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jun Guo</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05244">
<div class="article-summary-box-inner">
<span><p>Due to lack of data, overfitting ubiquitously exists in real-world
applications of deep neural networks (DNNs). We propose advanced dropout, a
model-free methodology, to mitigate overfitting and improve the performance of
DNNs. The advanced dropout technique applies a model-free and easily
implemented distribution with parametric prior, and adaptively adjusts dropout
rate. Specifically, the distribution parameters are optimized by stochastic
gradient variational Bayes in order to carry out an end-to-end training. We
evaluate the effectiveness of the advanced dropout against nine dropout
techniques on seven computer vision datasets (five small-scale datasets and two
large-scale datasets) with various base models. The advanced dropout
outperforms all the referred techniques on all the datasets.We further compare
the effectiveness ratios and find that advanced dropout achieves the highest
one on most cases. Next, we conduct a set of analysis of dropout rate
characteristics, including convergence of the adaptive dropout rate, the
learned distributions of dropout masks, and a comparison with dropout rate
generation without an explicit distribution. In addition, the ability of
overfitting prevention is evaluated and confirmed. Finally, we extend the
application of the advanced dropout to uncertainty inference, network pruning,
text classification, and regression. The proposed advanced dropout is also
superior to the corresponding referred methods. Codes are available at
https://github.com/PRIS-CV/AdvancedDropout.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Deep tree-ensembles for multi-output prediction.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Nakano_F/0/1/0/all/0/1">Felipe Kenji Nakano</a>, <a href="http://arxiv.org/find/cs/1/au:+Pliakos_K/0/1/0/all/0/1">Konstantinos Pliakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Vens_C/0/1/0/all/0/1">Celine Vens</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.02829">
<div class="article-summary-box-inner">
<span><p>Recently, deep neural networks have expanded the state-of-art in various
scientific fields and provided solutions to long standing problems across
multiple application domains. Nevertheless, they also suffer from weaknesses
since their optimal performance depends on massive amounts of training data and
the tuning of an extended number of parameters. As a countermeasure, some
deep-forest methods have been recently proposed, as efficient and low-scale
solutions. Despite that, these approaches simply employ label classification
probabilities as induced features and primarily focus on traditional
classification and regression tasks, leaving multi-output prediction
under-explored. Moreover, recent work has demonstrated that tree-embeddings are
highly representative, especially in structured output prediction. In this
direction, we propose a novel deep tree-ensemble (DTE) model, where every layer
enriches the original feature set with a representation learning component
based on tree-embeddings. In this paper, we specifically focus on two
structured output prediction tasks, namely multi-label classification and
multi-target regression. We conducted experiments using multiple benchmark
datasets and the obtained results confirm that our method provides superior
results to state-of-the-art methods in both tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ A Synthetic Over-sampling method with Minority and Majority classes for imbalance problems.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Khorshidi_H/0/1/0/all/0/1">Hadi A. Khorshidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Aickelin_U/0/1/0/all/0/1">Uwe Aickelin</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04170">
<div class="article-summary-box-inner">
<span><p>Class imbalance is a substantial challenge in classifying many real-world
cases. Synthetic over-sampling methods have been effective to improve the
performance of classifiers for imbalance problems. However, most synthetic
over-sampling methods generate non-diverse synthetic instances within the
convex hull formed by the existing minority instances as they only concentrate
on the minority class and ignore the vast information provided by the majority
class. They also often do not perform well for extremely imbalanced data as the
fewer the minority instances, the less information to generate synthetic
instances. Moreover, existing methods that generate synthetic instances using
the majority class distributional information cannot perform effectively when
the majority class has a multi-modal distribution. We propose a new method to
generate diverse and adaptable synthetic instances using Synthetic
Over-sampling with Minority and Majority classes (SOMM). SOMM generates
synthetic instances diversely within the minority data space. It updates the
generated instances adaptively to the neighbourhood including both classes.
Thus, SOMM performs well for both binary and multiclass imbalance problems. We
examine the performance of SOMM for binary and multiclass problems using
benchmark data sets for different imbalance levels. The empirical results show
the superiority of SOMM compared to other existing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Variational Laplace for Bayesian neural networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1">Ali Unlu</a>, <a href="http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1">Laurence Aitchison</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10443">
<div class="article-summary-box-inner">
<span><p>We develop variational Laplace for Bayesian neural networks (BNNs) which
exploits a local approximation of the curvature of the likelihood to estimate
the ELBO without the need for stochastic sampling of the neural-network
weights. The Variational Laplace objective is simple to evaluate, as it is (in
essence) the log-likelihood, plus weight-decay, plus a squared-gradient
regularizer. Variational Laplace gave better test performance and expected
calibration errors than maximum a-posteriori inference and standard
sampling-based variational inference, despite using the same variational
approximate posterior. Finally, we emphasise care needed in benchmarking
standard VI as there is a risk of stopping before the variance parameters have
converged. We show that early-stopping can be avoided by increasing the
learning rate for the variance parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Utilizing Concept Drift for Measuring the Effectiveness of Policy Interventions: The Case of the COVID-19 Pandemic.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Baier_L/0/1/0/all/0/1">Lucas Baier</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1">Niklas K&#xfc;hl</a>, <a href="http://arxiv.org/find/cs/1/au:+Schoffer_J/0/1/0/all/0/1">Jakob Sch&#xf6;ffer</a>, <a href="http://arxiv.org/find/cs/1/au:+Satzger_G/0/1/0/all/0/1">Gerhard Satzger</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03728">
<div class="article-summary-box-inner">
<span><p>As a reaction to the high infectiousness and lethality of the COVID-19 virus,
countries around the world have adopted drastic policy measures to contain the
pandemic. However, it remains unclear which effect these measures, so-called
non-pharmaceutical interventions (NPIs), have on the spread of the virus. In
this article, we use machine learning and apply drift detection methods in a
novel way to predict the time lag of policy interventions with respect to the
development of daily case numbers of COVID-19 across 9 European countries and
28 US states. Our analysis shows that there are, on average, more than two
weeks between NPI enactment and a drift in the case numbers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Estimating Average Treatment Effects via Orthogonal Regularization.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Hatt_T/0/1/0/all/0/1">Tobias Hatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1">Stefan Feuerriegel</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08490">
<div class="article-summary-box-inner">
<span><p>Decision-making often requires accurate estimation of treatment effects from
observational data. This is challenging as outcomes of alternative decisions
are not observed and have to be estimated. Previous methods estimate outcomes
based on unconfoundedness but neglect any constraints that unconfoundedness
imposes on the outcomes. In this paper, we propose a novel regularization
framework for estimating average treatment effects that exploits
unconfoundedness. To this end, we formalize unconfoundedness as an
orthogonality constraint, which ensures that the outcomes are orthogonal to the
treatment assignment. This orthogonality constraint is then included in the
loss function via a regularization. Based on our regularization framework, we
develop deep orthogonal networks for unconfounded treatments (DONUT), which
learn outcomes that are orthogonal to the treatment assignment. Using a variety
of benchmark datasets for estimating average treatment effects, we demonstrate
that DONUT outperforms the state-of-the-art substantially.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1">Ana Lucic</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1">Maartje ter Hoeve</a>, <a href="http://arxiv.org/find/cs/1/au:+Tolomei_G/0/1/0/all/0/1">Gabriele Tolomei</a>, <a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1">Maarten de Rijke</a>, <a href="http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1">Fabrizio Silvestri</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03322">
<div class="article-summary-box-inner">
<span><p>Given the increasing promise of Graph Neural Networks (GNNs) in real-world
applications, several methods have been developed for explaining their
predictions. So far, these methods have primarily focused on generating
subgraphs that are especially relevant for a particular prediction. However,
such methods do not provide a clear opportunity for recourse: given a
prediction, we want to understand how the prediction can be changed in order to
achieve a more desirable outcome. In this work, we propose a method for
generating counterfactual (CF) explanations for GNNs: the minimal perturbation
to the input (graph) data such that the prediction changes. Using only edge
deletions, we find that our method, CF-GNNExplainer can generate CF
explanations for the majority of instances across three widely used datasets
for GNN explanations, while removing less than 3 edges on average, with at
least 94\% accuracy. This indicates that CF-GNNExplainer primarily removes
edges that are crucial for the original predictions, resulting in minimal CF
explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/stat/1/au:+Xu_W/0/1/0/all/0/1">Winnie Xu</a>, <a href="http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1">Ricky T.Q. Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1">Xuechen Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Duvenaud_D/0/1/0/all/0/1">David Duvenaud</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.06559">
<div class="article-summary-box-inner">
<span><p>We perform scalable approximate inference in a continuous-depth Bayesian
neural network family. In this model class, uncertainty about separate weights
in each layer gives hidden units that follow a stochastic differential
equation. We demonstrate gradient-based stochastic variational inference in
this infinite-parameter setting, producing arbitrarily-flexible approximate
posteriors. We also derive a novel gradient estimator that approaches zero
variance as the approximate posterior over weights approaches the true
posterior. This approach brings continuous-depth Bayesian neural nets to a
competitive comparison against discrete-depth alternatives, while inheriting
the memory-efficient training and tunable precision of Neural ODEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Provable Super-Convergence with a Large Cyclical Learning Rate.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1">Samet Oymak</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10734">
<div class="article-summary-box-inner">
<span><p>Conventional wisdom dictates that learning rate should be in the stable
regime so that gradient-based algorithms don't blow up. This letter introduces
a simple scenario where an unstably large learning rate scheme leads to a super
fast convergence, with the convergence rate depending only logarithmically on
the condition number of the problem. Our scheme uses a Cyclical Learning Rate
(CLR) where we periodically take one large unstable step and several small
stable steps to compensate for the instability. These findings also help
explain the empirical observations of [Smith and Topin, 2019] where they show
that CLR with a large maximum learning rate can dramatically accelerate
learning and lead to so-called "super-convergence". We prove that our scheme
excels in the problems where Hessian exhibits a bimodal spectrum and the
eigenvalues can be grouped into two clusters (small and large). The unstably
large step is the key to enabling fast convergence over the small
eigen-spectrum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Memory-based Deep Reinforcement Learning for POMDPs.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1">Lingheng Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1">Rob Gorbet</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1">Dana Kuli&#x107;</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12344">
<div class="article-summary-box-inner">
<span><p>A promising characteristic of Deep Reinforcement Learning (DRL) is its
capability to learn optimal policy in an end-to-end manner without relying on
feature engineering. However, most approaches assume a fully observable state
space, i.e. fully observable Markov Decision Processes (MDPs). In real-world
robotics, this assumption is unpractical, because of issues such as sensor
sensitivity limitations and sensor noise, and the lack of knowledge about
whether the observation design is complete or not. These scenarios lead to
Partially Observable MDPs (POMDPs). In this paper, we propose
Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient
(LSTM-TD3) by introducing a memory component to TD3, and compare its
performance with other DRL algorithms in both MDPs and POMDPs. Our results
demonstrate the significant advantages of the memory component in addressing
POMDPs, including the ability to handle missing and noisy observation data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Embedded Knowledge <span class="highlight-title">Distillation</span> in Depth-Level Dynamic Neural Network.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qi Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1">Shuchang Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Ting-Bing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1">Guangliang Cheng</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00793">
<div class="article-summary-box-inner">
<span><p>In real applications, different computation-resource devices need
different-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,
existing methods either design multiple networks and train them independently,
or construct depth-level/width-level dynamic neural networks which is hard to
prove the accuracy of each sub-net. In this article, we propose an elegant
Depth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets
of similar architectures. To improve the generalization of sub-nets, we design
the Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to
implement knowledge transfer from the teacher (full-net) to multiple students
(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to
constrain the posterior class probability consistency between full-net and
sub-nets, and self-attention distillation on the same resolution feature of
different depth is addressed to drive more abundant feature representations of
sub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in
a DDNN via the online knowledge distillation in each training iteration without
extra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet
datasets demonstrate that sub-nets in DDNN with EKD training achieve better
performance than individually training networks while preserving the original
performance of full-nets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Aman Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1">Mayank Kothyari</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1">Vishwajeet Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1">Preethi Jyothi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1">Ganesh Ramakrishnan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1">Soumen Chakrabarti</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05568">
<div class="article-summary-box-inner">
<span><p>Multimodal IR, spanning text corpus, knowledge graph and images, called
outside knowledge visual question answering (OKVQA), is of much recent
interest. However, the popular data set has serious limitations. A surprisingly
large fraction of queries do not assess the ability to integrate cross-modal
information. Instead, some are independent of the image, some depend on
speculation, some require OCR or are otherwise answerable from the image alone.
To add to the above limitations, frequency-based guessing is very effective
because of (unintended) widespread answer overlaps between the train and test
folds. Overall, it is hard to determine when state-of-the-art systems exploit
these weaknesses rather than really infer the answers, because they are opaque
and their 'reasoning' process is uninterpretable. An equally important
limitation is that the dataset is designed for the quantitative assessment only
of the end-to-end answer retrieval task, with no provision for assessing the
correct(semantic) interpretation of the input query. In response, we identify a
key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and
build a new data set and challenge around it. Specifically, the questioner
identifies an entity in the image and asks a question involving that entity
which can be answered only by consulting a knowledge graph or corpus passage
mentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA
annotated based on the structural idiom and (ii)S3VQA, a new dataset built from
scratch. We also present a neural but structurally transparent OKVQA system,
S3, that explicitly addresses our challenge dataset, and outperforms recent
competitive baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1">Xianbo Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1">Shunquan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiwu Huang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13689">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that non-additive image steganographic frameworks
effectively improve security performance through adjusting distortion
distribution. However, as far as we know, all of the existing non-additive
proposals are based on handcrafted policies, and can only be applied to a
specific image domain, which heavily prevent non-additive steganography from
releasing its full potentiality. In this paper, we propose an automatic
non-additive steganographic distortion learning framework called MCTSteg to
remove the above restrictions. Guided by the reinforcement learning paradigm,
we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental
model to build MCTSteg. MCTS makes sequential decisions to adjust distortion
distribution without human intervention. Our proposed environmental model is
used to obtain feedbacks from each decision. Due to its self-learning
characteristic and domain-independent reward function, MCTSteg has become the
first reported universal non-additive steganographic framework which can work
in both spatial and JPEG domains. Extensive experimental results show that
MCTSteg can effectively withstand the detection of both hand-crafted
feature-based and deep-learning-based steganalyzers. In both spatial and JPEG
domains, the security performance of MCTSteg steadily outperforms the state of
the art by a clear margin under different scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Poisoning the Unlabeled Dataset of Semi-Supervised Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1">Nicholas Carlini</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01622">
<div class="article-summary-box-inner">
<span><p>Semi-supervised machine learning models learn from a (small) set of labeled
training examples, and a (large) set of unlabeled training examples.
State-of-the-art models can reach within a few percentage points of
fully-supervised training, while requiring 100x less labeled data.
</p>
<p>We study a new class of vulnerabilities: poisoning attacks that modify the
unlabeled dataset. In order to be useful, unlabeled datasets are given strictly
less review than labeled datasets, and adversaries can therefore poison them
easily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%
of the dataset size, we can manipulate a model trained on this poisoned dataset
to misclassify arbitrary examples at test time (as any desired label). Our
attacks are highly effective across datasets and semi-supervised learning
methods.
</p>
<p>We find that more accurate methods (thus more likely to be used) are
significantly more vulnerable to poisoning attacks, and as such better training
methods are unlikely to prevent this attack. To counter this we explore the
space of defenses, and propose two methods that mitigate our attack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ GIPA: General Information Propagation Algorithm for Graph Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1">Qinkai Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Houyi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhixiong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guowei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xintan Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yongchao Liu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06035">
<div class="article-summary-box-inner">
<span><p>Graph neural networks (GNNs) have been popularly used in analyzing
graph-structured data, showing promising results in various applications such
as node classification, link prediction and network recommendation. In this
paper, we present a new graph attention neural network, namely GIPA, for
attributed graph data learning. GIPA consists of three key components:
attention, feature propagation and aggregation. Specifically, the attention
component introduces a new multi-layer perceptron based multi-head to generate
better non-linear feature mapping and representation than conventional
implementations such as dot-product. The propagation component considers not
only node features but also edge features, which differs from existing GNNs
that merely consider node features. The aggregation component uses a residual
connection to generate the final embedding. We evaluate the performance of GIPA
using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The
experimental results reveal that GIPA can beat the state-of-the-art models in
terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of
$0.8700\pm 0.0010$ and outperforms all the previous methods listed in the
ogbn-proteins leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhi-Qin John Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hanxu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1">Tao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yaoyu Zhang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11686">
<div class="article-summary-box-inner">
<span><p>Studying the implicit regularization effect of the nonlinear training
dynamics of neural networks (NNs) is important for understanding why
over-parameterized neural networks often generalize well on real dataset.
Empirically, for two-layer NN, existing works have shown that input weights of
hidden neurons (the input weight of a hidden neuron consists of the weight from
its input layer to the hidden neuron and its bias term) condense on isolated
orientations with a small initialization. The condensation dynamics implies
that NNs can learn features from the training data with a network configuration
effectively equivalent to a much smaller network during the training. In this
work, we show that the multiple roots of activation function at origin
(referred as ``multiplicity'') is a key factor for understanding the
condensation at the initial stage of training. Our experiments of multilayer
networks suggest that the maximal number of condensed orientations is twice the
multiplicity of the activation function used. Our theoretical analysis of
two-layer networks confirms experiments for two cases, one is for the
activation function of multiplicity one, which contains many common activation
functions, and the other is for the one-dimensional input. This work makes a
step towards understanding how small initialization implicitly leads NNs to
condensation at initial training stage, which lays a foundation for the future
study of the nonlinear dynamics of NNs and its implicit regularization effect
at a later stage of training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1">Tolulope Odetola</a>, <a href="http://arxiv.org/find/cs/1/au:+Khalid_F/0/1/0/all/0/1">Faiq Khalid</a>, <a href="http://arxiv.org/find/cs/1/au:+Sandefur_T/0/1/0/all/0/1">Travis Sandefur</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1">Hawzhin Mohammed</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1">Syed Rafay Hasan</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06895">
<div class="article-summary-box-inner">
<span><p>To reduce the time-to-market and access to state-of-the-art techniques, CNN
hardware mapping and deployment on embedded accelerators are often outsourced
to untrusted third parties, which is going to be more prevalent in futuristic
artificial intelligence of things (AIoT) systems. These AIoT systems anticipate
horizontal collaboration among different resource-constrained AIoT node
devices, where CNN layers are partitioned and these devices collaboratively
compute complex CNN tasks. This horizontal collaboration opens another attack
surface to the CNN-based application, like inserting the hardware Trojans (HT)
into the embedded accelerators designed for the CNN. Therefore, there is a dire
need to explore this attack surface for designing secure embedded hardware
accelerators for CNNs. Towards this goal, in this paper, we exploited this
attack surface to propose an HT-based attack called FeSHI. Since in horizontal
collaboration of RC AIoT devices different sections of CNN architectures are
outsourced to different untrusted third parties, the attacker may not know the
input image, but it has access to the layer-by-layer output feature maps
information for the assigned sections of the CNN architecture. This attack
exploits the statistical distribution, i.e., Gaussian distribution, of the
layer-by-layer feature maps of the CNN to design two triggers for stealthy HT
with a very low probability of triggering. Also, three different novel,
stealthy and effective trigger designs are proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Learning Intrusion Prevention Policies through Optimal Stopping.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1">Kim Hammar</a>, <a href="http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1">Rolf Stadler</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07160">
<div class="article-summary-box-inner">
<span><p>We study automated intrusion prevention using reinforcement learning. In a
novel approach, we formulate the problem of intrusion prevention as an optimal
stopping problem. This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since the computation
of the optimal defender policy using dynamic programming is not feasible for
practical cases, we approximate the optimal policy through reinforcement
learning in a simulation environment. To define the dynamics of the simulation,
we emulate the target infrastructure and collect measurements. Our evaluations
show that the learned policies are close to optimal and that they indeed can be
expressed using thresholds.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ A deep generative model for probabilistic energy forecasting in power systems: normalizing flows.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1">Jonathan Dumas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1">Antoine Wehenkel Damien Lanaspeze</a>, <a href="http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1">Bertrand Corn&#xe9;lusse</a>, <a href="http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1">Antonio Sutera</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09370">
<div class="article-summary-box-inner">
<span><p>Greater direct electrification of end-use sectors with a higher share of
renewables is one of the pillars to power a carbon-neutral society by 2050.
However, in contrast to conventional power plants, renewable energy is subject
to uncertainty raising challenges for their interaction with power systems.
Scenario-based probabilistic forecasting models have become an important tool
to equip decision-makers. This paper proposes to present to the power systems
forecasting practitioners a recent deep learning technique, the normalizing
flows, to produce accurate scenario-based probabilistic forecasts that are
crucial to face the new challenges in power systems applications. The strength
of this technique is to directly learn the stochastic multivariate distribution
of the underlying process by maximizing the likelihood. Through comprehensive
empirical evaluations using the open data of the Global Energy Forecasting
Competition 2014, we demonstrate that this methodology is competitive with
other state-of-the-art deep learning generative models: generative adversarial
networks and variational autoencoders. The models producing weather-based wind,
solar power, and load scenarios are properly compared both in terms of forecast
value, by considering the case study of an energy retailer, and quality using
several complementary metrics. The numerical experiments are simple and easily
reproducible. Thus, we hope it will encourage other forecasting practitioners
to test and use normalizing flows in power system applications such as bidding
on electricity markets, scheduling of power systems with high renewable energy
sources penetration, energy management of virtual power plan or microgrids, and
unit commitment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Infante_G/0/1/0/all/0/1">Guillermo Infante</a>, <a href="http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1">Anders Jonsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1">Vicen&#xe7; G&#xf3;mez</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15380">
<div class="article-summary-box-inner">
<span><p>In this work we present a novel approach to hierarchical reinforcement
learning for linearly-solvable Markov decision processes. Our approach assumes
that the state space is partitioned, and the subtasks consist in moving between
the partitions. We represent value functions on several levels of abstraction,
and use the compositionality of subtasks to estimate the optimal values of the
states in each partition. The policy is implicitly defined on these optimal
value estimates, rather than being decomposed among the subtasks. As a
consequence, our approach can learn the globally optimal policy, and does not
suffer from the non-stationarity of high-level decisions. If several partitions
have equivalent dynamics, the subtasks of those partitions can be shared. If
the set of boundary states is smaller than the entire state space, our approach
can have significantly smaller sample complexity than that of a flat learner,
and we validate this empirically in several experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ A Distance Measure for Privacy-preserving Process Mining based on Feature Learning.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1">Fabian R&#xf6;sel</a>, <a href="http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1">Stephan A. Fahrenkrog-Petersen</a>, <a href="http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1">Han van der Aa</a>, <a href="http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1">Matthias Weidlich</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.06578">
<div class="article-summary-box-inner">
<span><p>To enable process analysis based on an event log without compromising the
privacy of individuals involved in process execution, a log may be anonymized.
Such anonymization strives to transform a log so that it satisfies provable
privacy guarantees, while largely maintaining its utility for process analysis.
Existing techniques perform anonymization using simple, syntactic measures to
identify suitable transformation operations. This way, the semantics of the
activities referenced by the events in a trace are neglected, potentially
leading to transformations in which events of unrelated activities are merged.
To avoid this and incorporate the semantics of activities during anonymization,
we propose to instead incorporate a distance measure based on feature learning.
Specifically, we show how embeddings of events enable the definition of a
distance measure for traces to guide event log anonymization. Our experiments
with real-world data indicate that anonymization using this measure, compared
to a syntactic one, yields logs that are closer to the original log in various
dimensions and, hence, have higher utility for process analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuesi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1">Guangda Huzhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qianying Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1">Qing Da</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08598">
<div class="article-summary-box-inner">
<span><p>Ensemble models in E-commerce combine predictions from multiple sub-models
for ranking and revenue improvement. Industrial ensemble models are typically
deep neural networks, following the supervised learning paradigm to infer
conversion rate given inputs from sub-models. However, this process has the
following two problems. Firstly, the point-wise scoring approach disregards the
relationships between items and leads to homogeneous displayed results, while
diversified display benefits user experience and revenue. Secondly, the
learning paradigm focuses on the ranking metrics and does not directly optimize
the revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework
RAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)
and explores the best weights of sub-models by the Evaluator-Generator
Optimization (EGO). To achieve the best online performance, we propose a new
rank aggregation algorithm TournamentGreedy as a refinement of classic rank
aggregators, which also produces the best average weighted Kendall Tau Distance
(KTD) amongst all the considered algorithms with quadratic time complexity.
Under the assumption that the best output list should be Pareto Optimal on the
KTD metric for sub-models, we show that our RA algorithm has higher efficiency
and coverage in exploring the optimal weights. Combined with the idea of
Bayesian Optimization and gradient descent, we solve the online contextual
Black-Box Optimization task that finds the optimal weights for sub-models given
a chosen RA model. RA-EGO has been deployed in our online system and has
improved the revenue significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Bias Loss for Mobile Neural Networks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1">Lusine Abrahamyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1">Valentin Ziatchin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1">Nikos Deligiannis</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11170">
<div class="article-summary-box-inner">
<span><p>Compact convolutional neural networks (CNNs) have witnessed exceptional
improvements in performance in recent years. However, they still fail to
provide the same predictive power as CNNs with a large number of parameters.
The diverse and even abundant features captured by the layers is an important
characteristic of these successful CNNs. However, differences in this
characteristic between large CNNs and their compact counterparts have rarely
been investigated. In compact CNNs, due to the limited number of parameters,
abundant features are unlikely to be obtained, and feature diversity becomes an
essential characteristic. Diverse features present in the activation maps
derived from a data point during model inference may indicate the presence of a
set of unique descriptors necessary to distinguish between objects of different
classes. In contrast, data points with low feature diversity may not provide a
sufficient amount of unique descriptors to make a valid prediction; we refer to
them as random predictions. Random predictions can negatively impact the
optimization process and harm the final performance. This paper proposes
addressing the problem raised by random predictions by reshaping the standard
cross-entropy to make it biased toward data points with a limited number of
unique descriptive features. Our novel Bias Loss focuses the training on a set
of valuable data points and prevents the vast number of samples with poor
learning features from misleading the optimization process. Furthermore, to
show the importance of diversity, we present a family of SkipNet models whose
architectures are brought to boost the number of unique descriptors in the last
layers. Our Skipnet-M can achieve 1% higher classification accuracy than
MobileNetV3 Large.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Federated Causal Inference in Heterogeneous Observational Data.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1">Ruoxuan Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1">Allison Koenecke</a>, <a href="http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1">Michael Powell</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1">Zhu Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1">Joshua T. Vogelstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1">Susan Athey</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11732">
<div class="article-summary-box-inner">
<span><p>Analyzing observational data from multiple sources can be useful for
increasing statistical power to detect a treatment effect; however, practical
constraints such as privacy considerations may restrict individual-level
information sharing across data sets. This paper develops federated methods
that only utilize summary-level information from heterogeneous data sets. Our
federated methods provide doubly-robust point estimates of treatment effects as
well as variance estimates. We derive the asymptotic distributions of our
federated estimators, which are shown to be asymptotically equivalent to the
corresponding estimators from the combined, individual-level data. We show that
to achieve these properties, federated methods should be adjusted based on
conditions such as whether models are correctly specified and stable across
heterogeneous data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1">Kiheiji Nishida</a>, <a href="http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1">Kanta Naito</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13430">
<div class="article-summary-box-inner">
<span><p>This study proposes multivariate kernel density estimation by stagewise
minimization algorithm based on $U$-divergence and a simple dictionary. The
dictionary consists of an appropriate scalar bandwidth matrix and a part of the
original data. The resulting estimator brings us data-adaptive weighting
parameters and bandwidth matrices, and realizes a sparse representation of
kernel density estimation. We develop the non-asymptotic error bound of
estimator obtained via the proposed stagewise minimization algorithm. It is
confirmed from simulation studies that the proposed estimator performs
competitive to or sometime better than other well-known density estimators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Developing Open Source Educational Resources for Machine Learning and Data Science.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1">Ludwig Bothmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1">Sven Strickroth</a>, <a href="http://arxiv.org/find/cs/1/au:+Casalicchio_G/0/1/0/all/0/1">Giuseppe Casalicchio</a>, <a href="http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1">David R&#xfc;gamer</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1">Marius Lindauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Scheipl_F/0/1/0/all/0/1">Fabian Scheipl</a>, <a href="http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1">Bernd Bischl</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14330">
<div class="article-summary-box-inner">
<span><p>Education should not be a privilege but a common good. It should be openly
accessible to everyone, with as few barriers as possible; even more so for key
technologies such as Machine Learning (ML) and Data Science (DS). Open
Educational Resources (OER) are a crucial factor for greater educational
equity. In this paper, we describe the specific requirements for OER in ML and
DS and argue that it is especially important for these fields to make source
files publicly available, leading to Open Source Educational Resources (OSER).
We present our view on the collaborative development of OSER, the challenges
this poses, and first steps towards their solutions. We outline how OSER can be
used for blended learning scenarios and share our experiences in university
education. Finally, we discuss additional challenges such as credit assignment
or granting certificates.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ NeuralDP Differentially private neural networks by design.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1">Moritz Knolle</a>, <a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1">Dmitrii Usynin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1">Alexander Ziller</a>, <a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1">Marcus R. Makowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1">Georgios Kaissis</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14582">
<div class="article-summary-box-inner">
<span><p>The application of differential privacy to the training of deep neural
networks holds the promise of allowing large-scale (decentralized) use of
sensitive data while providing rigorous privacy guarantees to the individual.
The predominant approach to differentially private training of neural networks
is DP-SGD, which relies on norm-based gradient clipping as a method for
bounding sensitivity, followed by the addition of appropriately calibrated
Gaussian noise. In this work we propose NeuralDP, a technique for privatising
activations of some layer within a neural network, which by the post-processing
properties of differential privacy yields a differentially private network. We
experimentally demonstrate on two datasets (MNIST and Pediatric Pneumonia
Dataset (PPD)) that our method offers substantially improved privacy-utility
trade-offs compared to DP-SGD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1">Ron Shmelkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1">Tomer Friedlander</a>, <a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1">Lior Wolf</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01077">
<div class="article-summary-box-inner">
<span><p>A master face is a face image that passes face-based identity-authentication
for a large portion of the population. These faces can be used to impersonate,
with a high probability of success, any user, without having access to any
user-information. We optimize these faces, by using an evolutionary algorithm
in the latent embedding space of the StyleGAN face generator. Multiple
evolutionary strategies are compared, and we propose a novel approach that
employs a neural network in order to direct the search in the direction of
promising samples, without adding fitness evaluations. The results we present
demonstrate that it is possible to obtain a high coverage of the LFW identities
(over 40%) with less than 10 master faces, for three leading deep face
recognition systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1">Andrei-Marius Avram</a>, <a href="http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1">Vasile Pais</a>, <a href="http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1">Dan Tufis</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01139">
<div class="article-summary-box-inner">
<span><p>EuroVoc is a multilingual thesaurus that was built for organizing the
legislative documentary of the European Union institutions. It contains
thousands of categories at different levels of specificity and its descriptors
are targeted by legal texts in almost thirty languages. In this work we propose
a unified framework for EuroVoc classification on 22 languages by fine-tuning
modern Transformer-based pretrained language models. We study extensively the
performance of our trained models and show that they significantly improve the
results obtained by a similar tool - JEX - on the same dataset. The code and
the fine-tuned models were open sourced, together with a programmatic interface
that eases the process of loading the weights of a trained model and of
classifying a new document.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ AdvRush: Searching for Adversarially Robust Neural Architectures.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1">Jisoo Mok</a>, <a href="http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1">Byunggook Na</a>, <a href="http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1">Hyeokjun Choe</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1">Sungroh Yoon</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01289">
<div class="article-summary-box-inner">
<span><p>Deep neural networks continue to awe the world with their remarkable
performance. Their predictions, however, are prone to be corrupted by
adversarial examples that are imperceptible to humans. Current efforts to
improve the robustness of neural networks against adversarial examples are
focused on developing robust training methods, which update the weights of a
neural network in a more robust direction. In this work, we take a step beyond
training of the weight parameters and consider the problem of designing an
adversarially robust neural architecture with high intrinsic robustness. We
propose AdvRush, a novel adversarial robustness-aware neural architecture
search algorithm, based upon a finding that independent of the training method,
the intrinsic robustness of a neural network can be represented with the
smoothness of its input loss landscape. Through a regularizer that favors a
candidate architecture with a smoother input loss landscape, AdvRush
successfully discovers an adversarially robust neural architecture. Along with
a comprehensive theoretical motivation for AdvRush, we conduct an extensive
amount of experiments to demonstrate the efficacy of AdvRush on various
benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust
accuracy under FGSM attack after standard training and 50.04% robust accuracy
under AutoAttack after 7-step PGD adversarial training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Personalized Federated Learning with Clustering: Non-IID Heart Rate Variability Data Application.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1">J.H.Yoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1">H.M.Son</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1">H.Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1">E-H.Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1">A.Y.Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">H.Y.Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1">H.J.Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1">T-M.Chung</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01903">
<div class="article-summary-box-inner">
<span><p>While machine learning techniques are being applied to various fields for
their exceptional ability to find complex relations in large datasets, the
strengthening of regulations on data ownership and privacy is causing
increasing difficulty in its application to medical data. In light of this,
Federated Learning has recently been proposed as a solution to train on private
data without breach of confidentiality. This conservation of privacy is
particularly appealing in the field of healthcare, where patient data is highly
confidential. However, many studies have shown that its assumption of
Independent and Identically Distributed data is unrealistic for medical data.
In this paper, we propose Personalized Federated Cluster Models, a hierarchical
clustering-based FL process, to predict Major Depressive Disorder severity from
Heart Rate Variability. By allowing clients to receive more personalized model,
we address problems caused by non-IID data, showing an accuracy increase in
severity prediction. This increase in performance may be sufficient to use
Personalized Federated Cluster Models in many existing Federated Learning
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Transferring Knowledge <span class="highlight-title">Distillation</span> for Multilingual Social Event Detection.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1">Jiaqian Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Lei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1">Yongxin Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lihong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xu Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03084">
<div class="article-summary-box-inner">
<span><p>Recently published graph neural networks (GNNs) show promising performance at
social event detection tasks. However, most studies are oriented toward
monolingual data in languages with abundant training samples. This has left the
more common multilingual settings and lesser-spoken languages relatively
unexplored. Thus, we present a GNN that incorporates cross-lingual word
embeddings for detecting events in multilingual data streams. The first exploit
is to make the GNN work with multilingual data. For this, we outline a
construction strategy that aligns messages in different languages at both the
node and semantic levels. Relationships between messages are established by
merging entities that are the same but are referred to in different languages.
Non-English message representations are converted into English semantic space
via the cross-lingual word embeddings. The resulting message graph is then
uniformly encoded by a GNN model. In special cases where a lesser-spoken
language needs to be detected, a novel cross-lingual knowledge distillation
framework, called CLKD, exploits prior knowledge learned from similar threads
in English to make up for the paucity of annotated data. Experiments on both
synthetic and real-world datasets show the framework to be highly effective at
detection in both multilingual data and in languages where training samples are
scarce.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
<span class="highlight-title">★</span> ♻ iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengshu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1">Fei Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1">Michael Lingelbach</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1">Sanjana Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1">Bokui Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1">Kent Vainio</a>, <a href="http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1">Cem Gokmen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1">Gokul Dharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1">Tanish Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1">Andrey Kurenkov</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">C. Karen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1">Hyowon Gweon</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"><span class="highlight-author">Jiajun Wu</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"><span class="highlight-author">Li Fei-Fei</span></a>, <a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1">Silvio Savarese</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03272">
<div class="article-summary-box-inner">
<span><p>Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset will be publicly available at
<a href="http://svl.stanford.edu/igibson/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Expressive Power and Loss Surfaces of Deep Learning Models.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1">Simant Dube</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03579">
<div class="article-summary-box-inner">
<span><p>The goals of this paper are two-fold. The first goal is to serve as an
expository tutorial on the working of deep learning models which emphasizes
geometrical intuition about the reasons for success of deep learning. The
second goal is to complement the current results on the expressive power of
deep learning models and their loss surfaces with novel insights and results.
In particular, we describe how deep neural networks carve out manifolds
especially when the multiplication neurons are introduced. Multiplication is
used in dot products and the attention mechanism and it is employed in capsule
networks and self-attention based transformers. We also describe how random
polynomial, random matrix, spin glass and computational complexity perspectives
on the loss surfaces are interconnected.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ Deep Neural Network for DrawiNg Networks, (DNN)^2.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Giovannangeli_L/0/1/0/all/0/1">Loann Giovannangeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Lalanne_F/0/1/0/all/0/1">Frederic Lalanne</a>, <a href="http://arxiv.org/find/cs/1/au:+Auber_D/0/1/0/all/0/1">David Auber</a>, <a href="http://arxiv.org/find/cs/1/au:+Giot_R/0/1/0/all/0/1">Romain Giot</a>, <a href="http://arxiv.org/find/cs/1/au:+Bourqui_R/0/1/0/all/0/1">Romain Bourqui</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03632">
<div class="article-summary-box-inner">
<span><p>By leveraging recent progress of stochastic gradient descent methods, several
works have shown that graphs could be efficiently laid out through the
optimization of a tailored objective function. In the meantime, Deep Learning
(DL) techniques achieved great performances in many applications. We
demonstrate that it is possible to use DL techniques to learn a graph-to-layout
sequence of operations thanks to a graph-related objective function. In this
paper, we present a novel graph drawing framework called (DNN)^2: Deep Neural
Network for DrawiNg Networks. Our method uses Graph Convolution Networks to
learn a model. Learning is achieved by optimizing a graph topology related loss
function that evaluates (DNN)^2 generated layouts during training. Once
trained, the (DNN)^ model is able to quickly lay any input graph out. We
experiment (DNN)^2 and statistically compare it to optimization-based and
regular graph layout algorithms. The results show that (DNN)^2 performs well
and are encouraging as the Deep Learning approach to Graph Drawing is novel and
many leads for future works are identified.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ AutoVideo: An Automated Video Action Recognition System.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1">Daochen Zha</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1">Zaid Pervaiz Bhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yi-Wei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yicheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1">Sirui Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1">Anmoll Kumar Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1">Mohammad Qazim Bhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1">Kwei-Herng Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiaben Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1">Na Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xia Hu</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04212">
<div class="article-summary-box-inner">
<span><p>Action recognition is a crucial task for video understanding. In this paper,
we present AutoVideo, a Python system for automated video action recognition.
It currently supports seven action recognition algorithms and various
pre-processing modules. Unlike the existing libraries that only provide model
zoos, AutoVideo is built with the standard pipeline language. The basic
building block is primitive, which wraps a pre-processing module or an
algorithm with some hyperparameters. AutoVideo is highly modular and
extendable. It can be easily combined with AutoML searchers. The pipeline
language is quite general so that we can easily enrich AutoVideo with
algorithms for various other video-related tasks in the future. AutoVideo is
released under MIT license at https://github.com/datamllab/autovideo
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1">Florian Kromp</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1">Lukas Fischer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1">Eva Bozsaky</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1">Inge Ambros</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1">Wolfgang Doerr</a>, <a href="http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1">Sabine Taschner-Mandl</a>, <a href="http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1">Peter Ambros</a>, <a href="http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1">Allan Hanbury</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.12975">
<div class="article-summary-box-inner">
<span><p>Separating and labeling each instance of a nucleus (instance-aware
segmentation) is the key challenge in segmenting single cell nuclei on
fluorescence microscopy images. Deep Neural Networks can learn the implicit
transformation of a nuclear image into a probability map indicating the class
membership of each pixel (nucleus or background), but the use of
post-processing steps to turn the probability map into a labeled object mask is
error-prone. This especially accounts for nuclear images of tissue sections and
nuclear images across varying tissue preparations. In this work, we aim to
evaluate the performance of state-of-the-art deep learning architectures to
segment nuclei in fluorescence images of various tissue origins and sample
preparation types without post-processing. We compare architectures that
operate on pixel to pixel translation and an architecture that operates on
object detection and subsequent locally applied segmentation. In addition, we
propose a novel strategy to create artificial images to extend the training
set. We evaluate the influence of ground truth annotation quality, image scale
and segmentation complexity on segmentation performance. Results show that
three out of four deep learning architectures (U-Net, U-Net with ResNet34
backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the
sample preparation types and tissue origins with satisfactory segmentation
performance. Mask R-CNN, an architecture designed to address instance aware
segmentation tasks, outperforms other architectures. Equal nuclear mean size,
consistent nuclear annotations and the use of artificially generated images
result in overall acceptable precision and recall across different tissues and
sample preparation types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jindi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1">Yang Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1">Kui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1">Kejie Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xiaohua Jia</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02940">
<div class="article-summary-box-inner">
<span><p>In recent years, many deep learning models have been adopted in autonomous
driving. At the same time, these models introduce new vulnerabilities that may
compromise the safety of autonomous vehicles. Specifically, recent studies have
demonstrated that adversarial attacks can cause a significant decline in
detection precision of deep learning-based 3D object detection models. Although
driving safety is the ultimate concern for autonomous driving, there is no
comprehensive study on the linkage between the performance of deep learning
models and the driving safety of autonomous vehicles under adversarial attacks.
In this paper, we investigate the impact of two primary types of adversarial
attacks, perturbation attacks and patch attacks, on the driving safety of
vision-based autonomous vehicles rather than the detection precision of deep
learning models. In particular, we consider two state-of-the-art models in
vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving
safety, we propose an end-to-end evaluation framework with a set of driving
safety performance metrics. By analyzing the results of our extensive
evaluation experiments, we find that (1) the attack's impact on the driving
safety of autonomous vehicles and the attack's impact on the precision of 3D
object detectors are decoupled, and (2) the DSGN model demonstrates stronger
robustness to adversarial attacks than the Stereo R-CNN model. In addition, we
further investigate the causes behind the two findings with an ablation study.
The findings of this paper provide a new perspective to evaluate adversarial
attacks and guide the selection of deep learning models in autonomous driving.
</p></span>
</div>
</a>
</details>
</article>
</div>
</details>
</article>
<article>
<details>
<Summary>cs.MM updates on arXiv.org</Summary>
<div class="details-content">
<p style="text-align:center;">Computer Science -- Multimedia (cs.MM) updates on the arXiv.org e-print archive</p>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Learning to Cut by Watching Movies.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1">Alejandro Pardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1">Fabian Caba Heilbron</a>, <a href="http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href="http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1">Ali Thabet</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1">Bernard Ghanem</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04294">
<div class="article-summary-box-inner">
<span><p>Video content creation keeps growing at an incredible pace; yet, creating
engaging stories remains challenging and requires non-trivial video editing
expertise. Many video editing components are astonishingly hard to automate
primarily due to the lack of raw video materials. This paper focuses on a new
task for computational video editing, namely the task of raking cut
plausibility. Our key idea is to leverage content that has already been edited
to learn fine-grained audiovisual patterns that trigger cuts. To do this, we
first collected a data source of more than 10K videos, from which we extract
more than 255K cuts. We devise a model that learns to discriminate between real
and artificial cuts via contrastive learning. We set up a new task and a set of
baselines to benchmark video cut generation. We observe that our proposed model
outperforms the baselines by large margins. To demonstrate our model in
real-world applications, we conduct human studies in a collection of unedited
videos. The results show that our model does a better job at cutting than
random and alternative baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ An Open Framework for Analyzing and Modeling XR Network Traffic.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Lecci_M/0/1/0/all/0/1">Mattia Lecci</a>, <a href="http://arxiv.org/find/cs/1/au:+Drago_M/0/1/0/all/0/1">Matteo Drago</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1">Andrea Zanella</a>, <a href="http://arxiv.org/find/cs/1/au:+Zorzi_M/0/1/0/all/0/1">Michele Zorzi</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04577">
<div class="article-summary-box-inner">
<span><p>Thanks to recent advancements in the technology, eXtended Reality (XR)
applications are gaining a lot of momentum, and they will surely become
increasingly popular in the next decade. These new applications, however,
require a step forward also in terms of models to simulate and analyze this
type of traffic sources in modern communication networks, in order to guarantee
to the users state of the art performance and Quality of Experience (QoE).
Recognizing this need, in this work, we present a novel open-source traffic
model, which researchers can use as a starting point both for improvements of
the model itself and for the design of optimized algorithms for the
transmission of these peculiar data flows. Along with the mathematical model
and the code, we also share with the community the traces that we gathered for
our study, collected from freely available applications such as Minecraft VR,
Google Earth VR, and Virus Popper. Finally, we propose a roadmap for the
construction of an end-to-end framework that fills this gap in the current
state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziwei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guangzhi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1">Yongkang Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1">Mohan Kankanhalli</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04603">
<div class="article-summary-box-inner">
<span><p>This paper proposes a novel model for recognizing images with composite
attribute-object concepts, notably for composite concepts that are unseen
during model training. We aim to explore the three key properties required by
the task --- relation-aware, consistent, and decoupled --- to learn rich and
robust features for primitive concepts that compose attribute-object pairs. To
this end, we propose the Blocked Message Passing Network (BMP-Net). The model
consists of two modules. The concept module generates semantically meaningful
features for primitive concepts, whereas the visual module extracts visual
features for attributes and objects from input images. A message passing
mechanism is used in the concept module to capture the relations between
primitive concepts. Furthermore, to prevent the model from being biased towards
seen composite concepts and reduce the entanglement between attributes and
objects, we propose a blocking mechanism that equalizes the information
available to the model for both seen and unseen concepts. Extensive experiments
and ablation studies on two benchmarks show the efficacy of the proposed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">
☆ ♻ MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography.
</summary>
<div class="article-authors">
<a href="http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1">Xianbo Mo</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1">Shunquan Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jiwu Huang</a>
</div>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13689">
<div class="article-summary-box-inner">
<span><p>Recent research has shown that non-additive image steganographic frameworks
effectively improve security performance through adjusting distortion
distribution. However, as far as we know, all of the existing non-additive
proposals are based on handcrafted policies, and can only be applied to a
specific image domain, which heavily prevent non-additive steganography from
releasing its full potentiality. In this paper, we propose an automatic
non-additive steganographic distortion learning framework called MCTSteg to
remove the above restrictions. Guided by the reinforcement learning paradigm,
we combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental
model to build MCTSteg. MCTS makes sequential decisions to adjust distortion
distribution without human intervention. Our proposed environmental model is
used to obtain feedbacks from each decision. Due to its self-learning
characteristic and domain-independent reward function, MCTSteg has become the
first reported universal non-additive steganographic framework which can work
in both spatial and JPEG domains. Extensive experimental results show that
MCTSteg can effectively withstand the detection of both hand-crafted
feature-based and deep-learning-based steganalyzers. In both spatial and JPEG
domains, the security performance of MCTSteg steadily outperforms the state of
the art by a clear margin under different scenarios.
</p></span>
</div>
</a>
</details>
</article>
</div>
</details>
</article>
</section>
</body>
<footer>
<time id="build-timestamp" datetime="2021-08-11 23:08:49.148586934 UTC">2021-08-11 23:08:49.148591934 UTC</time>
</footer>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
<script>var expanded=false;document.onkeydown=function(e){if(e.keyCode===9){expanded=!expanded;document.querySelectorAll("details").forEach(detail=>detail.open=expanded);return false}};const toggleSwitch=document.querySelector('.theme-switch input[type="checkbox"]');function switchTheme(e){if(e.target.checked){document.documentElement.setAttribute('data-theme','light');localStorage.setItem('theme','light')}else{document.documentElement.setAttribute('data-theme','dark');localStorage.setItem('theme','dark')}}toggleSwitch.addEventListener('change',switchTheme,false);const currentTheme=localStorage.getItem('theme')?localStorage.getItem('theme'):null;if(currentTheme){document.documentElement.setAttribute('data-theme',currentTheme);if(currentTheme==='light'){toggleSwitch.checked=true}}document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:'$',right:'$',display:false},{left:"\\(",right:"\\)",display:false},{left:"\\begin{equation}",right:"\\end{equation}",display:true},{left:"\\begin{align}",right:"\\end{align}",display:true},{left:"\\begin{alignat}",right:"\\end{alignat}",display:true},{left:"\\begin{gather}",right:"\\end{gather}",display:true},{left:"\\begin{CD}",right:"\\end{CD}",display:true},{left:"\\{",right:"\\}",display:false},{left:"\\[",right:"\\]",display:true}],throwOnError:false})})</script>
</html>