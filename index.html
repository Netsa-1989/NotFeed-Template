<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-25T01:30:00Z">08-25</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Open Vocabulary Keyword Search. (arXiv:2108.10357v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10357">
<div class="article-summary-box-inner">
<span><p>Recently, neural approaches to spoken content retrieval have become popular.
However, they tend to be restricted in their vocabulary or in their ability to
deal with imbalanced test settings. These restrictions limit their
applicability in keyword search, where the set of queries is not known
beforehand, and where the system should return not just whether an utterance
contains a query but the exact location of any such occurrences. In this work,
we propose a model directly optimized for keyword search. The model takes a
query and an utterance as input and returns a sequence of probabilities for
each frame of the utterance of the query having occurred in that frame.
Experiments show that the proposed model not only outperforms similar
end-to-end models on a task where the ratio of positive and negative trials is
artificially balanced, but it is also able to deal with the far more
challenging task of keyword search with its inherent imbalance. Furthermore,
using our system to rescore the outputs an LVCSR-based keyword search system
leads to significant improvements on the latter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Examining Covert Gender Bias: A Case Study in Turkish and English Machine Translation Models. (arXiv:2108.10379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10379">
<div class="article-summary-box-inner">
<span><p>As Machine Translation (MT) has become increasingly more powerful,
accessible, and widespread, the potential for the perpetuation of bias has
grown alongside its advances. While overt indicators of bias have been studied
in machine translation, we argue that covert biases expose a problem that is
further entrenched. Through the use of the gender-neutral language Turkish and
the gendered language English, we examine cases of both overt and covert gender
bias in MT models. Specifically, we introduce a method to investigate
asymmetrical gender markings. We also assess bias in the attribution of
personhood and examine occupational and personality stereotypes through overt
bias indicators in MT models. Our work explores a deeper layer of bias in MT
models and demonstrates the continued need for language-specific,
interdisciplinary methodology in MT model development.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recurrent multiple shared layers in Depth for Neural Machine Translation. (arXiv:2108.10417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10417">
<div class="article-summary-box-inner">
<span><p>Learning deeper models is usually a simple and effective approach to improve
model performance, but deeper models have larger model parameters and are more
difficult to train. To get a deeper model, simply stacking more layers of the
model seems to work well, but previous works have claimed that it cannot
benefit the model. We propose to train a deeper model with recurrent mechanism,
which loops the encoder and decoder blocks of Transformer in the depth
direction. To address the increasing of model parameters, we choose to share
parameters in different recursive moments. We conduct our experiments on WMT16
English-to-German and WMT14 English-to-France translation tasks, our model
outperforms the shallow Transformer-Base/Big baseline by 0.35, 1.45 BLEU
points, which is 27.23% of Transformer-Big model parameters. Compared to the
deep Transformer(20-layer encoder, 6-layer decoder), our model has similar
model performance and infer speed, but our model parameters are 54.72% of the
former.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One TTS Alignment To Rule Them All. (arXiv:2108.10447v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10447">
<div class="article-summary-box-inner">
<span><p>Speech-to-text alignment is a critical component of neural textto-speech
(TTS) models. Autoregressive TTS models typically use an attention mechanism to
learn these alignments on-line. However, these alignments tend to be brittle
and often fail to generalize to long utterances and out-of-domain text, leading
to missing or repeating words. Most non-autoregressive endto-end TTS models
rely on durations extracted from external sources. In this paper we leverage
the alignment mechanism proposed in RAD-TTS as a generic alignment learning
framework, easily applicable to a variety of neural TTS models. The framework
combines forward-sum algorithm, the Viterbi algorithm, and a simple and
efficient static prior. In our experiments, the alignment learning framework
improves all tested TTS architectures, both autoregressive (Flowtron, Tacotron
2) and non-autoregressive (FastPitch, FastSpeech 2, RAD-TTS). Specifically, it
improves alignment convergence speed of existing attention-based mechanisms,
simplifies the training pipeline, and makes the models more robust to errors on
long utterances. Most importantly, the framework improves the perceived speech
synthesis quality, as judged by human evaluators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taming the Beast: Learning to Control Neural Conversational Models. (arXiv:2108.10561v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10561">
<div class="article-summary-box-inner">
<span><p>This thesis investigates the controllability of deep learning-based,
end-to-end, generative dialogue systems in both task-oriented and chit-chat
scenarios. In particular, we study the different aspects of controlling
generative dialogue systems, including controlling styles and topics and
continuously adding and combining dialogue skills. In the three decades since
the first dialogue system was commercialized, the basic architecture of such
systems has remained substantially unchanged, consisting of four pipelined
basic components, namely, natural language understanding (NLU), dialogue state
tracking (DST), a dialogue manager (DM) and natural language generation (NLG).
The dialogue manager, which is the critical component of the modularized
system, controls the response content and style. This module is usually
programmed by rules and is designed to be highly controllable and easily
extendable. With the emergence of powerful "deep learning" architectures,
end-to-end generative dialogue systems have been proposed to optimize overall
system performance and simplify training. However, these systems cannot be
easily controlled and extended as the modularized dialogue manager can. This is
because a single neural system is used, which is usually a large pre-trained
language model (e.g., GPT-2), and thus it is hard to surgically change
desirable attributes (e.g., style, topics, etc.). More importantly,
uncontrollable dialogue systems can generate offensive and even toxic
responses. Therefore, in this thesis, we study controllable methods for
end-to-end generative dialogue systems in task-oriented and chit-chat
scenarios. Throughout the chapters, we describe 1) how to control the style and
topics of chit-chat models, 2) how to continuously control and extend
task-oriented dialogue systems, and 3) how to compose and control multi-skill
dialogue models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Criminal Texts for the Polish State Border Guard. (arXiv:2108.10580v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10580">
<div class="article-summary-box-inner">
<span><p>This paper describes research on the detection of Polish criminal texts
appearing on the Internet. We carried out experiments to find the best
available setup for the efficient classification of unbalanced and noisy data.
The best performance was achieved when our model was fine-tuned on a
pre-trained Polish-based transformer language model. For the detection task, a
large corpus of annotated Internet snippets was collected as training data. We
share this dataset and create a new task for the detection of criminal texts
using the Gonito platform as the benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prompt-Learning for Fine-Grained Entity Typing. (arXiv:2108.10604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10604">
<div class="article-summary-box-inner">
<span><p>As an effective approach to tune pre-trained language models (PLMs) for
specific tasks, prompt-learning has recently attracted much attention from
researchers. By using \textit{cloze}-style language prompts to stimulate the
versatile knowledge of PLMs, prompt-learning can achieve promising results on a
series of NLP tasks, such as natural language inference, sentiment
classification, and knowledge probing. In this work, we investigate the
application of prompt-learning on fine-grained entity typing in fully
supervised, few-shot and zero-shot scenarios. We first develop a simple and
effective prompt-learning pipeline by constructing entity-oriented verbalizers
and templates and conducting masked language modeling. Further, to tackle the
zero-shot regime, we propose a self-supervised strategy that carries out
distribution-level optimization in prompt-learning to automatically summarize
the information of entity types. Extensive experiments on three fine-grained
entity typing benchmarks (with up to 86 classes) under fully supervised,
few-shot and zero-shot settings show that prompt-learning methods significantly
outperform fine-tuning baselines, especially when the training data is
insufficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Cross-platform Teenager Detection with Adversarial BERT. (arXiv:2108.10619v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10619">
<div class="article-summary-box-inner">
<span><p>Teenager detection is an important case of the age detection task in social
media, which aims to detect teenage users to protect them from negative
influences. The teenager detection task suffers from the scarcity of labelled
data, which exacerbates the ability to perform well across social media
platforms. To further research in teenager detection in settings where no
labelled data is available for a platform, we propose a novel cross-platform
framework based on Adversarial BERT. Our framework can operate with a limited
amount of labelled instances from the source platform and with no labelled data
from the target platform, transferring knowledge from the source to the target
social media. We experiment on four publicly available datasets, obtaining
results demonstrating that our framework can significantly improve over
competitive baseline models on the cross-platform teenager detection task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are the Multilingual Models Better? Improving Czech Sentiment with Transformers. (arXiv:2108.10640v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10640">
<div class="article-summary-box-inner">
<span><p>In this paper, we aim at improving Czech sentiment with transformer-based
models and their multilingual versions. More concretely, we study the task of
polarity detection for the Czech language on three sentiment polarity datasets.
We fine-tune and perform experiments with five multilingual and three
monolingual models. We compare the monolingual and multilingual models'
performance, including comparison with the older approach based on recurrent
neural networks. Furthermore, we test the multilingual models and their ability
to transfer knowledge from English to Czech (and vice versa) with zero-shot
cross-lingual classification. Our experiments show that the huge multilingual
models can overcome the performance of the monolingual models. They are also
able to detect polarity in another language without any training data, with
performance not worse than 4.4 % compared to state-of-the-art monolingual
trained models. Moreover, we achieved new state-of-the-art results on all three
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Morality-based Assertion and Homophily on Social Media: A Cultural Comparison between English and Japanese Languages. (arXiv:2108.10643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10643">
<div class="article-summary-box-inner">
<span><p>Moral psychology is a domain that deals with moral identity, appraisals and
emotions. Previous work has greatly focused on moral development and the
associated role of culture. Knowing that language is an inherent element of a
culture, we used the social media platform Twitter for comparing the moral
behaviors of Japanese users with English users. The five basic moral
foundations i.e., Care, Fairness, Ingroup, Authority and Purity, along with the
associated emotional valence are compared for English and Japanese tweets. The
tweets from Japanese users depicted relatively higher Fairness, Ingroup and
Purity. As far as emotions related to morality are concerned, the English
tweets expressed more positive emotions for all moral dimensions. Considering
the role of moral similarities in connecting users on social media, we
quantified homophily concerning different moral dimensions using our proposed
method. The moral dimensions Care, Authority and Purity for English and Ingroup
for Japanese depicted homophily on Twitter. Overall, our study uncovers the
underlying cultural differences with respect to moral behavior in English and
Japanese speaking users.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Density-Based Dynamic Curriculum Learning for Intent Detection. (arXiv:2108.10674v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10674">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have achieved noticeable performance on the
intent detection task. However, due to assigning an identical weight to each
sample, they suffer from the overfitting of simple samples and the failure to
learn complex samples well. To handle this problem, we propose a density-based
dynamic curriculum learning model. Our model defines the sample's difficulty
level according to their eigenvectors' density. In this way, we exploit the
overall distribution of all samples' eigenvectors simultaneously. Then we apply
a dynamic curriculum learning strategy, which pays distinct attention to
samples of various difficulty levels and alters the proportion of samples
during the training process. Through the above operation, simple samples are
well-trained, and complex samples are enhanced. Experiments on three open
datasets verify that the proposed density-based algorithm can distinguish
simple and complex samples significantly. Besides, our model obtains obvious
improvement over the strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hybrid deep learning methods for phenotype prediction from clinical notes. (arXiv:2108.10682v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10682">
<div class="article-summary-box-inner">
<span><p>Identifying patient cohorts from clinical notes in secondary electronic
health records is a fundamental task in clinical information management. The
patient cohort identification needs to identify the patient phenotypes.
However, with the growing number of clinical notes, it becomes challenging to
analyze the data manually. Therefore, automatic extraction of clinical concepts
would be an essential task to identify the patient phenotypes correctly. This
paper proposes a novel hybrid model for automatically extracting patient
phenotypes using natural language processing and deep learning models to
determine the patient phenotypes without dictionaries and human intervention.
</p>
<p>The proposed hybrid model is based on a neural bidirectional sequence model
(BiLSTM or BiGRU) and a Convolutional Neural Network (CNN) for identifying
patient's phenotypes in discharge reports. Furthermore, to extract more
features related to each phenotype, an extra CNN layer is run parallel to the
hybrid proposed model. We used pre-trained embeddings such as FastText and
Word2vec separately as the input layers to evaluate other embedding's
performance in identifying patient phenotypes. We also measured the effect of
applying additional data cleaning steps on discharge reports to identify
patient phenotypes by deep learning models. We used discharge reports in the
Medical Information Mart for Intensive Care III (MIMIC III) database.
Experimental results in internal comparison demonstrate significant performance
improvement over existing models. The enhanced model with an extra CNN layer
obtained a relatively higher F1-score than the original hybrid model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression. (arXiv:2108.10684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10684">
<div class="article-summary-box-inner">
<span><p>Organizing complex peer production projects and advancing scientific
knowledge of open collaboration each depend on the ability to measure quality.
Article quality ratings on English language Wikipedia have been widely used by
both Wikipedia community members and academic researchers for purposes like
tracking knowledge gaps and studying how political polarization shapes
collaboration. Even so, measuring quality presents many methodological
challenges. The most widely used systems use labels on discrete ordinal scales
when assessing quality, but such labels can be inconvenient for statistics and
machine learning. Prior work handles this by assuming that different levels of
quality are "evenly spaced" from one another. This assumption runs counter to
intuitions about the relative degrees of effort needed to raise Wikipedia
encyclopedia articles to different quality levels. Furthermore, models from
prior work are fit to datasets that oversample high-quality articles. This
limits their accuracy for representative samples of articles or revisions. I
describe a technique extending the Wikimedia Foundations' ORES article quality
model to address these limitations. My method uses weighted ordinal regression
models to construct one-dimensional continuous measures of quality. While
scores from my technique and from prior approaches are correlated, my approach
improves accuracy for research datasets and provides evidence that the "evenly
spaced" assumption is unfounded in practice on English Wikipedia. I conclude
with recommendations for using quality scores in future research and include
the full code, data, and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Active Learning for Text Classification with Diverse Interpretations. (arXiv:2108.10687v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10687">
<div class="article-summary-box-inner">
<span><p>Recently, Deep Neural Networks (DNNs) have made remarkable progress for text
classification, which, however, still require a large number of labeled data.
To train high-performing models with the minimal annotation cost, active
learning is proposed to select and label the most informative samples, yet it
is still challenging to measure informativeness of samples used in DNNs. In
this paper, inspired by piece-wise linear interpretability of DNNs, we propose
a novel Active Learning with DivErse iNterpretations (ALDEN) approach. With
local interpretations in DNNs, ALDEN identifies linearly separable regions of
samples. Then, it selects samples according to their diversity of local
interpretations and queries their labels. To tackle the text classification
problem, we choose the word with the most diverse interpretations to represent
the whole sentence. Extensive experiments demonstrate that ALDEN consistently
outperforms several state-of-the-art deep active learning methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalized Optimal Linear Orders. (arXiv:2108.10692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10692">
<div class="article-summary-box-inner">
<span><p>The sequential structure of language, and the order of words in a sentence
specifically, plays a central role in human language processing. Consequently,
in designing computational models of language, the de facto approach is to
present sentences to machines with the words ordered in the same order as in
the original human-authored sentence. The very essence of this work is to
question the implicit assumption that this is desirable and inject theoretical
soundness into the consideration of word order in natural language processing.
In this thesis, we begin by uniting the disparate treatments of word order in
cognitive science, psycholinguistics, computational linguistics, and natural
language processing under a flexible algorithmic framework. We proceed to use
this heterogeneous theoretical foundation as the basis for exploring new word
orders with an undercurrent of psycholinguistic optimality. In particular, we
focus on notions of dependency length minimization given the difficulties in
human and computational language processing in handling long-distance
dependencies. We then discuss algorithms for finding optimal word orders
efficiently in spite of the combinatorial space of possibilities. We conclude
by addressing the implications of these word orders on human language and their
downstream impacts when integrated in computational models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficacy of BERT embeddings on predicting disaster from Twitter data. (arXiv:2108.10698v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10698">
<div class="article-summary-box-inner">
<span><p>Social media like Twitter provide a common platform to share and communicate
personal experiences with other people. People often post their life
experiences, local news, and events on social media to inform others. Many
rescue agencies monitor this type of data regularly to identify disasters and
reduce the risk of lives. However, it is impossible for humans to manually
check the mass amount of data and identify disasters in real-time. For this
purpose, many research works have been proposed to present words in
machine-understandable representations and apply machine learning methods on
the word representations to identify the sentiment of a text. The previous
research methods provide a single representation or embedding of a word from a
given document. However, the recent advanced contextual embedding method (BERT)
constructs different vectors for the same word in different contexts. BERT
embeddings have been successfully used in different natural language processing
(NLP) tasks, yet there is no concrete analysis of how these representations are
helpful in disaster-type tweet analysis. In this research work, we explore the
efficacy of BERT embeddings on predicting disaster from Twitter data and
compare these to traditional context-free word embedding methods (GloVe,
Skip-gram, and FastText). We use both traditional machine learning methods and
deep learning methods for this purpose. We provide both quantitative and
qualitative results for this study. The results show that the BERT embeddings
have the best results in disaster prediction task than the traditional word
embeddings. Our codes are made freely accessible to the research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Hateful are Movies? A Study and Prediction on Movie Subtitles. (arXiv:2108.10724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10724">
<div class="article-summary-box-inner">
<span><p>In this research, we investigate techniques to detect hate speech in movies.
We introduce a new dataset collected from the subtitles of six movies, where
each utterance is annotated either as hate, offensive or normal. We apply
transfer learning techniques of domain adaptation and fine-tuning on existing
social media datasets, namely from Twitter and Fox News. We evaluate different
representations, i.e., Bag of Words (BoW), Bi-directional Long short-term
memory (Bi-LSTM), and Bidirectional Encoder Representations from Transformers
(BERT) on 11k movie subtitles. The BERT model obtained the best macro-averaged
F1-score of 77%. Hence, we show that transfer learning from the social media
domain is efficacious in classifying hate and offensive speech in movies
through subtitles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misleading the Covid-19 vaccination discourse on Twitter: An exploratory study of infodemic around the pandemic. (arXiv:2108.10735v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10735">
<div class="article-summary-box-inner">
<span><p>In this work, we collect a moderate-sized representative corpus of tweets
(200,000 approx.) pertaining Covid-19 vaccination spanning over a period of
seven months (September 2020 - March 2021). Following a Transfer Learning
approach, we utilize the pre-trained Transformer-based XLNet model to classify
tweets as Misleading or Non-Misleading and validate against a random subset of
results manually. We build on this to study and contrast the characteristics of
tweets in the corpus that are misleading in nature against non-misleading ones.
This exploratory analysis enables us to design features (such as sentiments,
hashtags, nouns, pronouns, etc) that can, in turn, be exploited for classifying
tweets as (Non-)Misleading using various ML models in an explainable manner.
Specifically, several ML models are employed for prediction, with up to 90%
accuracy, and the importance of each feature is explained using SHAP
Explainable AI (XAI) tool. While the thrust of this work is principally
exploratory analysis in order to obtain insights on the online discourse on
Covid-19 vaccination, we conclude the paper by outlining how these insights
provide the foundations for a more actionable approach to mitigate
misinformation. The curated dataset and code is made available (Github
repository) so that the research community at large can reproduce, compare
against, or build upon this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10750">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) from tables is the task of identifying relations
between pairs of columns. Generally, RE models for this task require labelled
tables for training. Luckily, labelled tables can also be generated
artificially from a Knowledge Graph (KG), which makes the cost to acquire them
much lower in comparison to manual annotations. However, these tables have one
drawback compared to real tables, which is that they lack associated metadata,
such as column-headers, captions, etc. This is because synthetic tables are
created out of KGs that do not store such metadata. Unfortunately, metadata can
provide strong signals for RE from tables. To address this issue, we propose
methods to artificially create some of this metadata for synthetic tables. We
then experiment with a RE model that uses artificial metadata as input. Our
empirical results show that this leads to an improvement of 9\%-45\% in F1
score, in absolute terms, over 2 tabular datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More Than Words: Collocation Tokenization for Latent Dirichlet Allocation Models. (arXiv:2108.10755v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10755">
<div class="article-summary-box-inner">
<span><p>Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a
collection of documents to discover their latent topics using word-document
co-occurrences. However, it is unclear how to achieve the best results for
languages without marked word boundaries such as Chinese and Thai. Here, we
explore the use of Pearson's chi-squared test, t-statistics, and Word Pair
Encoding (WPE) to produce tokens as input to the LDA model. The Chi-squared, t,
and WPE tokenizers are trained on Wikipedia text to look for words that should
be grouped together, such as compound nouns, proper nouns, and complex event
verbs. We propose a new metric for measuring the clustering quality in settings
where the vocabularies of the models differ. Based on this metric and other
established metrics, we show that topics trained with merged tokens result in
topic keys that are clearer, more coherent, and more effective at
distinguishing topics than those unmerged models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ComSum: Commit Messages Summarization and Meaning Preservation. (arXiv:2108.10763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10763">
<div class="article-summary-box-inner">
<span><p>We present ComSum, a data set of 7 million commit messages for text
summarization. When documenting commits, software code changes, both a message
and its summary are posted. We gather and filter those to curate developers'
work summarization data set. Along with its growing size, practicality and
challenging language domain, the data set benefits from the living field of
empirical software engineering. As commits follow a typology, we propose to not
only evaluate outputs by Rouge, but by their meaning preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regularizing Transformers With Deep Probabilistic Layers. (arXiv:2108.10764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10764">
<div class="article-summary-box-inner">
<span><p>Language models (LM) have grown with non-stop in the last decade, from
sequence-to-sequence architectures to the state-of-the-art and utter
attention-based Transformers. In this work, we demonstrate how the inclusion of
deep generative models within BERT can bring more versatile models, able to
impute missing/noisy words with richer text or even improve BLEU score. More
precisely, we use a Gaussian Mixture Variational Autoencoder (GMVAE) as a
regularizer layer and prove its effectiveness not only in Transformers but also
in the most relevant encoder-decoder based LM, seq2seq with and without
attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensuring the Inclusive Use of Natural Language Processing in the Global Response to COVID-19. (arXiv:2108.10791v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10791">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) plays a significant role in tools for the
COVID-19 pandemic response, from detecting misinformation on social media to
helping to provide accurate clinical information or summarizing scientific
research. However, the approaches developed thus far have not benefited all
populations, regions or languages equally. We discuss ways in which current and
future NLP approaches can be made more inclusive by covering low-resource
languages, including alternative modalities, leveraging out-of-the-box tools
and forming meaningful partnerships. We suggest several future directions for
researchers interested in maximizing the positive societal impacts of NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reducing Exposure Bias in Training Recurrent Neural Network Transducers. (arXiv:2108.10803v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10803">
<div class="article-summary-box-inner">
<span><p>When recurrent neural network transducers (RNNTs) are trained using the
typical maximum likelihood criterion, the prediction network is trained only on
ground truth label sequences. This leads to a mismatch during inference, known
as exposure bias, when the model must deal with label sequences containing
errors. In this paper we investigate approaches to reducing exposure bias in
training to improve the generalization of RNNT models for automatic speech
recognition (ASR). A label-preserving input perturbation to the prediction
network is introduced. The input token sequences are perturbed using SwitchOut
and scheduled sampling based on an additional token language model. Experiments
conducted on the 300-hour Switchboard dataset demonstrate their effectiveness.
By reducing the exposure bias, we show that we can further improve the accuracy
of a high-performance RNNT ASR model and obtain state-of-the-art results on the
300-hour Switchboard dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document-level Neural Machine Translation with Associated Memory Network. (arXiv:1910.14528v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.14528">
<div class="article-summary-box-inner">
<span><p>Standard neural machine translation (NMT) is on the assumption that the
document-level context is independent. Most existing document-level NMT
approaches are satisfied with a smattering sense of global document-level
information, while this work focuses on exploiting detailed document-level
context in terms of a memory network. The capacity of the memory network that
detecting the most relevant part of the current sentence from memory renders a
natural solution to model the rich document-level context. In this work, the
proposed document-aware memory network is implemented to enhance the
Transformer NMT baseline. Experiments on several tasks show that the proposed
method significantly improves the NMT performance over strong Transformer
baselines and other related studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. (arXiv:1911.04942v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.04942">
<div class="article-summary-box-inner">
<span><p>When translating natural language questions into SQL queries to answer
questions from a database, contemporary semantic parsing models struggle to
generalize to unseen database schemas. The generalization challenge lies in (a)
encoding the database relations in an accessible way for the semantic parser,
and (b) modeling alignment between database columns and their mentions in a
given query. We present a unified framework, based on the relation-aware
self-attention mechanism, to address schema encoding, schema linking, and
feature representation within a text-to-SQL encoder. On the challenging Spider
dataset this framework boosts the exact match accuracy to 57.2%, surpassing its
best counterparts by 8.7% absolute improvement. Further augmented with BERT, it
achieves the new state-of-the-art performance of 65.6% on the Spider
leaderboard. In addition, we observe qualitative improvements in the model's
understanding of schema linking and alignment. Our implementation will be
open-sourced at https://github.com/Microsoft/rat-sql.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review on Fact Extraction and Verification. (arXiv:2010.03001v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03001">
<div class="article-summary-box-inner">
<span><p>We study the fact checking problem, which aims to identify the veracity of a
given claim. Specifically, we focus on the task of Fact Extraction and
VERification (FEVER) and its accompanied dataset. The task consists of the
subtasks of retrieving the relevant documents (and sentences) from Wikipedia
and validating whether the information in the documents supports or refutes a
given claim. This task is essential and can be the building block of
applications such as fake news detection and medical claim verification. In
this paper, we aim at a better understanding of the challenges of the task by
presenting the literature in a structured and comprehensive way. We describe
the proposed methods by analyzing the technical perspectives of the different
approaches and discussing the performance results on the FEVER dataset, which
is the most well-studied and formally structured dataset on the fact extraction
and verification task. We also conduct the largest experimental study to date
on identifying beneficial loss functions for the sentence retrieval component.
Our analysis indicates that sampling negative sentences is important for
improving the performance and decreasing the computational complexity. Finally,
we describe open issues and future challenges, and we motivate future research
in the task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable Visual Reasoning via Induced Symbolic Space. (arXiv:2011.11603v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11603">
<div class="article-summary-box-inner">
<span><p>We study the problem of concept induction in visual reasoning, i.e.,
identifying concepts and their hierarchical relationships from question-answer
pairs associated with images; and achieve an interpretable model via working on
the induced symbolic concept space. To this end, we first design a new
framework named object-centric compositional attention model (OCCAM) to perform
the visual reasoning task with object-level visual features. Then, we come up
with a method to induce concepts of objects and relations using clues from the
attention patterns between objects' visual features and question words.
Finally, we achieve a higher level of interpretability by imposing OCCAM on the
objects represented in the induced symbolic concept space. Our model design
makes this an easy adaption via first predicting the concepts of objects and
relations and then projecting the predicted concepts back to the visual feature
space so the compositional reasoning module can process normally. Experiments
on the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of
the art without human-annotated functional programs; 2) our induced concepts
are both accurate and sufficient as OCCAM achieves an on-par performance on
objects represented either in visual features or in the induced symbolic
concept space.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. (arXiv:2104.04473v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04473">
<div class="article-summary-box-inner">
<span><p>Large language models have led to state-of-the-art accuracies across a range
of tasks. However, training these models efficiently is challenging for two
reasons: a) GPU memory capacity is limited, making it impossible to fit large
models on even a multi-GPU server, and b) the number of compute operations
required to train these models can result in unrealistically long training
times. Consequently, new methods of model parallelism such as tensor and
pipeline parallelism have been proposed. Unfortunately, naive usage of these
methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to
expensive cross-node communication or devices spending significant time waiting
on other devices to make progress.
</p>
<p>In this paper, we show how different types of parallelism methods (tensor,
pipeline, and data parallelism) can be composed to scale to thousands of GPUs
and models with trillions of parameters. We survey techniques for pipeline
parallelism and propose a novel interleaved pipeline parallelism schedule that
can improve throughput by 10+% with memory footprint comparable to existing
approaches. We quantitatively study the trade-offs between tensor, pipeline,
and data parallelism, and provide intuition as to how to configure distributed
training of a large model. Our approach allows us to perform training
iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs
with achieved per-GPU throughput of 52% of theoretical peak. Our code is open
sourced at https://github.com/nvidia/megatron-lm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fibrational Initial Algebra-Final Coalgebra Coincidence over Initial Algebras: Turning Verification Witnesses Upside Down. (arXiv:2105.04817v3 [cs.LO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04817">
<div class="article-summary-box-inner">
<span><p>The coincidence between initial algebras (IAs) and final coalgebras (FCs) is
a phenomenon that underpins various important results in theoretical computer
science. In this paper, we identify a general fibrational condition for the
IA-FC coincidence, namely in the fiber over an initial algebra in the base
category. Identifying (co)algebras in a fiber as (co)inductive predicates, our
fibrational IA-FC coincidence allows one to use coinductive witnesses (such as
invariants) for verifying inductive properties (such as liveness). Our general
fibrational theory features the technical condition of stability of chain
colimits; we extend the framework to the presence of a monadic effect, too,
restricting to fibrations of complete lattice-valued predicates. Practical
benefits of our categorical theory are exemplified by new "upside-down" witness
notions for three verification problems: probabilistic liveness, and acceptance
and model-checking with respect to bottom-up tree automata.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08087">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI), along with the recent progress in biomedical
language understanding, is gradually changing medical practice. With the
development of biomedical language understanding benchmarks, AI applications
are widely used in the medical field. However, most benchmarks are limited to
English, which makes it challenging to replicate many of the successes in
English for other languages. To facilitate research in this direction, we
collect real-world biomedical data and present the first Chinese Biomedical
Language Understanding Evaluation (CBLUE) benchmark: a collection of natural
language understanding tasks including named entity recognition, information
extraction, clinical diagnosis normalization, single-sentence/sentence-pair
classification, and an associated online platform for model evaluation,
comparison, and analysis. To establish evaluation on these tasks, we report
empirical results with the current 11 pre-trained Chinese models, and
experimental results show that state-of-the-art neural models perform by far
worse than the human ceiling. Our benchmark is released at
\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&amp;lang=en-us}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Computer-assisted construct classification of organizational performance concerning different stakeholder groups. (arXiv:2107.05133v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05133">
<div class="article-summary-box-inner">
<span><p>The number of research articles in business and management has dramatically
increased along with terminology, constructs, and measures. Proper
classification of organizational performance constructs from research articles
plays an important role in categorizing the literature and understanding to
whom its research implications may be relevant. In this work, we classify
constructs (i.e., concepts and terminology used to capture different aspects of
organizational performance) in research articles into a three-level
categorization: (a) performance and non-performance categories (Level 0); (b)
for performance constructs, stakeholder group-level of performance concerning
investors, customers, employees, and the society (community and natural
environment) (Level 1); and (c) for each stakeholder group-level, subcategories
of different ways of measurement (Level 2). We observed that increasing
contextual information with features extracted from surrounding sentences and
external references improves classification of disaggregate-level labels, given
limited training data. Our research has implications for computer-assisted
construct identification and classification - an essential step for research
synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04539">
<div class="article-summary-box-inner">
<span><p>Understanding documents from their visual snapshots is an emerging problem
that requires both advanced computer vision and NLP methods. The recent advance
in OCR enables the accurate recognition of text blocks, yet it is still
challenging to extract key information from documents due to the diversity of
their layouts. Although recent studies on pre-trained language models show the
importance of incorporating layout information on this task, the conjugation of
texts and their layouts still follows the style of BERT optimized for
understanding the 1D text. This implies there is room for further improvement
considering the 2D nature of text layouts. This paper introduces a pre-trained
language model, BERT Relying On Spatiality (BROS), which effectively utilizes
the information included in individual text blocks and their layouts.
Specifically, BROS encodes spatial information by utilizing relative positions
and learns spatial dependencies between OCR blocks with a novel area-masking
strategy. These two novel approaches lead to an efficient encoding of spatial
layout information highlighted by the robust performance of BROS under
low-resource environments. We also introduce a general-purpose parser that can
be combined with BROS to extract key information even when there is no order
information between text blocks. BROS shows its superiority on four public
benchmarks -- FUNSD, SROIE*, CORD, and SciTSR -- and its robustness in
practical cases where order information of text blocks is not available.
Further experiments with a varying number of training examples demonstrate the
high training efficiency of our approach. Our code will be open to the public.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-25 23:08:50.715212233 UTC">2021-08-25 23:08:50 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>