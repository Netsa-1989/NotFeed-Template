<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-17T01:30:00Z">09-17</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Register Projection for Headline Part of Speech Tagging. (arXiv:2109.07483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07483">
<div class="article-summary-box-inner">
<span><p>Part of speech (POS) tagging is a familiar NLP task. State of the art taggers
routinely achieve token-level accuracies of over 97% on news body text,
evidence that the problem is well understood. However, the register of English
news headlines, "headlinese", is very different from the register of long-form
text, causing POS tagging models to underperform on headlines. In this work, we
automatically annotate news headlines with POS tags by projecting predicted
tags from corresponding sentences in news bodies. We train a multi-domain POS
tagger on both long-form and headline text and show that joint training on both
registers improves over training on just one or naively concatenating training
sets. We evaluate on a newly-annotated corpus of over 5,248 English news
headlines from the Google sentence compression corpus, and show that our model
yields a 23% relative error reduction per token and 19% per headline. In
addition, we demonstrate that better headline POS tags can improve the
performance of a syntax-based open information extraction system. We make POSH,
the POS-tagged Headline corpus, available to encourage research in improved NLP
models for news headlines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Euclidean and Hyperbolic Embeddings on the WordNet Nouns Hypernymy Graph. (arXiv:2109.07488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07488">
<div class="article-summary-box-inner">
<span><p>Nickel and Kiela (2017) present a new method for embedding tree nodes in the
Poincare ball, and suggest that these hyperbolic embeddings are far more
effective than Euclidean embeddings at embedding nodes in large, hierarchically
structured graphs like the WordNet nouns hypernymy tree. This is especially
true in low dimensions (Nickel and Kiela, 2017, Table 1). In this work, we seek
to reproduce their experiments on embedding and reconstructing the WordNet
nouns hypernymy graph. Counter to what they report, we find that Euclidean
embeddings are able to represent this tree at least as well as Poincare
embeddings, when allowed at least 50 dimensions. We note that this does not
diminish the significance of their work given the impressive performance of
hyperbolic embeddings in very low-dimensional settings. However, given the wide
influence of their work, our aim here is to present an updated and more
accurate comparison between the Euclidean and hyperbolic embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Making Heads and Tails of Models with Marginal Calibration for Sparse Tagsets. (arXiv:2109.07494v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07494">
<div class="article-summary-box-inner">
<span><p>For interpreting the behavior of a probabilistic model, it is useful to
measure a model's calibration--the extent to which it produces reliable
confidence scores. We address the open problem of calibration for tagging
models with sparse tagsets, and recommend strategies to measure and reduce
calibration error (CE) in such models. We show that several post-hoc
recalibration techniques all reduce calibration error across the marginal
distribution for two existing sequence taggers. Moreover, we propose tag
frequency grouping (TFG) as a way to measure calibration error in different
frequency bands. Further, recalibrating each group separately promotes a more
equitable reduction of calibration error across the tag frequency spectrum.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue State Tracking with a Language Model using Schema-Driven Prompting. (arXiv:2109.07506v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07506">
<div class="article-summary-box-inner">
<span><p>Task-oriented conversational systems often use dialogue state tracking to
represent the user's intentions, which involves filling in values of
pre-defined slots. Many approaches have been proposed, often using
task-specific architectures with special-purpose classifiers. Recently, good
results have been obtained using more general architectures based on pretrained
language models. Here, we introduce a new variation of the language modeling
approach that uses schema-driven prompting to provide task-aware history
encoding that is used for both categorical and non-categorical slots. We
further improve performance by augmenting the prompting with schema
descriptions, a naturally occurring source of in-domain knowledge. Our purely
generative system achieves state-of-the-art performance on MultiWOZ 2.2 and
achieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M.
The data and code will be available at
https://github.com/chiahsuan156/DST-as-Prompting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tied & Reduced RNN-T Decoder. (arXiv:2109.07513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07513">
<div class="article-summary-box-inner">
<span><p>Previous works on the Recurrent Neural Network-Transducer (RNN-T) models have
shown that, under some conditions, it is possible to simplify its prediction
network with little or no loss in recognition accuracy (<a href="/abs/2003.07705">arXiv:2003.07705</a>
[eess.AS], [2], <a href="/abs/2012.06749">arXiv:2012.06749</a> [cs.CL]). This is done by limiting the context
size of previous labels and/or using a simpler architecture for its layers
instead of LSTMs. The benefits of such changes include reduction in model size,
faster inference and power savings, which are all useful for on-device
applications.
</p>
<p>In this work, we study ways to make the RNN-T decoder (prediction network +
joint network) smaller and faster without degradation in recognition
performance. Our prediction network performs a simple weighted averaging of the
input embeddings, and shares its embedding matrix weights with the joint
network's output layer (a.k.a. weight tying, commonly used in language modeling
<a href="/abs/1611.01462">arXiv:1611.01462</a> [cs.LG]). This simple design, when used in conjunction with
additional Edit-based Minimum Bayes Risk (EMBR) training, reduces the RNN-T
Decoder from 23M parameters to just 2M, without affecting word-error rate
(WER).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text as Causal Mediators: Research Design for Causal Estimates of Differential Treatment of Social Groups via Language Aspects. (arXiv:2109.07542v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07542">
<div class="article-summary-box-inner">
<span><p>Using observed language to understand interpersonal interactions is important
in high-stakes decision making. We propose a causal research design for
observational (non-experimental) data to estimate the natural direct and
indirect effects of social group signals (e.g. race or gender) on speakers'
responses with separate aspects of language as causal mediators. We illustrate
the promises and challenges of this framework via a theoretical case study of
the effect of an advocate's gender on interruptions from justices during U.S.
Supreme Court oral arguments. We also discuss challenges conceptualizing and
operationalizing causal variables such as gender and language that comprise of
many components, and we articulate technical open challenges such as temporal
dependence between language mediators in conversational settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"It doesn't look good for a date": Transforming Critiques into Preferences for Conversational Recommendation Systems. (arXiv:2109.07576v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07576">
<div class="article-summary-box-inner">
<span><p>Conversations aimed at determining good recommendations are iterative in
nature. People often express their preferences in terms of a critique of the
current recommendation (e.g., "It doesn't look good for a date"), requiring
some degree of common sense for a preference to be inferred. In this work, we
present a method for transforming a user critique into a positive preference
(e.g., "I prefer more romantic") in order to retrieve reviews pertaining to
potentially better recommendations (e.g., "Perfect for a romantic dinner"). We
leverage a large neural language model (LM) in a few-shot setting to perform
critique-to-preference transformation, and we test two methods for retrieving
recommendations: one that matches embeddings, and another that fine-tunes an LM
for the task. We instantiate this approach in the restaurant domain and
evaluate it using a new dataset of restaurant critiques. In an ablation study,
we show that utilizing critique-to-preference transformation improves
recommendations, and that there are at least three general cases that explain
this improved performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An influencer-based approach to understanding radical right viral tweets. (arXiv:2109.07588v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07588">
<div class="article-summary-box-inner">
<span><p>Radical right influencers routinely use social media to spread highly
divisive, disruptive and anti-democratic messages. Assessing and countering the
challenge that such content poses is crucial for ensuring that online spaces
remain open, safe and accessible. Previous work has paid little attention to
understanding factors associated with radical right content that goes viral. We
investigate this issue with a new dataset ROT which provides insight into the
content, engagement and followership of a set of 35 radical right influencers.
It includes over 50,000 original entries and over 40 million retweets, quotes,
replies and mentions. We use a multilevel model to measure engagement with
tweets, which are nested in each influencer. We show that it is crucial to
account for the influencer-level structure, and find evidence of the importance
of both influencer- and content-level factors, including the number of
followers each influencer has, the type of content (original posts, quotes and
replies), the length and toxicity of content, and whether influencers request
retweets. We make ROT available for other researchers to use.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning. (arXiv:2109.07589v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07589">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) in Few-Shot setting is imperative for entity
tagging in low resource domains. Existing approaches only learn class-specific
semantic features and intermediate representations from source domains. This
affects generalizability to unseen target domains, resulting in suboptimal
performances. To this end, we present CONTaiNER, a novel contrastive learning
technique that optimizes the inter-token distribution distance for Few-Shot
NER. Instead of optimizing class-specific attributes, CONTaiNER optimizes a
generalized objective of differentiating between token categories based on
their Gaussian-distributed embeddings. This effectively alleviates overfitting
issues originating from training domains. Our experiments in several
traditional test domains (OntoNotes, CoNLL'03, WNUT '17, GUM) and a new large
scale Few-Shot NER dataset (Few-NERD) demonstrate that on average, CONTaiNER
outperforms previous methods by 3%-13% absolute F1 points while showing
consistent performance trends, even in challenging scenarios where previous
approaches could not achieve appreciable performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity of Data Selection and Fine Tuning for Domain Adaptation. (arXiv:2109.07591v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07591">
<div class="article-summary-box-inner">
<span><p>Domain adaptation of neural networks commonly relies on three training
phases: pretraining, selected data training and then fine tuning. Data
selection improves target domain generalization by training further on
pretraining data identified by relying on a small sample of target domain data.
This work examines the benefit of data selection for language modeling and
machine translation. Our experiments assess the complementarity of selection
with fine tuning and result in practical recommendations: (i) selected data
must be similar to the fine-tuning domain but not so much as to erode the
complementary effect of fine-tuning; (ii) there is a trade-off between
selecting little data for fast but limited progress or much data for slow but
long lasting progress; (iii) data selection can be applied early during
pretraining, with performance gains comparable to long pretraining session;
(iv) data selection from domain classifiers is often more effective than the
popular contrastive data selection method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification. (arXiv:2109.07604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07604">
<div class="article-summary-box-inner">
<span><p>Traditional hand-crafted linguistically-informed features have often been
used for distinguishing between translated and original non-translated texts.
By contrast, to date, neural architectures without manual feature engineering
have been less explored for this task. In this work, we (i) compare the
traditional feature-engineering-based approach to the feature-learning-based
one and (ii) analyse the neural architectures in order to investigate how well
the hand-crafted features explain the variance in the neural models'
predictions. We use pre-trained neural word embeddings, as well as several
end-to-end neural architectures in both monolingual and multilingual settings
and compare them to feature-engineering-based SVM classifiers. We show that (i)
neural architectures outperform other approaches by more than 20 accuracy
points, with the BERT-based model performing the best in both the monolingual
and multilingual settings; (ii) while many individual hand-crafted
translationese features correlate with neural model predictions, feature
importance analysis shows that the most important features for neural and
classical architectures differ; and (iii) our multilingual experiments provide
empirical evidence for translationese universals across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Ontology-Based Information Extraction System for Residential Land Use Suitability Analysis. (arXiv:2109.07672v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07672">
<div class="article-summary-box-inner">
<span><p>We propose an Ontology-Based Information Extraction (OBIE) system to automate
the extraction of the criteria and values applied in Land Use Suitability
Analysis (LUSA) from bylaw and regulation documents related to the geographic
area of interest. The results obtained by our proposed LUSA OBIE system (land
use suitability criteria and their values) are presented as an ontology
populated with instances of the extracted criteria and property values. This
latter output ontology is incorporated into a Multi-Criteria Decision Making
(MCDM) model applied for constructing suitability maps for different kinds of
land uses. The resulting maps may be the final desired product or can be
incorporated into the cellular automata urban modeling and simulation for
predicting future urban growth. A case study has been conducted where the
output from LUSA OBIE is applied to help produce a suitability map for the City
of Regina, Saskatchewan, to assist in the identification of suitable areas for
residential development. A set of Saskatchewan bylaw and regulation documents
were downloaded and input to the LUSA OBIE system. We accessed the extracted
information using both the populated LUSA ontology and the set of annotated
documents. In this regard, the LUSA OBIE system was effective in producing a
final suitability map.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset. (arXiv:2109.07679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07679">
<div class="article-summary-box-inner">
<span><p>Reasoning over commonsense knowledge bases (CSKB) whose elements are in the
form of free-text is an important yet hard task in NLP. While CSKB completion
only fills the missing links within the domain of the CSKB, CSKB population is
alternatively proposed with the goal of reasoning unseen assertions from
external resources. In this task, CSKBs are grounded to a large-scale
eventuality (activity, state, and event) graph to discriminate whether novel
triples from the eventuality graph are plausible or not. However, existing
evaluations on the population task are either not accurate (automatic
evaluation with randomly sampled negative examples) or of small scale (human
annotation). In this paper, we benchmark the CSKB population task with a new
large-scale dataset by first aligning four popular CSKBs, and then presenting a
high-quality human-annotated evaluation set to probe neural models' commonsense
reasoning ability. We also propose a novel inductive commonsense reasoning
model that reasons over graphs. Experimental results show that generalizing
commonsense reasoning on unseen assertions is inherently a hard task. Models
achieving high accuracy during training perform poorly on the evaluation set,
with a large gap between human performance. We will make the data publicly
available for future contributions. Codes and data are available at
https://github.com/HKUST-KnowComp/CSKB-Population.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07680">
<div class="article-summary-box-inner">
<span><p>Identification of user's opinions from natural language text has become an
exciting field of research due to its growing applications in the real world.
The research field is known as sentiment analysis and classification, where
aspect category detection (ACD) and aspect category polarity (ACP) are two
important sub-tasks of aspect-based sentiment analysis. The goal in ACD is to
specify which aspect of the entity comes up in opinion while ACP aims to
specify the polarity of each aspect category from the ACD task. The previous
works mostly propose separate solutions for these two sub-tasks. This paper
focuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The
proposed method carries out multi-label classification where four different
deep models were employed and comparatively evaluated to examine their
performance. A dataset of Persian reviews was collected from CinemaTicket
website including 2200 samples from 14 categories. The developed models were
evaluated using the collected dataset in terms of example-based and label-based
metrics. The results indicate the high applicability and preference of the CNN
and GRU models in comparison to LSTM and Bi-LSTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models are Few-shot Multilingual Learners. (arXiv:2109.07684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07684">
<div class="article-summary-box-inner">
<span><p>General-purpose language models have demonstrated impressive capabilities,
performing on par with state-of-the-art approaches on a range of downstream
natural language processing (NLP) tasks and benchmarks when inferring
instructions from very few examples. Here, we evaluate the multilingual skills
of the GPT and T5 models in conducting multi-class classification on
non-English languages without any parameter updates. We show that, given a few
English examples as context, pre-trained language models can predict not only
English test samples but also non-English ones. Finally, we find the in-context
few-shot cross-lingual prediction results of language models are significantly
better than random prediction, and they are competitive compared to the
existing state-of-the-art cross-lingual models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferable Persona-Grounded Dialogues via Grounded Minimal Edits. (arXiv:2109.07713v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07713">
<div class="article-summary-box-inner">
<span><p>Grounded dialogue models generate responses that are grounded on certain
concepts. Limited by the distribution of grounded dialogue data, models trained
on such data face the transferability challenges in terms of the data
distribution and the type of grounded concepts. To address the challenges, we
propose the grounded minimal editing framework, which minimally edits existing
responses to be grounded on the given concept. Focusing on personas, we propose
Grounded Minimal Editor (GME), which learns to edit by disentangling and
recombining persona-related and persona-agnostic parts of the response. To
evaluate persona-grounded minimal editing, we present the PersonaMinEdit
dataset, and experimental results show that GME outperforms competitive
baselines by a large margin. To evaluate the transferability, we experiment on
the test set of BlendedSkillTalk and show that GME can edit dialogue models'
responses to largely improve their persona consistency while preserving the use
of knowledge and empathy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sister Help: Data Augmentation for Frame-Semantic Role Labeling. (arXiv:2109.07725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07725">
<div class="article-summary-box-inner">
<span><p>While FrameNet is widely regarded as a rich resource of semantics in natural
language processing, a major criticism concerns its lack of coverage and the
relative paucity of its labeled data compared to other commonly used lexical
resources such as PropBank and VerbNet. This paper reports on a pilot study to
address these gaps. We propose a data augmentation approach, which uses
existing frame-specific annotation to automatically annotate other lexical
units of the same frame which are unannotated. Our rule-based approach defines
the notion of a sister lexical unit and generates frame-specific augmented data
for training. We present experiments on frame-semantic role labeling which
demonstrate the importance of this data augmentation: we obtain a large
improvement to prior results on frame identification and argument
identification for FrameNet, utilizing both full-text and lexicographic
annotations under FrameNet. Our findings on data augmentation highlight the
value of automatic resource creation for improved models in frame-semantic
parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MOVER: Mask, Over-generate and Rank for Hyperbole Generation. (arXiv:2109.07726v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07726">
<div class="article-summary-box-inner">
<span><p>Despite being a common figure of speech, hyperbole is under-researched with
only a few studies addressing its identification task. In this paper, we
introduce a new task of hyperbole generation to transfer a literal sentence
into its hyperbolic paraphrase. To tackle the lack of available hyperbolic
sentences, we construct HYPO-XL, the first large-scale hyperbole corpus
containing 17,862 hyperbolic sentences in a non-trivial way. Based on our
corpus, we propose an unsupervised method for hyperbole generation with no need
for parallel literal-hyperbole pairs. During training, we fine-tune BART to
infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we
mask part of an input literal sentence and over-generate multiple possible
hyperbolic versions. Then a BERT-based ranker selects the best candidate by
hyperbolicity and paraphrase quality. Human evaluation results show that our
model is capable of generating hyperbolic paraphrase sentences and outperforms
several baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scaling Laws for Neural Machine Translation. (arXiv:2109.07740v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07740">
<div class="article-summary-box-inner">
<span><p>We present an empirical study of scaling properties of encoder-decoder
Transformer models used in neural machine translation (NMT). We show that
cross-entropy loss as a function of model size follows a certain scaling law.
Specifically (i) We propose a formula which describes the scaling behavior of
cross-entropy loss as a bivariate function of encoder and decoder size, and
show that it gives accurate predictions under a variety of scaling approaches
and languages; we show that the total number of parameters alone is not
sufficient for such purposes. (ii) We observe different power law exponents
when scaling the decoder vs scaling the encoder, and provide recommendations
for optimal allocation of encoder/decoder capacity based on this observation.
(iii) We also report that the scaling behavior of the model is acutely
influenced by composition bias of the train/test sets, which we define as any
deviation from naturally generated text (either via machine generated or human
translated text). We observe that natural text on the target side enjoys
scaling, which manifests as successful reduction of the cross-entropy loss.
(iv) Finally, we investigate the relationship between the cross-entropy loss
and the quality of the generated translations. We find two different behaviors,
depending on the nature of the test data. For test sets which were originally
translated from target language to source language, both loss and BLEU score
improve as model size increases. In contrast, for test sets originally
translated from source language to target language, the loss improves, but the
BLEU score stops improving after a certain threshold. We release generated text
from all models used in this study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spanish Biomedical Crawled Corpus: A Large, Diverse Dataset for Spanish Biomedical Language Models. (arXiv:2109.07765v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07765">
<div class="article-summary-box-inner">
<span><p>We introduce CoWeSe (the Corpus Web Salud Espa\~nol), the largest Spanish
biomedical corpus to date, consisting of 4.5GB (about 750M tokens) of clean
plain text. CoWeSe is the result of a massive crawler on 3000 Spanish domains
executed in 2020. The corpus is openly available and already preprocessed.
CoWeSe is an important resource for biomedical and health NLP in Spanish and
has already been employed to train domain-specific language models and to
produce word embbedings. We released the CoWeSe corpus under a Creative Commons
Attribution 4.0 International license, both in Zenodo
(\url{https://zenodo.org/record/4561971\#.YTI5SnVKiEA}).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation. (arXiv:2109.07779v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07779">
<div class="article-summary-box-inner">
<span><p>Researches on dialogue empathy aim to endow an agent with the capacity of
accurate understanding and proper responding for emotions. Existing models for
empathetic dialogue generation focus on the emotion flow in one direction, that
is, from the context to response. We argue that conducting an empathetic
conversation is a bidirectional process, where empathy occurs when the emotions
of two interlocutors could converge on the same point, i.e., reaching an
emotion consensus. Besides, we also find that the empathetic dialogue corpus is
extremely limited, which further restricts the model performance. To address
the above issues, we propose a dual-generative model, Dual-Emp, to
simultaneously construct the emotion consensus and utilize some external
unpaired data. Specifically, our model integrates a forward dialogue model, a
backward dialogue model, and a discrete latent variable representing the
emotion consensus into a unified architecture. Then, to alleviate the
constraint of paired data, we extract unpaired emotional data from open-domain
conversations and employ Dual-Emp to produce pseudo paired empathetic samples,
which is more efficient and low-cost than the human annotation. Automatic and
human evaluations demonstrate that our method outperforms competitive baselines
in producing coherent and empathetic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Neural Machine Translation by Bidirectional Training. (arXiv:2109.07780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07780">
<div class="article-summary-box-inner">
<span><p>We present a simple and effective pretraining strategy -- bidirectional
training (BiT) for neural machine translation. Specifically, we bidirectionally
update the model parameters at the early stage and then tune the model
normally. To achieve bidirectional updating, we simply reconstruct the training
samples from "src$\rightarrow$tgt" to "src+tgt$\rightarrow$tgt+src" without any
complicated model modifications. Notably, our approach does not increase any
parameters or training steps, requiring the parallel data merely. Experimental
results show that BiT pushes the SOTA neural machine translation performance
across 15 translation tasks on 8 language pairs (data sizes range from 160K to
38M) significantly higher. Encouragingly, our proposed model can complement
existing data manipulation strategies, i.e. back translation, data
distillation, and data diversification. Extensive analyses show that our
approach functions as a novel bilingual code-switcher, obtaining better
bilingual alignment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transductive Learning for Unsupervised Text Style Transfer. (arXiv:2109.07812v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07812">
<div class="article-summary-box-inner">
<span><p>Unsupervised style transfer models are mainly based on an inductive learning
approach, which represents the style as embeddings, decoder parameters, or
discriminator parameters and directly applies these general rules to the test
cases. However, the lacking of parallel corpus hinders the ability of these
inductive learning methods on this task. As a result, it is likely to cause
severe inconsistent style expressions, like `the salad is rude`. To tackle this
problem, we propose a novel transductive learning approach in this paper, based
on a retrieval-based context-aware style representation. Specifically, an
attentional encoder-decoder with a retriever framework is utilized. It involves
top-K relevant sentences in the target style in the transfer process. In this
way, we can learn a context-aware style embedding to alleviate the above
inconsistency problem. In this paper, both sparse (BM25) and dense retrieval
functions (MIPS) are used, and two objective functions are designed to
facilitate joint learning. Experimental results show that our method
outperforms several strong baselines. The proposed transductive learning
approach is general and effective to the task of unsupervised style transfer,
and we will apply it to the other two typical methods in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reframing Instructional Prompts to GPTk's Language. (arXiv:2109.07830v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07830">
<div class="article-summary-box-inner">
<span><p>How can model designers turn task instructions into effective prompts for
language models? Backed by extensive empirical analysis on GPT3, we observe
important features for successful instructional prompts, and propose several
reframing techniques for model designers to create such prompts. For example, a
complex task can be decomposed into multiple simpler tasks. We experiment over
12 NLP tasks across 6 diverse categories (question generation, classification,
etc.). Our results show that reframing improves few-shot learning performance
by 14\% while reducing sample complexity over existing few-shot baselines. The
performance gains are particularly important on large language models, such as
GPT3 where tuning models or prompts on large datasets is not feasible.
Furthermore, we observe that such gains are not limited to GPT3; the reframed
tasks remain superior over raw instructions across different model
architectures, underscoring the cross-model generality of these guidelines. We
hope these empirical-driven techniques will pave way for more effective ways to
prompt LMs in future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings. (arXiv:2109.07833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07833">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) requires models to learn and apply
commonsense knowledge. These reasoning abilities are particularly important for
explainable NLI systems that generate a natural language explanation in
addition to their label prediction. The integration of external knowledge has
been shown to improve NLI systems, here we investigate whether it can also
improve their explanation capabilities. For this, we investigate different
sources of external knowledge and evaluate the performance of our models on
in-domain data as well as on special transfer datasets that are designed to
assess fine-grained reasoning capabilities. We find that different sources of
knowledge have a different effect on reasoning abilities, for example, implicit
knowledge stored in language models can hinder reasoning on numbers and
negations. Finally, we conduct the largest and most fine-grained explainable
NLI crowdsourcing study to date. It reveals that even large differences in
automatic performance scores do neither reflect in human ratings of label,
explanation, commonsense nor grammar correctness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation. (arXiv:2109.07848v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07848">
<div class="article-summary-box-inner">
<span><p>Temporary syntactic ambiguities arise when the beginning of a sentence is
compatible with multiple syntactic analyses. We inspect to which extent neural
language models (LMs) exhibit uncertainty over such analyses when processing
temporarily ambiguous inputs, and how that uncertainty is modulated by
disambiguating cues. We probe the LM's expectations by generating from it: we
use stochastic decoding to derive a set of sentence completions, and estimate
the probability that the LM assigns to each interpretation based on the
distribution of parses across completions. Unlike scoring-based methods for
targeted syntactic evaluation, this technique makes it possible to explore
completions that are not hypothesized in advance by the researcher. We apply
this method to study the behavior of two LMs (GPT2 and an LSTM) on three types
of temporary ambiguity, using materials from human sentence processing
experiments. We find that LMs can track multiple analyses simultaneously; the
degree of uncertainty varies across constructions and contexts. As a response
to disambiguating cues, the LMs often select the correct interpretation, but
occasional errors point to potential areas of improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translation Transformers Rediscover Inherent Data Domains. (arXiv:2109.07864v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07864">
<div class="article-summary-box-inner">
<span><p>Many works proposed methods to improve the performance of Neural Machine
Translation (NMT) models in a domain/multi-domain adaptation scenario. However,
an understanding of how NMT baselines represent text domain information
internally is still lacking. Here we analyze the sentence representations
learned by NMT Transformers and show that these explicitly include the
information on text domains, even after only seeing the input sentences without
domains labels. Furthermore, we show that this internal information is enough
to cluster sentences by their underlying domains without supervision. We show
that NMT models produce clusters better aligned to the actual domains compared
to pre-trained language models (LMs). Notably, when computed on document-level,
NMT cluster-to-domain correspondence nears 100%. We use these findings together
with an approach to NMT domain adaptation using automatically extracted
domains. Whereas previous work relied on external LMs for text clustering, we
propose re-using the NMT model as a source of unsupervised clusters. We perform
an extensive experimental study comparing two approaches across two data
scenarios, three language pairs, and both sentence-level and document-level
clustering, showing equal or significantly superior performance compared to
LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Humanly Certifying Superhuman Classifiers. (arXiv:2109.07867v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07867">
<div class="article-summary-box-inner">
<span><p>Estimating the performance of a machine learning system is a longstanding
challenge in artificial intelligence research. Today, this challenge is
especially relevant given the emergence of systems which appear to increasingly
outperform human beings. In some cases, this "superhuman" performance is
readily demonstrated; for example by defeating legendary human players in
traditional two player games. On the other hand, it can be challenging to
evaluate classification models that potentially surpass human performance.
Indeed, human annotations are often treated as a ground truth, which implicitly
assumes the superiority of the human over any models trained on human
annotations. In reality, human annotators can make mistakes and be subjective.
Evaluating the performance with respect to a genuine oracle may be more
objective and reliable, even when querying the oracle is expensive or
impossible. In this paper, we first raise the challenge of evaluating the
performance of both humans and models with respect to an oracle which is
unobserved. We develop a theory for estimating the accuracy compared to the
oracle, using only imperfect human annotations for reference. Our analysis
provides a simple recipe for detecting and certifying superhuman performance in
this setting, which we believe will assist in understanding the stage of
current research on classification. We validate the convergence of the bounds
and the assumptions of our theory on carefully designed toy experiments with
known oracles. Moreover, we demonstrate the utility of our theory by
meta-analyzing large-scale natural language processing tasks, for which an
oracle does not exist, and show that under our assumptions a number of models
from recent years are with high probability superhuman.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFE-NER: Multi-feature Fusion Embedding for Chinese Named Entity Recognition. (arXiv:2109.07877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07877">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models lead Named Entity Recognition (NER) into a new
era, while some more knowledge is needed to improve their performance in
specific problems. In Chinese NER, character substitution is a complicated
linguistic phenomenon. Some Chinese characters are quite similar for sharing
the same components or having similar pronunciations. People replace characters
in a named entity with similar characters to generate a new collocation but
referring to the same object. It becomes even more common in the Internet age
and is often used to avoid Internet censorship or just for fun. Such character
substitution is not friendly to those pre-trained language models because the
new collocations are occasional. As a result, it always leads to unrecognizable
or recognition errors in the NER task. In this paper, we propose a new method,
Multi-Feature Fusion Embedding for Chinese Named Entity Recognition (MFE-NER),
to strengthen the language pattern of Chinese and handle the character
substitution problem in Chinese Named Entity Recognition. MFE fuses semantic,
glyph, and phonetic features together. In the glyph domain, we disassemble
Chinese characters into components to denote structure features so that
characters with similar structures can have close embedding space
representation. Meanwhile, an improved phonetic system is also proposed in our
work, making it reasonable to calculate phonetic similarity among Chinese
characters. Experiments demonstrate that our method improves the overall
performance of Chinese NER and especially performs well in informal language
environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surveying the Research on Fake News in Social Media: a Tale of Networks and Language. (arXiv:2109.07909v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07909">
<div class="article-summary-box-inner">
<span><p>The history of journalism and news diffusion is tightly coupled with the
effort to dispel hoaxes, misinformation, propaganda, unverified rumours, poor
reporting, and messages containing hate and divisions. With the explosive
growth of online social media and billions of individuals engaged with
consuming, creating, and sharing news, this ancient problem has surfaced with a
renewed intensity threatening our democracies, public health, and news outlets
credibility. This has triggered many researchers to develop new methods for
studying, understanding, detecting, and preventing fake-news diffusion; as a
consequence, thousands of scientific papers have been published in a relatively
short period, making researchers of different disciplines to struggle in search
of open problems and most relevant trends. The aim of this survey is threefold:
first, we want to provide the researchers interested in this multidisciplinary
and challenging area with a network-based analysis of the existing literature
to assist them with a visual exploration of papers that can be of interest;
second, we present a selection of the main results achieved so far adopting the
network as an unifying framework to represent and make sense of data, to model
diffusion processes, and to evaluate different debunking strategies. Finally,
we present an outline of the most relevant research trends focusing on the
moving target of fake-news, bots, and trolls identification by means of data
mining and text technologies; despite scholars working on computational
linguistics and networks traditionally belong to different scientific
communities, we expect that forthcoming computational approaches to prevent
fake news from polluting the social media must be developed using hybrid and
up-to-date methodologies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Search for a Search Method -- Simple Heuristics Suffice for Adversarial Text Attacks. (arXiv:2109.07926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07926">
<div class="article-summary-box-inner">
<span><p>Recently more attention has been given to adversarial attacks on neural
networks for natural language processing (NLP). A central research topic has
been the investigation of search algorithms and search constraints, accompanied
by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth
order optimization-based attacks and compare with the benchmark results in the
TextAttack framework. Surprisingly, we find that optimization-based methods do
not yield any improvement in a constrained setup and slightly benefit from
approximate gradient information only in unconstrained setups where search
spaces are larger. In contrast, simple heuristics exploiting nearest neighbors
without querying the target function yield substantial success rates in
constrained setups, and nearly full success rate in unconstrained setups, at an
order of magnitude fewer queries. We conclude from these results that current
TextAttack benchmark tasks are too easy and constraints are too strict,
preventing meaningful research on black-box adversarial text attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RetrievalSum: A Retrieval Enhanced Framework for Abstractive Summarization. (arXiv:2109.07943v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07943">
<div class="article-summary-box-inner">
<span><p>Existing summarization systems mostly generate summaries purely relying on
the content of the source document. However, even for humans, we usually need
some references or exemplars to help us fully understand the source document
and write summaries in a particular format. But how to find the high-quality
exemplars and incorporate them into summarization systems is still challenging
and worth exploring. In this paper, we propose RetrievalSum, a novel retrieval
enhanced abstractive summarization framework consisting of a dense Retriever
and a Summarizer. At first, several closely related exemplars are retrieved as
supplementary input to help the generation model understand the text more
comprehensively. Furthermore, retrieved exemplars can also play a role in
guiding the model to capture the writing style of a specific corpus. We
validate our method on a wide range of summarization datasets across multiple
domains and two backbone models: BERT and BART. Results show that our framework
obtains significant improvement by 1.38~4.66 in ROUGE-1 score when compared
with the powerful pre-trained models, and achieve new state-of-the-art on
BillSum. Human evaluation demonstrates that our retrieval enhanced model can
better capture the domain-specific writing style.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Attribute Injection for Pretrained Language Models. (arXiv:2109.07953v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07953">
<div class="article-summary-box-inner">
<span><p>Metadata attributes (e.g., user and product IDs from reviews) can be
incorporated as additional inputs to neural-based NLP models, by modifying the
architecture of the models, in order to improve their performance. Recent
models however rely on pretrained language models (PLMs), where previously used
techniques for attribute injection are either nontrivial or ineffective. In
this paper, we propose a lightweight and memory-efficient method to inject
attributes to PLMs. We extend adapters, i.e. tiny plug-in feed-forward modules,
to include attributes both independently of or jointly with the text. To limit
the increase of parameters especially when the attribute vocabulary is large,
we use low-rank approximations and hypercomplex multiplications, significantly
decreasing the total parameters. We also introduce training mechanisms to
handle domains in which attributes can be multi-labeled or sparse. Extensive
experiments and analyses on eight datasets from different domains show that our
method outperforms previous attribute injection methods and achieves
state-of-the-art performance on various datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Unsupervised Question Answering via Summarization-Informed Question Generation. (arXiv:2109.07954v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07954">
<div class="article-summary-box-inner">
<span><p>Question Generation (QG) is the task of generating a plausible question for a
given &lt;passage, answer&gt; pair. Template-based QG uses linguistically-informed
heuristics to transform declarative sentences into interrogatives, whereas
supervised QG uses existing Question Answering (QA) datasets to train a system
to generate a question given a passage and an answer. A disadvantage of the
heuristic approach is that the generated questions are heavily tied to their
declarative counterparts. A disadvantage of the supervised approach is that
they are heavily tied to the domain/language of the QA dataset used as training
data. In order to overcome these shortcomings, we propose an unsupervised QG
method which uses questions generated heuristically from summaries as a source
of training data for a QG system. We make use of freely available news summary
data, transforming declarative summary sentences into appropriate questions
using heuristics informed by dependency parsing, named entity recognition and
semantic role labeling. The resulting questions are then combined with the
original news articles to train an end-to-end neural QG model. We extrinsically
evaluate our approach using unsupervised QA: our QG model is used to generate
synthetic QA pairs for training a QA model. Experimental results show that,
trained with only 20k English Wikipedia-based synthetic QA pairs, the QA model
substantially outperforms previous unsupervised models on three in-domain
datasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain
datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TruthfulQA: Measuring How Models Mimic Human Falsehoods. (arXiv:2109.07958v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07958">
<div class="article-summary-box-inner">
<span><p>We propose a benchmark to measure whether a language model is truthful in
generating answers to questions. The benchmark comprises 817 questions that
span 38 categories, including health, law, finance and politics. We crafted
questions that some humans would answer falsely due to a false belief or
misconception. To perform well, models must avoid generating false answers
learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a
T5-based model. The best model was truthful on 58% of questions, while human
performance was 94%. Models generated many false answers that mimic popular
misconceptions and have the potential to deceive humans. The largest models
were generally the least truthful. For example, the 6B-parameter GPT-J model
was 17% less truthful than its 125M-parameter counterpart. This contrasts with
other NLP tasks, where performance improves with model size. However, this
result is expected if false answers are learned from the training distribution.
We suggest that scaling up models alone is less promising for improving
truthfulness than fine-tuning using training objectives other than imitation of
text from the web.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alquist 4.0: Towards Social Intelligence Using Generative Models and Dialogue Personalization. (arXiv:2109.07968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07968">
<div class="article-summary-box-inner">
<span><p>The open domain-dialogue system Alquist has a goal to conduct a coherent and
engaging conversation that can be considered as one of the benchmarks of social
intelligence. The fourth version of the system, developed within the Alexa
Prize Socialbot Grand Challenge 4, brings two main innovations. The first
addresses coherence, and the second addresses the engagingness of the
conversation. For innovations regarding coherence, we propose a novel hybrid
approach combining hand-designed responses and a generative model. The proposed
approach utilizes hand-designed dialogues, out-of-domain detection, and a
neural response generator. Hand-designed dialogues walk the user through
high-quality conversational flows. The out-of-domain detection recognizes that
the user diverges from the predefined flow and prevents the system from
producing a scripted response that might not make sense for unexpected user
input. Finally, the neural response generator generates a response based on the
context of the dialogue that correctly reacts to the unexpected user input and
returns the dialogue to the boundaries of hand-designed dialogues. The
innovations for engagement that we propose are mostly inspired by the famous
exploration-exploitation dilemma. To conduct an engaging conversation with the
dialogue partners, one has to learn their preferences and interests --
exploration. Moreover, to engage the partner, we have to utilize the knowledge
we have already learned -- exploitation. In this work, we present the
principles and inner workings of individual components of the open-domain
dialogue system Alquist developed within the Alexa Prize Socialbot Grand
Challenge 4 and the experiments we have conducted to evaluate them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Language Models Know the Way to Rome?. (arXiv:2109.07971v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07971">
<div class="article-summary-box-inner">
<span><p>The global geometry of language models is important for a range of
applications, but language model probes tend to evaluate rather local
relations, for which ground truths are easily obtained. In this paper we
exploit the fact that in geography, ground truths are available beyond local
relations. In a series of experiments, we evaluate the extent to which language
model representations of city and country names are isomorphic to real-world
geography, e.g., if you tell a language model where Paris and Berlin are, does
it know the way to Rome? We find that language models generally encode limited
geographic information, but with larger models performing the best, suggesting
that geographic knowledge can be induced from higher-order co-occurrence
statistics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Let the CAT out of the bag: Contrastive Attributed explanations for Text. (arXiv:2109.07983v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07983">
<div class="article-summary-box-inner">
<span><p>Contrastive explanations for understanding the behavior of black box models
has gained a lot of attention recently as they provide potential for recourse.
In this paper, we propose a method Contrastive Attributed explanations for Text
(CAT) which provides contrastive explanations for natural language text data
with a novel twist as we build and exploit attribute classifiers leading to
more semantically meaningful explanations. To ensure that our contrastive
generated text has the fewest possible edits with respect to the original text,
while also being fluent and close to a human generated contrastive, we resort
to a minimal perturbation approach regularized using a BERT language model and
attribute classifiers trained on available attributes. We show through
qualitative examples and a user study that our method not only conveys more
insight because of these attributes, but also leads to better quality
(contrastive) text. Moreover, quantitatively we show that our method is more
efficient than other state-of-the-art methods with it also scoring higher on
benchmark metrics such as flip rate, (normalized) Levenstein distance, fluency
and content preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-aware Entity Typing in Knowledge Graphs. (arXiv:2109.07990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07990">
<div class="article-summary-box-inner">
<span><p>Knowledge graph entity typing aims to infer entities' missing types in
knowledge graphs which is an important but under-explored issue. This paper
proposes a novel method for this task by utilizing entities' contextual
information. Specifically, we design two inference mechanisms: i) N2T:
independently use each neighbor of an entity to infer its type; ii) Agg2T:
aggregate the neighbors of an entity to infer its type. Those mechanisms will
produce multiple inference results, and an exponentially weighted pooling
method is used to generate the final inference result. Furthermore, we propose
a novel loss function to alleviate the false-negative problem during training.
Experiments on two real-world KGs demonstrate the effectiveness of our method.
The source code and data of this paper can be obtained from
https://github.com/CCIIPLab/CET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowMAN: Weakly Supervised Multinomial Adversarial Networks. (arXiv:2109.07994v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07994">
<div class="article-summary-box-inner">
<span><p>The absence of labeled data for training neural models is often addressed by
leveraging knowledge about the specific task, resulting in heuristic but noisy
labels. The knowledge is captured in labeling functions, which detect certain
regularities or patterns in the training samples and annotate corresponding
labels for training. This process of weakly supervised training may result in
an over-reliance on the signals captured by the labeling functions and hinder
models to exploit other signals or to generalize well. We propose KnowMAN, an
adversarial scheme that enables to control influence of signals associated with
specific labeling functions. KnowMAN forces the network to learn
representations that are invariant to those signals and to pick up other
signals that are more generally associated with an output label. KnowMAN
strongly improves results compared to direct weakly supervised learning with a
pre-trained transformer language model and a feature-based baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The NiuTrans System for the WMT21 Efficiency Task. (arXiv:2109.08003v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08003">
<div class="article-summary-box-inner">
<span><p>This paper describes the NiuTrans system for the WMT21 translation efficiency
task (<a href="http://statmt.org/wmt21/efficiency-task.html">this http URL</a>). Following last year's
work, we explore various techniques to improve efficiency while maintaining
translation quality. We investigate the combinations of lightweight Transformer
architectures and knowledge distillation strategies. Also, we improve the
translation efficiency with graph optimization, low precision, dynamic
batching, and parallel pre/post-processing. Our system can translate 247,000
words per second on an NVIDIA A100, being 3$\times$ faster than last year's
system. Our system is the fastest and has the lowest memory consumption on the
GPU-throughput track. The code, model, and pipeline will be available at
NiuTrans.NMT (https://github.com/NiuTrans/NiuTrans.NMT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The NiuTrans System for WNGT 2020 Efficiency Task. (arXiv:2109.08008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08008">
<div class="article-summary-box-inner">
<span><p>This paper describes the submissions of the NiuTrans Team to the WNGT 2020
Efficiency Shared Task. We focus on the efficient implementation of deep
Transformer models \cite{wang-etal-2019-learning, li-etal-2019-niutrans} using
NiuTensor (https://github.com/NiuTrans/NiuTensor), a flexible toolkit for NLP
tasks. We explored the combination of deep encoder and shallow decoder in
Transformer models via model compression and knowledge distillation. The neural
machine translation decoding also benefits from FP16 inference, attention
caching, dynamic batching, and batch pruning. Our systems achieve promising
results in both translation quality and efficiency, e.g., our fastest system
can translate more than 40,000 tokens per second with an RTX 2080 Ti while
maintaining 42.9 BLEU on \textit{newstest2018}. The code, models, and docker
images are available at NiuTrans.NMT
(https://github.com/NiuTrans/NiuTrans.NMT).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Propaganda Techniques in Memes. (arXiv:2109.08013v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08013">
<div class="article-summary-box-inner">
<span><p>Propaganda can be defined as a form of communication that aims to influence
the opinions or the actions of people towards a specific goal; this is achieved
by means of well-defined rhetorical and psychological devices. Propaganda, in
the form we know it today, can be dated back to the beginning of the 17th
century. However, it is with the advent of the Internet and the social media
that it has started to spread on a much larger scale than before, thus becoming
major societal and political issue. Nowadays, a large fraction of propaganda in
social media is multimodal, mixing textual with visual content. With this in
mind, here we propose a new multi-label multimodal task: detecting the type of
propaganda techniques used in memes. We further create and release a new corpus
of 950 memes, carefully annotated with 22 propaganda techniques, which can
appear in the text, in the image, or in both. Our analysis of the corpus shows
that understanding both modalities together is essential for detecting these
techniques. This is further confirmed in our experiments with several
state-of-the-art multimodal models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Concept of Semantic Value in Social Network Analysis: an Application to Comparative Mythology. (arXiv:2109.08023v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08023">
<div class="article-summary-box-inner">
<span><p>Human sciences have traditionally relied on human reasoning and intelligence
to infer knowledge from a wide range of sources, such as oral and written
narrations, reports, and traditions. Here we develop an extension of classical
social network analysis approaches to incorporate the concept of meaning in
each actor, as a mean to quantify and infer further knowledge from the original
source of the network. This extension is based on a new affinity function, the
semantic affinity, that establishes fuzzy-like relationships between the
different actors in the network, using combinations of affinity functions. We
also propose a new heuristic algorithm based on the shortest capacity problem
to compute this affinity function. We use these concept of meaning and semantic
affinity to analyze and compare the gods and heroes from three different
classical mythologies: Greek, Celtic and Nordic. We study the relationships of
each individual mythology and those of common structure that is formed when we
fuse the three of them. We show a strong connection between the Celtic and
Nordic gods and that Greeks put more emphasis on heroic characters rather than
deities. Our approach provides a technique to highlight and quantify important
relationships in the original domain of the network not deducible from its
structural properties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Locating Language-Specific Information in Contextualized Embeddings. (arXiv:2109.08040v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08040">
<div class="article-summary-box-inner">
<span><p>Multilingual pretrained language models (MPLMs) exhibit multilinguality and
are well suited for transfer across languages. Most MPLMs are trained in an
unsupervised fashion and the relationship between their objective and
multilinguality is unclear. More specifically, the question whether MPLM
representations are language-agnostic or they simply interleave well with
learned task prediction heads arises. In this work, we locate language-specific
information in MPLMs and identify its dimensionality and the layers where this
information occurs. We show that language-specific information is scattered
across many dimensions, which can be projected into a linear subspace. Our
study contributes to a better understanding of MPLM representations, going
beyond treating them as unanalyzable blobs of information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Error Type Annotation for Arabic. (arXiv:2109.08068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08068">
<div class="article-summary-box-inner">
<span><p>We present ARETA, an automatic error type annotation system for Modern
Standard Arabic. We design ARETA to address Arabic's morphological richness and
orthographic ambiguity. We base our error taxonomy on the Arabic Learner Corpus
(ALC) Error Tagset with some modifications. ARETA achieves a performance of
85.8% (micro average F1 score) on a manually annotated blind test portion of
ALC. We also demonstrate ARETA's usability by applying it to a number of
submissions from the QALB 2014 shared task for Arabic grammatical error
correction. The resulting analyses give helpful insights on the strengths and
weaknesses of different submissions, which is more useful than the opaque M2
scoring metrics used in the shared task. ARETA employs a large Arabic
morphological analyzer, but is completely unsupervised otherwise. We make ARETA
publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Open Information Extraction using Question Generation and Reading Comprehension. (arXiv:2109.08079v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08079">
<div class="article-summary-box-inner">
<span><p>Typically, Open Information Extraction (OpenIE) focuses on extracting
triples, representing a subject, a relation, and the object of the relation.
However, most of the existing techniques are based on a predefined set of
relations in each domain which limits their applicability to newer domains
where these relations may be unknown such as financial documents. This paper
presents a zero-shot open information extraction technique that extracts the
entities (value) and their descriptions (key) from a sentence, using off the
shelf machine reading comprehension (MRC) Model. The input questions to this
model are created using a novel noun phrase generation method. This method
takes the context of the sentence into account and can create a wide variety of
questions making our technique domain independent. Given the questions and the
sentence, our technique uses the MRC model to extract entities (value). The
noun phrase corresponding to the question, with the highest confidence, is
taken as the description (key).
</p>
<p>This paper also introduces the EDGAR10-Q dataset which is based on publicly
available financial documents from corporations listed in US securities and
exchange commission (SEC). The dataset consists of paragraphs, tagged values
(entities), and their keys (descriptions) and is one of the largest among
entity extraction datasets. This dataset will be a valuable addition to the
research community, especially in the financial domain. Finally, the paper
demonstrates the efficacy of the proposed technique on the EDGAR10-Q and Ade
corpus drug dosage datasets, where it obtained 86.84 % and 97% accuracy,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MeLT: Message-Level Transformer with Masked Document Representations as Pre-Training for Stance Detection. (arXiv:2109.08113v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08113">
<div class="article-summary-box-inner">
<span><p>Much of natural language processing is focused on leveraging large capacity
language models, typically trained over single messages with a task of
predicting one or more tokens. However, modeling human language at
higher-levels of context (i.e., sequences of messages) is under-explored. In
stance detection and other social media tasks where the goal is to predict an
attribute of a message, we have contextual data that is loosely semantically
connected by authorship. Here, we introduce Message-Level Transformer (MeLT) --
a hierarchical message-encoder pre-trained over Twitter and applied to the task
of stance prediction. We focus on stance prediction as a task benefiting from
knowing the context of the message (i.e., the sequence of previous messages).
The model is trained using a variant of masked-language modeling; where instead
of predicting tokens, it seeks to generate an entire masked (aggregated)
message vector via reconstruction loss. We find that applying this pre-trained
masked message-level transformer to the downstream task of stance detection
achieves F1 performance of 67%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey of Online Hate Speech through the Causal Lens. (arXiv:2109.08120v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08120">
<div class="article-summary-box-inner">
<span><p>The societal issue of digital hostility has previously attracted a lot of
attention. The topic counts an ample body of literature, yet remains prominent
and challenging as ever due to its subjective nature. We posit that a better
understanding of this problem will require the use of causal inference
frameworks. This survey summarises the relevant research that revolves around
estimations of causal effects related to online hate speech. Initially, we
provide an argumentation as to why re-establishing the exploration of hate
speech in causal terms is of the essence. Following that, we give an overview
of the leading studies classified with respect to the direction of their
outcomes, as well as an outline of all related research, and a summary of open
research problems that can influence future work on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Tri-training of Dependency Parsers. (arXiv:2109.08122v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08122">
<div class="article-summary-box-inner">
<span><p>We compare two orthogonal semi-supervised learning techniques, namely
tri-training and pretrained word embeddings, in the task of dependency parsing.
We explore language-specific FastText and ELMo embeddings and multilingual BERT
embeddings. We focus on a low resource scenario as semi-supervised learning can
be expected to have the most impact here. Based on treebank size and available
ELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and
Vietnamese. Furthermore, we include English in a simulated low-resource
setting. We find that pretrained word embeddings make more effective use of
unlabelled data than tri-training but that the two approaches can be
successfully combined.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Summary Evaluation Survive Translation to Other Languages?. (arXiv:2109.08129v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08129">
<div class="article-summary-box-inner">
<span><p>The creation of a large summarization quality dataset is a considerable,
expensive, time-consuming effort, requiring careful planning and setup. It
includes producing human-written and machine-generated summaries and evaluation
of the summaries by humans, preferably by linguistic experts, and by automatic
evaluation tools. If such effort is made in one language, it would be
beneficial to be able to use it in other languages. To investigate how much we
can trust the translation of such dataset without repeating human annotations
in another language, we translated an existing English summarization dataset,
SummEval dataset, to four different languages and analyzed the scores from the
automatic evaluation metrics in translated languages, as well as their
correlation with human annotations in the source language. Our results reveal
that although translation changes the absolute value of automatic scores, the
scores keep the same rank order and approximately the same correlations with
human annotations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase Retrieval Learns Passage Retrieval, Too. (arXiv:2109.08133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08133">
<div class="article-summary-box-inner">
<span><p>Dense retrieval methods have shown great promise over sparse retrieval
methods in a range of NLP problems. Among them, dense phrase retrieval-the most
fine-grained retrieval unit-is appealing because phrases can be directly used
as the output for question answering and slot filling tasks. In this work, we
follow the intuition that retrieving phrases naturally entails retrieving
larger text blocks and study whether phrase retrieval can serve as the basis
for coarse-level retrieval including passages and documents. We first observe
that a dense phrase-retrieval system, without any retraining, already achieves
better passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage
retrievers, which also helps achieve superior end-to-end QA performance with
fewer passages. Then, we provide an interpretation for why phrase-level
supervision helps learn better fine-grained entailment compared to
passage-level supervision, and also show that phrase retrieval can be improved
to achieve competitive performance in document-retrieval tasks such as entity
linking and knowledge-grounded dialogue. Finally, we demonstrate how phrase
filtering and vector quantization can reduce the size of our index by 4-10x,
making dense phrase retrieval a practical and versatile solution in
multi-granularity retrieval.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Resources for Turkish Dependency Parsing: Introducing the BOUN Treebank and the BoAT Annotation Tool. (arXiv:2002.10416v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.10416">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the resources that we developed for Turkish
dependency parsing, which include a novel manually annotated treebank (BOUN
Treebank), along with the guidelines we adopted, and a new annotation tool
(BoAT). The manual annotation process we employed was shaped and implemented by
a team of four linguists and five Natural Language Processing (NLP)
specialists. Decisions regarding the annotation of the BOUN Treebank were made
in line with the Universal Dependencies (UD) framework as well as our recent
efforts for unifying the Turkish UD treebanks through manual re-annotation. To
the best of our knowledge, BOUN Treebank is the largest Turkish treebank. It
contains a total of 9,761 sentences from various topics including biographical
texts, national newspapers, instructional texts, popular culture articles, and
essays. In addition, we report the parsing results of a state-of-the-art
dependency parser obtained over the BOUN Treebank as well as two other
treebanks in Turkish. Our results demonstrate that the unification of the
Turkish annotation scheme and the introduction of a more comprehensive treebank
lead to improved performance with regard to dependency parsing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regex Queries over Incomplete Knowledge Bases. (arXiv:2005.00480v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00480">
<div class="article-summary-box-inner">
<span><p>We propose the novel task of answering regular expression queries (containing
disjunction ($\vee$) and Kleene plus ($+$) operators) over incomplete KBs. The
answer set of these queries potentially has a large number of entities, hence
previous works for single-hop queries in KBC that model a query as a point in
high-dimensional space are not as effective. In response, we develop RotatE-Box
-- a novel combination of RotatE and box embeddings. It can model more
relational inference patterns compared to existing embedding based models.
Furthermore, we define baseline approaches for embedding based KBC models to
handle regex operators. We demonstrate performance of RotatE-Box on two new
regex-query datasets introduced in this paper, including one where the queries
are harvested based on actual user query logs. We find that our final
RotatE-Box model significantly outperforms models based on just RotatE and just
box embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incremental Learning for End-to-End Automatic Speech Recognition. (arXiv:2005.04288v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04288">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose an incremental learning method for end-to-end
Automatic Speech Recognition (ASR) which enables an ASR system to perform well
on new tasks while maintaining the performance on its originally learned ones.
To mitigate catastrophic forgetting during incremental learning, we design a
novel explainability-based knowledge distillation for ASR models, which is
combined with a response-based knowledge distillation to maintain the original
model's predictions and the "reason" for the predictions. Our method works
without access to the training data of original tasks, which addresses the
cases where the previous data is no longer available or joint training is
costly. Results on a multi-stage sequential training task show that our method
outperforms existing ones in mitigating forgetting. Furthermore, in two
practical scenarios, compared to the target-reference joint training method,
the performance drop of our method is 0.02% Character Error Rate (CER), which
is 97% smaller than the drops of the baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset. (arXiv:2010.04480v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04480">
<div class="article-summary-box-inner">
<span><p>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality
Estimation (QE) and Automatic Post-Editing (APE). The dataset contains eleven
language pairs, with human labels for up to 10,000 translations per language
pair in the following formats: sentence-level direct assessments and
post-editing effort, and word-level good/bad labels. It also contains the
post-edited sentences, as well as titles of the articles where the sentences
were extracted from, and the neural MT models used to translate the text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enabling Zero-shot Multilingual Spoken Language Translation with Language-Specific Encoders and Decoders. (arXiv:2011.01097v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01097">
<div class="article-summary-box-inner">
<span><p>Current end-to-end approaches to Spoken Language Translation (SLT) rely on
limited training resources, especially for multilingual settings. On the other
hand, Multilingual Neural Machine Translation (MultiNMT) approaches rely on
higher-quality and more massive data sets. Our proposed method extends a
MultiNMT architecture based on language-specific encoders-decoders to the task
of Multilingual SLT (MultiSLT). Our method entirely eliminates the dependency
from MultiSLT data and it is able to translate while training only on ASR and
MultiNMT data.
</p>
<p>Our experiments on four different languages show that coupling the speech
encoder to the MultiNMT architecture produces similar quality translations
compared to a bilingual baseline ($\pm 0.2$ BLEU) while effectively allowing
for zero-shot MultiSLT. Additionally, we propose using an Adapter module for
coupling the speech inputs. This Adapter module produces consistent
improvements up to +6 BLEU points on the proposed architecture and +1 BLEU
point on the end-to-end baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">I Wish I Would Have Loved This One, But I Didn't -- A Multilingual Dataset for Counterfactual Detection in Product Reviews. (arXiv:2104.06893v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06893">
<div class="article-summary-box-inner">
<span><p>Counterfactual statements describe events that did not or cannot take place.
We consider the problem of counterfactual detection (CFD) in product reviews.
For this purpose, we annotate a multilingual CFD dataset from Amazon product
reviews covering counterfactual statements written in English, German, and
Japanese languages. The dataset is unique as it contains counterfactuals in
multiple languages, covers a new application area of e-commerce reviews, and
provides high quality professional annotations. We train CFD models using
different text representation methods and classifiers. We find that these
models are robust against the selectional biases introduced due to cue
phrase-based sentence selection. Moreover, our CFD dataset is compatible with
prior datasets and can be merged to learn accurate CFD models. Applying machine
translation on English counterfactual examples to create multilingual data
performs poorly, demonstrating the language-specificity of this problem, which
has been ignored so far.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TWEAC: Transformer with Extendable QA Agent Classifiers. (arXiv:2104.07081v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07081">
<div class="article-summary-box-inner">
<span><p>Question answering systems should help users to access knowledge on a broad
range of topics and to answer a wide array of different questions. Most systems
fall short of this expectation as they are only specialized in one particular
setting, e.g., answering factual questions with Wikipedia data. To overcome
this limitation, we propose composing multiple QA agents within a meta-QA
system. We argue that there exist a wide range of specialized QA agents in
literature. Thus, we address the central research question of how to
effectively and efficiently identify suitable QA agents for any given question.
We study both supervised and unsupervised approaches to address this challenge,
showing that TWEAC -- Transformer with Extendable Agent Classifiers -- achieves
the best performance overall with 94% accuracy. We provide extensive insights
on the scalability of TWEAC, demonstrating that it scales robustly to over 100
QA agents with each providing just 1000 examples of questions they can answer.
Our code and data is available:
https://github.com/UKPLab/TWEAC-qa-agent-selection
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation. (arXiv:2104.08678v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08678">
<div class="article-summary-box-inner">
<span><p>Despite recent progress, state-of-the-art question answering models remain
vulnerable to a variety of adversarial attacks. While dynamic adversarial data
collection, in which a human annotator tries to write examples that fool a
model-in-the-loop, can improve model robustness, this process is expensive
which limits the scale of the collected data. In this work, we are the first to
use synthetic adversarial data generation to make question answering models
more robust to human adversaries. We develop a data generation pipeline that
selects source passages, identifies candidate answers, generates questions,
then finally filters or re-labels them to improve quality. Using this approach,
we amplify a smaller human-written adversarial dataset to a much larger set of
synthetic question-answer pairs. By incorporating our synthetic data, we
improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve
model generalisation on nine of the twelve MRQA datasets. We further conduct a
novel human-in-the-loop evaluation to show that our models are considerably
more robust to new human-written adversarial examples: crowdworkers can fool
our model only 8.8% of the time on average, compared to 17.6% for a model
trained without synthetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete representations in neural models of spoken language. (arXiv:2105.05582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05582">
<div class="article-summary-box-inner">
<span><p>The distributed and continuous representations used by neural networks are at
odds with representations employed in linguistics, which are typically
symbolic. Vector quantization has been proposed as a way to induce discrete
neural representations that are closer in nature to their linguistic
counterparts. However, it is not clear which metrics are the best-suited to
analyze such discrete representations. We compare the merits of four commonly
used metrics in the context of weakly supervised models of spoken language. We
compare the results they show when applied to two different models, while
systematically studying the effect of the placement and size of the
discretization layer. We find that different evaluation regimes can give
inconsistent results. While we can attribute them to the properties of the
different metrics in most cases, one point of concern remains: the use of
minimal pairs of phoneme triples as stimuli disadvantages larger discrete unit
inventories, unlike metrics applied to complete utterances. Furthermore, while
in general vector quantization induces representations that correlate with
units posited in linguistics, the strength of this correlation is only
moderate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Divided We Rule: Influencer Polarization on Twitter During Political Crises in India. (arXiv:2105.08361v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08361">
<div class="article-summary-box-inner">
<span><p>Influencers are key to the nature and networks of information propagation on
social media. Influencers are particularly important in political discourse
through their engagement with issues, and may derive their legitimacy either
solely or in large part through online operation, or have an offline sphere of
expertise such as entertainers, journalists etc. To quantify influencers'
political engagement and polarity, we use Google's Universal Sentence Encoder
(USE) to encode the tweets of 6k influencers and 26k Indian politicians during
political crises in India. We then obtain aggregate vector representations of
the influencers based on their tweet embeddings, which alongside retweet graphs
help compute their stance and polarity with respect to these political issues.
We find that influencers engage with the topics in a partisan manner, with
polarized influencers being rewarded with increased retweeting and following.
Moreover, we observe that specific groups of influencers are consistently
polarized across all events. We conclude by discussing how our study provides
insights into the political schisms of present-day India, but also offers a
means to study the role of influencers in exacerbating political polarization
in other contexts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Directed Acyclic Graph Network for Conversational Emotion Recognition. (arXiv:2105.12907v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12907">
<div class="article-summary-box-inner">
<span><p>The modeling of conversational context plays a vital role in emotion
recognition from conversation (ERC). In this paper, we put forward a novel idea
of encoding the utterances with a directed acyclic graph (DAG) to better model
the intrinsic structure within a conversation, and design a directed acyclic
neural network, namely DAG-ERC, to implement this idea. In an attempt to
combine the strengths of conventional graph-based neural models and
recurrence-based neural models, DAG-ERC provides a more intuitive way to model
the information flow between long-distance conversation background and nearby
context. Extensive experiments are conducted on four ERC benchmarks with
state-of-the-art models employed as baselines for comparison. The empirical
results demonstrate the superiority of this new model and confirm the
motivation of the directed acyclic graph architecture for ERC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05707">
<div class="article-summary-box-inner">
<span><p>Fact verification has attracted a lot of attention in the machine learning
and natural language processing communities, as it is one of the key methods
for detecting misinformation. Existing large-scale benchmarks for this task
have focused mostly on textual sources, i.e. unstructured information, and thus
ignored the wealth of information available in structured formats, such as
tables. In this paper we introduce a novel dataset and benchmark, Fact
Extraction and VERification Over Unstructured and Structured information
(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated
with evidence in the form of sentences and/or cells from tables in Wikipedia,
as well as a label indicating whether this evidence supports, refutes, or does
not provide enough information to reach a verdict. Furthermore, we detail our
efforts to track and minimize the biases present in the dataset and could be
exploited by models, e.g. being able to predict the label without using
evidence. Finally, we develop a baseline for verifying claims against text and
tables which predicts both the correct evidence and verdict for 18% of the
claims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-utterance Reranking Models with BERT and Graph Convolutional Networks for Conversational Speech Recognition. (arXiv:2106.06922v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06922">
<div class="article-summary-box-inner">
<span><p>How to effectively incorporate cross-utterance information cues into a neural
language model (LM) has emerged as one of the intriguing issues for automatic
speech recognition (ASR). Existing research efforts on improving
contextualization of an LM typically regard previous utterances as a sequence
of additional input and may fail to capture complex global structural
dependencies among these utterances. In view of this, we in this paper seek to
represent the historical context information of an utterance as
graph-structured data so as to distill cross-utterances, global word
interaction relationships. To this end, we apply a graph convolutional network
(GCN) on the resulting graph to obtain the corresponding GCN embeddings of
historical words. GCN has recently found its versatile applications on
social-network analysis, text summarization, and among others due mainly to its
ability of effectively capturing rich relational information among elements.
However, GCN remains largely underexplored in the context of ASR, especially
for dealing with conversational speech. In addition, we frame ASR N-best
reranking as a prediction problem, leveraging bidirectional encoder
representations from transformers (BERT) as the vehicle to not only seize the
local intrinsic word regularity patterns inherent in a candidate hypothesis but
also incorporate the cross-utterance, historical word interaction cues
distilled by GCN for promoting performance. Extensive experiments conducted on
the AMI benchmark dataset seem to confirm the pragmatic utility of our methods,
in relation to some current top-of-the-line methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07340">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the original BERT (i.e., BASE BERT), researchers
have developed various customized BERT models with improved performance for
specific domains and tasks by exploiting the benefits of transfer learning. Due
to the nature of mathematical texts, which often use domain specific vocabulary
along with equations and math symbols, we posit that the development of a new
BERT model for mathematics would be useful for many mathematical downstream
tasks. In this resource paper, we introduce our multi-institutional effort
(i.e., two learning platforms and three academic institutions in the US) toward
this need: MathBERT, a model created by pre-training the BASE BERT model on a
large mathematical corpus ranging from pre-kindergarten (pre-k), to
high-school, to college graduate level mathematical content. In addition, we
select three general NLP tasks that are often used in mathematics education:
prediction of knowledge component, auto-grading open-ended Q&amp;A, and knowledge
tracing, to demonstrate the superiority of MathBERT over BASE BERT. Our
experiments show that MathBERT outperforms prior best methods by 1.2-22% and
BASE BERT by 2-8% on these tasks. In addition, we build a mathematics specific
vocabulary 'mathVocab' to train with MathBERT. We discover that MathBERT
pre-trained with 'mathVocab' outperforms MathBERT trained with the BASE BERT
vocabulary (i.e., 'origVocab'). MathBERT is currently being adopted at the
participated leaning platforms: Stride, Inc, a commercial educational resource
provider, and ASSISTments.org, a free online educational platform. We release
MathBERT for public usage at: https://github.com/tbs17/MathBERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN. (arXiv:2107.01366v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01366">
<div class="article-summary-box-inner">
<span><p>Despite their practical success, modern seq2seq architectures are unable to
generalize systematically on several SCAN tasks. Hence, it is not clear if
SCAN-style compositional generalization is useful in realistic NLP tasks. In
this work, we study the benefit that such compositionality brings about to
several machine translation tasks. We present several focused modifications of
Transformer that greatly improve generalization capabilities on SCAN and select
one that remains on par with a vanilla Transformer on a standard machine
translation (MT) task. Next, we study its performance in low-resource settings
and on a newly introduced distribution-shifted English-French translation task.
Overall, we find that improvements of a SCAN-capable model do not directly
transfer to the resource-rich MT setup. In contrast, in the low-resource setup,
general modifications lead to an improvement of up to 13.1% BLEU score w.r.t. a
vanilla Transformer. Similarly, an improvement of 14% in an accuracy-based
metric is achieved in the introduced compositional English-French translation
task. This provides experimental evidence that the compositional generalization
assessed in SCAN is particularly useful in resource-starved and domain-shifted
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models. (arXiv:2107.07610v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07610">
<div class="article-summary-box-inner">
<span><p>This paper improves the robustness of the pretrained language model, BERT,
against word substitution-based adversarial attacks by leveraging
self-supervised contrastive learning with adversarial perturbations. One
advantage of our method compared to previous works is that it is capable of
improving model robustness without using any labels. Additionally, we also
create an adversarial attack for word-level adversarial training on BERT. The
attack is efficient, allowing adversarial training for BERT on adversarial
examples generated \textit{on the fly} during training. Experimental results
show that our method improves the robustness of BERT against four different
word substitution-based adversarial attacks. Additionally, combining our method
with adversarial training gives higher robustness than adversarial training
alone. Furthermore, to understand why our method can improve the model
robustness against adversarial attacks, we study vector representations of
clean examples and their corresponding adversarial examples before and after
applying our method. As our method improves model robustness with unlabeled raw
data, it opens up the possibility of using large text datasets to train robust
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00412">
<div class="article-summary-box-inner">
<span><p>In multimodal sentiment analysis (MSA), the performance of a model highly
depends on the quality of synthesized embeddings. These embeddings are
generated from the upstream process called multimodal fusion, which aims to
extract and combine the input unimodal raw data to produce a richer multimodal
representation. Previous work either back-propagates the task loss or
manipulates the geometric property of feature spaces to produce favorable
fusion results, which neglects the preservation of critical task-related
information that flows from input to the fusion results. In this work, we
propose a framework named MultiModal InfoMax (MMIM), which hierarchically
maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)
and between multimodal fusion result and unimodal input in order to maintain
task-related information through multimodal fusion. The framework is jointly
trained with the main task (MSA) to improve the performance of the downstream
MSA task. To address the intractable issue of MI bounds, we further formulate a
set of computationally simple parametric and non-parametric methods to
approximate their truth value. Experimental results on the two widely used
datasets demonstrate the efficacy of our approach. The implementation of this
work is publicly available at
https://github.com/declare-lab/Multimodal-Infomax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Establishing Interlingua in Multilingual Language Models. (arXiv:2109.01207v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01207">
<div class="article-summary-box-inner">
<span><p>Large multilingual language models show remarkable zero-shot cross-lingual
transfer performance on a range of tasks. Follow-up works hypothesized that
these models internally project representations of different languages into a
shared interlingual space. However, they produced contradictory results. In
this paper, we correct the famous prior work claiming that "BERT is not an
Interlingua" and show that with the proper choice of sentence representation
different languages actually do converge to a shared space in such language
models. Furthermore, we demonstrate that this convergence pattern is robust
across four measures of correlation similarity and six mBERT-like models. We
then extend our analysis to 28 diverse languages and find that the interlingual
space exhibits a particular structure similar to the linguistic relatedness of
languages. We also highlight a few outlier languages that seem to fail to
converge to the shared space. The code for replicating our results is available
at the following URL: https://github.com/maksym-del/interlingua.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03772">
<div class="article-summary-box-inner">
<span><p>Multi-party dialogue machine reading comprehension (MRC) brings tremendous
challenge since it involves multiple speakers at one dialogue, resulting in
intricate speaker information flows and noisy dialogue contexts. To alleviate
such difficulties, previous models focus on how to incorporate these
information using complex graph-based modules and additional manually labeled
data, which is usually rare in real scenarios. In this paper, we design two
labour-free self- and pseudo-self-supervised prediction tasks on speaker and
key-utterance to implicitly model the speaker information flows, and capture
salient clues in a long dialogue. Experimental results on two benchmark
datasets have justified the effectiveness of our method over competitive
baselines and current state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03910">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage large language models (LMs) to perform zero-shot
text style transfer. We present a prompting method that we call augmented
zero-shot learning, which frames style transfer as a sentence rewriting task
and requires only a natural language instruction, without model fine-tuning or
exemplars in the target style. Augmented zero-shot learning is simple and
demonstrates promising results not just on standard style transfer tasks such
as sentiment, but also on arbitrary transformations such as "make this
melodramatic" or "insert a metaphor."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05494">
<div class="article-summary-box-inner">
<span><p>Building an automatic speech recognition (ASR) system from scratch requires a
large amount of annotated speech data, which is difficult to collect in many
languages. However, there are cases where the low-resource language shares a
common acoustic space with a high-resource language having enough annotated
data to build an ASR. In such cases, we show that the domain-independent
acoustic models learned from the high-resource language through unsupervised
domain adaptation (UDA) schemes can enhance the performance of the ASR in the
low-resource language. We use the specific example of Hindi in the source
domain and Sanskrit in the target domain. We explore two architectures: i)
domain adversarial training using gradient reversal layer (GRL) and ii) domain
separation networks (DSN). The GRL and DSN architectures give absolute
improvements of 6.71% and 7.32%, respectively, in word error rate over the
baseline deep neural network model when trained on just 5.5 hours of data in
the target domain. We also show that choosing a proper language (Telugu) in the
source domain can bring further improvement. The results suggest that UDA
schemes can be helpful in the development of ASR systems for low-resource
languages, mitigating the hassle of collecting large amounts of annotated
speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05611">
<div class="article-summary-box-inner">
<span><p>We propose a novel scheme to use the Levenshtein Transformer to perform the
task of word-level quality estimation. A Levenshtein Transformer is a natural
fit for this task: trained to perform decoding in an iterative manner, a
Levenshtein Transformer can learn to post-edit without explicit supervision. To
further minimize the mismatch between the translation task and the word-level
QE task, we propose a two-stage transfer learning procedure on both augmented
data and human post-editing data. We also propose heuristics to construct
reference labels that are compatible with subword-level finetuning and
inference. Results on WMT 2020 QE shared task dataset show that our proposed
method has superior data efficiency under the data-constrained setting and
competitive performance under the unconstrained setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Talking Space: inference from spatial linguistic meanings. (arXiv:2109.06554v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06554">
<div class="article-summary-box-inner">
<span><p>This paper concerns the intersection of natural language and the physical
space around us in which we live, that we observe and/or imagine things within.
Many important features of language have spatial connotations, for example,
many prepositions (like in, next to, after, on, etc.) are fundamentally
spatial. Space is also a key factor of the meanings of many
words/phrases/sentences/text, and space is a, if not the key, context for
referencing (e.g. pointing) and embodiment.
</p>
<p>We propose a mechanism for how space and linguistic structure can be made to
interact in a matching compositional fashion. Examples include Cartesian space,
subway stations, chesspieces on a chess-board, and Penrose's staircase. The
starting point for our construction is the DisCoCat model of compositional
natural language meaning, which we relax to accommodate physical space. We
address the issue of having multiple agents/objects in a space, including the
case that each agent has different capabilities with respect to that space,
e.g., the specific moves each chesspiece can make, or the different velocities
one may be able to reach.
</p>
<p>Once our model is in place, we show how inferences drawing from the structure
of physical space can be made. We also how how linguistic model of space can
interact with other such models related to our senses and/or embodiment, such
as the conceptual spaces of colour, taste and smell, resulting in a rich
compositional model of meaning that is close to human experience and embodiment
in the world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation. (arXiv:2109.07222v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07222">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have shown remarkable results on various NLP
tasks. Nevertheless, due to their bulky size and slow inference speed, it is
hard to deploy them on edge devices. In this paper, we have a critical insight
that improving the feed-forward network (FFN) in BERT has a higher gain than
improving the multi-head attention (MHA) since the computational cost of FFN is
2$\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to
designing efficient FFN as opposed to previous works that pay attention to MHA.
Since FFN comprises a multilayer perceptron (MLP) that is essential in BERT
optimization, we further design a thorough search space towards an advanced MLP
and perform a coarse-to-fine mechanism to search for an efficient BERT
architecture. Moreover, to accelerate searching and enhance model
transferability, we employ a novel warm-up knowledge distillation strategy at
each search stage. Extensive experiments show our searched EfficientBERT is
6.9$\times$ smaller and 4.4$\times$ faster than BERT$\rm_{BASE}$, and has
competitive performances on GLUE and SQuAD Benchmarks. Concretely,
EfficientBERT attains a 77.7 average score on GLUE \emph{test}, 0.7 higher than
MobileBERT$\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0
\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.
The code is released at https://github.com/cheneydon/efficient-bert.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop. (arXiv:1907.11184v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1907.11184">
<div class="article-summary-box-inner">
<span><p>While the role of humans is increasingly recognized in machine learning
community, representation of and interaction with models in current
human-in-the-loop machine learning (HITL-ML) approaches are too low-level and
far-removed from human's conceptual models. We demonstrate HEIDL, a prototype
HITL-ML system that exposes the machine-learned model through high-level,
explainable linguistic expressions formed of predicates representing semantic
structure of text. In HEIDL, human's role is elevated from simply evaluating
model predictions to interpreting and even updating the model logic directly by
enabling interaction with rule predicates themselves. Raising the currency of
interaction to such semantic levels calls for new interaction paradigms between
humans and machines that result in improved productivity for text analytics
model development process. Moreover, by involving humans in the process, the
human-machine co-created models generalize better to unseen data as domain
experts are able to instill their expertise by extrapolating from what has been
learned by automated algorithms from few labelled data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-17 23:09:38.830229317 UTC">2021-09-17 23:09:38 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>