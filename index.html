<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-24T01:30:00Z">09-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-linguistically Consistent Semantic and Syntactic Annotation of Child-directed Speech. (arXiv:2109.10952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10952">
<div class="article-summary-box-inner">
<span><p>While corpora of child speech and child-directed speech (CDS) have enabled
major contributions to the study of child language acquisition, semantic
annotation for such corpora is still scarce and lacks a uniform standard. We
compile two CDS corpora with sentential logical forms, one in English and the
other in Hebrew. In compiling the corpora we employ a methodology that enforces
a cross-linguistically consistent representation, building on recent advances
in dependency representation and semantic parsing. The corpora are based on a
sizable portion of Brown's Adam corpus from CHILDES (about 80% of its
child-directed utterances), and to all child-directed utterances from Berman's
Hebrew CHILDES corpus Hagar.
</p>
<p>We begin by annotating the corpora with the Universal Dependencies (UD)
scheme for syntactic annotation, motivated by its applicability to a wide
variety of domains and languages. We then proceed by applying an automatic
method for transducing sentential logical forms (LFs) from UD structures. The
two representations have complementary strengths: UD structures are
language-neutral and support direct annotation, whereas LFs are neutral as to
the interface between syntax and semantics, and transparently encode semantic
distinctions. We verify the quality of the annotated UD annotation using an
inter-annotator agreement study. We then demonstrate the utility of the
compiled corpora through a longitudinal corpus study of the prevalence of
different syntactic and semantic phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Fact-checking with Human-in-the-Loop. (arXiv:2109.10992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10992">
<div class="article-summary-box-inner">
<span><p>Researchers have been investigating automated solutions for fact-checking in
a variety of fronts. However, current approaches often overlook the fact that
the amount of information released every day is escalating, and a large amount
of them overlap. Intending to accelerate fact-checking, we bridge this gap by
grouping similar messages and summarizing them into aggregated claims.
Specifically, we first clean a set of social media posts (e.g., tweets) and
build a graph of all posts based on their semantics; Then, we perform two
clustering methods to group the messages for further claim summarization. We
evaluate the summaries both quantitatively with ROUGE scores and qualitatively
with human evaluation. We also generate a graph of summaries to verify that
there is no significant overlap among them. The results reduced 28,818 original
messages to 700 summary claims, showing the potential to speed up the
fact-checking process by organizing and selecting representative claims from
massive disorganized and redundant messages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT. (arXiv:2109.11010v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11010">
<div class="article-summary-box-inner">
<span><p>Alzheimers disease is a fatal progressive brain disorder that worsens with
time. It is high time we have inexpensive and quick clinical diagnostic
techniques for early detection and care. In previous studies, various Machine
Learning techniques and Pre-trained Deep Learning models have been used in
conjunction with the extraction of various acoustic and linguistic features.
Our study focuses on three models for the classification task in the ADReSS
(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021
Challenge. We use the well-balanced dataset provided by the ADReSS Challenge
for training and validating our models. Model 1 uses various acoustic features
from the eGeMAPs feature-set, Model 2 uses various linguistic features that we
generated from auto-generated transcripts and Model 3 uses the auto-generated
transcripts directly to extract features using a Pre-trained BERT and TF-IDF.
These models are described in detail in the models section.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Decomposition for Table-based Fact Verification. (arXiv:2109.11020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11020">
<div class="article-summary-box-inner">
<span><p>Fact verification based on structured data is challenging as it requires
models to understand both natural language and symbolic operations performed
over tables. Although pre-trained language models have demonstrated a strong
capability in verifying simple statements, they struggle with complex
statements that involve multiple operations. In this paper, we improve fact
verification by decomposing complex statements into simpler subproblems.
Leveraging the programs synthesized by a weakly supervised semantic parser, we
propose a program-guided approach to constructing a pseudo dataset for
decomposition model training. The subproblems, together with their predicted
answers, serve as the intermediate evidence to enhance our fact verification
model. Experiments show that our proposed approach achieves the new
state-of-the-art performance, an 82.7\% accuracy, on the TabFact benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Poisson Stochastic Beam Search. (arXiv:2109.11034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11034">
<div class="article-summary-box-inner">
<span><p>Beam search is the default decoding strategy for many sequence generation
tasks in NLP. The set of approximate K-best items returned by the algorithm is
a useful summary of the distribution for many applications; however, the
candidates typically exhibit high overlap and may give a highly biased estimate
for expectations under our model. These problems can be addressed by instead
using stochastic decoding strategies. In this work, we propose a new method for
turning beam search into a stochastic process: Conditional Poisson stochastic
beam search. Rather than taking the maximizing set at each iteration, we sample
K candidates without replacement according to the conditional Poisson sampling
design. We view this as a more natural alternative to Kool et. al. 2019's
stochastic beam search (SBS). Furthermore, we show how samples generated under
the CPSBS design can be used to build consistent estimators and sample diverse
sets from sequence models. In our experiments, we observe CPSBS produces lower
variance and more efficient estimators than SBS, even showing improvements in
high entropy settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models. (arXiv:2109.11058v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11058">
<div class="article-summary-box-inner">
<span><p>Prior work has shown that structural supervision helps English language
models learn generalizations about syntactic phenomena such as subject-verb
agreement. However, it remains unclear if such an inductive bias would also
improve language models' ability to learn grammatical dependencies in
typologically different languages. Here we investigate this question in
Mandarin Chinese, which has a logographic, largely syllable-based writing
system; different word order; and sparser morphology than English. We train
LSTMs, Recurrent Neural Network Grammars, Transformer language models, and
Transformer-parameterized generative parsing models on two Mandarin Chinese
datasets of different sizes. We evaluate the models' ability to learn different
aspects of Mandarin grammar that assess syntactic and semantic relationships.
We find suggestive evidence that structural supervision helps with representing
syntactic state across intervening content and improves performance in low-data
settings, suggesting that the benefits of hierarchical inductive biases in
acquiring dependency relationships may extend beyond English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender. (arXiv:2109.11061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11061">
<div class="article-summary-box-inner">
<span><p>Individuals signal aspects of their identity and beliefs through linguistic
choices. Studying these choices in aggregate allows us to examine large-scale
attitude shifts within a population. Here, we develop computational methods to
study word choice within a sociolinguistic lexical variable -- alternate words
used to express the same concept -- in order to test for change in the United
States towards sexuality and gender. We examine two variables: i) referents to
significant others, such as the word "partner" and ii) referents to an
indefinite person, both of which could optionally be marked with gender. The
linguistic choices in each variable allow us to study increased rates of
acceptances of gay marriage and gender equality, respectively. In longitudinal
analyses across Twitter and Reddit over 87M messages, we demonstrate that
attitudes are changing but that these changes are driven by specific
demographics within the United States. Further, in a quasi-causal analysis, we
show that passages of Marriage Equality Acts in different states are drivers of
linguistic change.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Actionable Conversational Quality Indicators for Improving Task-Oriented Dialog Systems. (arXiv:2109.11064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11064">
<div class="article-summary-box-inner">
<span><p>Automatic dialog systems have become a mainstream part of online customer
service. Many such systems are built, maintained, and improved by customer
service specialists, rather than dialog systems engineers and computer
programmers. As conversations between people and machines become commonplace,
it is critical to understand what is working, what is not, and what actions can
be taken to reduce the frequency of inappropriate system responses. These
analyses and recommendations need to be presented in terms that directly
reflect the user experience rather than the internal dialog processing.
</p>
<p>This paper introduces and explains the use of Actionable Conversational
Quality Indicators (ACQIs), which are used both to recognize parts of dialogs
that can be improved, and to recommend how to improve them. This combines
benefits of previous approaches, some of which have focused on producing dialog
quality scoring while others have sought to categorize the types of errors the
dialog system is making.
</p>
<p>We demonstrate the effectiveness of using ACQIs on LivePerson internal dialog
systems used in commercial customer service applications, and on the publicly
available CMU LEGOv2 conversational dataset (Raux et al. 2005). We report on
the annotation and analysis of conversational datasets showing which ACQIs are
important to fix in various situations.
</p>
<p>The annotated datasets are then used to build a predictive model which uses a
turn-based vector embedding of the message texts and achieves an 79% weighted
average f1-measure at the task of finding the correct ACQI for a given
conversation. We predict that if such a model worked perfectly, the range of
potential improvement actions a bot-builder must consider at each turn could be
reduced by an average of 81%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Universal Dense Retrieval for Open-domain Question Answering. (arXiv:2109.11085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11085">
<div class="article-summary-box-inner">
<span><p>In open-domain question answering, a model receives a text question as input
and searches for the correct answer using a large evidence corpus. The
retrieval step is especially difficult as typical evidence corpora have
\textit{millions} of documents, each of which may or may not have the correct
answer to the question. Very recently, dense models have replaced sparse
methods as the de facto retrieval method. Rather than focusing on lexical
overlap to determine similarity, dense methods build an encoding function that
captures semantic similarity by learning from a small collection of
question-answer or question-context pairs. In this paper, we investigate dense
retrieval models in the context of open-domain question answering across
different input distributions. To do this, first we introduce an entity-rich
question answering dataset constructed from Wikidata facts and demonstrate
dense models are unable to generalize to unseen input question distributions.
Second, we perform analyses aimed at better understanding the source of the
problem and propose new training techniques to improve out-of-domain
performance on a wide variety of datasets. We encourage the field to further
investigate the creation of a single, universal dense retrieval model that
generalizes well across all input distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles. (arXiv:2109.11087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11087">
<div class="article-summary-box-inner">
<span><p>A riddle is a question or statement with double or veiled meanings, followed
by an unexpected answer. Solving riddle is a challenging task for both machine
and human, testing the capability of understanding figurative, creative natural
language and reasoning with commonsense knowledge. We introduce BiRdQA, a
bilingual multiple-choice question answering dataset with 6614 English riddles
and 8751 Chinese riddles. For each riddle-answer pair, we provide four
distractors with additional information from Wikipedia. The distractors are
automatically generated at scale with minimal bias. Existing monolingual and
multilingual QA models fail to perform well on our dataset, indicating that
there is a long way to go before machine can beat human on solving tricky
riddles. The dataset has been released to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing. (arXiv:2109.11105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11105">
<div class="article-summary-box-inner">
<span><p>We aim to identify how different components in the KD pipeline affect the
resulting performance and how much the optimal KD pipeline varies across
different datasets/tasks, such as the data augmentation policy, the loss
function, and the intermediate representation for transferring the knowledge
between teacher and student. To tease apart their effects, we propose
Distiller, a meta KD framework that systematically combines a broad range of
techniques across different stages of the KD pipeline, which enables us to
quantify each component's contribution. Within Distiller, we unify commonly
used objectives for distillation of intermediate representations under a
universal mutual information (MI) objective and propose a class of MI-$\alpha$
objective functions with better bias/variance trade-off for estimating the MI
between the teacher and the student. On a diverse set of NLP datasets, the best
Distiller configurations are identified via large-scale hyperparameter
optimization. Our experiments reveal the following: 1) the approach used to
distill the intermediate representations is the most important factor in KD
performance, 2) among different objectives for intermediate distillation,
MI-$\alpha$ performs the best, and 3) data augmentation provides a large boost
for small training datasets or small student networks. Moreover, we find that
different datasets/tasks prefer different KD algorithms, and thus propose a
simple AutoDistiller algorithm that can recommend a good KD pipeline for a new
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Language Model Meta-Pretraining. (arXiv:2109.11129v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11129">
<div class="article-summary-box-inner">
<span><p>The success of pretrained cross-lingual language models relies on two
essential abilities, i.e., generalization ability for learning downstream tasks
in a source language, and cross-lingual transferability for transferring the
task knowledge to other languages. However, current methods jointly learn the
two abilities in a single-phase cross-lingual pretraining process, resulting in
a trade-off between generalization and cross-lingual transfer. In this paper,
we propose cross-lingual language model meta-pretraining, which learns the two
abilities in different training phases. Our method introduces an additional
meta-pretraining phase before cross-lingual pretraining, where the model learns
generalization ability on a large-scale monolingual corpus. Then, the model
focuses on learning cross-lingual transfer on a multilingual corpus.
Experimental results show that our method improves both generalization and
cross-lingual transfer, and produces better-aligned representations across
different languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Online Learning from Human Feedback for Neural Machine Translation. (arXiv:2109.11136v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11136">
<div class="article-summary-box-inner">
<span><p>We study the problem of online learning with human feedback in the
human-in-the-loop machine translation, in which the human translators revise
the machine-generated translations and then the corrected translations are used
to improve the neural machine translation (NMT) system. However, previous
methods require online model updating or additional translation memory networks
to achieve high-quality performance, making them inflexible and inefficient in
practice. In this paper, we propose a novel non-parametric online learning
method without changing the model structure. This approach introduces two
k-nearest-neighbor (KNN) modules: one module memorizes the human feedback,
which is the correct sentences provided by human translators, while the other
balances the usage of the history human feedback and original NMT models
adaptively. Experiments conducted on EMEA and JRC-Acquis benchmarks demonstrate
that our proposed method obtains substantial improvements on translation
accuracy and achieves better adaptation performance with less repeating human
correction operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint speaker diarisation and tracking in switching state-space model. (arXiv:2109.11140v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11140">
<div class="article-summary-box-inner">
<span><p>Speakers may move around while diarisation is being performed. When a
microphone array is used, the instantaneous locations of where the sounds
originated from can be estimated, and previous investigations have shown that
such information can be complementary to speaker embeddings in the diarisation
task. However, these approaches often assume that speakers are fairly
stationary throughout a meeting. This paper relaxes this assumption, by
proposing to explicitly track the movements of speakers while jointly
performing diarisation within a unified model. A state-space model is proposed,
where the hidden state expresses the identity of the current active speaker and
the predicted locations of all speakers. The model is implemented as a particle
filter. Experiments on a Microsoft rich meeting transcription task show that
the proposed joint location tracking and diarisation approach is able to
perform comparably with other methods that use location information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Information Extraction as a Unified Text-to-Triple Translation. (arXiv:2109.11171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11171">
<div class="article-summary-box-inner">
<span><p>We cast a suite of information extraction tasks into a text-to-triple
translation framework. Instead of solving each task relying on task-specific
datasets and models, we formalize the task as a translation between
task-specific input text and output triples. By taking the task-specific input,
we enable a task-agnostic translation by leveraging the latent knowledge that a
pre-trained language model has about the task. We further demonstrate that a
simple pre-training task of predicting which relational information corresponds
to which input text is an effective way to produce task-specific outputs. This
enables the zero-shot transfer of our framework to downstream tasks. We study
the zero-shot performance of this framework on open information extraction
(OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and
factual probe (Google-RE and T-REx). The model transfers non-trivially to most
tasks and is often competitive with a fully supervised method without the need
for any task-specific training. For instance, we significantly outperform the
F1 score of the supervised open information extraction without needing to use
its training set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Curriculum Learning in Unsupervised Neural Machine Translation. (arXiv:2109.11177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11177">
<div class="article-summary-box-inner">
<span><p>Back-translation (BT) has become one of the de facto components in
unsupervised neural machine translation (UNMT), and it explicitly makes UNMT
have translation ability. However, all the pseudo bi-texts generated by BT are
treated equally as clean data during optimization without considering the
quality diversity, leading to slow convergence and limited translation
performance. To address this problem, we propose a curriculum learning method
to gradually utilize pseudo bi-texts based on their quality from multiple
granularities. Specifically, we first apply cross-lingual word embedding to
calculate the potential translation difficulty (quality) for the monolingual
sentences. Then, the sentences are fed into UNMT from easy to hard batch by
batch. Furthermore, considering the quality of sentences/tokens in a particular
batch are also diverse, we further adopt the model itself to calculate the
fine-grained quality scores, which are served as learning factors to balance
the contributions of different parts when computing loss and encourage the UNMT
model to focus on pseudo data with higher quality. Experimental results on WMT
14 En-Fr, WMT 16 En-De, WMT 16 En-Ro, and LDC En-Zh translation tasks
demonstrate that the proposed method achieves consistent improvements with
faster convergence speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Linguistic Knowledge for Abstractive Multi-document Summarization. (arXiv:2109.11199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11199">
<div class="article-summary-box-inner">
<span><p>Within natural language processing tasks, linguistic knowledge can always
serve an important role in assisting the model to learn excel representations
and better guide the natural language generation. In this work, we develop a
neural network based abstractive multi-document summarization (MDS) model which
leverages dependency parsing to capture cross-positional dependencies and
grammatical structures. More concretely, we process the dependency information
into the linguistic-guided attention mechanism and further fuse it with the
multi-head attention for better feature representation. With the help of
linguistic signals, sentence-level relations can be correctly captured, thus
improving MDS performance. Our model has two versions based on Flat-Transformer
and Hierarchical Transformer respectively. Empirical studies on both versions
demonstrate that this simple but effective method outperforms existing works on
the benchmark dataset. Extensive analyses examine different settings and
configurations of the proposed model which provide a good reference to the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fuzzy Generalised Quantifiers for Natural Language in Categorical Compositional Distributional Semantics. (arXiv:2109.11227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11227">
<div class="article-summary-box-inner">
<span><p>Recent work on compositional distributional models shows that bialgebras over
finite dimensional vector spaces can be applied to treat generalised
quantifiers for natural language. That technique requires one to construct the
vector space over powersets, and therefore is computationally costly. In this
paper, we overcome this problem by considering fuzzy versions of quantifiers
along the lines of Zadeh, within the category of many valued relations. We show
that this category is a concrete instantiation of the compositional
distributional model. We show that the semantics obtained in this model is
equivalent to the semantics of the fuzzy quantifiers of Zadeh. As a result, we
are now able to treat fuzzy quantification without requiring a powerset
construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pregroup Grammars, their Syntax and Semantics. (arXiv:2109.11237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11237">
<div class="article-summary-box-inner">
<span><p>Pregroup grammars were developed in 1999 and stayed Lambek's preferred
algebraic model of grammar. The set-theoretic semantics of pregroups, however,
faces an ambiguity problem. In his latest book, Lambek suggests that this
problem might be overcome using finite dimensional vector spaces rather than
sets. What is the right notion of composition in this setting, direct sum or
tensor product of spaces?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21. (arXiv:2109.11247v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11247">
<div class="article-summary-box-inner">
<span><p>This paper describes the Volctrans' submission to the WMT21 news translation
shared task for German-&gt;English translation. We build a parallel (i.e.,
non-autoregressive) translation system using the Glancing Transformer, which
enables fast and accurate parallel decoding in contrast to the currently
prevailing autoregressive models. To the best of our knowledge, this is the
first parallel translation system that can be scaled to such a practical
scenario like WMT competition. More importantly, our parallel translation
system achieves the best BLEU score (35.0) on German-&gt;English translation task,
outperforming all strong autoregressive counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Question Generation Debias Question Answering Models? A Case Study on Question-Context Lexical Overlap. (arXiv:2109.11256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11256">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) models for reading comprehension have been
demonstrated to exploit unintended dataset biases such as question-context
lexical overlap. This hinders QA models from generalizing to under-represented
samples such as questions with low lexical overlap. Question generation (QG), a
method for augmenting QA datasets, can be a solution for such performance
degradation if QG can properly debias QA datasets. However, we discover that
recent neural QG models are biased towards generating questions with high
lexical overlap, which can amplify the dataset bias. Moreover, our analysis
reveals that data augmentation with these QG models frequently impairs the
performance on questions with low lexical overlap, while improving that on
questions with high lexical overlap. To address this problem, we use a synonym
replacement-based approach to augment questions with low lexical overlap. We
demonstrate that the proposed data augmentation approach is simple yet
effective to mitigate the degradation problem with only 70k synthetic examples.
Our data is publicly available at
https://github.com/KazutoshiShinoda/Synonym-Replacement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System. (arXiv:2109.11292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11292">
<div class="article-summary-box-inner">
<span><p>Consistency Identification has obtained remarkable success on open-domain
dialogue, which can be used for preventing inconsistent response generation.
However, in contrast to the rapid development in open-domain dialogue, few
efforts have been made to the task-oriented dialogue direction. In this paper,
we argue that consistency problem is more urgent in task-oriented domain. To
facilitate the research, we introduce CI-ToD, a novel dataset for Consistency
Identification in Task-oriented Dialog system. In addition, we not only
annotate the single label to enable the model to judge whether the system
response is contradictory, but also provide more fine-grained labels (i.e.,
Dialogue History Inconsistency, User Query Inconsistency and Knowledge Base
Inconsistency) to encourage model to know what inconsistent sources lead to it.
Empirical results show that state-of-the-art methods only achieve 51.3%, which
is far behind the human performance of 93.2%, indicating that there is ample
room for improving consistency identification ability. Finally, we conduct
exhaustive experiments and qualitative analysis to comprehend key challenges
and provide guidance for future directions. All datasets and models are
publicly available at \url{https://github.com/yizhen20133868/CI-ToD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Knowledge Distillation for Pre-trained Language Models. (arXiv:2109.11295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11295">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation~(KD) has been proved effective for compressing
large-scale pre-trained language models. However, existing methods conduct KD
statically, e.g., the student model aligns its output distribution to that of a
selected teacher model on the pre-defined training dataset. In this paper, we
explore whether a dynamic knowledge distillation that empowers the student to
adjust the learning procedure according to its competency, regarding the
student performance and learning efficiency. We explore the dynamical
adjustments on three aspects: teacher model adoption, data selection, and KD
objective adaptation. Experimental results show that (1) proper selection of
teacher model can boost the performance of student model; (2) conducting KD
with 10% informative instances achieves comparable performance while greatly
accelerates the training; (3) the student performance can be boosted by
adjusting the supervision contribution of different alignment objective. We
find dynamic knowledge distillation is promising and provide discussions on
potential future directions towards more efficient KD methods. Our code is
available at https://github.com/lancopku/DynamicKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11308">
<div class="article-summary-box-inner">
<span><p>Both generic and domain-specific BERT models are widely used for natural
language processing (NLP) tasks. In this paper we investigate the vulnerability
of BERT models to variation in input data for Named Entity Recognition (NER)
through adversarial attack. Experimental results show that the original as well
as the domain-specific BERT models are highly vulnerable to entity replacement:
They can be fooled in 89.2 to 99.4% of the cases to mislabel previously correct
entities. BERT models are also vulnerable to variation in the entity context
with 20.2 to 45.0% of entities predicted completely wrong and another 29.3 to
53.3% of entities predicted wrong partially. Often a single change is
sufficient to fool the model. BERT models seem most vulnerable to changes in
the local context of entities. Of the two domain-specific BERT models, the
vulnerability of BioBERT is comparable to the original BERT model whereas
SciBERT is even more vulnerable. Our results chart the vulnerabilities of BERT
models for NER and emphasize the importance of further research into uncovering
and reducing these weaknesses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaShoot: A Hebrew Question Answering Dataset. (arXiv:2109.11314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11314">
<div class="article-summary-box-inner">
<span><p>NLP research in Hebrew has largely focused on morphology and syntax, where
rich annotated datasets in the spirit of Universal Dependencies are available.
Semantic datasets, however, are in short supply, hindering crucial advances in
the development of NLP technology in Hebrew. In this work, we present
ParaShoot, the first question answering dataset in modern Hebrew. The dataset
follows the format and crowdsourcing methodology of SQuAD, and contains
approximately 3000 annotated examples, similar to other question-answering
datasets in low-resource languages. We provide the first baseline results using
recently-released BERT-style models for Hebrew, showing that there is
significant room for improvement on this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Argument Strength Estimation. (arXiv:2109.11319v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11319">
<div class="article-summary-box-inner">
<span><p>High-quality arguments are an essential part of decision-making.
Automatically predicting the quality of an argument is a complex task that
recently got much attention in argument mining. However, the annotation effort
for this task is exceptionally high. Therefore, we test uncertainty-based
active learning (AL) methods on two popular argument-strength data sets to
estimate whether sample-efficient learning can be enabled. Our extensive
empirical evaluation shows that uncertainty-based acquisition functions can not
surpass the accuracy reached with the random acquisition on these data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?. (arXiv:2109.11321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11321">
<div class="article-summary-box-inner">
<span><p>Large language models are known to suffer from the hallucination problem in
that they are prone to output statements that are false or inconsistent,
indicating a lack of knowledge. A proposed solution to this is to provide the
model with additional data modalities that complements the knowledge obtained
through text. We investigate the use of visual data to complement the knowledge
of large language models by proposing a method for evaluating visual knowledge
transfer to text for uni- or multimodal language models. The method is based on
two steps, 1) a novel task querying for knowledge of memory colors, i.e.
typical colors of well-known objects, and 2) filtering of model training data
to clearly separate knowledge contributions. Additionally, we introduce a model
architecture that involves a visual imagination step and evaluate it with our
proposed method. We find that our method can successfully be used to measure
visual knowledge transfer capabilities in models and that our novel model
architecture shows promising results for leveraging multimodal knowledge in a
unimodal setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Current State of Finnish NLP. (arXiv:2109.11326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11326">
<div class="article-summary-box-inner">
<span><p>There are a lot of tools and resources available for processing Finnish. In
this paper, we survey recent papers focusing on Finnish NLP related to many
different subcategories of NLP such as parsing, generation, semantics and
speech. NLP research is conducted in many different research groups in Finland,
and it is frequently the case that NLP tools and models resulting from academic
research are made available for others to use on platforms such as Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Pattern- and Fact-based Fake News Detection via Model Preference Learning. (arXiv:2109.11333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11333">
<div class="article-summary-box-inner">
<span><p>To defend against fake news, researchers have developed various methods based
on texts. These methods can be grouped as 1) pattern-based methods, which focus
on shared patterns among fake news posts rather than the claim itself; and 2)
fact-based methods, which retrieve from external sources to verify the claim's
veracity without considering patterns. The two groups of methods, which have
different preferences of textual clues, actually play complementary roles in
detecting fake news. However, few works consider their integration. In this
paper, we study the problem of integrating pattern- and fact-based models into
one framework via modeling their preference differences, i.e., making the
pattern- and fact-based models focus on respective preferred parts in a post
and mitigate interference from non-preferred parts as possible. To this end, we
build a Preference-aware Fake News Detection Framework (Pref-FEND), which
learns the respective preferences of pattern- and fact-based models for joint
detection. We first design a heterogeneous dynamic graph convolutional network
to generate the respective preference maps, and then use these maps to guide
the joint learning of pattern- and fact-based models for final prediction.
Experiments on two real-world datasets show that Pref-FEND effectively captures
model preferences and improves the performance of models based on patterns,
facts, or both.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Second Pandemic? Analysis of Fake News About COVID-19 Vaccines in Qatar. (arXiv:2109.11372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11372">
<div class="article-summary-box-inner">
<span><p>While COVID-19 vaccines are finally becoming widely available, a second
pandemic that revolves around the circulation of anti-vaxxer fake news may
hinder efforts to recover from the first one. With this in mind, we performed
an extensive analysis of Arabic and English tweets about COVID-19 vaccines,
with focus on messages originating from Qatar. We found that Arabic tweets
contain a lot of false information and rumors, while English tweets are mostly
factual. However, English tweets are much more propagandistic than Arabic ones.
In terms of propaganda techniques, about half of the Arabic tweets express
doubt, and 1/5 use loaded language, while English tweets are abundant in loaded
language, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in
terms of framing, Arabic tweets adopt a health and safety perspective, while in
English economic concerns dominate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WRENCH: A Comprehensive Benchmark for Weak Supervision. (arXiv:2109.11377v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11377">
<div class="article-summary-box-inner">
<span><p>Recent \emph{Weak Supervision (WS)} approaches have had widespread success in
easing the bottleneck of labeling training data for machine learning by
synthesizing labels from multiple potentially noisy supervision sources.
However, proper measurement and analysis of these approaches remain a
challenge. First, datasets used in existing works are often private and/or
custom, limiting standardization. Second, WS datasets with the same name and
base data often vary in terms of the labels and weak supervision sources used,
a significant "hidden" source of evaluation variance. Finally, WS studies often
diverge in terms of the evaluation protocol and ablations used. To address
these problems, we introduce a benchmark platform, \benchmark, for a thorough
and standardized evaluation of WS approaches. It consists of 22 varied
real-world datasets for classification and sequence tagging; a range of real,
synthetic, and procedurally-generated weak supervision sources; and a modular,
extensible framework for WS evaluation, including implementations for popular
WS methods. We use \benchmark to conduct extensive comparisons over more than
100 method variants to demonstrate its efficacy as a benchmark platform. The
code is available at \url{https://github.com/JieyuZ2/wrench}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster-based Mention Typing for Named Entity Disambiguation. (arXiv:2109.11389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11389">
<div class="article-summary-box-inner">
<span><p>An entity mention in text such as "Washington" may correspond to many
different named entities such as the city "Washington D.C." or the newspaper
"Washington Post." The goal of named entity disambiguation is to identify the
mentioned named entity correctly among all possible candidates. If the type
(e.g. location or person) of a mentioned entity can be correctly predicted from
the context, it may increase the chance of selecting the right candidate by
assigning low probability to the unlikely ones. This paper proposes
cluster-based mention typing for named entity disambiguation. The aim of
mention typing is to predict the type of a given mention based on its context.
Generally, manually curated type taxonomies such as Wikipedia categories are
used. We introduce cluster-based mention typing, where named entities are
clustered based on their contextual similarities and the cluster ids are
assigned as types. The hyperlinked mentions and their context in Wikipedia are
used in order to obtain these cluster-based types. Then, mention typing models
are trained on these mentions, which have been labeled with their cluster-based
types through distant supervision. At the named entity disambiguation phase,
first the cluster-based types of a given mention are predicted and then, these
types are used as features in a ranking model to select the best entity among
the candidates. We represent entities at multiple contextual levels and obtain
different clusterings (and thus typing models) based on each level. As each
clustering breaks the entity space differently, mention typing based on each
clustering discriminates the mention differently. When predictions from all
typing models are used together, our system achieves better or comparable
results based on randomization tests with respect to the state-of-the-art
levels on four defacto test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Recognition and Classification on Historical Documents: A Survey. (arXiv:2109.11406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11406">
<div class="article-summary-box-inner">
<span><p>After decades of massive digitisation, an unprecedented amount of historical
documents is available in digital format, along with their machine-readable
texts. While this represents a major step forward with respect to preservation
and accessibility, it also opens up new opportunities in terms of content
mining and the next fundamental challenge is to develop appropriate
technologies to efficiently search, retrieve and explore information from this
'big data of the past'. Among semantic indexing opportunities, the recognition
and classification of named entities are in great demand among humanities
scholars. Yet, named entity recognition (NER) systems are heavily challenged
with diverse, historical and noisy inputs. In this survey, we present the array
of challenges posed by historical documents to NER, inventory existing
resources, describe the main approaches deployed so far, and identify key
priorities for future developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Algorithm for Generating Gap-Fill Multiple Choice Questions of an Expert System. (arXiv:2109.11421v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11421">
<div class="article-summary-box-inner">
<span><p>This research is aimed to propose an artificial intelligence algorithm
comprising an ontology-based design, text mining, and natural language
processing for automatically generating gap-fill multiple choice questions
(MCQs). The simulation of this research demonstrated an application of the
algorithm in generating gap-fill MCQs about software testing. The simulation
results revealed that by using 103 online documents as inputs, the algorithm
could automatically produce more than 16 thousand valid gap-fill MCQs covering
a variety of topics in the software testing domain. Finally, in the discussion
section of this paper we suggest how the proposed algorithm should be applied
to produce gap-fill MCQs being collected in a question pool used by a knowledge
expert system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Fact-Checking: A Survey. (arXiv:2109.11427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11427">
<div class="article-summary-box-inner">
<span><p>As online false information continues to grow, automated fact-checking has
gained an increasing amount of attention in recent years. Researchers in the
field of Natural Language Processing (NLP) have contributed to the task by
building fact-checking datasets, devising automated fact-checking pipelines and
proposing NLP methods to further research in the development of different
components. This paper reviews relevant research on automated fact-checking
covering both the claim detection and claim validation components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Corpus and Models for Lemmatisation and POS-tagging of Old French. (arXiv:2109.11442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11442">
<div class="article-summary-box-inner">
<span><p>Old French is a typical example of an under-resourced historic languages,
that furtherly displays animportant amount of linguistic variation. In this
paper, we present the current results of a long going project (2015-...) and
describe how we broached the difficult question of providing lemmatisation
andPOS models for Old French with the help of neural taggers and the
progressive constitution of dedicated corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. (arXiv:2109.11491v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11491">
<div class="article-summary-box-inner">
<span><p>We present a method for exploring regions around individual points in a
contextualized vector space (particularly, BERT space), as a way to investigate
how these regions correspond to word senses. By inducing a contextualized
"pseudoword" as a stand-in for a static embedding in the input layer, and then
performing masked prediction of a word in the sentence, we are able to
investigate the geometry of the BERT-space in a controlled manner around
individual instances. Using our method on a set of carefully constructed
sentences targeting ambiguous English words, we find substantial regularity in
the contextualized space, with regions that correspond to distinct word senses;
but between these regions there are occasionally "sense voids" -- regions that
do not correspond to any intelligible sense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding a Balanced Degree of Automation for Summary Evaluation. (arXiv:2109.11503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11503">
<div class="article-summary-box-inner">
<span><p>Human evaluation for summarization tasks is reliable but brings in issues of
reproducibility and high costs. Automatic metrics are cheap and reproducible
but sometimes poorly correlated with human judgment. In this work, we propose
flexible semiautomatic to automatic summary evaluation metrics, following the
Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the
reusable human-labeled Summary Content Units (SCUs) for reference(s) but
replaces the manual work of judging SCUs' presence in system summaries with a
natural language inference (NLI) model. Fully automatic Lite3Pyramid further
substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via
a semantic role labeling (SRL) model. Finally, we propose in-between metrics,
Lite2.xPyramid, where we use a simple regressor to predict how well the STUs
can simulate SCUs and retain SCUs that are more difficult to simulate, which
provides a smooth transition and balance between automation and manual
evaluation. Comparing to 15 existing metrics, we evaluate human-metric
correlations on 3 existing meta-evaluation datasets and our newly-collected
PyrXSum (with 100/10 XSum examples/systems). It shows that Lite2Pyramid
consistently has the best summary-level correlations; Lite3Pyramid works better
than or comparable to other automatic metrics; Lite2.xPyramid trades off small
correlation drops for larger manual effort reduction, which can reduce costs
for future data collection. Our code and data are publicly available at:
https://github.com/ZhangShiyue/Lite2-3Pyramid
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MARMOT: A Deep Learning Framework for Constructing Multimodal Representations for Vision-and-Language Tasks. (arXiv:2109.11526v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11526">
<div class="article-summary-box-inner">
<span><p>Political activity on social media presents a data-rich window into political
behavior, but the vast amount of data means that almost all content analyses of
social media require a data labeling step. However, most automated machine
classification methods ignore the multimodality of posted content, focusing
either on text or images. State-of-the-art vision-and-language models are
unusable for most political science research: they require all observations to
have both image and text and require computationally expensive pretraining.
This paper proposes a novel vision-and-language framework called multimodal
representations using modality translation (MARMOT). MARMOT presents two
methodological contributions: it can construct representations for observations
missing image or text, and it replaces the computationally expensive
pretraining with modality translation. MARMOT outperforms an ensemble text-only
classifier in 19 of 20 categories in multilabel classifications of tweets
reporting election incidents during the 2016 U.S. general election. Moreover,
MARMOT shows significant improvements over the results of benchmark multimodal
models on the Hateful Memes dataset, improving the best result set by
VisualBERT in terms of accuracy from 0.6473 to 0.6760 and area under the
receiver operating characteristic curve (AUC) from 0.7141 to 0.7530.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06438">
<div class="article-summary-box-inner">
<span><p>Online advertising is an important revenue source for many IT companies. In
the search advertising scenario, advertisement text that meets the need of the
search query would be more attractive to the user. However, the manual creation
of query-variant advertisement texts for massive items is expensive.
Traditional text generation methods tend to focus on the general searching
needs with high frequency while ignoring the diverse personalized searching
needs with low frequency. In this paper, we propose the query-variant
advertisement text generation task that aims to generate candidate
advertisement texts for different web search queries with various needs based
on queries and item keywords. To solve the problem of ignoring low-frequency
needs, we propose a dynamic association mechanism to expand the receptive field
based on external knowledge, which can obtain associated words to be added to
the input. These associated words can serve as bridges to transfer the ability
of the model from the familiar high-frequency words to the unfamiliar
low-frequency words. With association, the model can make use of various
personalized needs in queries and generate query-variant advertisement texts.
Both automatic and human evaluations show that our model can generate more
attractive advertisement text than baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary-Source Proposition-level Alignment: Task, Datasets and Supervised Baseline. (arXiv:2009.00590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.00590">
<div class="article-summary-box-inner">
<span><p>Aligning sentences in a reference summary with their counterparts in source
documents was shown as a useful auxiliary summarization task, notably for
generating training data for salience detection. Despite its assessed utility,
the alignment step was mostly approached with heuristic unsupervised methods,
typically ROUGE-based, and was never independently optimized or evaluated. In
this paper, we propose establishing summary-source alignment as an explicit
task, while introducing two major novelties: (1) applying it at the more
accurate proposition span level, and (2) approaching it as a supervised
classification task. To that end, we created a novel training dataset for
proposition-level alignment, derived automatically from available summarization
evaluation data. In addition, we crowdsourced dev and test datasets, enabling
model development and proper evaluation. Utilizing these data, we present a
supervised proposition alignment baseline model, showing improved
alignment-quality over the unsupervised approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-LTP: An Open-source Neural Language Technology Platform for Chinese. (arXiv:2009.11616v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.11616">
<div class="article-summary-box-inner">
<span><p>We introduce \texttt{N-LTP}, an open-source neural language technology
platform supporting six fundamental Chinese NLP tasks: {lexical analysis}
(Chinese word segmentation, part-of-speech tagging, and named entity
recognition), {syntactic parsing} (dependency parsing), and {semantic parsing}
(semantic dependency parsing and semantic role labeling). Unlike the existing
state-of-the-art toolkits, such as \texttt{Stanza}, that adopt an independent
model for each task, \texttt{N-LTP} adopts the multi-task framework by using a
shared pre-trained model, which has the advantage of capturing the shared
knowledge across relevant Chinese tasks. In addition, a knowledge distillation
method \cite{DBLP:journals/corr/abs-1907-04829} where the single-task model
teaches the multi-task model is further introduced to encourage the multi-task
model to surpass its single-task teacher. Finally, we provide a collection of
easy-to-use APIs and a visualization tool to make users to use and view the
processing results more easily and directly. To the best of our knowledge, this
is the first toolkit to support six Chinese NLP fundamental tasks. Source code,
documentation, and pre-trained models are available at
\url{https://github.com/HIT-SCIR/ltp}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings. (arXiv:2010.11247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11247">
<div class="article-summary-box-inner">
<span><p>Simultaneous translation is vastly different from full-sentence translation,
in the sense that it starts translation before the source sentence ends, with
only a few words delay. However, due to the lack of large-scale, high-quality
simultaneous translation datasets, most such systems are still trained on
conventional full-sentence bitexts. This is far from ideal for the simultaneous
scenario due to the abundance of unnecessary long-distance reorderings in those
bitexts. We propose a novel method that rewrites the target side of existing
full-sentence corpora into simultaneous-style translation. Experiments on
Zh-&gt;En and Ja-&gt;En simultaneous translation show substantial improvements (up to
+2.7 BLEU) with the addition of these generated pseudo-references.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Validating Label Consistency in NER Data Annotation. (arXiv:2101.08698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08698">
<div class="article-summary-box-inner">
<span><p>Data annotation plays a crucial role in ensuring your named entity
recognition (NER) projects are trained with the right information to learn
from. Producing the most accurate labels is a challenge due to the complexity
involved with annotation. Label inconsistency between multiple subsets of data
annotation (e.g., training set and test set, or multiple training subsets) is
an indicator of label mistakes. In this work, we present an empirical method to
explore the relationship between label (in-)consistency and NER model
performance. It can be used to validate the label consistency (or catches the
inconsistency) in multiple sets of NER data annotation. In experiments, our
method identified the label inconsistency of test data in SCIERC and CoNLL03
datasets (with 26.7% and 5.4% label mistakes). It validated the consistency in
the corrected version of both datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking. (arXiv:2104.04466v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04466">
<div class="article-summary-box-inner">
<span><p>Dialogue State Tracking is central to multi-domain task-oriented dialogue
systems, responsible for extracting information from user utterances. We
present a novel hybrid architecture that augments GPT-2 with representations
derived from Graph Attention Networks in such a way to allow causal, sequential
prediction of slot values. The model architecture captures inter-slot
relationships and dependencies across domains that otherwise can be lost in
sequential prediction. We report improvements in state tracking performance in
MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified
sparse training scenario in which DST models are trained only on session-level
annotations but evaluated at the turn level. We further report detailed
analyses to demonstrate the effectiveness of graph models in DST by showing
that the proposed graph modules capture inter-slot dependencies and improve the
predictions of values that are common to multiple domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models. (arXiv:2104.08066v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08066">
<div class="article-summary-box-inner">
<span><p>A method for creating a vision-and-language (V&amp;L) model is to extend a
language model through structural modifications and V&amp;L pre-training. Such an
extension aims to make a V&amp;L model inherit the capability of natural language
understanding (NLU) from the original language model. To see how well this is
achieved, we propose to evaluate V&amp;L models using an NLU benchmark (GLUE). We
compare five V&amp;L models, including single-stream and dual-stream models,
trained with the same pre-training. Dual-stream models, with their higher
modality independence achieved by approximately doubling the number of
parameters, are expected to preserve the NLU capability better. Our main
finding is that the dual-stream scores are not much different than the
single-stream scores, contrary to expectation. Further analysis shows that
pre-training causes the performance drop in NLU tasks with few exceptions.
These results suggest that adopting a single-stream structure and devising the
pre-training could be an effective method for improving the maintenance of
language knowledge in V&amp;L extensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers: "The End of History" for NLP?. (arXiv:2105.00813v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00813">
<div class="article-summary-box-inner">
<span><p>Recent advances in neural architectures, such as the Transformer, coupled
with the emergence of large-scale pre-trained models such as BERT, have
revolutionized the field of Natural Language Processing (NLP), pushing the
state of the art for a number of NLP tasks. A rich family of variations of
these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but
fundamentally, they all remain limited in their ability to model certain kinds
of information, and they cannot cope with certain information sources, which
was easy for pre-existing models. Thus, here we aim to shed light on some
important theoretical limitations of pre-trained BERT-style models that are
inherent in the general Transformer architecture. First, we demonstrate in
practice on two general types of tasks -- segmentation and segment labeling --
and on four datasets that these limitations are indeed harmful and that
addressing them, even in some very simple and naive ways, can yield sizable
improvements over vanilla RoBERTa and XLNet models. Then, we offer a more
general discussion on desiderata for future additions to the Transformer
architecture that would increase its expressiveness, which we hope could help
in the design of the next generation of deep NLP architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors. (arXiv:2107.01545v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01545">
<div class="article-summary-box-inner">
<span><p>Attractor-based end-to-end diarization is achieving comparable accuracy to
the carefully tuned conventional clustering-based methods on challenging
datasets. However, the main drawback is that it cannot deal with the case where
the number of speakers is larger than the one observed during training. This is
because its speaker counting relies on supervised learning. In this work, we
introduce an unsupervised clustering process embedded in the attractor-based
end-to-end diarization. We first split a sequence of frame-wise embeddings into
short subsequences and then perform attractor-based diarization for each
subsequence. Given subsequence-wise diarization results, inter-subsequence
speaker correspondence is obtained by unsupervised clustering of the vectors
computed from the attractors from all the subsequences. This makes it possible
to produce diarization results of a large number of speakers for the whole
recording even if the number of output speakers for each subsequence is
limited. Experimental results showed that our method could produce accurate
diarization results of an unseen number of speakers. Our method achieved 11.84
%, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets,
respectively, each of which is better than the conventional end-to-end
diarization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09099">
<div class="article-summary-box-inner">
<span><p>Punctuation is critical in understanding natural language text. Currently,
most automatic speech recognition (ASR) systems do not generate punctuation,
which affects the performance of downstream tasks, such as intent detection and
slot filling. This gives rise to the need for punctuation restoration. Recent
work in punctuation restoration heavily utilizes pre-trained language models
without considering data imbalance when predicting punctuation classes. In this
work, we address this problem by proposing a token-level supervised contrastive
learning method that aims at maximizing the distance of representation of
different punctuation marks in the embedding space. The result shows that
training with token-level supervised contrastive learning obtains up to 3.2%
absolute F1 improvement on the test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning Pretrained Transformers into Variational Autoencoders. (arXiv:2108.02446v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02446">
<div class="article-summary-box-inner">
<span><p>Text variational autoencoders (VAEs) are notorious for posterior collapse, a
phenomenon where the model's decoder learns to ignore signals from the encoder.
Because posterior collapse is known to be exacerbated by expressive decoders,
Transformers have seen limited adoption as components of text VAEs. Existing
studies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et
al., 2021) mitigate posterior collapse using massive pretraining, a technique
unavailable to most of the research community without extensive computing
resources. We present a simple two-phase training scheme to convert a
sequence-to-sequence Transformer into a VAE with just finetuning. The resulting
language model is competitive with massively pretrained Transformer-based VAEs
in some internal metrics while falling short on others. To facilitate training
we comprehensively explore the impact of common posterior collapse alleviation
techniques in the literature. We release our code for reproducability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues. (arXiv:2109.00185v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00185">
<div class="article-summary-box-inner">
<span><p>We present an effective system adapted from the end-to-end neural coreference
resolution model, targeting on the task of anaphora resolution in dialogues.
Three aspects are specifically addressed in our approach, including the support
of singletons, encoding speakers and turns throughout dialogue interactions,
and knowledge transfer utilizing existing resources. Despite the simplicity of
our adaptation strategies, they are shown to bring significant impact to the
final performance, with up to 27 F1 improvement over the baseline. Our final
system ranks the 1st place on the leaderboard of the anaphora resolution track
in the CRAC 2021 shared task, and achieves the best evaluation results on all
four datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation. (arXiv:2109.00194v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00194">
<div class="article-summary-box-inner">
<span><p>Recent multilingual pre-trained language models have achieved remarkable
zero-shot performance, where the model is only finetuned on one source language
and directly evaluated on target languages. In this work, we propose a
self-learning framework that further utilizes unlabeled data of target
languages, combined with uncertainty estimation in the process to select
high-quality silver labels. Three different uncertainties are adapted and
analyzed specifically for the cross lingual transfer: Language
Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty
(EVI). We evaluate our framework with uncertainties on two cross-lingual tasks
including Named Entity Recognition (NER) and Natural Language Inference (NLI)
covering 40 languages in total, which outperforms the baselines significantly
by 10 F1 on average for NER and 2.5 accuracy score for NLI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Bias in NLP -- Application to Hate Speech Classification. (arXiv:2109.09725v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09725">
<div class="article-summary-box-inner">
<span><p>This document sums up our results forthe NLP lecture at ETH in the spring
semester 2021. In this work, a BERT based neural network model (Devlin et
al.,2018) is applied to the JIGSAW dataset (Jigsaw/Conversation AI, 2019) in
order to create a model identifying hateful and toxic comments (strictly
seperated from offensive language) in online social platforms (English
language), inthis case Twitter. Three other neural network architectures and a
GPT-2 (Radfordet al., 2019) model are also applied on the provided data set in
order to compare these different models. The trained BERT model is then applied
on two different data sets to evaluate its generalisation power, namely on
another Twitter data set (Tom Davidson, 2017) (Davidsonet al., 2017) and the
data set HASOC 2019 (Thomas Mandl, 2019) (Mandl et al.,2019) which includes
Twitter and also Facebook comments; we focus on the English HASOC 2019 data. In
addition, it can be shown that by fine-tuning the trained BERT model on these
two datasets by applying different transfer learning scenarios via retraining
partial or all layers the predictive scores improve compared to simply applying
the model pre-trained on the JIGSAW data set. Withour results, we get
precisions from 64% to around 90% while still achieving acceptable recall
values of at least lower 60s%, proving that BERT is suitable for real usecases
in social platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning. (arXiv:2109.10510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10510">
<div class="article-summary-box-inner">
<span><p>Despite the success of neural dialogue systems in achieving high performance
on the leader-board, they cannot meet users' requirements in practice, due to
their poor reasoning skills. The underlying reason is that most neural dialogue
models only capture the syntactic and semantic information, but fail to model
the logical consistency between the dialogue history and the generated
response. Recently, a new multi-turn dialogue reasoning task has been proposed,
to facilitate dialogue reasoning research. However, this task is challenging,
because there are only slight differences between the illogical response and
the dialogue history. How to effectively solve this challenge is still worth
exploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle
this problem. Inspired by human's behavior in reading comprehension, a
comparison mechanism is proposed to focus on the fine-grained differences in
the representation of each response candidate. Specifically, each candidate
representation is compared with the whole history to obtain a history
consistency representation. Furthermore, the consistency signals between each
candidate and the speaker's own history are considered to drive a model to
prefer a candidate that is logically consistent with the speaker's history
logic. Finally, the above consistency representations are employed to output a
ranking list of the candidate responses for multi-turn dialogue reasoning.
Experimental results on two public dialogue datasets show that our method
obtains higher ranking scores than the baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing. (arXiv:2109.10847v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10847">
<div class="article-summary-box-inner">
<span><p>Recent progress in the Natural Language Processing domain has given us
several State-of-the-Art (SOTA) pretrained models which can be finetuned for
specific tasks. These large models with billions of parameters trained on
numerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In
this paper, we discuss the need for a benchmark for cost and time effective
smaller models trained on a single GPU. This will enable researchers with
resource constraints experiment with novel and innovative ideas on
tokenization, pretraining tasks, architecture, fine tuning methods etc. We set
up Small-Bench NLP, a benchmark for small efficient neural language models
trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks
on the publicly available GLUE datasets and a leaderboard to track the progress
of the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture
achieves an average score of 81.53 which is comparable to that of BERT-Base's
82.20 (110M parameters). Our models, code and leaderboard are available at
https://github.com/smallbenchnlp
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-24 23:09:20.367565526 UTC">2021-09-24 23:09:20 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>