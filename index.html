<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-20T01:30:00Z">08-20</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks. (arXiv:2108.08375v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08375">
<div class="article-summary-box-inner">
<span><p>This paper studies the relative importance of attention heads in
Transformer-based models to aid their interpretability in cross-lingual and
multi-lingual tasks. Prior research has found that only a few attention heads
are important in each mono-lingual Natural Language Processing (NLP) task and
pruning the remaining heads leads to comparable or improved performance of the
model. However, the impact of pruning attention heads is not yet clear in
cross-lingual and multi-lingual tasks. Through extensive experiments, we show
that (1) pruning a number of attention heads in a multi-lingual
Transformer-based model has, in general, positive effects on its performance in
cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned
can be ranked using gradients and identified with a few trial experiments. Our
experiments focus on sequence labeling tasks, with potential applicability on
other cross-lingual and multi-lingual tasks. For comprehensiveness, we examine
two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and
XLM-R, on three tasks across 9 languages each. We also discuss the validity of
our findings and their extensibility to truly resource-scarce languages and
other task settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Dialog History into End-to-End Spoken Language Understanding Systems. (arXiv:2108.08405v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08405">
<div class="article-summary-box-inner">
<span><p>End-to-end spoken language understanding (SLU) systems that process
human-human or human-computer interactions are often context independent and
process each turn of a conversation independently. Spoken conversations on the
other hand, are very much context dependent, and dialog history contains useful
information that can improve the processing of each conversational turn. In
this paper, we investigate the importance of dialog history and how it can be
effectively integrated into end-to-end SLU systems. While processing a spoken
utterance, our proposed RNN transducer (RNN-T) based SLU model has access to
its dialog history in the form of decoded transcripts and SLU labels of
previous turns. We encode the dialog history as BERT embeddings, and use them
as an additional input to the SLU model along with the speech features for the
current utterance. We evaluate our approach on a recently released spoken
dialog data set, the HarperValleyBank corpus. We observe significant
improvements: 8% for dialog action and 30% for caller intent recognition tasks,
in comparison to a competitive context independent end-to-end baseline system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FeelsGoodMan: Inferring Semantics of Twitch Neologisms. (arXiv:2108.08411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08411">
<div class="article-summary-box-inner">
<span><p>Twitch chats pose a unique problem in natural language understanding due to a
large presence of neologisms, specifically emotes. There are a total of 8.06
million emotes, over 400k of which were used in the week studied. There is
virtually no information on the meaning or sentiment of emotes, and with a
constant influx of new emotes and drift in their frequencies, it becomes
impossible to maintain an updated manually-labeled dataset. Our paper makes a
two fold contribution. First we establish a new baseline for sentiment analysis
on Twitch data, outperforming the previous supervised benchmark by 7.9% points.
Secondly, we introduce a simple but powerful unsupervised framework based on
word embeddings and k-NN to enrich existing models with out-of-vocabulary
knowledge. This framework allows us to auto-generate a pseudo-dictionary of
emotes and we show that we can nearly match the supervised benchmark above even
when injecting such emote knowledge into sentiment classifiers trained on
extraneous datasets such as movie reviews or Twitter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation. (arXiv:2108.08447v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08447">
<div class="article-summary-box-inner">
<span><p>Conditional masked language models (CMLM) have shown impressive progress in
non-autoregressive machine translation (NAT). They learn the conditional
translation model by predicting the random masked subset in the target
sentence. Based on the CMLM framework, we introduce Multi-view Subset
Regularization (MvSR), a novel regularization method to improve the performance
of the NAT model. Specifically, MvSR consists of two parts: (1) \textit{shared
mask consistency}: we forward the same target with different mask strategies,
and encourage the predictions of shared mask positions to be consistent with
each other. (2) \textit{model consistency}, we maintain an exponential moving
average of the model weights, and enforce the predictions to be consistent
between the average model and the online model. Without changing the CMLM-based
architecture, our approach achieves remarkable performance on three public
benchmarks with 0.36-1.14 BLEU gains over previous NAT models. Moreover,
compared with the stronger Transformer baseline, we reduce the gap to 0.01-0.44
BLEU scores on small datasets (WMT16 RO$\leftrightarrow$EN and IWSLT
DE$\rightarrow$EN).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting Slot Values and Contexts for Spoken Language Understanding with Pretrained Models. (arXiv:2108.08451v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08451">
<div class="article-summary-box-inner">
<span><p>Spoken Language Understanding (SLU) is one essential step in building a
dialogue system. Due to the expensive cost of obtaining the labeled data, SLU
suffers from the data scarcity problem. Therefore, in this paper, we focus on
data augmentation for slot filling task in SLU. To achieve that, we aim at
generating more diverse data based on existing data. Specifically, we try to
exploit the latent language knowledge from pretrained language models by
finetuning them. We propose two strategies for finetuning process: value-based
and context-based augmentation. Experimental results on two public SLU datasets
have shown that compared with existing data augmentation methods, our proposed
method can generate more diverse sentences and significantly improve the
performance on SLU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08468">
<div class="article-summary-box-inner">
<span><p>We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student's performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Model Augmented Relevance Score. (arXiv:2108.08485v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08485">
<div class="article-summary-box-inner">
<span><p>Although automated metrics are commonly used to evaluate NLG systems, they
often correlate poorly with human judgements. Newer metrics such as BERTScore
have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which
rely on n-gram matching. These newer methods, however, are still limited in
that they do not consider the generation context, so they cannot properly
reward generated text that is correct but deviates from the given reference.
</p>
<p>In this paper, we propose Language Model Augmented Relevance Score (MARS), a
new context-aware metric for NLG evaluation. MARS leverages off-the-shelf
language models, guided by reinforcement learning, to create augmented
references that consider both the generation context and available human
references, which are then used as additional references to score generated
text. Compared with seven existing metrics in three common NLG tasks, MARS not
only achieves higher correlation with human reference judgements, but also
differentiates well-formed candidates from adversarial samples to a larger
degree.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021. (arXiv:2108.08556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08556">
<div class="article-summary-box-inner">
<span><p>This paper reports the Machine Translation (MT) systems submitted by the
IIITT team for the English-&gt;Marathi and English-&gt;Irish language pairs LoResMT
2021 shared task. The task focuses on getting exceptional translations for
rather low-resourced languages like Irish and Marathi. We fine-tune IndicTrans,
a pretrained multilingual NMT model for English-&gt;Marathi, using external
parallel corpus as input for additional training. We have used a pretrained
Helsinki-NLP Opus MT English-&gt;Irish model for the latter language pair. Our
approaches yield relatively promising results on the BLEU metrics. Under the
team name IIITT, our systems ranked 1, 1, and 2 in English-&gt;Marathi,
Irish-&gt;English, and English-&gt;Irish, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Contextualization using Top-k Operators for Question Answering over Knowledge Graphs. (arXiv:2108.08597v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08597">
<div class="article-summary-box-inner">
<span><p>Answering complex questions over knowledge bases (KB-QA) faces huge input
data with billions of facts, involving millions of entities and thousands of
predicates. For efficiency, QA systems first reduce the answer search space by
identifying a set of facts that is likely to contain all answers and relevant
cues. The most common technique is to apply named entity disambiguation (NED)
systems to the question, and retrieve KB facts for the disambiguated entities.
This work presents ECQA, an efficient method that prunes irrelevant parts of
the search space using KB-aware signals. ECQA is based on top-k query
processing over score-ordered lists of KB items that combine signals about
lexical matching, relevance to the question, coherence among candidate items,
and connectivity in the KB graph. Experiments with two recent QA benchmarks
demonstrate the superiority of ECQA over state-of-the-art baselines with
respect to answer presence, size of the search space, and runtimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08614">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good systems providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, systems from the IR
and NLP communities have addressed QA over text, but barely utilize semantic
data and knowledge. This paper presents the first QA system that can seamlessly
operate over RDF datasets and text corpora, or both together, in a unified
framework. Our method, called UNIQORN, builds a context graph on the fly, by
retrieving question-relevant triples from the RDF data and/or the text corpus,
where the latter case is handled by automatic information extraction. The
resulting graph is typically rich but highly noisy. UNIQORN copes with this
input by advanced graph algorithms for Group Steiner Trees, that identify the
best answer candidates in the context graph. Experimental results on several
benchmarks of complex questions with multiple entities and relations, show that
UNIQORN, an unsupervised method with only five parameters, produces results
comparable to the state-of-the-art on KGs, text corpora, and heterogeneous
sources. The graph-based methodology provides user-interpretable evidence for
the complete answering process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08676">
<div class="article-summary-box-inner">
<span><p>Existing system dealing with online complaint provides a final decision
without explanations. We propose to analyse the complaint text of internet
fraud in a fine-grained manner. Considering the complaint text includes
multiple clauses with various functions, we propose to identify the role of
each clause and classify them into different types of fraud element. We
construct a large labeled dataset originated from a real finance service
platform. We build an element identification model on top of BERT and propose
additional two modules to utilize the context of complaint text for better
element label classification, namely, global context encoder and label refiner.
Experimental results show the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Legislative Recipe: Syntax for Machine-Readable Legislation. (arXiv:2108.08678v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08678">
<div class="article-summary-box-inner">
<span><p>Legal interpretation is a linguistic venture. In judicial opinions, for
example, courts are often asked to interpret the text of statutes and
legislation. As time has shown, this is not always as easy as it sounds.
Matters can hinge on vague or inconsistent language and, under the surface,
human biases can impact the decision-making of judges. This raises an important
question: what if there was a method of extracting the meaning of statutes
consistently? That is, what if it were possible to use machines to encode
legislation in a mathematically precise form that would permit clearer
responses to legal questions? This article attempts to unpack the notion of
machine-readability, providing an overview of both its historical and recent
developments. The paper will reflect on logic syntax and symbolic language to
assess the capacity and limits of representing legal knowledge. In doing so,
the paper seeks to move beyond existing literature to discuss the implications
of various approaches to machine-readable legislation. Importantly, this study
hopes to highlight the challenges encountered in this burgeoning ecosystem of
machine-readable legislation against existing human-readable counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Language-Image Pre-training for the Italian Language. (arXiv:2108.08688v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08688">
<div class="article-summary-box-inner">
<span><p>CLIP (Contrastive Language-Image Pre-training) is a very recent multi-modal
model that jointly learns representations of images and texts. The model is
trained on a massive amount of English data and shows impressive performance on
zero-shot classification tasks. Training the same model on a different language
is not trivial, since data in other languages might be not enough and the model
needs high-quality translations of the texts to guarantee a good performance.
In this paper, we present the first CLIP model for the Italian Language
(CLIP-Italian), trained on more than 1.4 million image-text pairs. Results show
that CLIP-Italian outperforms the multilingual CLIP model on the tasks of image
retrieval and zero-shot classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Czech News Dataset for Semanic Textual Similarity. (arXiv:2108.08708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08708">
<div class="article-summary-box-inner">
<span><p>This paper describes a novel dataset consisting of sentences with semantic
similarity annotations. The data originate from the journalistic domain in the
Czech language. We describe the process of collecting and annotating the data
in detail. The dataset contains 138,556 human annotations divided into train
and test sets. In total, 485 journalism students participated in the creation
process. To increase the reliability of the test set, we compute the annotation
as an average of 9 individual annotations. We evaluate the quality of the
dataset by measuring inter and intra annotation annotators' agreements. Beside
agreement numbers, we provide detailed statistics of the collected dataset. We
conclude our paper with a baseline experiment of building a system for
predicting the semantic similarity of sentences. Due to the massive number of
training annotations (116 956), the model can perform significantly better than
an average annotator (0,92 versus 0,86 of Person's correlation coefficients).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DESYR: Definition and Syntactic Representation Based Claim Detection on the Web. (arXiv:2108.08759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08759">
<div class="article-summary-box-inner">
<span><p>The formulation of a claim rests at the core of argument mining. To demarcate
between a claim and a non-claim is arduous for both humans and machines, owing
to latent linguistic variance between the two and the inadequacy of extensive
definition-based formalization. Furthermore, the increase in the usage of
online social media has resulted in an explosion of unsolicited information on
the web presented as informal text. To account for the aforementioned, in this
paper, we proposed DESYR. It is a framework that intends on annulling the said
issues for informal web-based text by leveraging a combination of hierarchical
representation learning (dependency-inspired Poincare embedding),
definition-based alignment, and feature projection. We do away with fine-tuning
computer-heavy language models in favor of fabricating a more domain-centric
but lighter approach. Experimental results indicate that DESYR builds upon the
state-of-the-art system across four benchmark claim datasets, most of which
were constructed with informal texts. We see an increase of 3 claim-F1 points
on the LESA-Twitter dataset, an increase of 1 claim-F1 point and 9 macro-F1
points on the Online Comments(OC) dataset, an increase of 24 claim-F1 points
and 17 macro-F1 points on the Web Discourse(WD) dataset, and an increase of 8
claim-F1 points and 5 macro-F1 points on the Micro Texts(MT) dataset. We also
perform an extensive analysis of the results. We make a 100-D pre-trained
version of our Poincare-variant along with the source code.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval. (arXiv:2108.08787v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08787">
<div class="article-summary-box-inner">
<span><p>We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual
retrieval in eleven typologically diverse languages, designed to evaluate
ranking with learned dense representations. The goal of this resource is to
spur research in dense retrieval techniques in non-English languages, motivated
by recent observations that existing techniques for representation learning
perform poorly when applied to out-of-distribution data. As a starting point,
we provide zero-shot baselines for this new dataset based on a multi-lingual
adaptation of DPR that we call "mDPR". Experiments show that although the
effectiveness of mDPR is much lower than BM25, dense representations
nevertheless appear to provide valuable relevance signals, improving BM25
results in sparse-dense hybrids. In addition to analyses of our results, we
also discuss future challenges and present a research agenda in multi-lingual
dense retrieval. Mr. TyDi can be downloaded at
https://github.com/castorini/mr.tydi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations. (arXiv:2004.03744v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03744">
<div class="article-summary-box-inner">
<span><p>The recently proposed SNLI-VE corpus for recognising visual-textual
entailment is a large, real-world dataset for fine-grained multimodal
reasoning. However, the automatic way in which SNLI-VE has been assembled (via
combining parts of two related datasets) gives rise to a large number of errors
in the labels of this corpus. In this paper, we first present a data collection
effort to correct the class with the highest error rate in SNLI-VE. Secondly,
we re-evaluate an existing model on the corrected corpus, which we call
SNLI-VE-2.0, and provide a quantitative comparison with its performance on the
non-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends
human-written natural language explanations to SNLI-VE-2.0. Finally, we train
models that learn from these explanations at training time, and output such
explanations at testing time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pretrained Transformers for Text Ranking: BERT and Beyond. (arXiv:2010.06467v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06467">
<div class="article-summary-box-inner">
<span><p>The goal of text ranking is to generate an ordered list of texts retrieved
from a corpus in response to a query. Although the most common formulation of
text ranking is search, instances of the task can also be found in many natural
language processing applications. This survey provides an overview of text
ranking with neural network architectures known as transformers, of which BERT
is the best-known example. The combination of transformers and self-supervised
pretraining has been responsible for a paradigm shift in natural language
processing (NLP), information retrieval (IR), and beyond. In this survey, we
provide a synthesis of existing work as a single point of entry for
practitioners who wish to gain a better understanding of how to apply
transformers to text ranking problems and researchers who wish to pursue work
in this area. We cover a wide range of modern techniques, grouped into two
high-level categories: transformer models that perform reranking in multi-stage
architectures and dense retrieval techniques that perform ranking directly.
There are two themes that pervade our survey: techniques for handling long
documents, beyond typical sentence-by-sentence processing in NLP, and
techniques for addressing the tradeoff between effectiveness (i.e., result
quality) and efficiency (e.g., query latency, model and index size). Although
transformer architectures and pretraining techniques are recent innovations,
many aspects of how they are applied to text ranking are relatively well
understood and represent mature techniques. However, there remain many open
research questions, and thus in addition to laying out the foundations of
pretrained transformers for text ranking, this survey also attempts to
prognosticate where the field is heading.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning From How Human Correct. (arXiv:2102.00225v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
<div class="article-summary-box-inner">
<span><p>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we relabel the noisy
data in our dataset for our industry application. The experiment result shows
that our method improve the classification accuracy from 91.7% to 92.5%. The
91.7% baseline is based on BERT training on the corrected dataset, which is
hard to surpass.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval. (arXiv:2105.03599v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03599">
<div class="article-summary-box-inner">
<span><p>Recently, the retrieval models based on dense representations have been
gradually applied in the first stage of the document retrieval tasks, showing
better performance than traditional sparse vector space models. To obtain high
efficiency, the basic structure of these models is Bi-encoder in most cases.
However, this simple structure may cause serious information loss during the
encoding of documents since the queries are agnostic. To address this problem,
we design a method to mimic the queries on each of the documents by an
iterative clustering process and represent the documents by multiple pseudo
queries (i.e., the cluster centroids). To boost the retrieval process using
approximate nearest neighbor search library, we also optimize the matching
function with a two-step score calculation procedure. Experimental results on
several popular ranking and QA datasets show that our model can achieve
state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stylized Story Generation with Style-Guided Planning. (arXiv:2105.08625v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08625">
<div class="article-summary-box-inner">
<span><p>Current storytelling systems focus more ongenerating stories with coherent
plots regard-less of the narration style, which is impor-tant for controllable
text generation. There-fore, we propose a new task, stylized story gen-eration,
namely generating stories with speci-fied style given a leading context. To
tacklethe problem, we propose a novel generationmodel that first plans the
stylized keywordsand then generates the whole story with theguidance of the
keywords. Besides, we pro-pose two automatic metrics to evaluate theconsistency
between the generated story andthe specified style. Experiments
demonstratesthat our model can controllably generateemo-tion-driven
orevent-driven stories based onthe ROCStories dataset (Mostafazadeh et
al.,2016). Our study presents insights for stylizedstory generation in further
research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation. (arXiv:2106.04632v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04632">
<div class="article-summary-box-inner">
<span><p>Most existing video-and-language (VidL) research focuses on a single dataset,
or multiple datasets of a single task. In reality, a truly useful VidL system
is expected to be easily generalizable to diverse tasks, domains, and datasets.
To facilitate the evaluation of such systems, we introduce Video-And-Language
Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets
over 3 popular tasks: (i) text-to-video retrieval; (ii) video question
answering; and (iii) video captioning. VALUE benchmark aims to cover a broad
range of video genres, video lengths, data volumes, and task difficulty levels.
Rather than focusing on single-channel videos with visual information only,
VALUE promotes models that leverage information from both video frames and
their associated subtitles, as well as models that share knowledge across
multiple tasks. We evaluate various baseline methods with and without
large-scale VidL pre-training, and systematically investigate the impact of
video input channels, fusion methods, and different video representations. We
also study the transferability between tasks, and conduct multi-task learning
under different settings. The significant gap between our best model and human
performance calls for future study for advanced VidL models. VALUE is available
at https://value-benchmark.github.io/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">More but Correct: Generating Diversified and Entity-revised Medical Response. (arXiv:2108.01266v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01266">
<div class="article-summary-box-inner">
<span><p>Medical Dialogue Generation (MDG) is intended to build a medical dialogue
system for intelligent consultation, which can communicate with patients in
real-time, thereby improving the efficiency of clinical diagnosis with broad
application prospects. This paper presents our proposed framework for the
Chinese MDG organized by the 2021 China conference on knowledge graph and
semantic computing (CCKS) competition, which requires generating
context-consistent and medically meaningful responses conditioned on the
dialogue history. In our framework, we propose a pipeline system composed of
entity prediction and entity-aware dialogue generation, by adding predicted
entities to the dialogue model with a fusion mechanism, thereby utilizing
information from different sources. At the decoding stage, we propose a new
decoding mechanism named Entity-revised Diverse Beam Search (EDBS) to improve
entity correctness and promote the length and quality of the final response.
The proposed method wins both the CCKS and the International Conference on
Learning Representations (ICLR) 2021 Workshop Machine Learning for Preventing
and Combating Pandemics (MLPCP) Track 1 Entity-aware MED competitions, which
demonstrate the practicality and effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Single and Multiple Representations in Dense Passage Retrieval. (arXiv:2108.06279v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06279">
<div class="article-summary-box-inner">
<span><p>The advent of contextualised language models has brought gains in search
effectiveness, not just when applied for re-ranking the output of classical
weighting models such as BM25, but also when used directly for passage indexing
and retrieval, a technique which is called dense retrieval. In the existing
literature in neural ranking, two dense retrieval families have become
apparent: single representation, where entire passages are represented by a
single embedding (usually BERT's [CLS] token, as exemplified by the recent ANCE
approach), or multiple representations, where each token in a passage is
represented by its own embedding (as exemplified by the recent ColBERT
approach). These two families have not been directly compared. However, because
of the likely importance of dense retrieval moving forward, a clear
understanding of their advantages and disadvantages is paramount. To this end,
this paper contributes a direct study on their comparative effectiveness,
noting situations where each method under/over performs w.r.t. each other, and
w.r.t. a BM25 baseline. We observe that, while ANCE is more efficient than
ColBERT in terms of response time and memory usage, multiple representations
are statistically more effective than the single representations for MAP and
MRR@10. We also show that multiple representations obtain better improvements
than single representations for queries that are the hardest for BM25, as well
as for definitional queries, and those with complex information needs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Annotation Guidelines for the Turku Paraphrase Corpus. (arXiv:2108.07499v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07499">
<div class="article-summary-box-inner">
<span><p>This document describes the annotation guidelines used to construct the Turku
Paraphrase Corpus. These guidelines were developed together with the corpus
annotation, revising and extending the guidelines regularly during the
annotation work. Our paraphrase annotation scheme uses the base scale 1-4,
where labels 1 and 2 are used for negative candidates (not paraphrases), while
labels 3 and 4 are paraphrases at least in the given context if not everywhere.
In addition to base labeling, the scheme is enriched with additional
subcategories (flags) for categorizing different types of paraphrases inside
the two positive labels, making the annotation scheme suitable for more
fine-grained paraphrase categorization. The annotation scheme is used to
annotate over 100,000 Finnish paraphrase pairs.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-21 23:08:33.180252513 UTC">2021-08-21 23:08:33 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>