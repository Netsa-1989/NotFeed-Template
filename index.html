<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-12T01:30:00Z">10-12</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DPUV3INT8: A Compiler View to programmable FPGA Inference Engines. (arXiv:2110.04327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04327">
<div class="article-summary-box-inner">
<span><p>We have a FPGA design, we make it fast, efficient, and tested for a few
important examples. Now we must infer a general solution to deploy in the data
center. Here, we describe the FPGA DPUV3INT8 design and our compiler effort.
The hand-tuned SW-HW solution for Resnet50\_v1 has (close to) 2 times better
images per second (throughput) than our best FPGA implementation; the compiler
generalizes the hand written techniques achieving about 1.5 times better
performance for the same example, the compiler generalizes the optimizations to
a model zoo of networks, and it achieves 80+\% HW efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. (arXiv:2110.04330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04330">
<div class="article-summary-box-inner">
<span><p>Current Open-Domain Question Answering (ODQA) model paradigm often contains a
retrieving module and a reading module. Given an input question, the reading
module predicts the answer from the relevant passages which are retrieved by
the retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on
top of the pretrained generative model T5, achieves the state-of-the-art
performance in the reading module. Although being effective, it remains
constrained by inefficient attention on all retrieved passages which contain a
lot of noise. In this work, we propose a novel method KG-FiD, which filters
noisy passages by leveraging the structural relationship among the retrieved
passages with a knowledge graph. We initiate the passage node embedding from
the FiD encoder and then use graph neural network (GNN) to update the
representation for reranking. To improve the efficiency, we build the GNN on
top of the intermediate layer output of the FiD encoder and only pass a few top
reranked passages into the higher layers of encoder and decoder for answer
generation. We also apply the proposed GNN based reranking method to enhance
the passage retrieval results in the retrieving module. Extensive experiments
on common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate
that KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score
and achieve comparable performance with FiD with only 40% of computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Describe Solutions for Bug Reports Based on Developer Discussions. (arXiv:2110.04353v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04353">
<div class="article-summary-box-inner">
<span><p>When a software bug is reported, developers engage in a discussion to
collaboratively resolve it. While the solution is likely formulated within the
discussion, it is often buried in a large amount of text, making it difficult
to comprehend, which delays its implementation. To expedite bug resolution, we
propose generating a concise natural language description of the solution by
synthesizing relevant content within the discussion, which encompasses both
natural language and source code. Furthermore, to support generating an
informative description during an ongoing discussion, we propose a secondary
task of determining when sufficient context about the solution emerges in
real-time. We construct a dataset for these tasks with a novel technique for
obtaining noisy supervision from repository changes linked to bug reports. We
establish baselines for generating solution descriptions, and develop a
classifier which makes a prediction following each new utterance on whether or
not the necessary context for performing generation is available. Through
automated and human evaluation, we find these tasks to form an ideal testbed
for complex reasoning in long, bimodal dialogue context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards a Unified View of Parameter-Efficient Transfer Learning. (arXiv:2110.04366v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04366">
<div class="article-summary-box-inner">
<span><p>Fine-tuning large pre-trained language models on downstream tasks has become
the de-facto learning paradigm in NLP. However, conventional approaches
fine-tune all the parameters of the pre-trained model, which becomes
prohibitive as the model size and the number of tasks grow. Recent work has
proposed a variety of parameter-efficient transfer learning methods that only
fine-tune a small number of (extra) parameters to attain strong performance.
While effective, the critical ingredients for success and the connections among
the various methods are poorly understood. In this paper, we break down the
design of state-of-the-art parameter-efficient transfer learning methods and
present a unified framework that establishes connections between them.
Specifically, we re-frame them as modifications to specific hidden states in
pre-trained models, and define a set of design dimensions along which different
methods vary, such as the function to compute the modification and the position
to apply the modification. Through comprehensive empirical studies across
machine translation, text summarization, language understanding, and text
classification benchmarks, we utilize the unified view to identify important
design choices in previous methods. Furthermore, our unified framework enables
the transfer of design elements across different approaches, and as a result we
are able to instantiate new parameter-efficient fine-tuning methods that tune
less parameters than previous methods while being more effective, achieving
comparable results to fine-tuning all parameters on all four tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Few More Examples May Be Worth Billions of Parameters. (arXiv:2110.04374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04374">
<div class="article-summary-box-inner">
<span><p>We investigate the dynamics of increasing the number of model parameters
versus the number of labeled examples across a wide variety of tasks. Our
exploration reveals that while scaling parameters consistently yields
performance improvements, the contribution of additional examples highly
depends on the task's format. Specifically, in open question answering tasks,
enlarging the training set does not improve performance. In contrast,
classification, extractive question answering, and multiple choice tasks
benefit so much from additional examples that collecting a few hundred examples
is often "worth" billions of parameters. We hypothesize that unlike open
question answering, which involves recalling specific information, solving
strategies for tasks with a more restricted output space transfer across
examples, and can therefore be learned with small amounts of labeled data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Summarization Systems across Gender, Age, and Race. (arXiv:2110.04384v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04384">
<div class="article-summary-box-inner">
<span><p>Summarization systems are ultimately evaluated by human annotators and
raters. Usually, annotators and raters do not reflect the demographics of end
users, but are recruited through student populations or crowdsourcing platforms
with skewed demographics. For two different evaluation scenarios -- evaluation
against gold summaries and system output ratings -- we show that summary
evaluation is sensitive to protected attributes. This can severely bias system
development and evaluation, leading us to build models that cater for some
groups rather than others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Eval4NLP Shared Task on Explainable Quality Estimation: Overview and Results. (arXiv:2110.04392v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04392">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce the Eval4NLP-2021shared task on explainable
quality estimation. Given a source-translation pair, this shared task requires
not only to provide a sentence-level score indicating the overall quality of
the translation, but also to explain this score by identifying the words that
negatively impact translation quality. We present the data, annotation
guidelines and evaluation setup of the shared task, describe the six
participating systems, and analyze the results. To the best of our knowledge,
this is the first shared task on explainable NLP evaluation metrics. Datasets
and results are available at https://github.com/eval4nlp/SharedTask2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors. (arXiv:2110.04399v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04399">
<div class="article-summary-box-inner">
<span><p>Evaluation metrics are a key ingredient for progress of text generation
systems. In recent years, several BERT-based evaluation metrics have been
proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much
better with human assessment of text generation quality than BLEU or ROUGE,
invented two decades ago. However, little is known what these metrics, which
are based on black-box language model representations, actually capture (it is
typically assumed they model semantic similarity). In this work, we \wei{use a
simple regression based global explainability technique to} disentangle metric
scores along linguistic factors, including semantics, syntax, morphology, and
lexical overlap. We show that the different metrics capture all aspects to some
degree, but that they are all substantially sensitive to lexical overlap, just
like BLEU and ROUGE. This exposes limitations of these novelly proposed
metrics, which we also highlight in an adversarial test scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HydraSum -- Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04400">
<div class="article-summary-box-inner">
<span><p>Existing abstractive summarization models lack explicit control mechanisms
that would allow users to influence the stylistic features of the model
outputs. This results in generating generic summaries that do not cater to the
users needs or preferences. To address this issue we introduce HydraSum, a new
summarization architecture that extends the single decoder framework of current
models, e.g. BART, to a mixture-of-experts version consisting of multiple
decoders. Our proposed model encourages each expert, i.e. decoder, to learn and
generate stylistically-distinct summaries along dimensions such as
abstractiveness, length, specificity, and others. At each time step, HydraSum
employs a gating mechanism that decides the contribution of each individual
decoder to the next token's output probability distribution. Through
experiments on three summarization datasets (CNN, Newsroom, XSum), we
demonstrate that this gating mechanism automatically learns to assign
contrasting summary styles to different HydraSum decoders under the standard
training objective without the need for additional supervision. We further show
that a guided version of the training process can explicitly govern which
summary style is partitioned between decoders, e.g. high abstractiveness vs.
low abstractiveness or high specificity vs. low specificity, and also increase
the stylistic-difference between individual decoders. Finally, our experiments
demonstrate that our decoder framework is highly flexible: during inference, we
can sample from individual decoders or mixtures of different subsets of the
decoders to yield a diverse set of summaries and enforce single- and
multi-style control over summary generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content. (arXiv:2110.04406v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04406">
<div class="article-summary-box-inner">
<span><p>Natural language descriptions sometimes accompany visualizations to better
communicate and contextualize their insights, and to improve their
accessibility for readers with disabilities. However, it is difficult to
evaluate the usefulness of these descriptions, and how effectively they improve
access to meaningful information, because we have little understanding of the
semantic content they convey, and how different readers receive this content.
In response, we introduce a conceptual model for the semantic content conveyed
by natural language descriptions of visualizations. Developed through a
grounded theory analysis of 2,147 sentences, our model spans four levels of
semantic content: enumerating visualization construction properties (e.g.,
marks and encodings); reporting statistical concepts and relations (e.g.,
extrema and correlations); identifying perceptual and cognitive phenomena
(e.g., complex trends and patterns); and elucidating domain-specific insights
(e.g., social and political context). To demonstrate how our model can be
applied to evaluate the effectiveness of visualization descriptions, we conduct
a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that
these reader groups differ significantly on which semantic content they rank as
most useful. Together, our model and findings suggest that access to meaningful
information is strongly reader-specific, and that research in automatic
visualization captioning should orient toward descriptions that more richly
communicate overall trends and statistics, sensitive to reader preferences. Our
work further opens a space of research on natural language as a data interface
coequal with visualization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Community Sensitive Norm Violations in Online Conversations. (arXiv:2110.04419v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04419">
<div class="article-summary-box-inner">
<span><p>Online platforms and communities establish their own norms that govern what
behavior is acceptable within the community. Substantial effort in NLP has
focused on identifying unacceptable behaviors and, recently, on forecasting
them before they occur. However, these efforts have largely focused on toxicity
as the sole form of community norm violation. Such focus has overlooked the
much larger set of rules that moderators enforce. Here, we introduce a new
dataset focusing on a more complete spectrum of community norms and their
violations in the local conversational and global community contexts. We
introduce a series of models that use this data to develop context- and
community-sensitive norm violation detection, showing that these changes give
high performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning. (arXiv:2110.04429v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04429">
<div class="article-summary-box-inner">
<span><p>Distantly supervised named entity recognition (DS-NER) efficiently reduces
labor costs but meanwhile intrinsically suffers from the label noise due to the
strong assumption of distant supervision. Typically, the wrongly labeled
instances comprise numbers of incomplete and inaccurate annotation noise, while
most prior denoising works are only concerned with one kind of noise and fail
to fully explore useful information in the whole training set. To address this
issue, we propose a robust learning paradigm named Self-Collaborative Denoising
Learning (SCDL), which jointly trains two teacher-student networks in a
mutually-beneficial manner to iteratively perform noisy label refinery. Each
network is designed to exploit reliable labels via self denoising, and two
networks communicate with each other to explore unreliable annotations by
collaborative denoising. Extensive experimental results on five real-world
datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Two-stage Visual Cues Enhancement Network for Referring Image Segmentation. (arXiv:2110.04435v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04435">
<div class="article-summary-box-inner">
<span><p>Referring Image Segmentation (RIS) aims at segmenting the target object from
an image referred by one given natural language expression. The diverse and
flexible expressions as well as complex visual contents in the images raise the
RIS model with higher demands for investigating fine-grained matching behaviors
between words in expressions and objects presented in images. However, such
matching behaviors are hard to be learned and captured when the visual cues of
referents (i.e. referred objects) are insufficient, as the referents with weak
visual cues tend to be easily confused by cluttered background at boundary or
even overwhelmed by salient objects in the image. And the insufficient visual
cues issue can not be handled by the cross-modal fusion mechanisms as done in
previous work. In this paper, we tackle this problem from a novel perspective
of enhancing the visual information for the referents by devising a Two-stage
Visual cues enhancement Network (TV-Net), where a novel Retrieval and
Enrichment Scheme (RES) and an Adaptive Multi-resolution feature Fusion (AMF)
module are proposed. Through the two-stage enhancement, our proposed TV-Net
enjoys better performances in learning fine-grained matching behaviors between
the natural language expression and image, especially when the visual
information of the referent is inadequate, thus produces better segmentation
results. Extensive experiments are conducted to validate the effectiveness of
the proposed method on the RIS task, with our proposed TV-Net surpassing the
state-of-the-art approaches on four benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Natural Language for Human-Robot Collaboration: Problems Beyond Language Grounding. (arXiv:2110.04441v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04441">
<div class="article-summary-box-inner">
<span><p>To enable robots to instruct humans in collaborations, we identify several
aspects of language processing that are not commonly studied in this context.
These include location, planning, and generation. We suggest evaluations for
each task, offer baselines for simple methods, and close by discussing
challenges and opportunities in studying language for collaboration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging recent advances in Pre-Trained Language Models forEye-Tracking Prediction. (arXiv:2110.04475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04475">
<div class="article-summary-box-inner">
<span><p>Cognitively inspired Natural Language Pro-cessing uses human-derived
behavioral datalike eye-tracking data, which reflect the seman-tic
representations of language in the humanbrain to augment the neural nets to
solve arange of tasks spanning syntax and semanticswith the aim of teaching
machines about lan-guage processing mechanisms. In this paper,we use the ZuCo
1.0 and ZuCo 2.0 dataset con-taining the eye-gaze features to explore
differ-ent linguistic models to directly predict thesegaze features for each
word with respect to itssentence. We tried different neural networkmodels with
the words as inputs to predict thetargets. And after lots of experimentation
andfeature engineering finally devised a novel ar-chitecture consisting of
RoBERTa Token Clas-sifier with a dense layer on top for languagemodeling and a
stand-alone model consistingof dense layers followed by a transformer layerfor
the extra features we engineered. Finally,we took the mean of the outputs of
both thesemodels to make the final predictions. We eval-uated the models using
mean absolute error(MAE) and the R2 score for each target.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bayesian Active Summarization. (arXiv:2110.04480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04480">
<div class="article-summary-box-inner">
<span><p>Bayesian Active Learning has had significant impact to various NLP problems,
but nevertheless it's application to text summarization has been explored very
little. We introduce Bayesian Active Summarization (BAS), as a method of
combining active learning methods with state-of-the-art summarization models.
Our findings suggest that BAS achieves better and more robust performance,
compared to random selection, particularly for small and very small data
annotation budgets. Using BAS we showcase it is possible to leverage large
summarization models to effectively solve real-world problems with very limited
annotated data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis. (arXiv:2110.04482v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04482">
<div class="article-summary-box-inner">
<span><p>This work presents a lifelong learning approach to train a multilingual
Text-To-Speech (TTS) system, where each language was seen as an individual task
and was learned sequentially and continually. It does not require pooled data
from all languages altogether, and thus alleviates the storage and computation
burden. One of the challenges of lifelong learning methods is "catastrophic
forgetting": in TTS scenario it means that model performance quickly degrades
on previous languages when adapted to a new language. We approach this problem
via a data-replay-based lifelong learning method. We formulate the replay
process as a supervised learning problem, and propose a simple yet effective
dual-sampler framework to tackle the heavily language-imbalanced training
samples. Through objective and subjective evaluations, we show that this
supervised learning formulation outperforms other gradient-based and
regularization-based lifelong learning methods, achieving 43% Mel-Cepstral
Distortion reduction compared to a fine-tuning baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav2vec-S: Semi-Supervised Pre-Training for Speech Recognition. (arXiv:2110.04484v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04484">
<div class="article-summary-box-inner">
<span><p>Self-supervised pre-training has dramatically improved the performance of
automatic speech recognition (ASR). However, most existing self-supervised
pre-training approaches are task-agnostic, i.e., could be applied to various
downstream tasks. And there is a gap between the task-agnostic pre-training and
the task-specific downstream fine-tuning, which may degrade the downstream
performance. In this work, we propose a novel pre-training paradigm called
wav2vec-S, where we use task-specific semi-supervised pre-training to bridge
this gap. Specifically, the semi-supervised pre-training is conducted on the
basis of self-supervised pre-training such as wav2vec 2.0. Experiments on ASR
show that compared to wav2vec 2.0, wav2vec-S only requires marginal increment
of pre-training time but could significantly improve ASR performance on
in-domain, cross-domain and cross-lingual datasets. The average relative WER
reductions are 26.3% and 6.3% for 1h and 10h fine-tuning, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control. (arXiv:2110.04486v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04486">
<div class="article-summary-box-inner">
<span><p>Sequence expansion between encoder and decoder is a critical challenge in
sequence-to-sequence TTS. Attention-based methods achieve great naturalness but
suffer from unstable issues like missing and repeating phonemes, not to mention
accurate duration control. Duration-informed methods, on the contrary, seem to
easily adjust phoneme duration but show obvious degradation in speech
naturalness. This paper proposes PAMA-TTS to address the problem. It takes the
advantage of both flexible attention and explicit duration models. Based on the
monotonic attention mechanism, PAMA-TTS also leverages token duration and
relative position of a frame, especially countdown information, i.e. in how
many future frames the present phoneme will end. They help the attention to
move forward along the token sequence in a soft but reliable control.
Experimental results prove that PAMA-TTS achieves the highest naturalness,
while has on-par or even better duration controllability than the
duration-informed model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Isotropy Analysis in the Multilingual BERT Embedding Space. (arXiv:2110.04504v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04504">
<div class="article-summary-box-inner">
<span><p>Several studies have explored various advantages of multilingual pre-trained
models (e.g., multilingual BERT) in capturing shared linguistic knowledge.
However, their limitations have not been paid enough attention. In this paper,
we investigate the representation degeneration problem in multilingual
contextual word representations (CWRs) of BERT and show that the embedding
spaces of the selected languages suffer from anisotropy problem. Our
experimental results demonstrate that, similarly to their monolingual
counterparts, increasing the isotropy of multilingual embedding space can
significantly improve its representation power and performance. Our analysis
indicates that although the degenerated directions vary in different languages,
they encode similar linguistic knowledge, suggesting a shared linguistic space
among languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations. (arXiv:2110.04517v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04517">
<div class="article-summary-box-inner">
<span><p>NLP models that compare or consolidate information across multiple documents
often struggle when challenged with recognizing substantial information
redundancies across the texts. For example, in multi-document summarization it
is crucial to identify salient information across texts and then generate a
non-redundant summary, while facing repeated and usually differently-phrased
salient content. To facilitate researching such challenges, the sentence-level
task of \textit{sentence fusion} was proposed, yet previous datasets for this
task were very limited in their size and scope. In this paper, we revisit and
substantially extend previous dataset creation efforts. With careful
modifications, relabeling and employing complementing data sources, we were
able to triple the size of a notable earlier dataset. Moreover, we show that
our extended version uses more representative texts for multi-document tasks
and provides a larger and more diverse training set, which substantially
improves model training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing. (arXiv:2110.04518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04518">
<div class="article-summary-box-inner">
<span><p>Text discourse parsing weighs importantly in understanding information flow
and argumentative structure in natural language, making it beneficial for
downstream tasks. While previous work significantly improves the performance of
RST discourse parsing, they are not readily applicable to practical use cases:
(1) EDU segmentation is not integrated into most existing tree parsing
frameworks, thus it is not straightforward to apply such models on newly-coming
data. (2) Most parsers cannot be used in multilingual scenarios, because they
are developed only in English. (3) Parsers trained from single-domain treebanks
do not generalize well on out-of-domain inputs. In this work, we propose a
document-level multilingual RST discourse parsing framework, which conducts EDU
segmentation and discourse tree parsing jointly. Moreover, we propose a
cross-translation augmentation strategy to enable the framework to support
multilingual parsing and improve its domain generality. Experimental results
show that our model achieves state-of-the-art performance on document-level
multilingual RST parsing in all sub-tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks. (arXiv:2110.04522v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04522">
<div class="article-summary-box-inner">
<span><p>Rumors are rampant in the era of social media. Conversation structures
provide valuable clues to differentiate between real and fake claims. However,
existing rumor detection methods are either limited to the strict relation of
user responses or oversimplify the conversation structure. In this study, to
substantially reinforces the interaction of user opinions while alleviating the
negative impact imposed by irrelevant posts, we first represent the
conversation thread as an undirected interaction graph. We then present a
Claim-guided Hierarchical Graph Attention Network for rumor classification,
which enhances the representation learning for responsive posts considering the
entire social contexts and attends over the posts that can semantically infer
the target claim. Extensive experiments on three Twitter datasets demonstrate
that our rumor detection method achieves much better performance than
state-of-the-art methods and exhibits a superior capacity for detecting rumors
at early stages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Disentangled Arguments with Prompts: A Simple Event Extraction Framework that Works. (arXiv:2110.04525v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04525">
<div class="article-summary-box-inner">
<span><p>Event Extraction bridges the gap between text and event signals. Based on the
assumption of trigger-argument dependency, existing approaches have achieved
state-of-the-art performance with expert-designed templates or complicated
decoding constraints. In this paper, for the first time we introduce the
prompt-based learning strategy to the domain of Event Extraction, which
empowers the automatic exploitation of label semantics on both input and output
sides. To validate the effectiveness of the proposed generative method, we
conduct extensive experiments with 11 diverse baselines. Empirical results show
that, in terms of F1 score on Argument Extraction, our simple architecture is
stronger than any other generative counterpart and even competitive with
algorithms that require template engineering. Regarding the measure of recall,
it sets new overall records for both Argument and Trigger Extractions. We
hereby recommend this framework to the community, with the code publicly
available at https://git.io/GDAP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multi-Party Dialogue Discourse Parsing via Domain Integration. (arXiv:2110.04526v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04526">
<div class="article-summary-box-inner">
<span><p>While multi-party conversations are often less structured than monologues and
documents, they are implicitly organized by semantic level correlations across
the interactive turns, and dialogue discourse analysis can be applied to
predict the dependency structure and relations between the elementary discourse
units, and provide feature-rich structural information for downstream tasks.
However, the existing corpora with dialogue discourse annotation are collected
from specific domains with limited sample sizes, rendering the performance of
data-driven approaches poor on incoming dialogues without any domain
adaptation. In this paper, we first introduce a Transformer-based parser, and
assess its cross-domain performance. We next adopt three methods to gain domain
integration from both data and language modeling perspectives to improve the
generalization capability. Empirical results show that the neural parser can
benefit from our proposed methods, and performs better on cross-domain dialogue
samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04541">
<div class="article-summary-box-inner">
<span><p>Pretraining Neural Language Models (NLMs) over a large corpus involves
chunking the text into training examples, which are contiguous text segments of
sizes processable by the neural architecture. We highlight a bias introduced by
this common practice: we prove that the pretrained NLM can model much stronger
dependencies between text segments that appeared in the same training example,
than it can between text segments that appeared in different training examples.
This intuitive result has a twofold role. First, it formalizes the motivation
behind a broad line of recent successful NLM training heuristics, proposed for
the pretraining and fine-tuning stages, which do not necessarily appear related
at first glance. Second, our result clearly indicates further improvements to
be made in NLM pretraining for the benefit of Natural Language Understanding
tasks. As an example, we propose "kNN-Pretraining": we show that including
semantically related non-neighboring sentences in the same pretraining example
yields improved sentence representations and open domain question answering
abilities. This theoretically motivated degree of freedom for "pretraining
example design" indicates new training schemes for self-improving
representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIP-Adapter: Better Vision-Language Models with Feature Adapters. (arXiv:2110.04544v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04544">
<div class="article-summary-box-inner">
<span><p>Large-scale contrastive vision-language pre-training has shown significant
progress in visual representation learning. Unlike traditional visual systems
trained by a fixed set of discrete labels, a new paradigm was introduced in
\cite{radford2021learning} to directly learn to align images with raw texts in
an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt
is employed to make zero-shot predictions.~To avoid non-trivial prompt
engineering, context optimization \cite{zhou2021coop} has been proposed to
learn continuous vectors as task-specific prompts with few-shot training
examples.~In this paper, we show that there is an alternative path to achieve
better vision-language models other than prompt tuning.~While prompt tuning is
for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with
feature adapters on either visual or language branch. Specifically,
CLIP-Adapter adopts an additional bottleneck layer to learn new features and
performs residual-style feature blending with the original pre-trained
features.~As a consequence, CLIP-Adapter is able to outperform context
optimization while maintains a simple design. Experiments and extensive
ablation studies on various visual classification tasks demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition. (arXiv:2110.04590v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04590">
<div class="article-summary-box-inner">
<span><p>Self-supervised pretraining on speech data has achieved a lot of progress.
High-fidelity representation of the speech signal is learned from a lot of
untranscribed data and shows promising performance. Recently, there are several
works focusing on evaluating the quality of self-supervised pretrained
representations on various tasks without domain restriction, e.g. SUPERB.
However, such evaluations do not provide a comprehensive comparison among many
ASR benchmark corpora. In this paper, we focus on the general applications of
pretrained speech representations, on advanced end-to-end automatic speech
recognition (E2E-ASR) models. We select several pretrained speech
representations and present the experimental results on various open-source and
publicly available corpora for E2E-ASR. Without any modification of the
back-end model architectures or training strategy, some of the experiments with
pretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or
outperform current state-of-the-art (SOTA) recognition performance. Moreover,
we further explore more scenarios for whether the pretraining representations
are effective, such as the cross-language or overlapped speech. The scripts,
configuratons and the trained models have been released in ESPnet to let the
community reproduce our experiments and improve them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets. (arXiv:2110.04612v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04612">
<div class="article-summary-box-inner">
<span><p>This study investigates the performance of personalized automatic speech
recognition (ASR) for recognizing disordered speech using small amounts of
per-speaker adaptation data. We trained personalized models for 195 individuals
with different types and severities of speech impairment with training sets
ranging in size from &lt;1 minute to 18-20 minutes of speech data. Word error rate
(WER) thresholds were selected to determine Success Percentage (the percentage
of personalized models reaching the target WER) in different application
scenarios. For the home automation scenario, 79% of speakers reached the target
WER with 18-20 minutes of speech; but even with only 3-4 minutes of speech, 63%
of speakers reached the target WER. Further evaluation found similar
improvement on test sets with conversational and out-of-domain, unprompted
phrases. Our results demonstrate that with only a few minutes of recordings,
individuals with disordered speech could benefit from personalized ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Response Generation through Graph-based Multi-hop Reasoning on Emotional Causality. (arXiv:2110.04614v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04614">
<div class="article-summary-box-inner">
<span><p>Empathetic response generation aims to comprehend the user emotion and then
respond to it appropriately. Most existing works merely focus on what the
emotion is and ignore how the emotion is evoked, thus weakening the capacity of
the model to understand the emotional experience of the user for generating
empathetic responses. To tackle this problem, we consider the emotional
causality, namely, what feelings the user expresses (i.e., emotion) and why the
user has such feelings (i.e., cause). Then, we propose a novel graph-based
model with multi-hop reasoning to model the emotional causality of the
empathetic conversation. Finally, we demonstrate the effectiveness of our model
on EMPATHETICDIALOGUES in comparison with several competitive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Framework for Rationale Extraction for Deep QA models. (arXiv:2110.04620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04620">
<div class="article-summary-box-inner">
<span><p>As neural-network-based QA models become deeper and more complex, there is a
demand for robust frameworks which can access a model's rationale for its
prediction. Current techniques that provide insights on a model's working are
either dependent on adversarial datasets or are proposing models with explicit
explanation generation components. These techniques are time-consuming and
challenging to extend to existing models and new datasets. In this work, we use
`Integrated Gradients' to extract rationale for existing state-of-the-art
models in the task of Reading Comprehension based Question Answering (RCQA). On
detailed analysis and comparison with collected human rationales, we find that
though ~40-80% words of extracted rationale coincide with the human rationale
(precision), only 6-19% of human rationale is present in the extracted
rationale (recall).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Relation between Syntactic Divergence and Zero-Shot Performance. (arXiv:2110.04644v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04644">
<div class="article-summary-box-inner">
<span><p>We explore the link between the extent to which syntactic relations are
preserved in translation and the ease of correctly constructing a parse tree in
a zero-shot setting. While previous work suggests such a relation, it tends to
focus on the macro level and not on the level of individual edges-a gap we aim
to address. As a test case, we take the transfer of Universal Dependencies (UD)
parsing from English to a diverse set of languages and conduct two sets of
experiments. In one, we analyze zero-shot performance based on the extent to
which English source edges are preserved in translation. In another, we apply
three linguistically motivated transformations to UD, creating more
cross-lingually stable versions of it, and assess their zero-shot parsability.
In order to compare parsing performance across different schemes, we perform
extrinsic evaluation on the downstream task of cross-lingual relation
extraction (RE) using a subset of a popular English RE benchmark translated to
Russian and Korean. In both sets of experiments, our results suggest a strong
relation between cross-lingual stability and zero-shot parsing performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Follow Language Instructions with Compositional Policies. (arXiv:2110.04647v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04647">
<div class="article-summary-box-inner">
<span><p>We propose a framework that learns to execute natural language instructions
in an environment consisting of goal-reaching tasks that share components of
their task descriptions. Our approach leverages the compositionality of both
value functions and language, with the aim of reducing the sample complexity of
learning novel tasks. First, we train a reinforcement learning agent to learn
value functions that can be subsequently composed through a Boolean algebra to
solve novel tasks. Second, we fine-tune a seq2seq model pretrained on web-scale
corpora to map language to logical expressions that specify the required value
function compositions. Evaluating our agent in the BabyAI domain, we observe a
decrease of 86% in the number of training steps needed to learn a second task
after mastering a single task. Results from ablation studies further indicate
that it is the combination of compositional value functions and language
representations that allows the agent to quickly generalize to new tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangled Sequence to Sequence Learning for Compositional Generalization. (arXiv:2110.04655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04655">
<div class="article-summary-box-inner">
<span><p>There is mounting evidence that existing neural network models, in particular
the very popular sequence-to-sequence architecture, struggle with compositional
generalization, i.e., the ability to systematically generalize to unseen
compositions of seen components. In this paper we demonstrate that one of the
reasons hindering compositional generalization relates to the representations
being entangled. We propose an extension to sequence-to-sequence models which
allows us to learn disentangled representations by adaptively re-encoding (at
each time step) the source input. Specifically, we condition the source
representations on the newly decoded target context which makes it easier for
the encoder to exploit specialized information for each prediction rather than
capturing all source information in a single forward pass. Experimental results
on semantic parsing and machine translation empirically show that our proposal
yields more disentangled representations and better generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Audio Captions Be Evaluated with Image Caption Metrics?. (arXiv:2110.04684v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04684">
<div class="article-summary-box-inner">
<span><p>Automated audio captioning aims at generating textual descriptions for an
audio clip. To evaluate the quality of generated audio captions, previous works
directly adopt image captioning metrics like SPICE and CIDEr, without
justifying their suitability in this new domain, which may mislead the
development of advanced models. This problem is still unstudied due to the lack
of human judgment datasets on caption quality. Therefore, we firstly construct
two evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established
with pairwise comparison instead of absolute rating to achieve better
inter-annotator agreement. Current metrics are found in poor correlation with
human annotations on these datasets. To overcome their limitations, we propose
a metric named FENSE, where we combine the strength of Sentence-BERT in
capturing similarity, and a novel Error Detector to penalize erroneous
sentences for robustness. On the newly established benchmarks, FENSE
outperforms current metrics by 14-25% accuracy. Code, data and web demo
available at: https://github.com/blmoistawinde/fense
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Channel End-to-End Neural Diarization with Distributed Microphones. (arXiv:2110.04694v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04694">
<div class="article-summary-box-inner">
<span><p>Recent progress on end-to-end neural diarization (EEND) has enabled
overlap-aware speaker diarization with a single neural network. This paper
proposes to enhance EEND by using multi-channel signals from distributed
microphones. We replace Transformer encoders in EEND with two types of encoders
that process a multi-channel input: spatio-temporal and co-attention encoders.
Both are independent of the number and geometry of microphones and suitable for
distributed microphone settings. We also propose a model adaptation method
using only single-channel recordings. With simulated and real-recorded
datasets, we demonstrated that the proposed method outperformed conventional
EEND when a multi-channel input was given while maintaining comparable
performance with a single-channel input. We also showed that the proposed
method performed well even when spatial information is inoperative given
multi-channel inputs, such as in hybrid meetings in which the utterances of
multiple remote participants are played back from the same loudspeaker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention in Natural Language Processing. (arXiv:1902.02181v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1902.02181">
<div class="article-summary-box-inner">
<span><p>Attention is an increasingly popular mechanism used in a wide range of neural
architectures. The mechanism itself has been realized in a variety of formats.
However, because of the fast-paced advances in this domain, a systematic
overview of attention is still missing. In this article, we define a unified
model for attention architectures in natural language processing, with a focus
on those designed to work with vector representations of the textual data. We
propose a taxonomy of attention models according to four dimensions: the
representation of the input, the compatibility function, the distribution
function, and the multiplicity of the input and/or output. We present the
examples of how prior information can be exploited in attention models and
discuss ongoing research efforts and open challenges in the area, providing the
first extensive categorization of the vast body of literature in this exciting
domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Relational Image Captioning via Multi-task Triple-Stream Networks. (arXiv:2010.03855v3 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03855">
<div class="article-summary-box-inner">
<span><p>We introduce dense relational captioning, a novel image captioning task which
aims to generate multiple captions with respect to relational information
between objects in a visual scene. Relational captioning provides explicit
descriptions for each relationship between object combinations. This framework
is advantageous in both diversity and amount of information, leading to a
comprehensive image understanding based on relationships, e.g., relational
proposal generation. For relational understanding between objects, the
part-of-speech (POS; i.e., subject-object-predicate categories) can be a
valuable prior information to guide the causal sequence of words in a caption.
We enforce our framework to learn not only to generate captions but also to
understand the POS of each word. To this end, we propose the multi-task
triple-stream network (MTTSNet) which consists of three recurrent units
responsible for each POS which is trained by jointly predicting the correct
captions and POS for each word. In addition, we found that the performance of
MTTSNet can be improved by modulating the object embeddings with an explicit
relational module. We demonstrate that our proposed model can generate more
diverse and richer captions, via extensive experimental analysis on large scale
datasets and several metrics. Then, we present applications of our framework to
holistic image captioning, scene graph generation, and retrieval tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset. (arXiv:2010.04480v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04480">
<div class="article-summary-box-inner">
<span><p>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality
Estimation (QE) and Automatic Post-Editing (APE). The dataset contains eleven
language pairs, with human labels for up to 10,000 translations per language
pair in the following formats: sentence-level direct assessments and
post-editing effort, and word-level good/bad labels. It also contains the
post-edited sentences, as well as titles of the articles where the sentences
were extracted from, and the neural MT models used to translate the text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SemMT: A Semantic-based Testing Approach for Machine Translation Systems. (arXiv:2012.01815v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01815">
<div class="article-summary-box-inner">
<span><p>Machine translation has wide applications in daily life. In mission-critical
applications such as translating official documents, incorrect translation can
have unpleasant or sometimes catastrophic consequences. This motivates recent
research on testing methodologies for machine translation systems. Existing
methodologies mostly rely on metamorphic relations designed at the textual
level (e.g., Levenshtein distance) or syntactic level (e.g., the distance
between grammar structures) to determine the correctness of translation
results. However, these metamorphic relations do not consider whether the
original and translated sentences have the same meaning (i.e., Semantic
similarity). Therefore, in this paper, we propose SemMT, an automatic testing
approach for machine translation systems based on semantic similarity checking.
SemMT applies round-trip translation and measures the semantic similarity
between the original and translated sentences. Our insight is that the
semantics expressed by the logic and numeric constraint in sentences can be
captured using regular expressions (or deterministic finite automata) where
efficient equivalence/similarity checking algorithms are available. Leveraging
the insight, we propose three semantic similarity metrics and implement them in
SemMT. The experiment result reveals SemMT can achieve higher effectiveness
compared with state-of-the-art works, achieving an increase of 21% and 23% on
accuracy and F-Score, respectively. We also explore potential improvements that
can be achieved when proper combinations of metrics are adopted. Finally, we
discuss a solution to locate the suspicious trip in round-trip translation,
which may shed lights on further exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Approximating How Single Head Attention Learns. (arXiv:2103.07601v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07601">
<div class="article-summary-box-inner">
<span><p>Why do models often attend to salient words, and how does this evolve
throughout training? We approximate model training as a two stage process:
early on in training when the attention weights are uniform, the model learns
to translate individual input word `i` to `o` if they co-occur frequently.
Later, the model learns to attend to `i` while the correct output is $o$
because it knows `i` translates to `o`. To formalize, we define a model
property, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`
translates to `o`), and claim that it drives the learning of the attention.
This claim is supported by the fact that before the attention mechanism is
learned, KTIW can be learned from word co-occurrence statistics, but not the
other way around. Particularly, we can construct a training distribution that
makes KTIW hard to learn, the learning of the attention fails, and the model
cannot even learn the simple task of copying the input words to the output. Our
approximation explains why models sometimes attend to salient words, and
inspires a toy example where a multi-head attention model can overcome the
above hard training distribution by improving learning dynamics rather than
expressiveness. We end by discussing the limitation of our approximation
framework and suggest future directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LSTM Based Sentiment Analysis for Cryptocurrency Prediction. (arXiv:2103.14804v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14804">
<div class="article-summary-box-inner">
<span><p>Recent studies in big data analytics and natural language processing develop
automatic techniques in analyzing sentiment in the social media information. In
addition, the growing user base of social media and the high volume of posts
also provide valuable sentiment information to predict the price fluctuation of
the cryptocurrency. This research is directed to predicting the volatile price
movement of cryptocurrency by analyzing the sentiment in social media and
finding the correlation between them. While previous work has been developed to
analyze sentiment in English social media posts, we propose a method to
identify the sentiment of the Chinese social media posts from the most popular
Chinese social media platform Sina-Weibo. We develop the pipeline to capture
Weibo posts, describe the creation of the crypto-specific sentiment dictionary,
and propose a long short-term memory (LSTM) based recurrent neural network
along with the historical cryptocurrency price movement to predict the price
trend for future time frames. The conducted experiments demonstrate the
proposed approach outperforms the state of the art auto regressive based model
by 18.5% in precision and 15.4% in recall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. (arXiv:2104.06378v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06378">
<div class="article-summary-box-inner">
<span><p>The problem of answering questions using knowledge from pre-trained language
models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA
context (question and answer choice), methods need to (i) identify relevant
knowledge from large KGs, and (ii) perform joint reasoning over the QA context
and KG. In this work, we propose a new model, QA-GNN, which addresses the above
challenges through two key innovations: (i) relevance scoring, where we use LMs
to estimate the importance of KG nodes relative to the given QA context, and
(ii) joint reasoning, where we connect the QA context and KG to form a joint
graph, and mutually update their representations through graph neural networks.
We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its
improvement over existing LM and LM+KG models, as well as its capability to
perform interpretable and structured reasoning, e.g., correctly handling
negation in questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph. (arXiv:2104.07302v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07302">
<div class="article-summary-box-inner">
<span><p>Multi-hop Question Answering (QA) is a challenging task because it requires
precise reasoning with entity relations at every step towards the answer. The
relations can be represented in terms of labels in knowledge graph (e.g.,
\textit{spouse}) or text in text corpus (e.g., \textit{they have been married
for 26 years}). Existing models usually infer the answer by predicting the
sequential relation path or aggregating the hidden graph features. The former
is hard to optimize, and the latter lacks interpretability. In this paper, we
propose TransferNet, an effective and transparent model for multi-hop QA, which
supports both label and text relations in a unified framework. TransferNet
jumps across entities at multiple steps. At each step, it attends to different
parts of the question, computes activated scores for relations, and then
transfer the previous entity scores along activated relations in a
differentiable way. We carry out extensive experiments on three datasets and
demonstrate that TransferNet surpasses the state-of-the-art models by a large
margin. In particular, on MetaQA, it achieves 100\% accuracy in 2-hop and 3-hop
questions. By qualitative analysis, we show that TransferNet has transparent
and interpretable intermediate results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation. (arXiv:2104.08200v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08200">
<div class="article-summary-box-inner">
<span><p>Natural language generation (NLG) benchmarks provide an important avenue to
measure progress and develop better NLG systems. Unfortunately, the lack of
publicly available NLG benchmarks for low-resource languages poses a
challenging barrier for building NLG systems that work well for languages with
limited amounts of data. Here we introduce IndoNLG, the first benchmark to
measure natural language generation (NLG) progress in three low-resource -- yet
widely spoken -- languages of Indonesia: Indonesian, Javanese, and Sundanese.
Altogether, these languages are spoken by more than 100 million native
speakers, and hence constitute an important use case of NLG systems today.
Concretely, IndoNLG covers six tasks: summarization, question answering,
chit-chat, and three different pairs of machine translation (MT) tasks. We
collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese
datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and
IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on
all tasks -- despite using only one-fifth the parameters of a larger
multilingual model, mBART-LARGE (Liu et al., 2020). This finding emphasizes the
importance of pretraining on closely related, local languages to achieve more
efficient learning and faster inference for very low-resource languages like
Javanese and Sundanese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08808">
<div class="article-summary-box-inner">
<span><p>The ability to continuously expand knowledge over time and utilize it to
rapidly generalize to new tasks is a key feature of human linguistic
intelligence. Existing models that pursue rapid generalization to new tasks
(e.g., few-shot learning methods), however, are mostly trained in a single shot
on fixed datasets, unable to dynamically expand their knowledge; while
continual learning algorithms are not specifically designed for rapid
generalization. We present a new learning setup, Continual Learning of Few-Shot
Learners (CLIF), to address the challenges of both learning settings in a
unified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks
arriving sequentially, accumulating knowledge for improved generalization to
new tasks, while also retaining performance on the tasks learned earlier. We
examine how the generalization ability is affected in the continual learning
setup, evaluate a number of continual learning algorithms, and propose a novel
regularized adapter generation approach. We find that catastrophic forgetting
affects generalization ability to a less degree than performance on seen tasks;
while continual learning algorithms can still bring considerable benefit to the
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoFormer: Enhanced Transformer with Rotary Position Embedding. (arXiv:2104.09864v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09864">
<div class="article-summary-box-inner">
<span><p>Position encoding in transformer architecture provides supervision for
dependency modeling between elements at different positions in the sequence. We
investigate various methods to encode positional information in
transformer-based language models and propose a novel implementation named
Rotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional
information with rotation matrix and naturally incorporates explicit relative
position dependency in self-attention formulation. Notably, RoPE comes with
valuable properties such as flexibility of being expand to any sequence
lengths, decaying inter-token dependency with increasing relative distances,
and capability of equipping the linear self-attention with relative position
encoding. As a result, the enhanced transformer with rotary position embedding,
or RoFormer, achieves superior performance in tasks with long texts. We release
the theoretical analysis along with some preliminary experiment results on
Chinese data. The undergoing experiment for English benchmark will soon be
updated.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12227">
<div class="article-summary-box-inner">
<span><p>Classic information extraction techniques consist in building questions and
answers about the facts. Indeed, it is still a challenge to subjective
information extraction systems to identify opinions and feelings in context. In
sentiment-based NLP tasks, there are few resources to information extraction,
above all offensive or hateful opinions in context. To fill this important gap,
this short paper provides a new cross-lingual and contextual offensive lexicon,
which consists of explicit and implicit offensive and swearing expressions of
opinion, which were annotated in two different classes: context dependent and
context-independent offensive. In addition, we provide markers to identify hate
speech. Annotation approach was evaluated at the expression-level and achieves
high human inter-annotator agreement. The provided offensive lexicon is
available in Portuguese and English languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12265">
<div class="article-summary-box-inner">
<span><p>This paper provides a new approach for offensive language and hate speech
detection on social media. Our approach incorporates an offensive lexicon
composed of implicit and explicit offensive and swearing expressions annotated
with binary classes: context-dependent and context-independent offensive. Due
to the severity of the hate speech and offensive comments in Brazil, and the
lack of research in Portuguese, Brazilian Portuguese is the language used to
validate the proposed method. Nevertheless, our proposal may be applied to any
other language or domain. Based on the obtained results, the proposed approach
showed high-performance overcoming the current baselines for European and
Brazilian Portuguese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WhatTheWikiFact: Fact-Checking Claims Against Wikipedia. (arXiv:2105.00826v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00826">
<div class="article-summary-box-inner">
<span><p>The rise of Internet has made it a major source of information.
Unfortunately, not all information online is true, and thus a number of
fact-checking initiatives have been launched, both manual and automatic, to
deal with the problem. Here, we present our contribution in this regard:
\emph{WhatTheWikiFact}, a system for automatic claim verification using
Wikipedia. The system can predict the veracity of an input claim, and it
further shows the evidence it has retrieved as part of the verification
process. It shows confidence scores and a list of relevant Wikipedia articles,
together with detailed information about each article, including the phrase
used to retrieve it, the most relevant sentences extracted from it and their
stance with respect to the input claim, as well as the associated
probabilities. The system supports several languages: Bulgarian, English, and
Russian.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Black or White but never neutral: How readers perceive identity from yellow or skin-toned emoji. (arXiv:2105.05887v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.05887">
<div class="article-summary-box-inner">
<span><p>Research in sociology and linguistics shows that people use language not only
to express their own identity but to understand the identity of others. Recent
work established a connection between expression of identity and emoji usage on
social media, through use of emoji skin tone modifiers. Motivated by that
finding, this work asks if, as with language, readers are sensitive to such
acts of self-expression and use them to understand the identity of authors. In
behavioral experiments (n=488), where text and emoji content of social media
posts were carefully controlled before being presented to participants, we find
in the affirmative -- emoji are a salient signal of author identity. That
signal is distinct from, and complementary to, the one encoded in language.
Participant groups (based on self-identified ethnicity) showed no differences
in how they perceive this signal, except in the case of the default yellow
emoji. While both groups associate this with a White identity, the effect was
stronger in White participants. Our finding that emoji can index social
variables will have experimental applications for researchers but also
implications for designers: supposedly ``neutral`` defaults may be more
representative of some users than others.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doc2Dict: Information Extraction as Text Generation. (arXiv:2105.07510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07510">
<div class="article-summary-box-inner">
<span><p>Typically, information extraction (IE) requires a pipeline approach: first, a
sequence labeling model is trained on manually annotated documents to extract
relevant spans; then, when a new document arrives, a model predicts spans which
are then post-processed and standardized to convert the information into a
database entry. We replace this labor-intensive workflow with a transformer
language model trained on existing database records to directly generate
structured JSON. Our solution removes the workload associated with producing
token-level annotations and takes advantage of a data source which is generally
quite plentiful (e.g. database records). As long documents are common in
information extraction tasks, we use gradient checkpointing and chunked
encoding to apply our method to sequences of up to 32,000 tokens on a single
GPU. Our Doc2Dict approach is competitive with more complex, hand-engineered
pipelines and offers a simple but effective baseline for document-level
information extraction. We release our Doc2Dict model and code to reproduce our
experiments and facilitate future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interpretable agent communication from scratch (with a generic visual processor emerging on the side). (arXiv:2106.04258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04258">
<div class="article-summary-box-inner">
<span><p>As deep networks begin to be deployed as autonomous agents, the issue of how
they can communicate with each other becomes important. Here, we train two deep
nets from scratch to perform realistic referent identification through
unsupervised emergent communication. We show that the largely interpretable
emergent protocol allows the nets to successfully communicate even about object
types they did not see at training time. The visual representations induced as
a by-product of our training regime, moreover, show comparable quality, when
re-used as generic visual features, to a recent self-supervised learning model.
Our results provide concrete evidence of the viability of (interpretable)
emergent deep net communication in a more realistic scenario than previously
considered, as well as establishing an intriguing link between this field and
self-supervised visual learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08858">
<div class="article-summary-box-inner">
<span><p>Language is an interface to the outside world. In order for embodied agents
to use it, language must be grounded in other, sensorimotor modalities. While
there is an extended literature studying how machines can learn grounded
language, the topic of how to learn spatio-temporal linguistic concepts is
still largely uncharted. To make progress in this direction, we here introduce
a novel spatio-temporal language grounding task where the goal is to learn the
meaning of spatio-temporal descriptions of behavioral traces of an embodied
agent. This is achieved by training a truth function that predicts if a
description matches a given history of observations. The descriptions involve
time-extended predicates in past and present tense as well as spatio-temporal
references to objects in the scene. To study the role of architectural biases
in this task, we train several models including multimodal Transformer
architectures; the latter implement different attention computations between
words and objects across space and time. We test models on two classes of
generalization: 1) generalization to randomly held-out sentences; 2)
generalization to grammar primitives. We observe that maintaining object
identity in the attention computation of our Transformers is instrumental to
achieving good performance on generalization overall, and that summarizing
object traces in a single token has little influence on performance. We then
discuss how this opens new perspectives for language-guided autonomous embodied
agents. We also release our code under open-source license as well as
pretrained models and datasets to encourage the wider community to build upon
and extend our work in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04734">
<div class="article-summary-box-inner">
<span><p>Recently proposed self-supervised learning approaches have been successful
for pre-training speech representation models. The utility of these learned
representations has been observed empirically, but not much has been studied
about the type or extent of information encoded in the pre-trained
representations themselves. Developing such insights can help understand the
capabilities and limits of these models and enable the research community to
more efficiently develop their usage for downstream applications. In this work,
we begin to fill this gap by examining one recent and successful pre-trained
model (wav2vec 2.0), via its intermediate representation vectors, using a suite
of analysis tools. We use the metrics of canonical correlation, mutual
information, and performance on simple downstream tasks with non-parametric
probes, in order to (i) query for acoustic and linguistic information content,
(ii) characterize the evolution of information across model layers, and (iii)
understand how fine-tuning the model for automatic speech recognition (ASR)
affects these observations. Our findings motivate modifying the fine-tuning
protocol for ASR, which produces improved word error rates in a low-resource
setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation. (arXiv:2107.09278v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09278">
<div class="article-summary-box-inner">
<span><p>Transcripts generated by automatic speech recognition (ASR) systems for
spoken documents lack structural annotations such as paragraphs, significantly
reducing their readability. Automatically predicting paragraph segmentation for
spoken documents may both improve readability and downstream NLP performance
such as summarization and machine reading comprehension. We propose a sequence
model with self-adaptive sliding window for accurate and efficient paragraph
segmentation. We also propose an approach to exploit phonetic information,
which significantly improves robustness of spoken document segmentation to ASR
errors. Evaluations are conducted on the English Wiki-727K document
segmentation benchmark, a Chinese Wikipedia-based document segmentation dataset
we created, and an in-house Chinese spoken document dataset. Our proposed model
outperforms the state-of-the-art (SOTA) model based on the same BERT-Base,
increasing segmentation F1 on the English benchmark by 4.2 points and on
Chinese datasets by 4.3-10.1 points, while reducing inference time to less than
1/6 of inference time of the current SOTA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01135">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence learning with neural networks has become the de facto
standard for sequence prediction tasks. This approach typically models the
local distribution over the next word with a powerful neural network that can
condition on arbitrary context. While flexible and performant, these models
often require large datasets for training and can fail spectacularly on
benchmarks designed to test for compositional generalization. This work
explores an alternative, hierarchical approach to sequence-to-sequence learning
with quasi-synchronous grammars, where each node in the target tree is
transduced by a node in the source tree. Both the source and target trees are
treated as latent and induced during training. We develop a neural
parameterization of the grammar which enables parameter sharing over the
combinatorial space of derivation rules without the need for manual feature
engineering. We apply this latent neural grammar to various domains -- a
diagnostic language navigation task designed to test for compositional
generalization (SCAN), style transfer, and small-scale machine translation --
and find that it performs respectably compared to standard baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02401">
<div class="article-summary-box-inner">
<span><p>Multimodal abstractive summarization (MAS) models that summarize videos
(vision modality) and their corresponding transcripts (text modality) are able
to extract the essential information from massive multimodal data on the
Internet. Recently, large-scale generative pre-trained language models (GPLMs)
have been shown to be effective in text generation tasks. However, existing MAS
models cannot leverage GPLMs' powerful generation ability. To fill this
research gap, we aim to study two research questions: 1) how to inject visual
information into GPLMs without hurting their generation ability; and 2) where
is the optimal place in GPLMs to inject the visual information? In this paper,
we present a simple yet effective method to construct vision guided (VG) GPLMs
for the MAS task using attention-based add-on layers to incorporate visual
information while maintaining their original text generation ability. Results
show that our best model significantly surpasses the prior state-of-the-art
model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,
and our visual guidance method contributes 83.6% of the overall improvement.
Furthermore, we conduct thorough ablation studies to analyze the effectiveness
of various modality fusion methods and fusion locations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04947">
<div class="article-summary-box-inner">
<span><p>Large-scale, pre-trained language models (LMs) have achieved human-level
performance on a breadth of language understanding tasks. However, evaluations
only based on end task performance shed little light on machines' true ability
in language understanding and reasoning. In this paper, we highlight the
importance of evaluating the underlying reasoning process in addition to end
performance. Toward this goal, we introduce Tiered Reasoning for Intuitive
Physics (TRIP), a novel commonsense reasoning dataset with dense annotations
that enable multi-tiered evaluation of machines' reasoning process. Our
empirical results show that while large LMs can achieve high end performance,
they struggle to support their predictions with valid supporting evidence. The
TRIP dataset and our baseline results will motivate verifiable evaluation of
commonsense reasoning and facilitate future research toward developing better
language understanding and reasoning models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pack Together: Entity and Relation Extraction with Levitated Marker. (arXiv:2109.06067v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06067">
<div class="article-summary-box-inner">
<span><p>Named Entity Recognition (NER) and Relation Extraction (RE) are the core
sub-tasks for information extraction. Many recent works formulate these two
tasks as the span (pair) classification problem, and thus focus on
investigating how to obtain a better span representation from the pre-trained
encoder. However, a major limitation of existing works is that they ignore the
dependencies between spans (pairs). In this work, we propose a novel span
representation approach, named Packed Levitated Markers, to consider the
dependencies between the spans (pairs) by strategically packing the markers in
the encoder. In particular, we propose a group packing strategy to enable our
model to process massive spans together to consider their dependencies with
limited resources. Furthermore, for those more complicated span pair
classification tasks, we design a subject-oriented packing strategy, which
packs each subject and all its objects into an instance to model the
dependencies between the same-subject span pairs. Our experiments show that our
model with packed levitated markers outperforms the sequence labeling model by
0.4%-1.9% F1 on three flat NER tasks, beats the token concat model on six NER
benchmarks, and obtains a 3.5%-3.6% strict relation F1 improvement with higher
speed over previous SOTA models on ACE04 and ACE05. Code and models are
publicly available at https://github.com/thunlp/PL-Marker.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition. (arXiv:2109.09161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09161">
<div class="article-summary-box-inner">
<span><p>Unifying acoustic and linguistic representation learning has become
increasingly crucial to transfer the knowledge learned on the abundance of
high-resource language data for low-resource speech recognition. Existing
approaches simply cascade pre-trained acoustic and language models to learn the
transfer from speech to text. However, how to solve the representation
discrepancy of speech and text is unexplored, which hinders the utilization of
acoustic and linguistic information. Moreover, previous works simply replace
the embedding layer of the pre-trained language model with the acoustic
features, which may cause the catastrophic forgetting problem. In this work, we
introduce Wav-BERT, a cooperative acoustic and linguistic representation
learning method to fuse and utilize the contextual information of speech and
text. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a
language model (BERT) into an end-to-end trainable framework. A Representation
Aggregation Module is designed to aggregate acoustic and linguistic
representation, and an Embedding Attention Module is introduced to incorporate
acoustic information into BERT, which can effectively facilitate the
cooperation of two pre-trained models and thus boost the representation
learning. Extensive experiments show that our Wav-BERT significantly
outperforms the existing approaches and achieves state-of-the-art performance
on low-resource speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques. (arXiv:2109.09725v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09725">
<div class="article-summary-box-inner">
<span><p>In this paper, a BERT based neural network model is applied to the JIGSAW
data set in order to create a model identifying hateful and toxic comments
(strictly seperated from offensive language) in online social platforms
(English language), in this case Twitter. Three other neural network
architectures and a GPT-2 model are also applied on the provided data set in
order to compare these different models. The trained BERT model is then applied
on two different data sets to evaluate its generalisation power, namely on
another Twitter data set and the data set HASOC 2019 which includes Twitter and
also Facebook comments; we focus on the English HASOC 2019 data. In addition,
it can be shown that by fine-tuning the trained BERT model on these two data
sets by applying different transfer learning scenarios via retraining partial
or all layers the predictive scores improve compared to simply applying the
model pre-trained on the JIGSAW data set. With our results, we get precisions
from 64% to around 90% while still achieving acceptable recall values of at
least lower 60s%, proving that BERT is suitable for real use cases in social
platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WRENCH: A Comprehensive Benchmark for Weak Supervision. (arXiv:2109.11377v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11377">
<div class="article-summary-box-inner">
<span><p>Recent Weak Supervision (WS) approaches have had widespread success in easing
the bottleneck of labeling training data for machine learning by synthesizing
labels from multiple potentially noisy supervision sources. However, proper
measurement and analysis of these approaches remain a challenge. First,
datasets used in existing works are often private and/or custom, limiting
standardization. Second, WS datasets with the same name and base data often
vary in terms of the labels and weak supervision sources used, a significant
"hidden" source of evaluation variance. Finally, WS studies often diverge in
terms of the evaluation protocol and ablations used. To address these problems,
we introduce a benchmark platform, WRENCH, for thorough and standardized
evaluation of WS approaches. It consists of 22 varied real-world datasets for
classification and sequence tagging; a range of real, synthetic, and
procedurally-generated weak supervision sources; and a modular, extensible
framework for WS evaluation, including implementations for popular WS methods.
We use WRENCH to conduct extensive comparisons over more than 120 method
variants to demonstrate its efficacy as a benchmark platform. The code is
available at https://github.com/JieyuZ2/wrench.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12584">
<div class="article-summary-box-inner">
<span><p>In recent times, there has been definitive progress in the field of NLP, with
its applications growing as the utility of our language models increases with
advances in their performance. However, these models require a large amount of
computational power and data to train, consequently leading to large carbon
footprints. Therefore, it is imperative that we study the carbon efficiency and
look for alternatives to reduce the overall environmental impact of training
models, in particular large language models. In our work, we assess the
performance of models for machine translation, across multiple language pairs
to assess the difference in computational power required to train these models
for each of these language pairs and examine the various components of these
models to analyze aspects of our pipeline that can be optimized to reduce these
carbon emissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications. (arXiv:2109.14776v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14776">
<div class="article-summary-box-inner">
<span><p>Certainty and uncertainty are fundamental to science communication. Hedges
have widely been used as proxies for uncertainty. However, certainty is a
complex construct, with authors expressing not only the degree but the type and
aspects of uncertainty in order to give the reader a certain impression of what
is known. Here, we introduce a new study of certainty that models both the
level and the aspects of certainty in scientific findings. Using a new dataset
of 2167 annotated scientific findings, we demonstrate that hedges alone account
for only a partial explanation of certainty. We show that both the overall
certainty and individual aspects can be predicted with pre-trained language
models, providing a more complete picture of the author's intended
communication. Downstream analyses on 431K scientific findings from news and
scientific abstracts demonstrate that modeling sentence-level and aspect-level
certainty is meaningful for areas like science communication. Both the model
and datasets used in this paper are released at
https://blablablab.si.umich.edu/projects/certainty/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">#ContextMatters: Advantages and Limitations of Using Machine Learning to Support Women in Politics. (arXiv:2110.00116v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00116">
<div class="article-summary-box-inner">
<span><p>The United Nations identified gender equality as a Sustainable Development
Goal in 2015, recognizing the underrepresentation of women in politics as a
specific barrier to achieving gender equality. Political systems around the
world experience gender inequality across all levels of elected government as
fewer women run for office than men. This is due in part to online abuse,
particularly on social media platforms like Twitter, where women seeking or in
power tend to be targeted with more toxic maltreatment than their male
counterparts. In this paper, we present reflections on ParityBOT - the first
natural language processing-based intervention designed to affect online
discourse for women in politics for the better, at scale. Deployed across
elections in Canada, the United States and New Zealand, ParityBOT was used to
analyse and classify more than 12 million tweets directed at women candidates
and counter toxic tweets with supportive ones. From these elections we present
three case studies highlighting the current limitations of, and future research
and application opportunities for, using a natural language processing-based
system to detect online toxicity, specifically with regards to contextually
important microaggressions. We examine the rate of false negatives, where
ParityBOT failed to pick up on insults directed at specific high profile women,
which would be obvious to human users. We examine the unaddressed harms of
microaggressions and the potential of yet unseen damage they cause for women in
these communities, and for progress towards gender equality overall, in light
of these technological blindspots. This work concludes with a discussion on the
benefits of partnerships between nonprofit social groups and technology experts
to develop responsible, socially impactful approaches to addressing online
hate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles. (arXiv:2110.03179v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03179">
<div class="article-summary-box-inner">
<span><p>We present HowSumm, a novel large-scale dataset for the task of query-focused
multi-document summarization (qMDS), which targets the use-case of generating
actionable instructions from a set of sources. This use-case is different from
the use-cases covered in existing multi-document summarization (MDS) datasets
and is applicable to educational and industrial scenarios. We employed
automatic methods, and leveraged statistics from existing human-crafted qMDS
datasets, to create HowSumm from wikiHow website articles and the sources they
cite. We describe the creation of the dataset and discuss the unique features
that distinguish it from other summarization corpora. Automatic and human
evaluations of both extractive and abstractive summarization models on the
dataset reveal that there is room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over. (arXiv:2110.03342v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03342">
<div class="article-summary-box-inner">
<span><p>In this paper, we formulate a novel task to synthesize speech in sync with a
silent pre-recorded video, denoted as automatic voice over (AVO). Unlike
traditional speech synthesis, AVO seeks to generate not only human-sounding
speech, but also perfect lip-speech synchronization. A natural solution to AVO
is to condition the speech rendering on the temporal progression of lip
sequence in the video. We propose a novel text-to-speech model that is
conditioned on visual input, named VisualTTS, for accurate lip-speech
synchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)
textual-visual attention, and 2) visual fusion strategy during acoustic
decoding, which both contribute to forming accurate alignment between the input
text content and lip motion in input lip sequence. Experimental results show
that VisualTTS achieves accurate lip-speech synchronization and outperforms all
baseline systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Applying Phonological Features in Multilingual Text-To-Speech. (arXiv:2110.03609v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03609">
<div class="article-summary-box-inner">
<span><p>This study investigates whether phonological features can be applied in
text-to-speech systems to generate native and non-native speech in English and
Mandarin. We present a mapping of ARPABET/pinyin to SAMPA/SAMPA-SC and then to
phonological features. We tested whether this mapping could lead to the
successful generation of native, non-native, and code-switched speech in the
two languages. We ran two experiments, one with a small dataset and one with a
larger dataset. The results proved that phonological features could be used as
a feasible input system, although further investigation is needed to improve
model performance. The accented output generated by the TTS models also helps
with understanding human second language acquisition processes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation of professions in entertainment media: Insights into frequency and sentiment trends through computational text analysis. (arXiv:2110.03873v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03873">
<div class="article-summary-box-inner">
<span><p>Societal ideas and trends dictate media narratives and cinematic depictions
which in turn influences people's beliefs and perceptions of the real world.
Media portrayal of culture, education, government, religion, and family affect
their function and evolution over time as people interpret and perceive these
representations and incorporate them into their beliefs and actions. It is
important to study media depictions of these social structures so that they do
not propagate or reinforce negative stereotypes, or discriminate against any
demographic section. In this work, we examine media representation of
professions and provide computational insights into their incidence, and
sentiment expressed, in entertainment media content. We create a searchable
taxonomy of professional groups and titles to facilitate their retrieval from
speaker-agnostic text passages like movie and television (TV) show subtitles.
We leverage this taxonomy and relevant natural language processing (NLP) models
to create a corpus of professional mentions in media content, spanning more
than 136,000 IMDb titles over seven decades (1950-2017). We analyze the
frequency and sentiment trends of different occupations, study the effect of
media attributes like genre, country of production, and title type on these
trends, and investigate if the incidence of professions in media subtitles
correlate with their real-world employment statistics. We observe increased
media mentions of STEM, arts, sports, and entertainment occupations in the
analyzed subtitles, and a decreased frequency of manual labor jobs and military
occupations. The sentiment expressed toward lawyers, police, and doctors is
becoming negative over time, whereas astronauts, musicians, singers, and
engineers are mentioned favorably. Professions that employ more people have
increased media frequency, supporting our hypothesis that media acts as a
mirror to society.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-12 23:09:23.722999917 UTC">2021-10-12 23:09:23 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>