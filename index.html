<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-30T01:30:00Z">09-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Simplification for Comprehension-based Question-Answering. (arXiv:2109.13984v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13984">
<div class="article-summary-box-inner">
<span><p>Text simplification is the process of splitting and rephrasing a sentence to
a sequence of sentences making it easier to read and understand while
preserving the content and approximating the original meaning. Text
simplification has been exploited in NLP applications like machine translation,
summarization, semantic role labeling, and information extraction, opening a
broad avenue for its exploitation in comprehension-based question-answering
downstream tasks. In this work, we investigate the effect of text
simplification in the task of question-answering using a comprehension context.
We release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset.
</p>
<p>Firstly, we outline each step in the dataset creation pipeline, including
style transfer, thresholding of sentences showing correct transfer, and offset
finding for each answer. Secondly, we verify the quality of the transferred
sentences through various methodologies involving both automated and human
evaluation. Thirdly, we benchmark the newly created corpus and perform an
ablation study for examining the effect of the simplification process in the
SQuAD-based question answering task. Our experiments show that simplification
leads to up to 2.04% and 1.74% increase in Exact Match and F1, respectively.
Finally, we conclude with an analysis of the transfer process, investigating
the types of edits made by the model, and the effect of sentence length on the
transfer model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations. (arXiv:2109.14017v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14017">
<div class="article-summary-box-inner">
<span><p>Recent research has adopted a new experimental field centered around the
concept of text perturbations which has revealed that shuffled word order has
little to no impact on the downstream performance of Transformer-based language
models across many NLP tasks. These findings contradict the common
understanding of how the models encode hierarchical and structural information
and even question if the word order is modeled with position embeddings. To
this end, this paper proposes nine probing datasets organized by the type of
\emph{controllable} text perturbation for three Indo-European languages with a
varying degree of word order flexibility: English, Swedish and Russian. Based
on the probing analysis of the M-BERT and M-BART models, we report that the
syntactic sensitivity depends on the language and model pre-training
objectives. We also find that the sensitivity grows across layers together with
the increase of the perturbation granularity. Last but not least, we show that
the models barely use the positional information to induce syntactic trees from
their intermediate self-attention and contextualized representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Marked Attribute Bias in Natural Language Inference. (arXiv:2109.14039v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14039">
<div class="article-summary-box-inner">
<span><p>Reporting and providing test sets for harmful bias in NLP applications is
essential for building a robust understanding of the current problem. We
present a new observation of gender bias in a downstream NLP application:
marked attribute bias in natural language inference. Bias in downstream
applications can stem from training data, word embeddings, or be amplified by
the model in use. However, focusing on biased word embeddings is potentially
the most impactful first step due to their universal nature. Here we seek to
understand how the intrinsic properties of word embeddings contribute to this
observed marked attribute effect, and whether current post-processing methods
address the bias successfully. An investigation of the current debiasing
landscape reveals two open problems: none of the current debiased embeddings
mitigate the marked attribute error, and none of the intrinsic bias measures
are predictive of the marked attribute effect. By noticing that a new type of
intrinsic bias measure correlates meaningfully with the marked attribute
effect, we propose a new postprocessing debiasing scheme for static word
embeddings. The proposed method applied to existing embeddings achieves new
best results on the marked attribute bias test set. See
https://github.com/hillary-dawkins/MAB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution. (arXiv:2109.14047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14047">
<div class="article-summary-box-inner">
<span><p>We observe an instance of gender-induced bias in a downstream application,
despite the absence of explicit gender words in the test cases. We provide a
test set, SoWinoBias, for the purpose of measuring such latent gender bias in
coreference resolution systems. We evaluate the performance of current
debiasing methods on the SoWinoBias test set, especially in reference to the
method's design and altered embedding space properties. See
https://github.com/hillarydawkins/SoWinoBias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating Summaries for Scientific Paper Review. (arXiv:2109.14059v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14059">
<div class="article-summary-box-inner">
<span><p>The review process is essential to ensure the quality of publications.
Recently, the increase of submissions for top venues in machine learning and
NLP has caused a problem of excessive burden on reviewers and has often caused
concerns regarding how this may not only overload reviewers, but also may
affect the quality of the reviews. An automatic system for assisting with the
reviewing process could be a solution for ameliorating the problem. In this
paper, we explore automatic review summary generation for scientific papers. We
posit that neural language models have the potential to be valuable candidates
for this task. In order to test this hypothesis, we release a new dataset of
scientific papers and their reviews, collected from papers published in the
NeurIPS conference from 2013 to 2020. We evaluate state of the art neural
summarization models, present initial results on the feasibility of automatic
review summary generation, and propose directions for the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAFT: A Real-World Few-Shot Text Classification Benchmark. (arXiv:2109.14076v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14076">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models have shown promise for few-shot learning,
completing text-based tasks given only a few task-specific examples. Will
models soon solve classification tasks that have so far been reserved for human
research assistants? Existing benchmarks are not designed to measure progress
in applied settings, and so don't directly answer this question. The RAFT
benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring
tasks and uses an evaluation setup that mirrors deployment. Baseline
evaluations on RAFT reveal areas current techniques struggle with: reasoning
over long texts and tasks with many classes. Human baselines show that some
classification tasks are difficult for non-expert humans, reflecting that
real-world value sometimes depends on domain expertise. Yet even non-expert
human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets
and leaderboard will track which model improvements translate into real-world
benefits at https://raft.elicit.org .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14084">
<div class="article-summary-box-inner">
<span><p>We present VideoCLIP, a contrastive approach to pre-train a unified model for
zero-shot video and text understanding, without using any labels on downstream
tasks. VideoCLIP trains a transformer for video and text by contrasting
temporally overlapping positive video-text pairs with hard negatives from
nearest neighbor retrieval. Our experiments on a diverse series of downstream
tasks, including sequence-level text-video retrieval, VideoQA, token-level
action localization, and action segmentation reveal state-of-the-art
performance, surpassing prior work, and in some cases even outperforming
supervised approaches. Code is made available at
https://github.com/pytorch/fairseq/examples/MMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Video-Language Segmentation. (arXiv:2109.14131v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14131">
<div class="article-summary-box-inner">
<span><p>We focus on the problem of segmenting a certain object referred by a natural
language sentence in video content, at the core of formulating a pinpoint
vision-language relation. While existing attempts mainly construct such
relation in an implicit way, i.e., grid-level multi-modal feature fusion, it
has been proven problematic to distinguish semantically similar objects under
this paradigm. In this work, we propose to interwind the visual and linguistic
modalities in an explicit way via the contrastive learning objective, which
directly aligns the referred object and the language description and separates
the unreferred content apart across frames. Moreover, to remedy for the
degradation problem, we present two complementary hard instance mining
strategies, i.e., Language-relevant Channel Filter and Relative Hard Instance
Construction. They encourage the network to exclude visual-distinguishable
feature and to focus on easy-confused objects during the contrastive training.
Extensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB
Sentences, quantitatively demonstrate the state-of-the-arts performance of our
method and qualitatively show the more accurate distinguishment between
semantically similar objects over baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Dialogue State Tracking by Joint Slot Modeling. (arXiv:2109.14144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14144">
<div class="article-summary-box-inner">
<span><p>Dialogue state tracking models play an important role in a task-oriented
dialogue system. However, most of them model the slot types conditionally
independently given the input. We discover that it may cause the model to be
confused by slot types that share the same data type. To mitigate this issue,
we propose TripPy-MRF and TripPy-LSTM that models the slots jointly. Our
results show that they are able to alleviate the confusion mentioned above, and
they push the state-of-the-art on dataset MultiWoZ 2.1 from 58.7 to 61.3. Our
implementation is available at https://github.com/CTinRay/Trippy-Joint.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Arabic Diacritization by Learning to Diacritize and Translate. (arXiv:2109.14150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14150">
<div class="article-summary-box-inner">
<span><p>We propose a novel multitask learning method for diacritization which trains
a model to both diacritize and translate. Our method addresses data sparsity by
exploiting large, readily available bitext corpora. Furthermore, translation
requires implicit linguistic and semantic knowledge, which is helpful for
resolving ambiguities in the diacritization task. We apply our method to the
Penn Arabic Treebank and report a new state-of-the-art word error rate of
4.79%. We also conduct manual and automatic analysis to better understand our
method and highlight some of the remaining challenges in diacritization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reflexivity in Issues of Scale and Representation in a Digital Humanities Project. (arXiv:2109.14184v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14184">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore issues that we have encountered in developing a
pipeline that combines natural language processing with data analysis and
visualization techniques. The characteristics of the corpus - being comprised
of diaries of a single person spanning several decades - present both
conceptual challenges in terms of issues of representation, and affordances as
a source for historical research. We consider these issues in a team context
with a particular focus on the generation and interpretation of visualizations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context based Roman-Urdu to Urdu Script Transliteration System. (arXiv:2109.14197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14197">
<div class="article-summary-box-inner">
<span><p>Now a day computer is necessary for human being and it is very useful in many
fields like search engine, text processing, short messaging services, voice
chatting and text recognition. Since last many years there are many tools and
techniques that have been developed to support the writing of language script.
Most of the Asian languages like Arabic, Urdu, Persian, Chains and Korean are
written in Roman alphabets. Roman alphabets are the most commonly used for
transliteration of languages, which have non-Latin scripts. For writing Urdu
characters as an input, there are many layouts which are already exist. Mostly
Urdu speaker prefer to use Roman-Urdu for different applications, because
mostly user is not familiar with Urdu language keyboard. The objective of this
work is to improve the context base transliteration of Roman-Urdu to Urdu
script. In this paper, we propose an algorithm which effectively solve the
transliteration issues. The algorithm work like, convert the encoding roman
words into the words in the standard Urdu script and match it with the lexicon.
If match found, then display the word in the text editor. The highest frequency
words are displayed if more than one match found in the lexicon. Display the
first encoded and converted instance and set it to the default if there is not
a single instance of the match is found and then adjust the given ambiguous
word to their desire location according to their context. The outcome of this
algorithm proved the efficiency and significance as compare to other models and
algorithms which work for transliteration of Raman-Urdu to Urdu on context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Who says like a style of Vitamin: Towards Syntax-Aware DialogueSummarization using Multi-task Learning. (arXiv:2109.14199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14199">
<div class="article-summary-box-inner">
<span><p>Abstractive dialogue summarization is a challenging task for several reasons.
First, most of the important pieces of information in a conversation are
scattered across utterances through multi-party interactions with different
textual styles. Second, dialogues are often informal structures, wherein
different individuals express personal perspectives, unlike text summarization,
tasks that usually target formal documents such as news articles. To address
these issues, we focused on the association between utterances from individual
speakers and unique syntactic structures. Speakers have unique textual styles
that can contain linguistic information, such as voiceprint. Therefore, we
constructed a syntax-aware model by leveraging linguistic information (i.e.,
POS tagging), which alleviates the above issues by inherently distinguishing
sentences uttered from individual speakers. We employed multi-task learning of
both syntax-aware information and dialogue summarization. To the best of our
knowledge, our approach is the first method to apply multi-task learning to the
dialogue summarization task. Experiments on a SAMSum corpus (a large-scale
dialogue summarization corpus) demonstrated that our method improved upon the
vanilla model. We further analyze the costs and benefits of our approach
relative to baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation. (arXiv:2109.14200v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14200">
<div class="article-summary-box-inner">
<span><p>Decades of research has studied how language learning infants learn to
discriminate speech sounds, segment words, and associate words with their
meanings. While gradual development of such capabilities is unquestionable, the
exact nature of these skills and the underlying mental representations yet
remains unclear. In parallel, computational studies have shown that basic
comprehension of speech can be achieved by statistical learning between speech
and concurrent referentially ambiguous visual input. These models can operate
without prior linguistic knowledge such as representations of linguistic units,
and without learning mechanisms specifically targeted at such units. This has
raised the question of to what extent knowledge of linguistic units, such as
phone(me)s, syllables, and words, could actually emerge as latent
representations supporting the translation between speech and representations
in other modalities, and without the units being proximal learning targets for
the learner. In this study, we formulate this idea as the so-called latent
language hypothesis (LLH), connecting linguistic representation learning to
general predictive processing within and across sensory modalities. We review
the extent that the audiovisual aspect of LLH is supported by the existing
computational studies. We then explore LLH further in extensive learning
simulations with different neural network models for audiovisual
cross-situational learning, and comparing learning from both synthetic and real
speech data. We investigate whether the latent representations learned by the
networks reflect phonetic, syllabic, or lexical structure of input speech by
utilizing an array of complementary evaluation metrics related to linguistic
selectivity and temporal characteristics of the representations. As a result,
we find that representations associated...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BLEU, METEOR, BERTScore: Evaluation of Metrics Performance in Assessing Critical Translation Errors in Sentiment-oriented Text. (arXiv:2109.14250v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14250">
<div class="article-summary-box-inner">
<span><p>Social media companies as well as authorities make extensive use of
artificial intelligence (AI) tools to monitor postings of hate speech,
celebrations of violence or profanity. Since AI software requires massive
volumes of data to train computers, Machine Translation (MT) of the online
content is commonly used to process posts written in several languages and
hence augment the data needed for training. However, MT mistakes are a regular
occurrence when translating sentiment-oriented user-generated content (UGC),
especially when a low-resource language is involved. The adequacy of the whole
process relies on the assumption that the evaluation metrics used give a
reliable indication of the quality of the translation. In this paper, we assess
the ability of automatic quality metrics to detect critical machine translation
errors which can cause serious misunderstanding of the affect message. We
compare the performance of three canonical metrics on meaningless translations
where the semantic content is seriously impaired as compared to meaningful
translations with a critical error which exclusively distorts the sentiment of
the source text. We conclude that there is a need for fine-tuning of automatic
metrics to make them more robust in detecting sentiment critical errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Character Tagger for Short Text Spelling Error Correction. (arXiv:2109.14259v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14259">
<div class="article-summary-box-inner">
<span><p>State-of-the-art approaches to spelling error correction problem include
Transformer-based Seq2Seq models, which require large training sets and suffer
from slow inference time; and sequence labeling models based on Transformer
encoders like BERT, which involve token-level label space and therefore a large
pre-defined vocabulary dictionary. In this paper we present a Hierarchical
Character Tagger model, or HCTagger, for short text spelling error correction.
We use a pre-trained language model at the character level as a text encoder,
and then predict character-level edits to transform the original text into its
error-free form with a much smaller label space. For decoding, we propose a
hierarchical multi-task approach to alleviate the issue of long-tail label
distribution without introducing extra model parameters. Experiments on two
public misspelling correction datasets demonstrate that HCTagger is an accurate
and much faster approach than many existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Call Larisa Ivanovna: Code-Switching Fools Multilingual NLU Models. (arXiv:2109.14350v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14350">
<div class="article-summary-box-inner">
<span><p>Practical needs of developing task-oriented dialogue assistants require the
ability to understand many languages. Novel benchmarks for multilingual natural
language understanding (NLU) include monolingual sentences in several
languages, annotated with intents and slots. In such setup models for
cross-lingual transfer show remarkable performance in joint intent recognition
and slot filling. However, existing benchmarks lack of code-switched
utterances, which are difficult to gather and label due to complexity in the
grammatical structure. The evaluation of NLU models seems biased and limited,
since code-switching is being left out of scope.
</p>
<p>Our work adopts recognized methods to generate plausible and
naturally-sounding code-switched utterances and uses them to create a synthetic
code-switched test set. Based on experiments, we report that the
state-of-the-art NLU models are unable to handle code-switching. At worst, the
performance, evaluated by semantic accuracy, drops as low as 15\% from 80\%
across languages. Further we show, that pre-training on synthetic code-mixed
data helps to maintain performance on the proposed test set at a comparable
level with monolingual data. Finally, we analyze different language pairs and
show that the closer the languages are, the better the NLU model handles their
alternation. This is in line with the common understanding of how multilingual
models conduct transferring between languages
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Fact Linking. (arXiv:2109.14364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14364">
<div class="article-summary-box-inner">
<span><p>Knowledge-intensive NLP tasks can benefit from linking natural language text
with facts from a Knowledge Graph (KG). Although facts themselves are
language-agnostic, the fact labels (i.e., language-specific representation of
the fact) in the KG are often present only in a few languages. This makes it
challenging to link KG facts to sentences in languages other than the limited
set of languages. To address this problem, we introduce the task of
Multilingual Fact Linking (MFL) where the goal is to link fact expressed in a
sentence to corresponding fact in the KG, even when the fact label in the KG is
not available in the language of the sentence. To facilitate research in this
area, we present a new evaluation dataset, IndicLink. This dataset contains
11,293 linked WikiData facts and 6,429 sentences spanning English and six
Indian languages. We propose a Retrieval+Generation model, ReFCoG, that can
scale to millions of KG facts by combining Dual Encoder based retrieval with a
Seq2Seq based generation model which is constrained to output only valid KG
facts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in
Precision@1. In spite of this gain, the model achieves an overall score of
52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink
data are available at https://github.com/SaiKeshav/mfl
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EdinSaar@WMT21: North-Germanic Low-Resource Multilingual NMT. (arXiv:2109.14368v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14368">
<div class="article-summary-box-inner">
<span><p>We describe the EdinSaar submission to the shared task of Multilingual
Low-Resource Translation for North Germanic Languages at the Sixth Conference
on Machine Translation (WMT2021). We submit multilingual translation models for
translations to/from Icelandic (is), Norwegian-Bokmal (nb), and Swedish (sv).
We employ various experimental approaches, including multilingual pre-training,
back-translation, fine-tuning, and ensembling. In most translation directions,
our models outperform other submitted systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EDGAR-CORPUS: Billions of Tokens Make The World Go Round. (arXiv:2109.14394v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14394">
<div class="article-summary-box-inner">
<span><p>We release EDGAR-CORPUS, a novel corpus comprising annual reports from all
the publicly traded companies in the US spanning a period of more than 25
years. To the best of our knowledge, EDGAR-CORPUSis the largest financial NLP
corpus available to date. All the reports are downloaded, split into their
corresponding items (sections), and provided in a clean, easy-to-use JSON
format. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC
embeddings for the financial domain. We employ these embeddings in a battery of
financial NLP tasks and showcase their superiority over generic GloVe
embeddings and other existing financial word embeddings. We also open-source
EDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future
annual reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StoryDB: Broad Multi-language Narrative Dataset. (arXiv:2109.14396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14396">
<div class="article-summary-box-inner">
<span><p>This paper presents StoryDB - a broad multi-language dataset of narratives.
StoryDB is a corpus of texts that includes stories in 42 different languages.
Every language includes 500+ stories. Some of the languages include more than
20 000 stories. Every story is indexed across languages and labeled with tags
such as a genre or a topic. The corpus shows rich topical and language
variation and can serve as a resource for the study of the role of narrative in
natural language processing across various languages including low resource
ones. We also demonstrate how the dataset could be used to benchmark three
modern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiQUE: Biquaternionic Embeddings of Knowledge Graphs. (arXiv:2109.14401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14401">
<div class="article-summary-box-inner">
<span><p>Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge
graphs (KGs). Existing KGE models rely on geometric operations to model
relational patterns. Euclidean (circular) rotation is useful for modeling
patterns such as symmetry, but cannot represent hierarchical semantics. In
contrast, hyperbolic models are effective at modeling hierarchical relations,
but do not perform as well on patterns on which circular rotation excels. It is
crucial for KGE models to unify multiple geometric transformations so as to
fully cover the multifarious relations in KGs. To do so, we propose BiQUE, a
novel model that employs biquaternions to integrate multiple geometric
transformations, viz., scaling, translation, Euclidean rotation, and hyperbolic
rotation. BiQUE makes the best trade-offs among geometric operators during
training, picking the best one (or their best combination) for each relation.
Experiments on five datasets show BiQUE's effectiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14420">
<div class="article-summary-box-inner">
<span><p>Error correction is widely used in automatic speech recognition (ASR) to
post-process the generated sentence, and can further reduce the word error rate
(WER). Although multiple candidates are generated by an ASR system through beam
search, current error correction approaches can only correct one sentence at a
time, failing to leverage the voting effect from multiple candidates to better
detect and correct error tokens. In this work, we propose FastCorrect 2, an
error correction model that takes multiple ASR candidates as input for better
correction accuracy. FastCorrect 2 adopts non-autoregressive generation for
fast inference, which consists of an encoder that processes multiple source
sentences and a decoder that generates the target sentence in parallel from the
adjusted source sentence, where the adjustment is based on the predicted
duration of each source token. However, there are some issues when handling
multiple source sentences. First, it is non-trivial to leverage the voting
effect from multiple source sentences since they usually vary in length. Thus,
we propose a novel alignment algorithm to maximize the degree of token
alignment among multiple sentences in terms of token and pronunciation
similarity. Second, the decoder can only take one adjusted source sentence as
input, while there are multiple source sentences. Thus, we develop a candidate
predictor to detect the most suitable candidate for the decoder. Experiments on
our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce
the WER over the previous correction model with single candidate by 3.2% and
2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR
error correction. FastCorrect 2 achieves better performance than the cascaded
re-scoring and correction pipeline and can serve as a unified post-processing
module for ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the Arabic Sentiment Analysis 2021 Competition at KAUST. (arXiv:2109.14456v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14456">
<div class="article-summary-box-inner">
<span><p>This paper provides an overview of the Arabic Sentiment Analysis Challenge
organized by King Abdullah University of Science and Technology (KAUST). The
task in this challenge is to develop machine learning models to classify a
given tweet into one of the three categories Positive, Negative, or Neutral.
From our recently released ASAD dataset, we provide the competitors with 55K
tweets for training, and 20K tweets for validation, based on which the
performance of participating teams are ranked on a leaderboard,
https://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust. The competition
received in total 1247 submissions from 74 teams (99 team members). The final
winners are determined by another private set of 20K tweets that have the same
distribution as the training and validation set. In this paper, we present the
main findings in the competition and summarize the methods and tools used by
the top ranked teams. The full dataset of 100K labeled tweets is also released
for public usage, at
https://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust/data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition. (arXiv:2004.10663v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.10663">
<div class="article-summary-box-inner">
<span><p>We present a fast and scalable architecture called Explicit Modular
Decomposition (EMD), in which we incorporate both classification-based and
extraction-based methods and design four modules (for classification and
sequence labelling) to jointly extract dialogue states. Experimental results
based on the MultiWoz 2.0 dataset validates the superiority of our proposed
model in terms of both complexity and scalability when compared to the
state-of-the-art methods, especially in the scenario of multi-domain dialogues
entangled with many turns of utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems. (arXiv:2010.05740v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05740">
<div class="article-summary-box-inner">
<span><p>Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue
systems is challenging, since it requires to properly represent the entity of
KB, which is associated with its KB context and dialogue context. The existing
works represent the entity with only perceiving a part of its KB context, which
can lead to the less effective representation due to the information loss, and
adversely favor KB reasoning and response generation. To tackle this issue, we
explore to fully contextualize the entity representation by dynamically
perceiving all the relevant entities} and dialogue history. To achieve this, we
propose a COntext-aware Memory Enhanced Transformer framework (COMET), which
treats the KB as a sequence and leverages a novel Memory Mask to enforce the
entity to only focus on its relevant entities and dialogue history, while
avoiding the distraction from the irrelevant entities. Through extensive
experiments, we show that our COMET framework can achieve superior performance
over the state of the arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Influence Patterns for Explaining Information Flow in BERT. (arXiv:2011.00740v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.00740">
<div class="article-summary-box-inner">
<span><p>While attention is all you need may be proving true, we do not know why:
attention-based transformer models such as BERT are superior but how
information flows from input tokens to output predictions are unclear. We
introduce influence patterns, abstractions of sets of paths through a
transformer model. Patterns quantify and localize the flow of information to
paths passing through a sequence of model nodes. Experimentally, we find that
significant portion of information flow in BERT goes through skip connections
instead of attention heads. We further show that consistency of patterns across
instances is an indicator of BERT's performance. Finally, We demonstrate that
patterns account for far more model performance than previous attention-based
and layer-based methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Decomposing and Recomposing Event Structure. (arXiv:2103.10387v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10387">
<div class="article-summary-box-inner">
<span><p>We present an event structure classification empirically derived from
inferential properties annotated on sentence- and document-level Universal
Decompositional Semantics (UDS) graphs. We induce this classification jointly
with semantic role, entity, and event-event relation classifications using a
document-level generative model structured by these graphs. To support this
induction, we augment existing annotations found in the UDS1.0 dataset, which
covers the entirety of the English Web Treebank, with an array of inferential
properties capturing fine-grained aspects of the temporal and aspectual
structure of events. The resulting dataset (available at decomp.io) is the
largest annotation of event structure and (partial) event coreference to date.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09996">
<div class="article-summary-box-inner">
<span><p>We present a simplified, task-agnostic multi-modal pre-training approach that
can accept either video or text input, or both for a variety of end tasks.
Existing pre-training are task-specific by adopting either a single cross-modal
encoder that requires both modalities, limiting their use for retrieval-style
end tasks or more complex multitask learning with two unimodal encoders,
limiting early cross-modal fusion. We instead introduce new pretraining masking
schemes that better mix across modalities (e.g. by forcing masks for text to
predict the closest video embeddings) while also maintaining separability (e.g.
unimodal predictions are sometimes required, without using all the input).
Experimental results show strong performance across a wider range of tasks than
any previous methods, often outperforming task-specific pre-training. Code is
made available at https://github.com/pytorch/fairseq/examples/MMPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Various Tokenizers for Arabic Text Classification. (arXiv:2106.07540v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07540">
<div class="article-summary-box-inner">
<span><p>The first step in any NLP pipeline is to split the text into individual
tokens. The most obvious and straightforward approach is to use words as
tokens. However, given a large text corpus, representing all the words is not
efficient in terms of vocabulary size. In the literature, many tokenization
algorithms have emerged to tackle this problem by creating subwords which in
turn limits the vocabulary size in a given text corpus. Most tokenization
techniques are language-agnostic i.e they don't incorporate the linguistic
features of a given language. Not to mention the difficulty of evaluating such
techniques in practice. In this paper, we introduce three new tokenization
algorithms for Arabic and compare them to three other baselines using
unsupervised evaluations. In addition to that, we compare all the six
algorithms by evaluating them on three supervised classification tasks which
are sentiment analysis, news classification and poetry classification using six
publicly available datasets. Our experiments show that none of the tokenization
technique is the best choice overall and that the performance of a given
tokenization algorithm depends on the size of the dataset, type of the task,
and the amount of morphology that exists in the dataset. However, some
tokenization techniques are better overall as compared to others on various
text classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07498">
<div class="article-summary-box-inner">
<span><p>Pretrained Language Models (PLMs) have achieved tremendous success in natural
language understanding tasks. While different learning schemes -- fine-tuning,
zero-shot, and few-shot learning -- have been widely explored and compared for
languages such as English, there is comparatively little work in Chinese to
fairly and comprehensively evaluate and compare these methods and thus hinders
cumulative progress. In this paper, we introduce the Chinese Few-shot Learning
Evaluation Benchmark (FewCLUE), the first comprehensive few-shot evaluation
benchmark in Chinese. It includes nine tasks, ranging from single-sentence and
sentence-pair classification tasks to machine reading comprehension tasks. We
systematically evaluate five state-of-the-art (SOTA) few-shot learning methods
(including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their
performance with fine-tuning and zero-shot learning schemes on the newly
constructed FewCLUE benchmark. Experimental results reveal that: 1) The effect
of different few-shot learning methods is sensitive to the pre-trained model to
which the methods are applied; 2) PET and P-tuning achieve the best overall
performance with RoBERTa and ERNIE respectively. Our benchmark is used in the
few-shot learning contest of NLPCC 2021. In addition, we provide a
user-friendly toolkit, as well as an online leaderboard to help facilitate
further progress on Chinese few-shot learning. We provide a baseline
performance on different learning methods, a reference for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter. (arXiv:2108.04816v2 [cs.SI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04816">
<div class="article-summary-box-inner">
<span><p>The understanding of the public response to COVID-19 vaccines is the key
success factor to control the COVID-19 pandemic. To understand the public
response, there is a need to explore public opinion. Traditional surveys are
expensive and time-consuming, address limited health topics, and obtain
small-scale data. Twitter can provide a great opportunity to understand public
opinion regarding COVID-19 vaccines. The current study proposes an approach
using computational and human coding methods to collect and analyze a large
number of tweets to provide a wider perspective on the COVID-19 vaccine. This
study identifies the sentiment of tweets using a machine learning rule-based
approach, discovers major topics, explores temporal trend and compares topics
of negative and non-negative tweets using statistical tests, and discloses top
topics of tweets having negative and non-negative sentiment. Our findings show
that the negative sentiment regarding the COVID-19 vaccine had a decreasing
trend between November 2020 and February 2021. We found Twitter users have
discussed a wide range of topics from vaccination sites to the 2020 U.S.
election between November 2020 and February 2021. The findings show that there
was a significant difference between tweets having negative and non-negative
sentiment regarding the weight of most topics. Our results also indicate that
the negative and non-negative tweets had different topic priorities and
focuses. This research illustrates that Twitter data can be used to explore
public opinion regarding the COVID-19 vaccine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13602">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) is one of the most reliable methods for defending
against adversarial attacks in machine learning. Variants of this method have
been used as regularization mechanisms to achieve SOTA results on NLP
benchmarks, and they have been found to be useful for transfer learning and
continual learning. We search for the reasons for the effectiveness of AT by
contrasting vanilla and adversarially fine-tuned BERT models. We identify
partial preservation of BERT's syntactic abilities during fine-tuning as the
key to the success of AT. We observe that adversarially fine-tuned models
remain more faithful to BERT's language modeling behavior and are more
sensitive to the word order. As concrete examples of syntactic abilities, an
adversarially fine-tuned model could have an advantage of up to 38% on anaphora
agreement and up to 11% on dependency parsing. Our analysis demonstrates that
vanilla fine-tuning oversimplifies the sentence representation by focusing
heavily on a small subset of words. AT, however, moderates the effect of these
influential words and encourages representational diversity. This allows for a
more hierarchical representation of a sentence and leads to the mitigation of
BERT's loss of syntactic abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13897">
<div class="article-summary-box-inner">
<span><p>The MS MARCO ranking dataset has been widely used for training deep learning
models for IR tasks, achieving considerable effectiveness on diverse zero-shot
scenarios. However, this type of resource is scarce in other languages than
English. In this work we present mMARCO, a multilingual version of the MS MARCO
passage ranking dataset comprising 8 languages that was created using machine
translation. We evaluated mMARCO by fine-tuning mono and multilingual
re-ranking models on it. Experimental results demonstrate that multilingual
models fine-tuned on our translated dataset achieve superior effectiveness than
models fine-tuned on the original English version alone. Also, our distilled
multilingual re-ranker is competitive with non-distilled models while having
5.4 times fewer parameters. The translated datasets as well as fine-tuned
models are available at https://github.com/unicamp-dl/mMARCO.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patterns of Lexical Ambiguity in Contextualised Language Models. (arXiv:2109.13032v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13032">
<div class="article-summary-box-inner">
<span><p>One of the central aspects of contextualised language models is that they
should be able to distinguish the meaning of lexically ambiguous words by their
contexts. In this paper we investigate the extent to which the contextualised
embeddings of word forms that display multiplicity of sense reflect traditional
distinctions of polysemy and homonymy. To this end, we introduce an extended,
human-annotated dataset of graded word sense similarity and co-predication
acceptability, and evaluate how well the similarity of embeddings predicts
similarity in meaning. Both types of human judgements indicate that the
similarity of polysemic interpretations falls in a continuum between identity
of meaning and homonymy. However, we also observe significant differences
within the similarity ratings of polysemes, forming consistent patterns for
different types of polysemic sense alternation. Our dataset thus appears to
capture a substantial part of the complexity of lexical ambiguity, and can
provide a realistic test bed for contextualised embeddings. Among the tested
models, BERT Large shows the strongest correlation with the collected word
sense similarity ratings, but struggles to consistently replicate the observed
similarity patterns. When clustering ambiguous word forms based on their
embeddings, the model displays high confidence in discerning homonyms and some
types of polysemic alternations, but consistently fails for others.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-30 23:09:14.372750144 UTC">2021-09-30 23:09:14 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>