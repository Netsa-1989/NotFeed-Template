<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-02T01:30:00Z">09-02</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Working Memory Connections for LSTM. (arXiv:2109.00020v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00020">
<div class="article-summary-box-inner">
<span><p>Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of
gating mechanisms to mitigate exploding and vanishing gradients when learning
long-term dependencies. For this reason, LSTMs and other gated RNNs are widely
adopted, being the standard de facto for many sequence modeling tasks. Although
the memory cell inside the LSTM contains essential information, it is not
allowed to influence the gating mechanism directly. In this work, we improve
the gate potential by including information coming from the internal cell
state. The proposed modification, named Working Memory Connection, consists in
adding a learnable nonlinear projection of the cell content into the network
gates. This modification can fit into the classical LSTM gates without any
assumption on the underlying task, being particularly effective when dealing
with longer sequences. Previous research effort in this direction, which goes
back to the early 2000s, could not bring a consistent improvement over vanilla
LSTM. As part of this paper, we identify a key issue tied to previous
connections that heavily limits their effectiveness, hence preventing a
successful integration of the knowledge coming from the internal cell state. We
show through extensive experimental evaluation that Working Memory Connections
constantly improve the performance of LSTMs on a variety of tasks. Numerical
results suggest that the cell state contains useful information that is worth
including in the gate structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Machine-Learning media bias. (arXiv:2109.00024v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00024">
<div class="article-summary-box-inner">
<span><p>We present an automated method for measuring media bias. Inferring which
newspaper published a given article, based only on the frequencies with which
it uses different phrases, leads to a conditional probability distribution
whose analysis lets us automatically map newspapers and phrases into a bias
space. By analyzing roughly a million articles from roughly a hundred
newspapers for bias in dozens of news topics, our method maps newspapers into a
two-dimensional bias landscape that agrees well with previous bias
classifications based on human judgement. One dimension can be interpreted as
traditional left-right bias, the other as establishment bias. This means that
although news bias is inherently political, its measurement need not be.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sense representations for Portuguese: experiments with sense embeddings and deep neural language models. (arXiv:2109.00025v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00025">
<div class="article-summary-box-inner">
<span><p>Sense representations have gone beyond word representations like Word2Vec,
GloVe and FastText and achieved innovative performance on a wide range of
natural language processing tasks. Although very useful in many applications,
the traditional approaches for generating word embeddings have a strict
drawback: they produce a single vector representation for a given word ignoring
the fact that ambiguous words can assume different meanings. In this paper, we
explore unsupervised sense representations which, different from traditional
word embeddings, are able to induce different senses of a word by analyzing its
contextual semantics in a text. The unsupervised sense representations
investigated in this paper are: sense embeddings and deep neural language
models. We present the first experiments carried out for generating sense
embeddings for Portuguese. Our experiments show that the sense embedding model
(Sense2vec) outperformed traditional word embeddings in syntactic and semantic
analogies task, proving that the language resource generated here can improve
the performance of NLP tasks in Portuguese. We also evaluated the performance
of pre-trained deep neural language models (ELMo and BERT) in two transfer
learning approaches: feature based and fine-tuning, in the semantic textual
similarity task. Our experiments indicate that the fine tuned Multilingual and
Portuguese BERT language models were able to achieve better accuracy than the
ELMo model and baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00055">
<div class="article-summary-box-inner">
<span><p>Representation learning for text via pretraining a language model on a large
corpus has become a standard starting point for building NLP systems. This
approach stands in contrast to autoencoders, also trained on raw text, but with
the objective of learning to encode each input as a vector that allows full
reconstruction. Autoencoders are attractive because of their latent space
structure and generative properties. We therefore explore the construction of a
sentence-level autoencoder from a pretrained, frozen transformer language
model. We adapt the masked language modeling objective as a generative,
denoising one, while only training a sentence bottleneck and a single-layer
modified transformer decoder. We demonstrate that the sentence representations
discovered by our model achieve better quality than previous methods that
extract representations from pretrained transformers on text similarity tasks,
style transfer (an example of controlled generation), and single-sentence
classification tasks in the GLUE benchmark, while using fewer parameters than
large pretrained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effectiveness of Deep Networks in NLP using BiDAF as an example architecture. (arXiv:2109.00074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00074">
<div class="article-summary-box-inner">
<span><p>Question Answering with NLP has progressed through the evolution of advanced
model architectures like BERT and BiDAF and earlier word, character, and
context-based embeddings. As BERT has leapfrogged the accuracy of models, an
element of the next frontier can be the introduction of deep networks and an
effective way to train them. In this context, I explored the effectiveness of
deep networks focussing on the model encoder layer of BiDAF. BiDAF with its
heterogeneous layers provides the opportunity not only to explore the
effectiveness of deep networks but also to evaluate whether the refinements
made in lower layers are additive to the refinements made in the upper layers
of the model architecture. I believe the next greatest model in NLP will in
fact fold in a solid language modeling like BERT with a composite architecture
which will bring in refinements in addition to generic language modeling and
will have a more extensive layered architecture. I experimented with the Bypass
network, Residual Highway network, and DenseNet architectures. In addition, I
evaluated the effectiveness of ensembling the last few layers of the network. I
also studied the difference character embeddings make in adding them to the
word embeddings, and whether the effects are additive with deep networks. My
studies indicate that deep networks are in fact effective in giving a boost.
Also, the refinements in the lower layers like embeddings are passed on
additively to the gains made through deep networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactive Machine Comprehension with Dynamic Knowledge Graphs. (arXiv:2109.00077v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00077">
<div class="article-summary-box-inner">
<span><p>Interactive machine reading comprehension (iMRC) is machine comprehension
tasks where knowledge sources are partially observable. An agent must interact
with an environment sequentially to gather necessary knowledge in order to
answer a question. We hypothesize that graph representations are good inductive
biases, which can serve as an agent's memory mechanism in iMRC tasks. We
explore four different categories of graphs that can capture text information
at various levels. We describe methods that dynamically build and update these
graphs during information gathering, as well as neural models to encode graph
representations in RL agents. Extensive experiments on iSQuAD suggest that
graph representations can result in significant performance improvements for RL
agents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v1 [cs.SE])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00084">
<div class="article-summary-box-inner">
<span><p>Collaborative software development is an integral part of the modern software
development life cycle, essential to the success of large-scale software
projects. When multiple developers make concurrent changes around the same
lines of code, a merge conflict may occur. Such conflicts stall pull requests
and continuous integration pipelines for hours to several days, seriously
hurting developer productivity.
</p>
<p>In this paper, we introduce MergeBERT, a novel neural program merge framework
based on the token-level three-way differencing and a transformer encoder
model. Exploiting restricted nature of merge conflict resolutions, we
reformulate the task of generating the resolution sequence as a classification
task over a set of primitive merge patterns extracted from real-world merge
commit data.
</p>
<p>Our model achieves 64--69% precision of merge resolution synthesis, yielding
nearly a 2x performance improvement over existing structured and neural program
merge tools. Finally, we demonstrate versatility of our model, which is able to
perform program merge in a multilingual setting with Java, JavaScript,
TypeScript, and C# programming languages, generalizing zero-shot to unseen
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It's not Rocket Science : Interpreting Figurative Language in Narratives. (arXiv:2109.00087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00087">
<div class="article-summary-box-inner">
<span><p>Figurative language is ubiquitous in English. Yet, the vast majority of NLP
research focuses on literal language. Existing text representations by design
rely on compositionality, while figurative language is often non-compositional.
In this paper, we study the interpretation of two non-compositional figurative
languages (idioms and similes). We collected datasets of fictional narratives
containing a figurative expression along with crowd-sourced plausible and
implausible continuations relying on the correct interpretation of the
expression. We then trained models to choose or generate the plausible
continuation. Our experiments show that models based solely on pre-trained
language models perform substantially worse than humans on these tasks. We
additionally propose knowledge-enhanced models, adopting human strategies for
interpreting figurative language: inferring meaning from the context and
relying on the constituent word's literal meanings. The knowledge-enhanced
models improve the performance on both the discriminative and generative tasks,
further bridging the gap from human performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00122">
<div class="article-summary-box-inner">
<span><p>The sheer volume of financial statements makes it difficult for humans to
access and analyze a business's financials. Robust numerical reasoning likewise
faces unique challenges in this domain. In this work, we focus on answering
deep questions over financial data, aiming to automate the analysis of a large
corpus of financial documents. In contrast to existing tasks on general domain,
the finance domain includes complex numerical reasoning and understanding of
heterogeneous representations. To facilitate analytical progress, we propose a
new large-scale dataset, FinQA, with Question-Answering pairs over Financial
reports, written by financial experts. We also annotate the gold reasoning
programs to ensure full explainability. We further introduce baselines and
conduct comprehensive experiments in our dataset. The results demonstrate that
popular, large, pre-trained models fall far short of expert humans in acquiring
finance knowledge and in complex multi-step numerical reasoning on that
knowledge. Our dataset -- the first of its kind -- should therefore enable
significant, new community research into complex application domains. The
dataset and code are publicly available\url{https://github.com/czyssrs/FinQA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Unsupervised Method for Building Sentence Simplification Corpora in Multiple Languages. (arXiv:2109.00165v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00165">
<div class="article-summary-box-inner">
<span><p>The availability of parallel sentence simplification (SS) is scarce for
neural SS modelings. We propose an unsupervised method to build SS corpora from
large-scale bilingual translation corpora, alleviating the need for SS
supervised corpora. Our method is motivated by the following two findings:
neural machine translation model usually tends to generate more high-frequency
tokens and the difference of text complexity levels exists between the source
and target language of a translation corpus. By taking the pair of the source
sentences of translation corpus and the translations of their references in a
bridge language, we can construct large-scale pseudo parallel SS data. Then, we
keep these sentence pairs with a higher complexity difference as SS sentence
pairs. The building SS corpora with an unsupervised approach can satisfy the
expectations that the aligned sentences preserve the same meanings and have
difference in text complexity levels. Experimental results show that SS methods
trained by our corpora achieve the state-of-the-art results and significantly
outperform the results on English benchmark WikiLarge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Have Been Learned & What Should Be Learned? An Empirical Study of How to Selectively Augment Text for Classification. (arXiv:2109.00175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00175">
<div class="article-summary-box-inner">
<span><p>Text augmentation techniques are widely used in text classification problems
to improve the performance of classifiers, especially in low-resource
scenarios. Whilst lots of creative text augmentation methods have been
designed, they augment the text in a non-selective manner, which means the less
important or noisy words have the same chances to be augmented as the
informative words, and thereby limits the performance of augmentation. In this
work, we systematically summarize three kinds of role keywords, which have
different functions for text classification, and design effective methods to
extract them from the text. Based on these extracted role keywords, we propose
STA (Selective Text Augmentation) to selectively augment the text, where the
informative, class-indicating words are emphasized but the irrelevant or noisy
words are diminished. Extensive experiments on four English and Chinese text
classification benchmark datasets demonstrate that STA can substantially
outperform the non-selective text augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Problem Learning: Towards the Free Will of Machines. (arXiv:2109.00177v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00177">
<div class="article-summary-box-inner">
<span><p>A machine intelligence pipeline usually consists of six components: problem,
representation, model, loss, optimizer and metric. Researchers have worked hard
trying to automate many components of the pipeline. However, one key component
of the pipeline--problem definition--is still left mostly unexplored in terms
of automation. Usually, it requires extensive efforts from domain experts to
identify, define and formulate important problems in an area. However,
automatically discovering research or application problems for an area is
beneficial since it helps to identify valid and potentially important problems
hidden in data that are unknown to domain experts, expand the scope of tasks
that we can do in an area, and even inspire completely new findings.
</p>
<p>This paper describes Problem Learning, which aims at learning to discover and
define valid and ethical problems from data or from the machine's interaction
with the environment. We formalize problem learning as the identification of
valid and ethical problems in a problem space and introduce several possible
approaches to problem learning. In a broader sense, problem learning is an
approach towards the free will of intelligent machines. Currently, machines are
still limited to solving the problems defined by humans, without the ability or
flexibility to freely explore various possible problems that are even unknown
to humans. Though many machine learning techniques have been developed and
integrated into intelligent systems, they still focus on the means rather than
the purpose in that machines are still solving human defined problems. However,
proposing good problems is sometimes even more important than solving problems,
because a good problem can help to inspire new ideas and gain deeper
understandings. The paper also discusses the ethical implications of problem
learning under the background of Responsible AI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues. (arXiv:2109.00185v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00185">
<div class="article-summary-box-inner">
<span><p>We present an effective system adapted from the end-to-end neural coreference
resolution model, targeting on the task of anaphora resolution in dialogues.
Three aspects are specifically addressed in our approach, including the support
of singletons, encoding speakers and turns throughout dialogue interactions,
and knowledge transfer utilizing existing resources. Despite the simplicity of
our adaptation strategies, they are shown to bring significant impact to the
final performance, with up to 27 F1 improvement over the baseline. Our final
system ranks the 1st place on the leaderboard of the anaphora resolution track
in the CRAC 2021 shared task, and achieves the best evaluation results on all
four datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Predictive Uncertainty under Distributional Shift on Dialogue Dataset. (arXiv:2109.00186v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00186">
<div class="article-summary-box-inner">
<span><p>In open-domain dialogues, predictive uncertainties are mainly evaluated in a
domain shift setting to cope with out-of-distribution inputs. However, in
real-world conversations, there could be more extensive distributional shifted
inputs than the out-of-distribution. To evaluate this, we first propose two
methods, Unknown Word (UW) and Insufficient Context (IC), enabling gradual
distributional shifts by corruption on the dialogue dataset. We then
investigate the effect of distributional shifts on accuracy and calibration.
Our experiments show that the performance of existing uncertainty estimation
methods consistently degrades with intensifying the shift. The results suggest
that the proposed methods could be useful for evaluating the calibration of
dialogue systems under distributional shifts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation. (arXiv:2109.00194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00194">
<div class="article-summary-box-inner">
<span><p>Recent multilingual pre-trained language models have achieved remarkable
zero-shot performance, where the model is only finetuned on one source language
and directly evaluated on target languages. In this work, we propose a
self-learning framework that further utilizes unlabeled data of target
languages, combined with uncertainty estimation in the process to select
high-quality silver labels. Three different uncertainties are adapted and
analyzed specifically for the cross lingual transfer: Language
Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty
(EVI). We evaluate our framework with uncertainties on two cross-lingual tasks
including Named Entity Recognition (NER) and Natural Language Inference (NLI)
covering 40 languages in total, which outperforms the baselines significantly
by 10 F1 on average for NER and 2.5 accuracy score for NLI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pattern-based Acquisition of Scientific Entities from Scholarly Article Titles. (arXiv:2109.00199v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00199">
<div class="article-summary-box-inner">
<span><p>We describe a rule-based approach for the automatic acquisition of scientific
entities from scholarly article titles. Two observations motivated the
approach: (i) noting the concentration of an article's contribution information
in its title; and (ii) capturing information pattern regularities via a system
of rules that alleviate the human annotation task in creating gold standards
that annotate single instances at a time. We identify a set of lexico-syntactic
patterns that are easily recognizable, that occur frequently, and that
generally indicates the scientific entity type of interest about the scholarly
contribution.
</p>
<p>A subset of the acquisition algorithm is implemented for article titles in
the Computational Linguistics (CL) scholarly domain. The tool called
ORKG-Title-Parser, in its first release, identifies the following six concept
types of scientific terminology from the CL paper titles, viz. research
problem, solution, resource, language, tool, and method. It has been
empirically evaluated on a collection of 50,237 titles that cover nearly all
articles in the ACL Anthology. It has extracted 19,799 research problems;
18,111 solutions; 20,033 resources; 1,059 languages; 6,878 tools; and 21,687
methods at an average extraction precision of 75%. The code and related data
resources are publicly available at
https://gitlab.com/TIBHannover/orkg/orkg-title-parser.
</p>
<p>Finally, in the article, we discuss extensions and applications to areas such
as scholarly knowledge graph (SKG) creation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset for Identification of Homophobia and Transophobia in Multilingual YouTube Comments. (arXiv:2109.00227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00227">
<div class="article-summary-box-inner">
<span><p>The increased proliferation of abusive content on social media platforms has
a negative impact on online users. The dread, dislike, discomfort, or mistrust
of lesbian, gay, transgender or bisexual persons is defined as
homophobia/transphobia. Homophobic/transphobic speech is a type of offensive
language that may be summarized as hate speech directed toward LGBT+ people,
and it has been a growing concern in recent years. Online
homophobia/transphobia is a severe societal problem that can make online
platforms poisonous and unwelcome to LGBT+ people while also attempting to
eliminate equality, diversity, and inclusion. We provide a new hierarchical
taxonomy for online homophobia and transphobia, as well as an expert-labelled
dataset that will allow homophobic/transphobic content to be automatically
identified. We educated annotators and supplied them with comprehensive
annotation rules because this is a sensitive issue, and we previously
discovered that untrained crowdsourcing annotators struggle with diagnosing
homophobia due to cultural and other prejudices. The dataset comprises 15,141
annotated multilingual comments. This paper describes the process of building
the dataset, qualitative analysis of data, and inter-annotator agreement. In
addition, we create baseline models for the dataset. To the best of our
knowledge, our dataset is the first such dataset created. Warning: This paper
contains explicit statements of homophobia, transphobia, stereotypes which may
be distressing to some readers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OptAGAN: Entropy-based finetuning on text VAE-GAN. (arXiv:2109.00239v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00239">
<div class="article-summary-box-inner">
<span><p>Transfer learning through large pre-trained models has changed the landscape
of current applications in natural language processing (NLP). Recently Optimus,
a variational autoencoder (VAE) which combines two pre-trained models, BERT and
GPT-2, has been released, and its combination with generative adversial
networks (GANs) has been shown to produce novel, yet very human-looking text.
The Optimus and GANs combination avoids the troublesome application of GANs to
the discrete domain of text, and prevents the exposure bias of standard maximum
likelihood methods. We combine the training of GANs in the latent space, with
the finetuning of the decoder of Optimus for single word generation. This
approach lets us model both the high-level features of the sentences, and the
low-level word-by-word generation. We finetune using reinforcement learning
(RL) by exploiting the structure of GPT-2 and by adding entropy-based
intrinsically motivated rewards to balance between quality and diversity. We
benchmark the results of the VAE-GAN model, and show the improvements brought
by our RL finetuning on three widely used datasets for text generation, with
results that greatly surpass the current state-of-the-art for the quality of
the generated texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast. (arXiv:2109.00253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00253">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to align sentence representations from different
languages into a unified embedding space, where semantic similarities (both
cross-lingual and monolingual) can be computed with a simple dot product.
Pre-trained language models are fine-tuned with the translation ranking task.
Existing work (Feng et al., 2020) uses sentences within the same batch as
negatives, which can suffer from the issue of easy negatives. We adapt MoCo (He
et al., 2020) to further improve the quality of alignment. As the experimental
results show, the sentence representations produced by our model achieve the
new state-of-the-art on several tasks, including Tatoeba en-zh similarity
search (Artetxe and Schwenk, 2019b), BUCC en-zh bitext mining, and semantic
textual similarity on 7 datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting all Aspect-polarity Pairs Jointly in a Text with Relation Extraction Approach. (arXiv:2109.00256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00256">
<div class="article-summary-box-inner">
<span><p>Extracting aspect-polarity pairs from texts is an important task of
fine-grained sentiment analysis. While the existing approaches to this task
have gained many progresses, they are limited at capturing relationships among
aspect-polarity pairs in a text, thus degrading the extraction performance.
Moreover, the existing state-of-the-art approaches, namely token-based
se-quence tagging and span-based classification, have their own defects such as
polarity inconsistency resulted from separately tagging tokens in the former
and the heterogeneous categorization in the latter where aspect-related and
polarity-related labels are mixed. In order to remedy the above defects,
in-spiring from the recent advancements in relation extraction, we propose to
generate aspect-polarity pairs directly from a text with relation extraction
technology, regarding aspect-pairs as unary relations where aspects are
enti-ties and the corresponding polarities are relations. Based on the
perspective, we present a position- and aspect-aware sequence2sequence model
for joint extraction of aspect-polarity pairs. The model is characterized with
its ability to capture not only relationships among aspect-polarity pairs in a
text through the sequence decoding, but also correlations between an aspect and
its polarity through the position- and aspect-aware attentions. The
experi-ments performed on three benchmark datasets demonstrate that our model
outperforms the existing state-of-the-art approaches, making significant
im-provement over them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs. (arXiv:2109.00269v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00269">
<div class="article-summary-box-inner">
<span><p>We tackle the problem of weakly-supervised conversational Question Answering
over large Knowledge Graphs using a neural semantic parsing approach. We
introduce a new Logical Form (LF) grammar that can model a wide range of
queries on the graph while remaining sufficiently simple to generate
supervision data efficiently. Our Transformer-based model takes a JSON-like
structure as input, allowing us to easily incorporate both Knowledge Graph and
conversational contexts. This structured input is transformed to lists of
embeddings and then fed to standard attention layers. We validate our approach,
both in terms of grammar coverage and LF execution accuracy, on two publicly
available datasets, CSQA and ConvQuestions, both grounded in Wikidata. On CSQA,
our approach increases the coverage from $80\%$ to $96.2\%$, and the LF
execution accuracy from $70.6\%$ to $75.6\%$, with respect to previous
state-of-the-art results. On ConvQuestions, we achieve competitive results with
respect to the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discovering Representation Sprachbund For Multilingual Pre-Training. (arXiv:2109.00271v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00271">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained models have demonstrated their effectiveness in many
multilingual NLP tasks and enabled zero-shot or few-shot transfer from
high-resource languages to low resource ones. However, due to significant
typological differences and contradictions between some languages, such models
usually perform poorly on many languages and cross-lingual settings, which
shows the difficulty of learning a single model to handle massive diverse
languages well at the same time. To alleviate this issue, we present a new
multilingual pre-training pipeline. We propose to generate language
representation from multilingual pre-trained models and conduct linguistic
analysis to show that language representation similarity reflect linguistic
similarity from multiple perspectives, including language family, geographical
sprachbund, lexicostatistics and syntax. Then we cluster all the target
languages into multiple groups and name each group as a representation
sprachbund. Thus, languages in the same representation sprachbund are supposed
to boost each other in both pre-training and fine-tuning as they share rich
linguistic similarity. We pre-train one multilingual model for each
representation sprachbund. Experiments are conducted on cross-lingual
benchmarks and significant improvements are achieved compared to strong
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discourse Analysis of Covid-19 in Persian Twitter Social Networks Using Graph Mining and Natural Language Processing. (arXiv:2109.00298v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00298">
<div class="article-summary-box-inner">
<span><p>One of the new scientific ways of understanding discourse dynamics is
analyzing the public data of social networks. This research's aim is
Post-structuralist Discourse Analysis (PDA) of Covid-19 phenomenon (inspired by
Laclau and Mouffe's Discourse Theory) by using Intelligent Data Mining for
Persian Society. The examined big data is five million tweets from 160,000
users of the Persian Twitter network to compare two discourses. Besides
analyzing the tweet texts individually, a social network graph database has
been created based on retweets relationships. We use the VoteRank algorithm to
introduce and rank people whose posts become word of mouth, provided that the
total information spreading scope is maximized over the network. These users
are also clustered according to their word usage pattern (the Gaussian Mixture
Model is used). The constructed discourse of influential spreaders is compared
to the most active users. This analysis is done based on Covid-related posts
over eight episodes. Also, by relying on the statistical content analysis and
polarity of tweet words, discourse analysis is done for the whole mentioned
subpopulations, especially for the top individuals. The most important result
of this research is that the Twitter subjects' discourse construction is
government-based rather than community-based. The analyzed Iranian society does
not consider itself responsible for the Covid-19 wicked problem, does not
believe in participation, and expects the government to solve all problems. The
most active and most influential users' similarity is that political, national,
and critical discourse construction is the predominant one. In addition to the
advantages of its research methodology, it is necessary to pay attention to the
study's limitations. Suggestion for future encounters of Iranian society with
similar crises is given.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\infty$-former: Infinite Memory Transformer. (arXiv:2109.00301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00301">
<div class="article-summary-box-inner">
<span><p>Transformers struggle when attending to long contexts, since the amount of
computation grows with the context length, and therefore they cannot model
long-term memories effectively. Several variations have been proposed to
alleviate this problem, but they all have a finite memory capacity, being
forced to drop old information. In this paper, we propose the $\infty$-former,
which extends the vanilla transformer with an unbounded long-term memory. By
making use of a continuous-space attention mechanism to attend over the
long-term memory, the $\infty$-former's attention complexity becomes
independent of the context length. Thus, it is able to model arbitrarily long
contexts and maintain "sticky memories" while keeping a fixed computation
budget. Experiments on a synthetic sorting task demonstrate the ability of the
$\infty$-former to retain information from long sequences. We also perform
experiments on language modeling, by training a model from scratch and by
fine-tuning a pre-trained language model, which show benefits of unbounded
long-term memories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring deep learning methods for recognizing rare diseases and their clinical manifestations from texts. (arXiv:2109.00343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00343">
<div class="article-summary-box-inner">
<span><p>Although rare diseases are characterized by low prevalence, approximately 300
million people are affected by a rare disease. The early and accurate diagnosis
of these conditions is a major challenge for general practitioners, who do not
have enough knowledge to identify them. In addition to this, rare diseases
usually show a wide variety of manifestations, which might make the diagnosis
even more difficult. A delayed diagnosis can negatively affect the patient's
life. Therefore, there is an urgent need to increase the scientific and medical
knowledge about rare diseases. Natural Language Processing (NLP) and Deep
Learning can help to extract relevant information about rare diseases to
facilitate their diagnosis and treatments. The paper explores the use of
several deep learning techniques such as Bidirectional Long Short Term Memory
(BiLSTM) networks or deep contextualized word representations based on
Bidirectional Encoder Representations from Transformers (BERT) to recognize
rare diseases and their clinical manifestations (signs and symptoms) in the
RareDis corpus. This corpus contains more than 5,000 rare diseases and almost
6,000 clinical manifestations. BioBERT, a domain-specific language
representation based on BERT and trained on biomedical corpora, obtains the
best results. In particular, this model obtains an F1-score of 85.2% for rare
diseases, outperforming all the other models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConRPG: Paraphrase Generation using Contexts as Regularizer. (arXiv:2109.00363v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00363">
<div class="article-summary-box-inner">
<span><p>A long-standing issue with paraphrase generation is how to obtain reliable
supervision signals. In this paper, we propose an unsupervised paradigm for
paraphrase generation based on the assumption that the probabilities of
generating two sentences with the same meaning given the same context should be
the same. Inspired by this fundamental idea, we propose a pipelined system
which consists of paraphrase candidate generation based on contextual language
models, candidate filtering using scoring functions, and paraphrase model
training based on the selected candidates. The proposed paradigm offers merits
over existing paraphrase generation methods: (1) using the context regularizer
on meanings, the model is able to generate massive amounts of high-quality
paraphrase pairs; and (2) using human-interpretable scoring functions to select
paraphrase pairs from candidates, the proposed framework provides a channel for
developers to intervene with the data generation process, leading to a more
controllable model. Experimental results across different tasks and datasets
demonstrate that the effectiveness of the proposed model in both supervised and
unsupervised setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chronic Pain and Language: A Topic Modelling Approach to Personal Pain Descriptions. (arXiv:2109.00402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00402">
<div class="article-summary-box-inner">
<span><p>Chronic pain is recognized as a major health problem, with impacts not only
at the economic, but also at the social, and individual levels. Being a private
and subjective experience, it is impossible to externally and impartially
experience, describe, and interpret chronic pain as a purely noxious stimulus
that would directly point to a causal agent and facilitate its mitigation,
contrary to acute pain, the assessment of which is usually straightforward.
Verbal communication is, thus, key to convey relevant information to health
professionals that would otherwise not be accessible to external entities,
namely, intrinsic qualities about the painful experience and the patient. We
propose and discuss a topic modelling approach to recognize patterns in verbal
descriptions of chronic pain, and use these patterns to quantify and qualify
experiences of pain. Our approaches allow for the extraction of novel insights
on chronic pain experiences from the obtained topic models and latent spaces.
We argue that our results are clinically relevant for the assessment and
management of chronic pain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00412">
<div class="article-summary-box-inner">
<span><p>In multimodal sentiment analysis (MSA), the performance of a model highly
depends on the quality of synthesized embeddings. These embeddings are
generated from the upstream process called multimodal fusion, which aims to
extract and combine the input unimodal raw data to produce a richer multimodal
representation. Previous work either back-propagates the task loss or
manipulates the geometric property of feature spaces to produce favorable
fusion results, which neglects the preservation of critical task-related
information that flows from input to the fusion results. In this work, we
propose a framework named MultiModal InfoMax (MMIM), which hierarchically
maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)
and between multimodal fusion result and unimodal input in order to maintain
task-related information through multimodal fusion. The framework is jointly
trained with the main task (MSA) to improve the performance of the downstream
MSA task. To address the intractable issue of MI bounds, we further formulate a
set of computationally simple parametric and non-parametric methods to
approximate their truth value. Experimental results on the two widely used
datasets demonstrate the efficacy of our approach. The implementation of this
work is publicly available at
https://github.com/declare-lab/Multimodal-Infomax.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Masked Adversarial Generation for Neural Machine Translation. (arXiv:2109.00417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00417">
<div class="article-summary-box-inner">
<span><p>Attacking Neural Machine Translation models is an inherently combinatorial
task on discrete sequences, solved with approximate heuristics. Most methods
use the gradient to attack the model on each sample independently. Instead of
mechanically applying the gradient, could we learn to produce meaningful
adversarial attacks ? In contrast to existing approaches, we learn to attack a
model by training an adversarial generator based on a language model. We
propose the Masked Adversarial Generation (MAG) model, that learns to perturb
the translation model throughout the training process. The experiments show
that it improves the robustness of machine translation models, while being
faster than competing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00430">
<div class="article-summary-box-inner">
<span><p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a
range of professional medical services, i.e., diagnosis, consultation, and
treatment. However, one-stop MDS is still unexplored because: (1) no dataset
has so large-scale dialogues contains both multiple medical services and
fine-grained medical labels (i.e., intents, slots, values); (2) no model has
addressed a MDS based on multiple-service conversations in a unified framework.
In this work, we first build a Multiple-domain Multiple-service medical
dialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between
doctors and patients, covering 276 types of diseases, 2,468 medical entities,
and 3 specialties of medical services. To the best of our knowledge, it is the
only medical dialogue dataset that includes both multiple medical services and
fine-grained medical labels. Then, we formulate a one-stop MDS as a
sequence-to-sequence generation problem. We unify a MDS with causal language
modeling and conditional causal language modeling, respectively. Specifically,
we employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)
and their variants to get benchmarks on M^2-MedDialog dataset. We also propose
pseudo labeling and natural perturbation methods to expand M2-MedDialog dataset
and enhance the state-of-the-art pretrained models. We demonstrate the results
achieved by the benchmarks so far through extensive experiments on
M2-MedDialog. We release the dataset, the code, as well as the evaluation
scripts to facilitate future research in this important research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position Masking for Improved Layout-Aware Document Understanding. (arXiv:2109.00442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00442">
<div class="article-summary-box-inner">
<span><p>Natural language processing for document scans and PDFs has the potential to
enormously improve the efficiency of business processes. Layout-aware word
embeddings such as LayoutLM have shown promise for classification of and
information extraction from such documents. This paper proposes a new
pre-training task called that can improve performance of layout-aware word
embeddings that incorporate 2-D position embeddings. We compare models
pre-trained with only language masking against models pre-trained with both
language masking and position masking, and we find that position masking
improves performance by over 5% on a form understanding task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Capturing Stance Dynamics in Social Media: Open Challenges and Research Directions. (arXiv:2109.00475v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00475">
<div class="article-summary-box-inner">
<span><p>Social media platforms provide a goldmine for mining public opinion on issues
of wide societal interest. Opinion mining is a problem that can be
operationalised by capturing and aggregating the stance of individual social
media posts as supporting, opposing or being neutral towards the issue at hand.
While most prior work in stance detection has investigated datasets with
limited time coverage, interest in investigating longitudinal datasets has
recently increased. Evolving dynamics in linguistic and behavioural patterns
observed in new data require in turn adapting stance detection systems to deal
with the changes. In this survey paper, we investigate the intersection between
computational linguistics and the temporal evolution of human communication in
digital media. We perform a critical review in emerging research considering
dynamics, exploring different semantic and pragmatic factors that impact
linguistic data in general, and stance particularly. We further discuss current
directions in capturing stance dynamics in social media. We organise the
challenges of dealing with stance dynamics, identify open challenges and
discuss future directions in three key dimensions: utterance, context and
influence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey of Low-Resource Machine Translation. (arXiv:2109.00486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00486">
<div class="article-summary-box-inner">
<span><p>We present a survey covering the state of the art in low-resource machine
translation. There are currently around 7000 languages spoken in the world and
almost all language pairs lack significant resources for training machine
translation models. There has been increasing interest in research addressing
the challenge of producing useful translation models when very little
translated training data is available. We present a high level summary of this
topical field and provide an overview of best practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Iterative Knowledge Transfer Network with Routing for Aspect-based Sentiment Analysis. (arXiv:2004.01935v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.01935">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect
term extraction, opinion term extraction, and aspect-level sentiment
classification, which are typically handled in a separate or joint manner.
However, previous approaches do not well exploit the interactive relations
among three subtasks and do not pertinently leverage the easily available
document-level labeled domain/sentiment knowledge, which restricts their
performances. To address these issues, we propose a novel Iterative
Multi-Knowledge Transfer Network (IMKTN) for end-to-end ABSA. For one thing,
through the interactive correlations between the ABSA subtasks, our IMKTN
transfers the task-specific knowledge from any two of the three subtasks to
another one at the token level by utilizing a well-designed routing algorithm,
that is, any two of the three subtasks will help the third one. For another,
our IMKTN pertinently transfers the document-level knowledge, i.e.,
domain-specific and sentiment-related knowledge, to the aspect-level subtasks
to further enhance the corresponding performance. Experimental results on three
benchmark datasets demonstrate the effectiveness and superiority of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speaker anonymisation using the McAdams coefficient. (arXiv:2011.01130v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01130">
<div class="article-summary-box-inner">
<span><p>Anonymisation has the goal of manipulating speech signals in order to degrade
the reliability of automatic approaches to speaker recognition, while
preserving other aspects of speech, such as those relating to intelligibility
and naturalness. This paper reports an approach to anonymisation that, unlike
other current approaches, requires no training data, is based upon well-known
signal processing techniques and is both efficient and effective. The proposed
solution uses the McAdams coefficient to transform the spectral envelope of
speech signals. Results derived using common VoicePrivacy 2020 databases and
protocols show that random, optimised transformations can outperform competing
solutions in terms of anonymisation while causing only modest, additional
degradations to intelligibility, even in the case of a semi-informed privacy
adversary.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Evolution of Word Order. (arXiv:2101.09579v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09579">
<div class="article-summary-box-inner">
<span><p>Most natural languages have a predominant or fixed word order. For example in
English the word order is usually Subject-Verb-Object. This work attempts to
explain this phenomenon as well as other typological findings regarding word
order from a functional perspective. In particular, we examine whether fixed
word order provides a functional advantage, explaining why these languages are
prevalent. To this end, we consider an evolutionary model of language and
demonstrate, both theoretically and using genetic algorithms, that a language
with a fixed word order is optimal. We also show that adding information to the
sentence, such as case markers and noun-verb distinction, reduces the need for
fixed word order, in accordance with the typological findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01345">
<div class="article-summary-box-inner">
<span><p>Emotion dynamics is a framework for measuring how an individual's emotions
change over time. It is a powerful tool for understanding how we behave and
interact with the world. In this paper, we introduce a framework to track
emotion dynamics through one's utterances. Specifically we introduce a number
of utterance emotion dynamics (UED) metrics inspired by work in Psychology. We
use this approach to trace emotional arcs of movie characters. We analyze
thousands of such character arcs to test hypotheses that inform our broader
understanding of stories. Notably, we show that there is a tendency for
characters to use increasingly more negative words and become increasingly
emotionally discordant with each other until about 90 percent of the narrative
length. UED also has applications in behavior studies, social sciences, and
public health.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Data-Centric Framework for Composable NLP Workflows. (arXiv:2103.01834v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01834">
<div class="article-summary-box-inner">
<span><p>Empirical natural language processing (NLP) systems in application domains
(e.g., healthcare, finance, education) involve interoperation among multiple
components, ranging from data ingestion, human annotation, to text retrieval,
analysis, generation, and visualization. We establish a unified open-source
framework to support fast development of such sophisticated NLP workflows in a
composable manner. The framework introduces a uniform data representation to
encode heterogeneous results by a wide range of NLP tasks. It offers a large
repository of processors for NLP tasks, visualization, and annotation, which
can be easily assembled with full interoperability under the unified
representation. The highly extensible framework allows plugging in custom
processors from external off-the-shelf NLP and deep learning libraries. The
whole framework is delivered through two modularized yet integratable
open-source projects, namely Forte (for workflow infrastructure and NLP
function processors) and Stave (for user interaction, visualization, and
annotation).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts. (arXiv:2104.08809v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08809">
<div class="article-summary-box-inner">
<span><p>Determining coreference of concept mentions across multiple documents is a
fundamental task in natural language understanding. Previous work on
cross-document coreference resolution (CDCR) typically considers mentions of
events in the news, which seldom involve abstract technical concepts that are
prevalent in science and technology. These complex concepts take diverse or
ambiguous forms and have many hierarchical levels of granularity (e.g., tasks
and subtasks), posing challenges for CDCR. We present a new task of
Hierarchical CDCR (H-CDCR) with the goal of jointly inferring coreference
clusters and hierarchy between them. We create SciCo, an expert-annotated
dataset for H-CDCR in scientific papers, 3X larger than the prominent ECB+
resource. We study strong baseline models that we customize for H-CDCR, and
highlight challenges for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning. (arXiv:2104.10357v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10357">
<div class="article-summary-box-inner">
<span><p>In the traditional cascading architecture for spoken language understanding
(SLU), it has been observed that automatic speech recognition errors could be
detrimental to the performance of natural language understanding. End-to-end
(E2E) SLU models have been proposed to directly map speech input to desired
semantic frame with a single model, hence mitigating ASR error propagation.
Recently, pre-training technologies have been explored for these E2E models. In
this paper, we propose a novel joint textual-phonetic pre-training approach for
learning spoken language representations, aiming at exploring the full
potentials of phonetic information to improve SLU robustness to ASR errors. We
explore phoneme labels as high-level speech features, and design and compare
pre-training tasks based on conditional masked language model objectives and
inter-sentence relation objectives. We also investigate the efficacy of
combining textual and phonetic information during fine-tuning. Experimental
results on spoken language understanding benchmarks, Fluent Speech Commands and
SNIPS, show that the proposed approach significantly outperforms strong
baseline models and improves robustness of spoken language understanding to ASR
errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07464">
<div class="article-summary-box-inner">
<span><p>Recently, considerable literature has grown up around the theme of few-shot
named entity recognition (NER), but little published benchmark data
specifically focused on the practical and challenging task. Current approaches
collect existing supervised NER datasets and re-organize them to the few-shot
setting for empirical study. These strategies conventionally aim to recognize
coarse-grained entity types with few examples, while in practice, most unseen
entity types are fine-grained. In this paper, we present Few-NERD, a
large-scale human-annotated few-shot NER dataset with a hierarchy of 8
coarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238
sentences from Wikipedia, 4,601,160 words are included and each is annotated as
context or a part of a two-level entity type. To the best of our knowledge,
this is the first few-shot NER dataset and the largest human-crafted NER
dataset. We construct benchmark tasks with different emphases to
comprehensively assess the generalization capability of models. Extensive
empirical results and analysis show that Few-NERD is challenging and the
problem requires further research. We make Few-NERD public at
https://ningding97.github.io/fewnerd/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08122">
<div class="article-summary-box-inner">
<span><p>In recent years, Neural Machine Translation (NMT) has achieved notable
results in various translation tasks. However, the word-by-word generation
manner determined by the autoregressive mechanism leads to high translation
latency of the NMT and restricts its low-latency applications.
Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive
mechanism and achieves significant decoding speedup through generating target
words independently and simultaneously. Nevertheless, NAT still takes the
word-level cross-entropy loss as the training objective, which is not optimal
because the output of NAT cannot be properly evaluated due to the multimodality
problem. In this article, we propose using sequence-level training objectives
to train NAT models, which evaluate the NAT outputs as a whole and correlates
well with the real translation quality. Firstly, we propose training NAT models
to optimize sequence-level evaluation metrics (e.g., BLEU) based on several
novel reinforcement algorithms customized for NAT, which outperforms the
conventional method by reducing the variance of gradient estimation. Secondly,
we introduce a novel training objective for NAT models, which aims to minimize
the Bag-of-Ngrams (BoN) difference between the model output and the reference
sentence. The BoN training objective is differentiable and can be calculated
efficiently without doing any approximations. Finally, we apply a three-stage
training strategy to combine these two methods to train the NAT model. We
validate our approach on four translation tasks (WMT14 En$\leftrightarrow$De,
WMT16 En$\leftrightarrow$Ro), which shows that our approach largely outperforms
NAT baselines and achieves remarkable performance on all translation tasks. The
source code is available at https://github.com/ictnlp/Seq-NAT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00956">
<div class="article-summary-box-inner">
<span><p>Building embodied autonomous agents capable of participating in social
interactions with humans is one of the main challenges in AI. Within the Deep
Reinforcement Learning (DRL) field, this objective motivated multiple works on
embodied language use. However, current approaches focus on language as a
communication tool in very simplified and non-diverse social situations: the
"naturalness" of language is reduced to the concept of high vocabulary size and
variability. In this paper, we argue that aiming towards human-level AI
requires a broader set of key social skills: 1) language use in complex and
variable social contexts; 2) beyond language, complex embodied communication in
multimodal settings within constantly evolving social worlds. We explain how
concepts from cognitive sciences could help AI to draw a roadmap towards
human-like intelligence, with a focus on its social dimensions. As a first
step, we propose to expand current research to a broader set of core social
skills. To do this, we present SocialAI, a benchmark to assess the acquisition
of social skills of DRL agents using multiple grid-world environments featuring
other (scripted) social agents. We then study the limits of a recent SOTA DRL
approach when tested on SocialAI and discuss important next steps towards
proficient social agents. Videos and code are available at
https://sites.google.com/view/socialai.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Higher-Order Concurrency for Microcontrollers. (arXiv:2108.07805v2 [cs.PL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07805">
<div class="article-summary-box-inner">
<span><p>Programming microcontrollers involves low-level interfacing with hardware and
peripherals that are concurrent and reactive. Such programs are typically
written in a mixture of C and assembly using concurrent language extensions
(like $\texttt{FreeRTOS tasks}$ and $\texttt{semaphores}$), resulting in
unsafe, callback-driven, error-prone and difficult-to-maintain code.
</p>
<p>We address this challenge by introducing $\texttt{SenseVM}$ - a
bytecode-interpreted virtual machine that provides a message-passing based
$\textit{higher-order concurrency}$ model, originally introduced by Reppy, for
microcontroller programming. This model treats synchronous operations as
first-class values (called $\texttt{Events}$) akin to the treatment of
first-class functions in functional languages. This primarily allows the
programmer to compose and tailor their own concurrency abstractions and,
additionally, abstracts away unsafe memory operations, common in shared-memory
concurrency models, thereby making microcontroller programs safer, composable
and easier-to-maintain.
</p>
<p>Our VM is made portable via a low-level $\textit{bridge}$ interface, built
atop the embedded OS - Zephyr. The bridge is implemented by all drivers and
designed such that programming in response to a software message or a hardware
interrupt remains uniform and indistinguishable. In this paper we demonstrate
the features of our VM through an example, written in a Caml-like functional
language, running on the $\texttt{nRF52840}$ and $\texttt{STM32F4}$
microcontrollers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021. (arXiv:2108.08556v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08556">
<div class="article-summary-box-inner">
<span><p>This paper reports the Machine Translation (MT) systems submitted by the
IIITT team for the English-&gt;Marathi and English-&gt;Irish language pairs LoResMT
2021 shared task. The task focuses on getting exceptional translations for
rather low-resourced languages like Irish and Marathi. We fine-tune IndicTrans,
a pretrained multilingual NMT model for English-&gt;Marathi, using external
parallel corpus as input for additional training. We have used a pretrained
Helsinki-NLP Opus MT English-&gt;Irish model for the latter language pair. Our
approaches yield relatively promising results on the BLEU metrics. Under the
team name IIITT, our systems ranked 1, 1, and 2 in English-&gt;Marathi,
Irish-&gt;English, and English-&gt;Irish, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09084">
<div class="article-summary-box-inner">
<span><p>Transformer is a powerful model for text understanding. However, it is
inefficient due to its quadratic complexity to input sequence length. Although
there are many methods on Transformer acceleration, they are still either
inefficient on long sequences or not effective enough. In this paper, we
propose Fastformer, which is an efficient Transformer model based on additive
attention. In Fastformer, instead of modeling the pair-wise interactions
between tokens, we first use additive attention mechanism to model global
contexts, and then further transform each token representation based on its
interaction with global context representations. In this way, Fastformer can
achieve effective context modeling with linear complexity. Extensive
experiments on five datasets show that Fastformer is much more efficient than
many existing Transformer models and can meanwhile achieve comparable or even
better long text modeling performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression. (arXiv:2108.10684v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10684">
<div class="article-summary-box-inner">
<span><p>Organizing complex peer production projects and advancing scientific
knowledge of open collaboration each depend on the ability to measure quality.
Article quality ratings on English language Wikipedia have been widely used by
both Wikipedia community members and academic researchers for purposes like
tracking knowledge gaps and studying how political polarization shapes
collaboration. Even so, measuring quality presents many methodological
challenges. The most widely used systems use labels on discrete ordinal scales
when assessing quality, but such labels can be inconvenient for statistics and
machine learning. Prior work handles this by assuming that different levels of
quality are "evenly spaced" from one another. This assumption runs counter to
intuitions about the relative degrees of effort needed to raise Wikipedia
encyclopedia articles to different quality levels. Furthermore, models from
prior work are fit to datasets that oversample high-quality articles. This
limits their accuracy for representative samples of articles or revisions. I
describe a technique extending the Wikimedia Foundations' ORES article quality
model to address these limitations. My method uses weighted ordinal regression
models to construct one-dimensional continuous measures of quality. While
scores from my technique and from prior approaches are correlated, my approach
improves accuracy for research datasets and provides evidence that the "evenly
spaced" assumption is unfounded in practice on English Wikipedia. I conclude
with recommendations for using quality scores in future research and include
the full code, data, and models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Why Intermediate-Task Fine-Tuning Works. (arXiv:2108.11696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11696">
<div class="article-summary-box-inner">
<span><p>Supplementary Training on Intermediate Labeled-data Tasks (STILTs) is a
widely applied technique, which first fine-tunes the pretrained language models
on an intermediate task before on the target task of interest. While STILTs is
able to further improve the performance of pretrained language models, it is
still unclear why and when it works. Previous research shows that those
intermediate tasks involving complex inference, such as commonsense reasoning,
work especially well for RoBERTa. In this paper, we discover that the
improvement from an intermediate task could be orthogonal to it containing
reasoning or other complex skills -- a simple real-fake discrimination task
synthesized by GPT2 can benefit diverse target tasks. We conduct extensive
experiments to study the impact of different factors on STILTs. These findings
suggest rethinking the role of intermediate fine-tuning in the STILTs pipeline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position-Invariant Truecasing with a Word-and-Character Hierarchical Recurrent Neural Network. (arXiv:2108.11943v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11943">
<div class="article-summary-box-inner">
<span><p>Truecasing is the task of restoring the correct case (uppercase or lowercase)
of noisy text generated either by an automatic system for speech recognition or
machine translation or by humans. It improves the performance of downstream NLP
tasks such as named entity recognition and language modeling. We propose a
fast, accurate and compact two-level hierarchical word-and-character-based
recurrent neural network model, the first of its kind for this problem. Using
sequence distillation, we also address the problem of truecasing while ignoring
token positions in the sentence, i.e. in a position-invariant manner.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13679">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to formulate the task-oriented dialogue system as
the purely natural language generation task, so as to fully leverage the
large-scale pre-trained models like GPT-2 and simplify complicated
delexicalization prepossessing. However, directly applying this method heavily
suffers from the dialogue entity inconsistency caused by the removal of
delexicalized tokens, as well as the catastrophic forgetting problem of the
pre-trained model during fine-tuning, leading to unsatisfactory performance. To
alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which
incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve
better performance on transfer learning and dialogue entity generation.
Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ
dataset demonstrate that our proposed approach significantly outperforms
baseline models with a remarkable performance on automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-02 23:09:07.844628477 UTC">2021-09-02 23:09:07 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>