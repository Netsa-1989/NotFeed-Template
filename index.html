<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-01T01:30:00Z">10-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy Policy Question Answering Assistant: A Query-Guided Extractive Summarization Approach. (arXiv:2109.14638v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14638">
<div class="article-summary-box-inner">
<span><p>Existing work on making privacy policies accessible has explored new
presentation forms such as color-coding based on the risk factors or
summarization to assist users with conscious agreement. To facilitate a more
personalized interaction with the policies, in this work, we propose an
automated privacy policy question answering assistant that extracts a summary
in response to the input user query. This is a challenging task because users
articulate their privacy-related questions in a very different language than
the legal language of the policy, making it difficult for the system to
understand their inquiry. Moreover, existing annotated data in this domain are
limited. We address these problems by paraphrasing to bring the style and
language of the user's question closer to the language of privacy policies. Our
content scoring module uses the existing in-domain data to find relevant
information in the policy and incorporates it in a summary. Our pipeline is
able to find an answer for 89% of the user queries in the privacyQA dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Classifying Tweet Sentiment Using the Hidden State and Attention Matrix of a Fine-tuned BERTweet Model. (arXiv:2109.14692v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14692">
<div class="article-summary-box-inner">
<span><p>This paper introduces a study on tweet sentiment classification. Our task is
to classify a tweet as either positive or negative. We approach the problem in
two steps, namely embedding and classifying. Our baseline methods include
several combinations of traditional embedding methods and classification
algorithms. Furthermore, we explore the current state-of-the-art tweet analysis
model, BERTweet, and propose a novel approach in which features are engineered
from the hidden states and attention matrices of the model, inspired by
empirical study of the tweets. Using a multi-layer perceptron trained with a
high dropout rate for classification, our proposed approach achieves a
validation accuracy of 0.9111.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Approach For Sparse Representations Using The Locally Competitive Algorithm For Audio. (arXiv:2109.14705v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14705">
<div class="article-summary-box-inner">
<span><p>Gammachirp filterbank has been used to approximate the cochlea in sparse
coding algorithms. An oriented grid search optimization was applied to adapt
the gammachirp's parameters and improve the Matching Pursuit (MP) algorithm's
sparsity along with the reconstruction quality. However, this combination of a
greedy algorithm with a grid search at each iteration is computationally
demanding and not suitable for real-time applications. This paper presents an
adaptive approach to optimize the gammachirp's parameters but in the context of
the Locally Competitive Algorithm (LCA) that requires much fewer computations
than MP. The proposed method consists of taking advantage of the LCA's neural
architecture to automatically adapt the gammachirp's filterbank using the
backpropagation algorithm. Results demonstrate an improvement in the LCA's
performance with our approach in terms of sparsity, reconstruction quality, and
convergence time. This approach can yield a significant advantage over existing
approaches for real-time applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief. (arXiv:2109.14723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14723">
<div class="article-summary-box-inner">
<span><p>Although pretrained language models (PTLMs) contain significant amounts of
world knowledge, they can still produce inconsistent answers to questions when
probed, even after specialized training. As a result, it can be hard to
identify what the model actually "believes" about the world, making it
susceptible to inconsistent behavior and simple errors. Our goal is to reduce
these problems. Our approach is to embed a PTLM in a broader system that also
includes an evolving, symbolic memory of beliefs -- a BeliefBank -- that
records but then may modify the raw PTLM answers. We describe two mechanisms to
improve belief consistency in the overall system. First, a reasoning component
-- a weighted MaxSAT solver -- revises beliefs that significantly clash with
others. Second, a feedback component issues future queries to the PTLM using
known beliefs as context. We show that, in a controlled experimental setting,
these two mechanisms result in more consistent beliefs in the overall system,
improving both the accuracy and consistency of its answers over time. This is
significant as it is a first step towards PTLM-based architectures with a
systematic notion of belief, enabling them to construct a more coherent picture
of the world, and improve over time without model retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System. (arXiv:2109.14739v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14739">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have been recently shown to benefit task-oriented
dialogue (TOD) systems. Despite their success, existing methods often formulate
this task as a cascaded generation problem which can lead to error accumulation
across different sub-tasks and greater data annotation overhead. In this study,
we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In
addition, we introduce a new dialogue multi-task pre-training strategy that
allows the model to learn the primary TOD task completion skills from
heterogeneous dialog corpora. We extensively test our model on three benchmark
TOD tasks, including end-to-end dialogue modelling, dialogue state tracking,
and intent classification. Experimental results show that PPTOD achieves new
state of the art on all evaluated tasks in both high-resource and low-resource
scenarios. Furthermore, comparisons against previous SOTA methods show that the
responses generated by PPTOD are more factually correct and semantically
coherent as judged by human annotators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications. (arXiv:2109.14776v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14776">
<div class="article-summary-box-inner">
<span><p>Certainty and uncertainty are fundamental to science communication. Hedges
have widely been used as proxies for uncertainty. However, certainty is a
complex construct, with authors expressing not only the degree but the type and
aspects of uncertainty in order to give the reader a certain impression of what
is known. Here, we introduce a new study of certainty that models both the
level and the aspects of certainty in scientific findings. Using a new dataset
of 2167 annotated scientific findings, we demonstrate that hedges alone account
for only a partial explanation of certainty. We show that both the overall
certainty and individual aspects can be predicted with pre-trained language
models, providing a more complete picture of the author's intended
communication. Downstream analyses on 431K scientific findings from news and
scientific abstracts demonstrate that modeling sentence-level and aspect-level
certainty is meaningful for areas like science communication. Both the model
and datasets used in this paper are released at
https://blablablab.si.umich.edu/projects/certainty/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tipping the Scales: A Corpus-Based Reconstruction of Adjective Scales in the McGill Pain Questionnaire. (arXiv:2109.14788v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14788">
<div class="article-summary-box-inner">
<span><p>Modern medical diagnosis relies on precise pain assessment tools in
translating clinical information from patient to physician. The McGill Pain
Questionnaire (MPQ) is a clinical pain assessment technique that utilizes 78
adjectives of different intensities in 20 different categories to quantity a
patient's pain. The questionnaire's efficacy depends on a predictable pattern
of adjective use by patients experiencing pain. In this study, I recreate the
MPQ's adjective intensity orderings using data gathered from patient forums and
modern NLP techniques. I extract adjective intensity relationships by searching
for key linguistic contexts, and then combine the relationship information to
form robust adjective scales. Of 17 adjective relationships predicted by this
research, 10 show agreement with the MPQ, which is statistically significant at
the .1 alpha level. The results suggest predictable patterns of adjective use
by people experiencing pain, but call into question the MPQ's categories for
grouping adjectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phonetic Word Embeddings. (arXiv:2109.14796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14796">
<div class="article-summary-box-inner">
<span><p>This work presents a novel methodology for calculating the phonetic
similarity between words taking motivation from the human perception of sounds.
This metric is employed to learn a continuous vector embedding space that
groups similar sounding words together and can be used for various downstream
computational phonology tasks. The efficacy of the method is presented for two
different languages (English, Hindi) and performance gains over previous
reported works are discussed on established tests for predicting phonetic
similarity. To address limited benchmarking mechanisms in this field, we also
introduce a heterographic pun dataset based evaluation methodology to compare
the effectiveness of acoustic similarity algorithms. Further, a visualization
of the embedding space is presented with a discussion on the various possible
use-cases of this novel algorithm. An open-source implementation is also shared
to aid reproducibility and enable adoption in related tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">COVID-19 Fake News Detection Using Bidirectiona lEncoder Representations from Transformers Based Models. (arXiv:2109.14816v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14816">
<div class="article-summary-box-inner">
<span><p>Nowadays, the development of social media allows people to access the latest
news easily. During the COVID-19 pandemic, it is important for people to access
the news so that they can take corresponding protective measures. However, the
fake news is flooding and is a serious issue especially under the global
pandemic. The misleading fake news can cause significant loss in terms of the
individuals and the society. COVID-19 fake news detection has become a novel
and important task in the NLP field. However, fake news always contain the
correct portion and the incorrect portion. This fact increases the difficulty
of the classification task. In this paper, we fine tune the pre-trained
Bidirectional Encoder Representations from Transformers (BERT) model as our
base model. We add BiLSTM layers and CNN layers on the top of the finetuned
BERT model with frozen parameters or not frozen parameters methods
respectively. The model performance evaluation results showcase that our best
model (BERT finetuned model with frozen parameters plus BiLSTM layers) achieves
state-of-the-art results towards COVID-19 fake news detection task. We also
explore keywords evaluation methods using our best model and evaluate the model
performance after removing keywords.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Process discovery on deviant traces and other stranger things. (arXiv:2109.14883v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14883">
<div class="article-summary-box-inner">
<span><p>As the need to understand and formalise business processes into a model has
grown over the last years, the process discovery research field has gained more
and more importance, developing two different classes of approaches to model
representation: procedural and declarative. Orthogonally to this
classification, the vast majority of works envisage the discovery task as a
one-class supervised learning process guided by the traces that are recorded
into an input log. In this work instead, we focus on declarative processes and
embrace the less-popular view of process discovery as a binary supervised
learning task, where the input log reports both examples of the normal system
execution, and traces representing "stranger" behaviours according to the
domain semantics. We therefore deepen how the valuable information brought by
both these two sets can be extracted and formalised into a model that is
"optimal" according to user-defined goals. Our approach, namely NegDis, is
evaluated w.r.t. other relevant works in this field, and shows promising
results as regards both the performance and the quality of the obtained
solution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems. (arXiv:2109.14895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14895">
<div class="article-summary-box-inner">
<span><p>In translating text where sentiment is the main message, human translators
give particular attention to sentiment-carrying words. The reason is that an
incorrect translation of such words would miss the fundamental aspect of the
source text, i.e. the author's sentiment. In the online world, MT systems are
extensively used to translate User-Generated Content (UGC) such as reviews,
tweets, and social media posts, where the main message is often the author's
positive or negative attitude towards the topic of the text. It is important in
such scenarios to accurately measure how far an MT system can be a reliable
real-life utility in transferring the correct affect message. This paper
tackles an under-recognised problem in the field of machine translation
evaluation which is judging to what extent automatic metrics concur with the
gold standard of human evaluation for a correct translation of sentiment. We
evaluate the efficacy of conventional quality metrics in spotting a
mistranslation of sentiment, especially when it is the sole error in the MT
output. We propose a numerical `sentiment-closeness' measure appropriate for
assessing the accuracy of a translated affect message in UGC text by an MT
system. We will show that incorporating this sentiment-aware measure can
significantly enhance the correlation of some available quality metrics with
the human judgement of an accurate translation of sentiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DICoE@FinSim-3: Financial Hypernym Detection using Augmented Terms and Distance-based Features. (arXiv:2109.14906v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14906">
<div class="article-summary-box-inner">
<span><p>We present the submission of team DICoE for FinSim-3, the 3rd Shared Task on
Learning Semantic Similarities for the Financial Domain. The task provides a
set of terms in the financial domain and requires to classify them into the
most relevant hypernym from a financial ontology. After augmenting the terms
with their Investopedia definitions, our system employs a Logistic Regression
classifier over financial word embeddings and a mix of hand-crafted and
distance-based features. Also, for the first time in this task, we employ
different replacement methods for out-of-vocabulary terms, leading to improved
performance. Finally, we have also experimented with word representations
generated from various financial corpora. Our best-performing submission ranked
4th on the task's leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14927">
<div class="article-summary-box-inner">
<span><p>Temporal expressions in text play a significant role in language
understanding and correctly identifying them is fundamental to various
retrieval and natural language processing systems. Previous works have slowly
shifted from rule-based to neural architectures, capable of tagging expressions
with higher accuracy. However, neural models can not yet distinguish between
different expression types at the same level as their rule-based counterparts.
n this work, we aim to identify the most suitable transformer architecture for
joint temporal tagging and type classification, as well as, investigating the
effect of semi-supervised training on the performance of these systems. After
studying variants of token classification and encoder-decoder architectures, we
ultimately present a transformer encoder-decoder model using RoBERTa language
model as our best performing system. By supplementing training resources with
weakly labeled data from rule-based systems, our model surpasses previous works
in temporal tagging and type classification, especially on rare classes.
Additionally, we make the code and pre-trained experiment publicly available
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prose2Poem: The blessing of Transformer-based Language Models in translating Prose to Persian Poetry. (arXiv:2109.14934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14934">
<div class="article-summary-box-inner">
<span><p>Persian Poetry has consistently expressed its philosophy, wisdom, speech, and
rationale on the basis of its couplets, making it an enigmatic language on its
own to both native and non-native speakers. Nevertheless, the notice able gap
between Persian prose and poem has left the two pieces of literature
medium-less. Having curated a parallel corpus of prose and their equivalent
poems, we introduce a novel Neural Machine Translation (NMT) approach to
translate prose to ancient Persian poetry using transformer-based Language
Models in an extremely low-resource setting. More specifically, we trained a
Transformer model from scratch to obtain initial translations and pretrained
different variations of BERT to obtain final translations. To address the
challenge of using masked language modelling under poeticness criteria, we
heuristically joined the two models and generated valid poems in terms of
automatic and human assessments. Final results demonstrate the eligibility and
creativity of our novel heuristically aided approach among Literature
professionals and non-professionals in generating novel Persian poems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Syntactic Persistence in Language Models: Priming as a Window into Abstract Language Representations. (arXiv:2109.14989v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14989">
<div class="article-summary-box-inner">
<span><p>We investigate the extent to which modern, neural language models are
susceptible to syntactic priming, the phenomenon where the syntactic structure
of a sentence makes the same structure more probable in a follow-up sentence.
We explore how priming can be used to study the nature of the syntactic
knowledge acquired by these models. We introduce a novel metric and release
Prime-LM, a large corpus where we control for various linguistic factors which
interact with priming strength. We find that recent large Transformer models
indeed show evidence of syntactic priming, but also that the syntactic
generalisations learned by these models are to some extent modulated by
semantic information. We report surprisingly strong priming effects when
priming with multiple sentences, each with different words and meaning but with
identical syntactic structure. We conclude that the syntactic priming paradigm
is a highly useful, additional tool for gaining insights into the capacities of
language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A surprisal--duration trade-off across and within the world's languages. (arXiv:2109.15000v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15000">
<div class="article-summary-box-inner">
<span><p>While there exist scores of natural languages, each with its unique features
and idiosyncrasies, they all share a unifying theme: enabling human
communication. We may thus reasonably predict that human cognition shapes how
these languages evolve and are used. Assuming that the capacity to process
information is roughly constant across human populations, we expect a
surprisal--duration trade-off to arise both across and within languages. We
analyse this trade-off using a corpus of 600 languages and, after controlling
for several potential confounds, we find strong supporting evidence in both
settings. Specifically, we find that, on average, phones are produced faster in
languages where they are less surprising, and vice versa. Further, we confirm
that more surprising phones are longer, on average, in 319 languages out of the
600. We thus conclude that there is strong evidence of a surprisal--duration
trade-off in operation, both across and within the world's languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep Neural Compression Via Concurrent Pruning and Self-Distillation. (arXiv:2109.15014v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15014">
<div class="article-summary-box-inner">
<span><p>Pruning aims to reduce the number of parameters while maintaining performance
close to the original network. This work proposes a novel
\emph{self-distillation} based pruning strategy, whereby the representational
similarity between the pruned and unpruned versions of the same network is
maximized. Unlike previous approaches that treat distillation and pruning
separately, we use distillation to inform the pruning criteria, without
requiring a separate student network as in knowledge distillation. We show that
the proposed {\em cross-correlation objective for self-distilled pruning}
implicitly encourages sparse solutions, naturally complementing magnitude-based
pruning criteria. Experiments on the GLUE and XGLUE benchmarks show that
self-distilled pruning increases mono- and cross-lingual language model
performance. Self-distilled pruned models also outperform smaller Transformers
with an equal number of parameters and are competitive against (6 times) larger
distilled networks. We also observe that self-distillation (1) maximizes class
separability, (2) increases the signal-to-noise ratio, and (3) converges faster
after pruning steps, providing further insights into why self-distilled pruning
improves generalization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient Post-training Quantization of Pre-trained Language Models. (arXiv:2109.15082v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15082">
<div class="article-summary-box-inner">
<span><p>Network quantization has gained increasing attention with the rapid growth of
large pre-trained language models~(PLMs). However, most existing quantization
methods for PLMs follow quantization-aware training~(QAT) that requires
end-to-end training with full access to the entire dataset. Therefore, they
suffer from slow training, large memory overhead, and data security issues. In
this paper, we study post-training quantization~(PTQ) of PLMs, and propose
module-wise quantization error minimization~(MREM), an efficient solution to
mitigate these issues. By partitioning the PLM into multiple modules, we
minimize the reconstruction error incurred by quantization for each module. In
addition, we design a new model parallel training strategy such that each
module can be trained locally on separate computing devices without waiting for
preceding modules, which brings nearly the theoretical training speed-up (e.g.,
$4\times$ on $4$ GPUs). Experiments on GLUE and SQuAD benchmarks show that our
proposed PTQ solution not only performs close to QAT, but also enjoys
significant reductions in training time, memory overhead, and data consumption.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Key Point Analysis via Contrastive Learning and Extractive Argument Summarization. (arXiv:2109.15086v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15086">
<div class="article-summary-box-inner">
<span><p>Key point analysis is the task of extracting a set of concise and high-level
statements from a given collection of arguments, representing the gist of these
arguments. This paper presents our proposed approach to the Key Point Analysis
shared task, collocated with the 8th Workshop on Argument Mining. The approach
integrates two complementary components. One component employs contrastive
learning via a siamese neural network for matching arguments to key points; the
other is a graph-based extractive summarization model for generating key
points. In both automatic and manual evaluation, our approach was ranked best
among all submissions to the shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional generalization in semantic parsing with pretrained transformers. (arXiv:2109.15101v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15101">
<div class="article-summary-box-inner">
<span><p>Large-scale pretraining instills large amounts of knowledge in deep neural
networks. This, in turn, improves the generalization behavior of these models
in downstream tasks. What exactly are the limits to the generalization benefits
of large-scale pretraining? Here, we report observations from some simple
experiments aimed at addressing this question in the context of two semantic
parsing tasks involving natural language, SCAN and COGS. We show that language
models pretrained exclusively with non-English corpora, or even with
programming language corpora, significantly improve out-of-distribution
generalization in these benchmarks, compared with models trained from scratch,
even though both benchmarks are English-based. This demonstrates the
surprisingly broad transferability of pretrained representations and knowledge.
Pretraining with a large-scale protein sequence prediction task, on the other
hand, mostly deteriorates the generalization performance in SCAN and COGS,
suggesting that pretrained representations do not transfer universally and that
there are constraints on the similarity between the pretraining and downstream
domains for successful transfer. Finally, we show that larger models are harder
to train from scratch and their generalization accuracy is lower when trained
up to convergence on the relatively small SCAN and COGS datasets, but the
benefits of large-scale pretraining become much clearer with larger models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact Verification Models. (arXiv:2109.15107v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15107">
<div class="article-summary-box-inner">
<span><p>Fact verification datasets are typically constructed using crowdsourcing
techniques due to the lack of text sources with veracity labels. However, the
crowdsourcing process often produces undesired biases in data that cause models
to learn spurious patterns. In this paper, we propose CrossAug, a contrastive
data augmentation method for debiasing fact verification models. Specifically,
we employ a two-stage augmentation pipeline to generate new claims and
evidences from existing samples. The generated samples are then paired
cross-wise with the original pair, forming contrastive samples that facilitate
the model to rely less on spurious patterns and learn more robust
representations. Experimental results show that our method outperforms the
previous state-of-the-art debiasing technique by 3.6% on the debiased extension
of the FEVER dataset, with a total performance boost of 10.13% from the
baseline. Furthermore, we evaluate our approach in data-scarce settings, where
models can be more susceptible to biases due to the lack of training data.
Experimental results demonstrate that our approach is also effective at
debiasing in these low-resource conditions, exceeding the baseline performance
on the Symmetric dataset with just 1% of the original data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims. (arXiv:2109.15118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15118">
<div class="article-summary-box-inner">
<span><p>We present an overview of the second edition of the CheckThat! Lab at CLEF
2019. The lab featured two tasks in two different languages: English and
Arabic. Task 1 (English) challenged the participating systems to predict which
claims in a political debate or speech should be prioritized for fact-checking.
Task 2 (Arabic) asked to (A) rank a given set of Web pages with respect to a
check-worthy claim based on their usefulness for fact-checking that claim, (B)
classify these same Web pages according to their degree of usefulness for
fact-checking the target claim, (C) identify useful passages from these pages,
and (D) use the useful pages to predict the claim's factuality. CheckThat!
provided a full evaluation framework, consisting of data in English (derived
from fact-checking sources) and Arabic (gathered and annotated from scratch)
and evaluation based on mean average precision (MAP) and normalized discounted
cumulative gain (nDCG) for ranking, and F1 for classification. A total of 47
teams registered to participate in this lab, and fourteen of them actually
submitted runs (compared to nine last year). The evaluation results show that
the most successful approaches to Task 1 used various neural networks and
logistic regression. As for Task 2, learning-to-rank was used by the highest
scoring runs for subtask A, while different classifiers were used in the other
subtasks. We release to the research community all datasets from the lab as
well as the evaluation scripts, which should enable further research in the
important tasks of check-worthiness estimation and automatic claim
verification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improved statistical machine translation using monolingual paraphrases. (arXiv:2109.15119v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15119">
<div class="article-summary-box-inner">
<span><p>We propose a novel monolingual sentence paraphrasing method for augmenting
the training data for statistical machine translation systems "for free" -- by
creating it from data that is already available rather than having to create
more aligned data. Starting with a syntactic tree, we recursively generate new
sentence variants where noun compounds are paraphrased using suitable
prepositions, and vice-versa -- preposition-containing noun phrases are turned
into noun compounds. The evaluation shows an improvement equivalent to 33%-50%
of that of doubling the amount of training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SUper Team at SemEval-2016 Task 3: Building a feature-rich system for community question answering. (arXiv:2109.15120v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15120">
<div class="article-summary-box-inner">
<span><p>We present the system we built for participating in SemEval-2016 Task 3 on
Community Question Answering. We achieved the best results on subtask C, and
strong results on subtasks A and B, by combining a rich set of various types of
features: semantic, lexical, metadata, and user-related. The most important
group turned out to be the metadata for the question and for the comment,
semantic vectors trained on QatarLiving data and similarities between the
question and the comment for subtasks A and C, and between the original and the
related question for Subtask B.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature-Rich Named Entity Recognition for Bulgarian Using Conditional Random Fields. (arXiv:2109.15121v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15121">
<div class="article-summary-box-inner">
<span><p>The paper presents a feature-rich approach to the automatic recognition and
categorization of named entities (persons, organizations, locations, and
miscellaneous) in news text for Bulgarian. We combine well-established features
used for other languages with language-specific lexical, syntactic and
morphological information. In particular, we make use of the rich tagset
annotation of the BulTreeBank (680 morpho-syntactic tags), from which we derive
suitable task-specific tagsets (local and nonlocal). We further add
domain-specific gazetteers and additional unlabeled data, achieving F1=89.4%,
which is comparable to the state-of-the-art results for English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Review of Text Style Transfer using Deep Learning. (arXiv:2109.15144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15144">
<div class="article-summary-box-inner">
<span><p>Style is an integral component of a sentence indicated by the choice of words
a person makes. Different people have different ways of expressing themselves,
however, they adjust their speaking and writing style to a social context, an
audience, an interlocutor or the formality of an occasion. Text style transfer
is defined as a task of adapting and/or changing the stylistic manner in which
a sentence is written, while preserving the meaning of the original sentence.
</p>
<p>A systematic review of text style transfer methodologies using deep learning
is presented in this paper. We point out the technological advances in deep
neural networks that have been the driving force behind current successes in
the fields of natural language understanding and generation. The review is
structured around two key stages in the text style transfer process, namely,
representation learning and sentence generation in a new style. The discussion
highlights the commonalities and differences between proposed solutions as well
as challenges and opportunities that are expected to direct and foster further
research in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Modal Sarcasm Detection Based on Contrastive Attention Mechanism. (arXiv:2109.15153v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15153">
<div class="article-summary-box-inner">
<span><p>In the past decade, sarcasm detection has been intensively conducted in a
textual scenario. With the popularization of video communication, the analysis
in multi-modal scenarios has received much attention in recent years.
Therefore, multi-modal sarcasm detection, which aims at detecting sarcasm in
video conversations, becomes increasingly hot in both the natural language
processing community and the multi-modal analysis community. In this paper,
considering that sarcasm is often conveyed through incongruity between
modalities (e.g., text expressing a compliment while acoustic tone indicating a
grumble), we construct a Contras-tive-Attention-based Sarcasm Detection
(ConAttSD) model, which uses an inter-modality contrastive attention mechanism
to extract several contrastive features for an utterance. A contrastive feature
represents the incongruity of information between two modalities. Our
experiments on MUStARD, a benchmark multi-modal sarcasm dataset, demonstrate
the effectiveness of the proposed ConAttSD model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Focused Contrastive Training for Test-based Constituency Analysis. (arXiv:2109.15159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15159">
<div class="article-summary-box-inner">
<span><p>We propose a scheme for self-training of grammaticality models for
constituency analysis based on linguistic tests. A pre-trained language model
is fine-tuned by contrastive estimation of grammatical sentences from a corpus,
and ungrammatical sentences that were perturbed by a syntactic test, a
transformation that is motivated by constituency theory. We show that
consistent gains can be achieved if only certain positive instances are chosen
for training, depending on whether they could be the result of a test
transformation. This way, the positives, and negatives exhibit similar
characteristics, which makes the objective more challenging for the language
model, and also allows for additional markup that indicates the position of the
test application within the sentence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual AMR Parsing with Noisy Knowledge Distillation. (arXiv:2109.15196v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15196">
<div class="article-summary-box-inner">
<span><p>We study multilingual AMR parsing from the perspective of knowledge
distillation, where the aim is to learn and improve a multilingual AMR parser
by using an existing English parser as its teacher. We constrain our
exploration in a strict multilingual setting: there is but one model to parse
all different languages including English. We identify that noisy input and
precise output are the key to successful distillation. Together with extensive
pre-training, we obtain an AMR parser whose performances surpass all previously
published results on four different foreign languages, including German,
Spanish, Italian, and Chinese, by large margins (up to 18.8 \textsc{Smatch}
points on Chinese and on average 11.3 \textsc{Smatch} points). Our parser also
achieves comparable performance on English to the latest state-of-the-art
English-only parser.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments. (arXiv:2109.15207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15207">
<div class="article-summary-box-inner">
<span><p>In the Vision-and-Language Navigation (VLN) task an embodied agent navigates
a 3D environment, following natural language instructions. A challenge in this
task is how to handle 'off the path' scenarios where an agent veers from a
reference path. Prior work supervises the agent with actions based on the
shortest path from the agent's location to the goal, but such goal-oriented
supervision is often not in alignment with the instruction. Furthermore, the
evaluation metrics employed by prior work do not measure how much of a language
instruction the agent is able to follow. In this work, we propose a simple and
effective language-aligned supervision scheme, and a new metric that measures
the number of sub-instructions the agent has completed during navigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A formal model for ledger management systems based on contracts and temporal logic. (arXiv:2109.15212v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15212">
<div class="article-summary-box-inner">
<span><p>A key component of blockchain technology is the ledger, viz., a database
that, unlike standard databases, keeps in memory the complete history of past
transactions as in a notarial archive for the benefit of any future test. In
second-generation blockchains such as Ethereum the ledger is coupled with smart
contracts, which enable the automation of transactions associated with
agreements between the parties of a financial or commercial nature. The
coupling of smart contracts and ledgers provides the technological background
for very innovative application areas, such as Decentralized Autonomous
Organizations (DAOs), Initial Coin Offerings (ICOs) and Decentralized Finance
(DeFi), which propelled blockchains beyond cryptocurrencies that were the only
focus of first generation blockchains such as the Bitcoin. However, the
currently used implementation of smart contracts as arbitrary programming
constructs has made them susceptible to dangerous bugs that can be exploited
maliciously and has moved their semantics away from that of legal contracts. We
propose here to recompose the split and recover the reliability of databases by
formalizing a notion of contract modelled as a finite-state automaton with
well-defined computational characteristics derived from an encoding in terms of
allocations of resources to actors, as an alternative to the approach based on
programming. To complete the work, we use temporal logic as the basis for an
abstract query language that is effectively suited to the historical nature of
the information kept in the ledger.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SlovakBERT: Slovak Masked Language Model. (arXiv:2109.15254v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15254">
<div class="article-summary-box-inner">
<span><p>We introduce a new Slovak masked language model called SlovakBERT in this
paper. It is the first Slovak-only transformers-based model trained on a
sizeable corpus. We evaluate the model on several NLP tasks and achieve
state-of-the-art results. We publish the masked language model, as well as the
subsequently fine-tuned models for part-of-speech tagging, sentiment analysis
and semantic textual similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks. (arXiv:2109.15256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15256">
<div class="article-summary-box-inner">
<span><p>Systematic compositionality is an essential mechanism in human language,
allowing the recombination of known parts to create novel expressions. However,
existing neural models have been shown to lack this basic ability in learning
symbolic structures. Motivated by the failure of a Transformer model on the
SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing
a command into actions, we propose two auxiliary sequence prediction tasks that
track the progress of function and argument semantics, as additional training
supervision. These automatically-generated sequences are more representative of
the underlying compositional symbolic structures of the input data. During
inference, the model jointly predicts the next action and the next tokens in
the auxiliary sequences at each step. Experiments on the SCAN dataset show that
our method encourages the Transformer to understand compositional structures of
the command, improving its accuracy on multiple challenging splits from &lt;= 10%
to 100%. With only 418 (5%) training instances, our approach still achieves
97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can
be induced in Transformers given minimal but proper guidance. We also show that
a better result is achieved using less contextualized vectors as the
attention's query, providing insights into architecture choices in achieving
systematic compositionality. Finally, we show positive generalization results
on the groundedSCAN task (Ruis et al., 2020). Our code is publicly available
at: https://github.com/jiangycTarheel/compositional-auxseq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MatSciBERT: A Materials Domain Language Model for Text Mining and Information Extraction. (arXiv:2109.15290v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15290">
<div class="article-summary-box-inner">
<span><p>An overwhelmingly large amount of knowledge in the materials domain is
generated and stored as text published in peer-reviewed scientific literature.
Recent developments in natural language processing, such as bidirectional
encoder representations from transformers (BERT) models, provide promising
tools to extract information from these texts. However, direct application of
these models in the materials domain may yield suboptimal results as the models
themselves may not be trained on notations and jargon that are specific to the
domain. Here, we present a materials-aware language model, namely, MatSciBERT,
which is trained on a large corpus of scientific literature published in the
materials domain. We further evaluate the performance of MatSciBERT on three
downstream tasks, namely, abstract classification, named entity recognition,
and relation extraction, on different materials datasets. We show that
MatSciBERT outperforms SciBERT, a language model trained on science corpus, on
all the tasks. Further, we discuss some of the applications of MatSciBERT in
the materials domain for extracting information, which can, in turn, contribute
to materials discovery or optimization. Finally, to make the work accessible to
the larger materials community, we make the pretrained and finetuned weights
and the models of MatSciBERT freely accessible.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-granular Legal Topic Classification on Greek Legislation. (arXiv:2109.15298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15298">
<div class="article-summary-box-inner">
<span><p>In this work, we study the task of classifying legal texts written in the
Greek language. We introduce and make publicly available a novel dataset based
on Greek legislation, consisting of more than 47 thousand official, categorized
Greek legislation resources. We experiment with this dataset and evaluate a
battery of advanced methods and classifiers, ranging from traditional machine
learning and RNN-based methods to state-of-the-art Transformer-based methods.
We show that recurrent architectures with domain-specific word embeddings offer
improved overall performance while being competitive even to transformer-based
models. Finally, we show that cutting-edge multilingual and monolingual
transformer-based models brawl on the top of the classifiers' ranking, making
us question the necessity of training monolingual transfer learning models as a
rule of thumb. To the best of our knowledge, this is the first time the task of
Greek legal text classification is considered in an open research project,
while also Greek is a language with very limited NLP resources in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Text Classification via Self-Pretraining. (arXiv:2109.15300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15300">
<div class="article-summary-box-inner">
<span><p>We present a neural semi-supervised learning model termed Self-Pretraining.
Our model is inspired by the classic self-training algorithm. However, as
opposed to self-training, Self-Pretraining is threshold-free, it can
potentially update its belief about previously labeled documents, and can cope
with the semantic drift problem. Self-Pretraining is iterative and consists of
two classifiers. In each iteration, one classifier draws a random set of
unlabeled documents and labels them. This set is used to initialize the second
classifier, to be further trained by the set of labeled documents. The
algorithm proceeds to the next iteration and the classifiers' roles are
reversed. To improve the flow of information across the iterations and also to
cope with the semantic drift problem, Self-Pretraining employs an iterative
distillation process, transfers hypotheses across the iterations, utilizes a
two-stage training model, uses an efficient learning rate schedule, and employs
a pseudo-label transformation heuristic. We have evaluated our model in three
publicly available social media datasets. Our experiments show that
Self-Pretraining outperforms the existing state-of-the-art semi-supervised
classifiers across multiple settings. Our code is available at
https://github.com/p-karisani/self_pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Graph Contextualized Knowledge into Pre-trained Language Models. (arXiv:1912.00147v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.00147">
<div class="article-summary-box-inner">
<span><p>Complex node interactions are common in knowledge graphs, and these
interactions also contain rich knowledge information. However, traditional
methods usually treat a triple as a training unit during the knowledge
representation learning (KRL) procedure, neglecting contextualized information
of the nodes in knowledge graphs (KGs). We generalize the modeling object to a
very general form, which theoretically supports any subgraph extracted from the
knowledge graph, and these subgraphs are fed into a novel transformer-based
model to learn the knowledge embeddings. To broaden usage scenarios of
knowledge, pre-trained language models are utilized to build a model that
incorporates the learned knowledge representations. Experimental results
demonstrate that our model achieves the state-of-the-art performance on several
medical NLP tasks, and improvement above TransE indicates that our KRL method
captures the graph contextualized information effectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Meta Sequence Learning for Generating Adequate Question-Answer Pairs. (arXiv:2010.01620v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01620">
<div class="article-summary-box-inner">
<span><p>Creating multiple-choice questions to assess reading comprehension of a given
article involves generating question-answer pairs (QAPs) on the main points of
the document. We present a learning scheme to generate adequate QAPs via
meta-sequence representations of sentences. A meta sequence is a sequence of
vectors comprising semantic and syntactic tags. In particular, we devise a
scheme called MetaQA to learn meta sequences from training data to form pairs
of a meta sequence for a declarative sentence (MD) and a corresponding
interrogative sentence (MIs). On a given declarative sentence, a trained MetaQA
model converts it to a meta sequence, finds a matched MD, and uses the
corresponding MIs and the input sentence to generate QAPs. We implement MetaQA
for the English language using semantic-role labeling, part-of-speech tagging,
and named-entity recognition, and show that trained on a small dataset, MetaQA
generates efficiently over the official SAT practice reading tests a large
number of syntactically and semantically correct QAPs with over 97\% accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v4 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09697">
<div class="article-summary-box-inner">
<span><p>The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such "saturated" networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does My Representation Capture X? Probe-Ably. (arXiv:2104.05807v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05807">
<div class="article-summary-box-inner">
<span><p>Probing (or diagnostic classification) has become a popular strategy for
investigating whether a given set of intermediate features is present in the
representations of neural models. Probing studies may have misleading results,
but various recent works have suggested more reliable methodologies that
compensate for the possible pitfalls of probing. However, these best practices
are numerous and fast-evolving. To simplify the process of running a set of
probing experiments in line with suggested methodologies, we introduce
Probe-Ably: an extendable probing framework which supports and automates the
application of probing methods to the user's inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media. (arXiv:2104.06999v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06999">
<div class="article-summary-box-inner">
<span><p>Online social media platforms increasingly rely on Natural Language
Processing (NLP) techniques to detect abusive content at scale in order to
mitigate the harms it causes to their users. However, these techniques suffer
from various sampling and association biases present in training data, often
resulting in sub-par performance on content relevant to marginalized groups,
potentially furthering disproportionate harms towards them. Studies on such
biases so far have focused on only a handful of axes of disparities and
subgroups that have annotations/lexicons available. Consequently, biases
concerning non-Western contexts are largely ignored in the literature. In this
paper, we introduce a weakly supervised method to robustly detect lexical
biases in broader geocultural contexts. Through a case study on a publicly
available toxicity detection model, we demonstrate that our method identifies
salient groups of cross-geographic errors, and, in a follow up, demonstrate
that these groupings reflect human judgments of offensive and inoffensive
language in those geographic contexts. We also conduct analysis of a model
trained on a dataset with ground truth labels to better understand these
biases, and present preliminary mitigation experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. (arXiv:2104.08758v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08758">
<div class="article-summary-box-inner">
<span><p>Large language models have led to remarkable progress on many NLP tasks, and
researchers are turning to ever-larger text corpora to train them. Some of the
largest corpora available are made by scraping significant portions of the
internet, and are frequently introduced with only minimal documentation. In
this work we provide some of the first documentation for the Colossal Clean
Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set
of filters to a single snapshot of Common Crawl. We begin by investigating
where the data came from, and find a significant amount of text from unexpected
sources like patents and US military websites. Then we explore the content of
the text itself, and find machine-generated text (e.g., from machine
translation systems) and evaluation examples from other benchmark NLP datasets.
To understand the impact of the filters applied to create this dataset, we
evaluate the text that was removed, and show that blocklist filtering
disproportionately removes text from and about minority individuals. Finally,
we conclude with some recommendations for how to created and document web-scale
datasets from a scrape of the internet.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.01542">
<div class="article-summary-box-inner">
<span><p>Machine reading comprehension (MRC) is a sub-field in natural language
processing that aims to assist computers understand unstructured texts and then
answer questions related to them. In practice, the conversation is an essential
way to communicate and transfer information. To help machines understand
conversation texts, we present UIT-ViCoQA, a new corpus for conversational
machine reading comprehension in the Vietnamese language. This corpus consists
of 10,000 questions with answers over 2,000 conversations about health news
articles. Then, we evaluate several baseline approaches for conversational
machine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1
score of 45.27%, which is 30.91 points behind human performance (76.18%),
indicating that there is ample room for improvement. Our dataset is available
at our website: <a href="http://nlp.uit.edu.vn/datasets/">this http URL</a> for research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.02605">
<div class="article-summary-box-inner">
<span><p>The representation learning on textual graph is to generate low-dimensional
embeddings for the nodes based on the individual textual features and the
neighbourhood information. Recent breakthroughs on pretrained language models
and graph neural networks push forward the development of corresponding
techniques. The existing works mainly rely on the cascaded model architecture:
the textual features of nodes are independently encoded by language models at
first; the textual embeddings are aggregated by graph neural networks
afterwards. However, the above architecture is limited due to the independent
modeling of textual features. In this work, we propose GraphFormers, where
layerwise GNN components are nested alongside the transformer blocks of
language models. With the proposed architecture, the text encoding and the
graph aggregation are fused into an iterative workflow, making each node's
semantic accurately comprehended from the global perspective. In addition, a
progressive learning strategy is introduced, where the model is successively
trained on manipulated data and original data to reinforce its capability of
integrating information on graph. Extensive evaluations are conducted on three
large-scale benchmark datasets, where GraphFormers outperform the SOTA
baselines with comparable running efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation. (arXiv:2106.11096v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11096">
<div class="article-summary-box-inner">
<span><p>In this work, we propose a novel and easy-to-apply data augmentation
strategy, namely Bilateral Generation (BiG), with a contrastive training
objective for improving the performance of ranking question answer pairs with
existing labeled data. In specific, we synthesize pseudo-positive QA pairs in
contrast to the original negative QA pairs with two pre-trained generation
models, one for question generation, the other for answer generation, which are
fine-tuned on the limited positive QA pairs from the original dataset. With the
augmented dataset, we design a contrastive training objective for learning to
rank question answer pairs. Experimental results on three benchmark datasets
show that our method significantly improves the performance of ranking models
by making full use of existing labeled data and can be easily applied to
different ranking models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13658">
<div class="article-summary-box-inner">
<span><p>The understanding of time expressions includes two sub-tasks: recognition and
normalization. In recent years, significant progress has been made in the
recognition of time expressions while research on normalization has lagged
behind. Existing SOTA normalization methods highly rely on rules or grammars
designed by experts, which limits their performance on emerging corpora, such
as social media texts. In this paper, we model time expression normalization as
a sequence of operations to construct the normalized temporal value, and we
present a novel method called ARTime, which can automatically generate
normalization rules from training data without expert interventions.
Specifically, ARTime automatically captures possible operation sequences from
annotated data and generates normalization rules on time expressions with
common surface forms. The experimental results show that ARTime can
significantly surpass SOTA methods on the Tweets benchmark, and achieves
competitive results with existing expert-engineered rule methods on the
TempEval-3 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques. (arXiv:2109.09725v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09725">
<div class="article-summary-box-inner">
<span><p>In this paper, a BERT based neural network model is applied to the JIGSAW
data set in order to create a model identifying hateful and toxic comments
(strictly seperated from offensive language) in online social platforms
(English language), in this case Twitter. Three other neural network
architectures and a GPT-2 model are also applied on the provided data set in
order to compare these different models. The trained BERT model is then applied
on two different data sets to evaluate its generalisation power, namely on
another Twitter data set and the data set HASOC 2019 which includes Twitter and
also Facebook comments; we focus on the English HASOC 2019 data. In addition,
it can be shown that by fine-tuning the trained BERT model on these two data
sets by applying different transfer learning scenarios via retraining partial
or all layers the predictive scores improve compared to simply applying the
model pre-trained on the JIGSAW data set. With our results, we get precisions
from 64% to around 90% while still achieving acceptable recall values of at
least lower 60s%, proving that BERT is suitable for real use cases in social
platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?. (arXiv:2109.11321v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11321">
<div class="article-summary-box-inner">
<span><p>Large language models are known to suffer from the hallucination problem in
that they are prone to output statements that are false or inconsistent,
indicating a lack of knowledge. A proposed solution to this is to provide the
model with additional data modalities that complements the knowledge obtained
through text. We investigate the use of visual data to complement the knowledge
of large language models by proposing a method for evaluating visual knowledge
transfer to text for uni- or multimodal language models. The method is based on
two steps, 1) a novel task querying for knowledge of memory colors, i.e.
typical colors of well-known objects, and 2) filtering of model training data
to clearly separate knowledge contributions. Additionally, we introduce a model
architecture that involves a visual imagination step and evaluate it with our
proposed method. We find that our method can successfully be used to measure
visual knowledge transfer capabilities in models and that our novel model
architecture shows promising results for leveraging multimodal knowledge in a
unimodal setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12212">
<div class="article-summary-box-inner">
<span><p>Online conversations include more than just text. Increasingly, image-based
responses such as memes and animated gifs serve as culturally recognized and
often humorous responses in conversation. However, while NLP has broadened to
multimodal models, conversational dialog systems have largely focused only on
generating text replies. Here, we introduce a new dataset of 1.56M text-gif
conversation turns and introduce a new multimodal conversational model Pepe the
King Prawn for selecting gif-based replies. We demonstrate that our model
produces relevant and high-quality gif responses and, in a large randomized
control trial of multiple models replying to real users, we show that our model
replies with gifs that are significantly better received by the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions. (arXiv:2109.13066v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13066">
<div class="article-summary-box-inner">
<span><p>Existing text-to-SQL research only considers complete questions as the input,
but lay-users might strive to formulate a complete question. To build a smarter
natural language interface to database systems (NLIDB) that also processes
incomplete questions, we propose a new task, prefix-to-SQL which takes question
prefix from users as the input and predicts the intended SQL. We construct a
new benchmark called PAGSAS that contains 124K user question prefixes and the
intended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.
Additionally, we propose a new metric SAVE to measure how much effort can be
saved by users. Experimental results show that PAGSAS is challenging even for
strong baseline models such as T5. As we observe the difficulty of
prefix-to-SQL is related to the number of omitted tokens, we incorporate
curriculum learning of feeding examples with an increasing number of omitted
tokens. This improves scores on various sub-tasks by as much as 9% recall
scores on sub-task GeoQuery in PAGSAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP). (arXiv:2109.13123v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13123">
<div class="article-summary-box-inner">
<span><p>Digital learning platforms enable students to learn on a flexible and
individual schedule as well as providing instant feedback mechanisms. The field
of STEM education requires students to solve numerous training exercises to
grasp underlying concepts. It is apparent that there are restrictions in
current online education in terms of exercise diversity and individuality. Many
exercises show little variance in structure and content, hindering the adoption
of abstraction capabilities by students. This thesis proposes an approach to
generate diverse, context rich word problems. In addition to requiring the
generated language to be grammatically correct, the nature of word problems
implies additional constraints on the validity of contents. The proposed
approach is proven to be effective in generating valid word problems for
mathematical statistics. The experimental results present a tradeoff between
generation time and exercise validity. The system can easily be parametrized to
handle this tradeoff according to the requirements of specific use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v2 [eess.SY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13662">
<div class="article-summary-box-inner">
<span><p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce
an end-to-end trainable system that integrates reasoning and perception. PSL
represents first-order logic in terms of a convex graphical model -- Hinge Loss
Markov random fields (HL-MRFs). PSL stands out among probabilistic logic
frameworks due to its tractability having been applied to systems of more than
1 billion ground rules. The key to our approach is to represent predicates in
first-order logic using deep neural networks and then to approximately
back-propagate through the HL-MRF and thus train every aspect of the
first-order system being represented. We believe that this approach represents
an interesting direction for the integration of deep learning and reasoning
techniques with applications to knowledge base learning, multi-task learning,
and explainability. We evaluate DeepPSL on a zero shot learning problem in
image classification. State of the art results demonstrate the utility and
flexibility of our approach.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-03 23:09:04.405082142 UTC">2021-10-03 23:09:04 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>