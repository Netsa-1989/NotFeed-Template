<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-09T01:30:00Z">09-09</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03264">
<div class="article-summary-box-inner">
<span><p>Speech pre-training has primarily demonstrated efficacy on classification
tasks, while its capability of generating novel speech, similar to how GPT-2
can generate coherent paragraphs, has barely been explored. Generative Spoken
Language Modeling (GSLM) (Lakhotia et al., 2021) is the only prior work
addressing the generative aspects of speech pre-training, which replaces text
with discovered phone-like units for language modeling and shows the ability to
generate meaningful novel sentences. Unfortunately, despite eliminating the
need of text, the units used in GSLM discard most of the prosodic information.
Hence, GSLM fails to leverage prosody for better comprehension, and does not
generate expressive speech. In this work, we present a prosody-aware generative
spoken language model (pGSLM). It is composed of a multi-stream transformer
language model (MS-TLM) of speech, represented as discovered unit and prosodic
feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to
waveforms. We devise a series of metrics for prosody modeling and generation,
and re-use metrics from GSLM for content modeling. Experimental results show
that the pGSLM can utilize prosody to improve both prosody and content
modeling, and also generate natural, meaningful, and coherent speech given a
spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Decoder Conformer for Multilingual Speech Recognition. (arXiv:2109.03277v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03277">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have recently become very popular for
sequence-to-sequence applications such as machine translation and speech
recognition. This work proposes a dual-decoder transformer model for
low-resource multilingual speech recognition for Indian languages. Our proposed
model consists of a Conformer [1] encoder, two parallel transformer decoders,
and a language classifier. We use a phoneme decoder (PHN-DEC) for the phoneme
recognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence
along with language information. We consider phoneme recognition and language
identification as auxiliary tasks in the multi-task learning framework. We
jointly optimize the network for phoneme recognition, grapheme recognition, and
language identification tasks with Joint CTC-Attention [2] training. Our
experiments show that we can obtain a significant reduction in WER over the
baseline approaches. We also show that our dual-decoder approach obtains
significant improvement over the single decoder approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models. (arXiv:2109.03300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03300">
<div class="article-summary-box-inner">
<span><p>All AI models are susceptible to learning biases in data that they are
trained on. For generative dialogue models, being trained on real human
conversations containing unbalanced gender and race/ethnicity references can
lead to models that display learned biases, which we define here broadly as any
measurable differences in the distributions of words or semantic content of
conversations based on demographic groups. We measure the strength of such
biases by producing artificial conversations between two copies of a dialogue
model, conditioning one conversational partner to state a name commonly
associated with a certain gender and/or race/ethnicity. We find that larger
capacity models tend to exhibit more gender bias and greater stereotyping of
occupations by gender. We show that several methods of tuning these dialogue
models, specifically name scrambling, controlled generation, and unlikelihood
training, are effective in reducing bias in conversation, including on a
downstream conversational task. Name scrambling is also effective in lowering
differences in token usage across conversations where partners have names
associated with different genders or races/ethnicities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Corpus-based Open-Domain Event Type Induction. (arXiv:2109.03322v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03322">
<div class="article-summary-box-inner">
<span><p>Traditional event extraction methods require predefined event types and their
corresponding annotations to learn event extractors. These prerequisites are
often hard to be satisfied in real-world applications. This work presents a
corpus-based open-domain event type induction method that automatically
discovers a set of event types from a given corpus. As events of the same type
could be expressed in multiple ways, we propose to represent each event type as
a cluster of &lt;predicate sense, object head&gt; pairs. Specifically, our method (1)
selects salient predicates and object heads, (2) disambiguates predicate senses
using only a verb sense dictionary, and (3) obtains event types by jointly
embedding and clustering &lt;predicate sense, object head&gt; pairs in a latent
spherical space. Our experiments, on three datasets from different domains,
show our method can discover salient and high-quality event types, according to
both automatic and human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings. (arXiv:2109.03334v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03334">
<div class="article-summary-box-inner">
<span><p>Building compositional explanations requires models to combine two or more
facts that, together, describe why the answer to a question is correct.
Typically, these "multi-hop" explanations are evaluated relative to one (or a
small number of) gold explanations. In this work, we show these evaluations
substantially underestimate model performance, both in terms of the relevance
of included facts, as well as the completeness of model-generated explanations,
because models regularly discover and produce valid explanations that are
different than gold explanations. To address this, we construct a large corpus
of 126k domain-expert (science teacher) relevance ratings that augment a corpus
of explanations to standardized science exam questions, discovering 80k
additional relevant facts not rated as gold. We build three strong models based
on different methodologies (generation, ranking, and schemas), and empirically
show that while expert-augmented ratings provide better estimates of
explanation quality, both original (gold) and expert-augmented automatic
evaluations still substantially underestimate performance by up to 36% when
compared with full manual expert judgements, with different models being
disproportionately affected. This poses a significant methodological challenge
to accurately evaluating explanations produced by compositional reasoning
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. (arXiv:2109.03381v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03381">
<div class="article-summary-box-inner">
<span><p>Spoken question answering (SQA) requires fine-grained understanding of both
spoken documents and questions for the optimal answer prediction. In this
paper, we propose novel training schemes for spoken question answering with a
self-supervised training stage and a contrastive representation learning stage.
In the self-supervised stage, we propose three auxiliary self-supervised tasks,
including utterance restoration, utterance insertion, and question
discrimination, and jointly train the model to capture consistency and
coherence among speech documents without any additional data or annotations. We
then propose to learn noise-invariant utterance representations in a
contrastive objective by adopting multiple augmentation strategies, including
span deletion and span substitution. Besides, we design a Temporal-Alignment
attention to semantically align the speech-text clues in the learned common
space and benefit the SQA tasks. By this means, the training schemes can more
effectively guide the generation model to predict more proper answers.
Experimental results show that our model achieves state-of-the-art results on
three SQA benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepZensols: Deep Natural Language Processing Framework. (arXiv:2109.03383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03383">
<div class="article-summary-box-inner">
<span><p>Reproducing results in publications by distributing publicly available source
code is becoming ever more popular. Given the difficulty of reproducing machine
learning (ML) experiments, there have been significant efforts in reducing the
variance of these results. As in any science, the ability to consistently
reproduce results effectively strengthens the underlying hypothesis of the
work, and thus, should be regarded as important as the novel aspect of the
research itself. The contribution of this work is a framework that is able to
reproduce consistent results and provides a means of easily creating, training,
and evaluating natural language processing (NLP) deep learning (DL) models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03402">
<div class="article-summary-box-inner">
<span><p>Diverse machine translation aims at generating various target language
translations for a given source language sentence. Leveraging the linear
relationship in the sentence latent space introduced by the mixup training, we
propose a novel method, MixDiversity, to generate different translations for
the input sentence by linearly interpolating it with different sentence pairs
sampled from the training corpus when decoding. To further improve the
faithfulness and diversity of the translations, we propose two simple but
effective approaches to select diverse sentence pairs in the training corpus
and adjust the interpolation weight for each pair correspondingly. Moreover, by
controlling the interpolation weight, our method can achieve the trade-off
between faithfulness and diversity without any additional training, which is
required in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14
en-de, and WMT'17 zh-en are conducted to show that our method substantially
outperforms all previous diverse machine translation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models. (arXiv:2109.03415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03415">
<div class="article-summary-box-inner">
<span><p>Multimodal machine translation (MMT) systems have been shown to outperform
their text-only neural machine translation (NMT) counterparts when visual
context is available. However, recent studies have also shown that the
performance of MMT models is only marginally impacted when the associated image
is replaced with an unrelated image or noise, which suggests that the visual
context might not be exploited by the model at all. We hypothesize that this
might be caused by the nature of the commonly used evaluation benchmark, also
known as Multi30K, where the translations of image captions were prepared
without actually showing the images to human translators. In this paper, we
present a qualitative study that examines the role of datasets in stimulating
the leverage of visual modality and we propose methods to highlight the
importance of visual signals in the datasets which demonstrate improvements in
reliance of models on the source images. Our findings suggest the research on
effective MMT architectures is currently impaired by the lack of suitable
datasets and careful consideration must be taken in creation of future MMT
datasets, for which we also provide useful insights.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It is AI's Turn to Ask Human a Question: Question and Answer Pair Generation for Children Storybooks in FairytaleQA Dataset. (arXiv:2109.03423v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03423">
<div class="article-summary-box-inner">
<span><p>Existing question answering (QA) datasets are created mainly for the
application of having AI to be able to answer questions asked by humans. But in
educational applications, teachers and parents sometimes may not know what
questions they should ask a child that can maximize their language learning
results. With a newly released book QA dataset (FairytaleQA), which educational
experts labeled on 46 fairytale storybooks for early childhood readers, we
developed an automated QA generation model architecture for this novel
application. Our model (1) extracts candidate answers from a given storybook
passage through carefully designed heuristics based on a pedagogical framework;
(2) generates appropriate questions corresponding to each extracted answer
using a language model; and, (3) uses another QA model to rank top QA-pairs.
Automatic and human evaluations show that our model outperforms baselines. We
also demonstrate that our method can help with the scarcity issue of the
children's book QA dataset via data augmentation on 200 unlabeled storybooks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03438">
<div class="article-summary-box-inner">
<span><p>In the last few years, open-domain question answering (ODQA) has advanced
rapidly due to the development of deep learning techniques and the availability
of large-scale QA datasets. However, the current datasets are essentially
designed for synchronic document collections (e.g., Wikipedia). Temporal news
collections such as long-term news archives spanning several decades, are
rarely used in training the models despite they are quite valuable for our
society. In order to foster the research in the field of ODQA on such
historical collections, we present ArchivalQA, a large question answering
dataset consisting of 1,067,056 question-answer pairs which is designed for
temporal news QA. In addition, we create four subparts of our dataset based on
the question difficulty levels and the containment of temporal expressions,
which we believe could be useful for training or testing ODQA systems
characterized by different strengths and abilities. The novel QA
dataset-constructing framework that we introduce can be also applied to create
datasets over other types of collections.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Referee: Towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis. (arXiv:2109.03439v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03439">
<div class="article-summary-box-inner">
<span><p>Cross-speaker style transfer (CSST) in text-to-speech (TTS) synthesis aims at
transferring a speaking style to the synthesised speech in a target speaker's
voice. Most previous CSST approaches rely on expensive high-quality data
carrying desired speaking style during training and require a reference
utterance to obtain speaking style descriptors as conditioning on the
generation of a new sentence. This work presents Referee, a robust
reference-free CSST approach for expressive TTS, which fully leverages
low-quality data to learn speaking styles from text. Referee is built by
cascading a text-to-style (T2S) model with a style-to-wave (S2W) model.
Phonetic PosteriorGram (PPG), phoneme-level pitch and energy contours are
adopted as fine-grained speaking style descriptors, which are predicted from
text using the T2S model. A novel pretrain-refinement method is adopted to
learn a robust T2S model by only using readily accessible low-quality data. The
S2W model is trained with high-quality target data, which is adopted to
effectively aggregate style descriptors and generate high-fidelity speech in
the target speaker's voice. Experimental results are presented, showing that
Referee outperforms a global-style-token (GST)-based baseline approach in CSST.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03481">
<div class="article-summary-box-inner">
<span><p>Contrastive learning models have achieved great success in unsupervised
visual representation learning, which maximize the similarities between feature
representations of different views of the same image, while minimize the
similarities between feature representations of views of different images. In
text summarization, the output summary is a shorter form of the input document
and they have similar meanings. In this paper, we propose a contrastive
learning model for supervised abstractive text summarization, where we view a
document, its gold summary and its model generated summaries as different views
of the same mean representation and maximize the similarities between them
during training. We improve over a strong sequence-to-sequence text generation
model (i.e., BART) on three different summarization datasets. Human evaluation
also shows that our model achieves better faithfulness ratings compared to its
counterpart without contrastive objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Social Analysis of Young Basque Speaking Communities in Twitter. (arXiv:2109.03487v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03487">
<div class="article-summary-box-inner">
<span><p>In this paper we take into account both social and linguistic aspects to
perform demographic analysis by processing a large amount of tweets in Basque
language. The study of demographic characteristics and social relationships are
approached by applying machine learning and modern deep-learning Natural
Language Processing (NLP) techniques, combining social sciences with automatic
text processing. More specifically, our main objective is to combine
demographic inference and social analysis in order to detect young Basque
Twitter users and to identify the communities that arise from their
relationships or shared content. This social and demographic analysis will be
entirely based on the~automatically collected tweets using NLP to convert
unstructured textual information into interpretable knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spelling provides a precise (but sometimes misplaced) phonological target. Orthography and acoustic variability in second language word learning. (arXiv:2109.03490v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03490">
<div class="article-summary-box-inner">
<span><p>L1 French participants learned novel L2 English words over two days of
learning sessions, with half of the words presented with their orthographic
forms (Audio-Ortho) and half without (Audio only). One group heard the words
pronounced by a single talker, while another group heard them pronounced by
multiple talkers. On the third day, they completed a variety of tasks to
evaluate their learning. Our results show a robust influence of orthography,
with faster response times in both production (picture naming) and recognition
(picture mapping) tasks for words learned in the Audio-Ortho condition.
Moreover, formant analyses of the picture naming responses show that
orthographic input pulls pronunciations of English novel words towards a
non-native (French) phonological target. Words learned with their orthographic
forms were pronounced more precisely (with smaller Dispersion Scores), but were
misplaced in the vowel space (as reflected by smaller Euclidian distances with
respect to French vowels). Finally, we found only limited evidence of an effect
of talker-based acoustic variability: novel words learned with multiple talkers
showed faster responses times in the picture naming task, but only in the
Audio-only condition, which suggests that orthographic information may have
overwhelmed any advantage of talker-based acoustic variability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03502">
<div class="article-summary-box-inner">
<span><p>This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank
twice, reaD twice). The pipeline is composed of a retriever, passage reranker,
extractive reader, generative reader and a mechanism that aggregates the final
prediction from all system's components. We demonstrate its strength across
three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,
surpassing state-of-the-art on the first two. Our analysis demonstrates that:
(i) combining extractive and generative reader yields absolute improvements up
to 5 exact match and it is at least twice as effective as the posterior
averaging ensemble of the same models with different parameters, (ii) the
extractive reader with fewer parameters can match the performance of the
generative reader on extractive QA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RefineCap: Concept-Aware Refinement for Image Captioning. (arXiv:2109.03529v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03529">
<div class="article-summary-box-inner">
<span><p>Automatically translating images to texts involves image scene understanding
and language modeling. In this paper, we propose a novel model, termed
RefineCap, that refines the output vocabulary of the language decoder using
decoder-guided visual semantics, and implicitly learns the mapping between
visual tag words and images. The proposed Visual-Concept Refinement method can
allow the generator to attend to semantic details in the image, thereby
generating more semantically descriptive captions. Our model achieves superior
performance on the MS-COCO dataset in comparison with previous visual-concept
based models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets. (arXiv:2109.03537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03537">
<div class="article-summary-box-inner">
<span><p>Pre-training language models (LMs) on large-scale unlabeled text data makes
the model much easier to achieve exceptional downstream performance than their
counterparts directly trained on the downstream tasks. In this work, we study
what specific traits in the pre-training data, other than the semantics, make a
pre-trained LM superior to their counterparts trained from scratch on
downstream tasks. We propose to use artificially constructed datasets as the
pre-training data to exclude the effect of semantics, and further control what
characteristics the pre-training corpora have. By fine-tuning the pre-trained
models on GLUE benchmark, we can learn how beneficial it is to transfer the
knowledge from the model trained on the dataset possessing that specific trait.
We define and discuss three different characteristics in the artificial
dataset: 1) matching the token's uni-gram or bi-gram distribution between
pre-training and downstream fine-tuning, 2) the presence of the explicit
dependencies among the tokens in a sequence, 3) the length of the implicit
dependencies among the tokens in a sequence. Our experiments show that the
explicit dependencies in the sequences of the pre-training data are critical to
the downstream performance. Our results also reveal that models achieve better
downstream performance when pre-trained on a dataset with a longer range of
implicit dependencies. Based on our analysis, we find that models pre-trained
with artificial datasets are prone to learn spurious correlation in downstream
tasks. Our work reveals that even if the LMs are not pre-trained on natural
language, they still gain transferability on certain human language downstream
tasks once the LMs learn to model the token dependencies in the sequences. This
result helps us understand the exceptional transferability of pre-trained LMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03551">
<div class="article-summary-box-inner">
<span><p>Voice conversion (VC) is an effective approach to electrolaryngeal (EL)
speech enhancement, a task that aims to improve the quality of the artificial
voice from an electrolarynx device. In frame-based VC methods, time alignment
needs to be performed prior to model training, and the dynamic time warping
(DTW) algorithm is widely adopted to compute the best time alignment between
each utterance pair. The validity is based on the assumption that the same
phonemes of the speakers have similar features and can be mapped by measuring a
pre-defined distance between speech frames of the source and the target.
However, the special characteristics of the EL speech can break the assumption,
resulting in a sub-optimal DTW alignment. In this work, we propose to use lip
images for time alignment, as we assume that the lip movements of laryngectomee
remain normal compared to healthy people. We investigate two naive lip
representations and distance metrics, and experimental results demonstrate that
the proposed method can significantly outperform the audio-only alignment in
terms of objective and subjective evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi. (arXiv:2109.03552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03552">
<div class="article-summary-box-inner">
<span><p>The widespread presence of offensive language on social media motivated the
development of systems capable of recognizing such content automatically. Apart
from a few notable exceptions, most research on automatic offensive language
identification has dealt with English. To address this shortcoming, we
introduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first
dataset of its kind compiled for Marathi, thus opening a new domain for
research in low-resource Indo-Aryan languages. We present results from several
machine learning experiments on this dataset, including zero-short and other
transfer learning experiments on state-of-the-art cross-lingual transformers
from existing data in Bengali, English, and Hindi.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. (arXiv:2109.03564v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03564">
<div class="article-summary-box-inner">
<span><p>Using prompts to utilize language models to perform various downstream tasks,
also known as prompt-based learning or prompt-learning, has lately gained
significant success in comparison to the pre-train and fine-tune paradigm.
Nonetheless, virtually all prompt-based methods are token-level, meaning they
all utilize GPT's left-to-right language model or BERT's masked language model
to perform cloze-style tasks. In this paper, we attempt to accomplish several
NLP tasks in the zero-shot scenario using a BERT original pre-training task
abandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike
token-level techniques, our sentence-level prompt-based method NSP-BERT does
not need to fix the length of the prompt or the position to be predicted,
allowing it to handle tasks such as entity linking with ease. Based on the
characteristics of NSP-BERT, we offer several quick building templates for
various downstream tasks. We suggest a two-stage prompt method for word sense
disambiguation tasks in particular. Our strategies for mapping the labels
significantly enhance the model's performance on sentence pair tasks. On the
FewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of
these tasks and comes close to the few-shot methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03570">
<div class="article-summary-box-inner">
<span><p>This work presents biomedical and clinical language models for Spanish by
experimenting with different pretraining choices, such as masking at word and
subword level, varying the vocabulary size and testing with domain data,
looking for better language representations. Interestingly, in the absence of
enough clinical data to train a model from scratch, we applied mixed-domain
pretraining and cross-domain transfer approaches to generate a performant
bio-clinical model suitable for real-world clinical data. We evaluated our
models on Named Entity Recognition (NER) tasks for biomedical documents and
challenging hospital discharge reports. When compared against the competitive
mBERT and BETO models, we outperform them in all NER tasks by a significant
margin. Finally, we studied the impact of the model's vocabulary on the NER
performances by offering an interesting vocabulary-centric analysis. The
results confirm that domain-specific pretraining is fundamental to achieving
higher performances in downstream NER tasks, even within a mid-resource
scenario. To the best of our knowledge, we provide the first biomedical and
clinical transformer-based pretrained language models for Spanish, intending to
boost native Spanish NLP applications in biomedicine. Our models will be made
freely available after publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes. (arXiv:2109.03571v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03571">
<div class="article-summary-box-inner">
<span><p>Research into the classification of Image with Text (IWT) troll memes has
recently become popular. Since the online community utilizes the refuge of
memes to express themselves, there is an abundance of data in the form of
memes. These memes have the potential to demean, harras, or bully targeted
individuals. Moreover, the targeted individual could fall prey to opinion
manipulation. To comprehend the use of memes in opinion manipulation, we define
three specific domains (product, political or others) which we classify into
troll or not-troll, with or without opinion manipulation. To enable this
analysis, we enhanced an existing dataset by annotating the data with our
defined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the
English language (TrollsWithOpinion dataset). We perform baseline experiments
on the annotated dataset, and our result shows that existing state-of-the-art
techniques could only reach a weighted-average F1-score of 0.37. This shows the
need for a development of a specific technique to deal with multimodal troll
memes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. (arXiv:2109.03587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03587">
<div class="article-summary-box-inner">
<span><p>Sarcasm employs ambivalence, where one says something positive but actually
means negative, and vice versa. Due to the sophisticated and obscure sentiment,
sarcasm brings in great challenges to sentiment analysis. In this paper, we
show up the essence of sarcastic text is that the literal sentiment (expressed
by the surface form of the text) is opposite to the deep sentiment (expressed
by the actual meaning of the text). To this end, we propose a Dual-Channel
Framework by modeling both literal and deep sentiments to recognize the
sentiment conflict. Specifically, the proposed framework is capable of
detecting the sentiment conflict between the literal and deep meanings of the
input text. Experiments on the political debates and the Twitter datasets show
that our framework achieves the best performance on sarcasm recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base. (arXiv:2109.03614v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03614">
<div class="article-summary-box-inner">
<span><p>Formal query building is an important part of complex question answering over
knowledge bases. It aims to build correct executable queries for questions.
Recent methods try to rank candidate queries generated by a state-transition
strategy. However, this candidate generation strategy ignores the structure of
queries, resulting in a considerable number of noisy queries. In this paper, we
propose a new formal query building approach that consists of two stages. In
the first stage, we predict the query structure of the question and leverage
the structure to constrain the generation of the candidate queries. We propose
a novel graph generation framework to handle the structure prediction task and
design an encoder-decoder model to predict the argument of the predetermined
operation in each generative step. In the second stage, we follow the previous
methods to rank the candidate queries. The experimental results show that our
formal query building approach outperforms existing methods on complex
questions while staying competitive on simple questions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discrete and Soft Prompting for Multilingual Models. (arXiv:2109.03630v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03630">
<div class="article-summary-box-inner">
<span><p>It has been shown for English that discrete and soft prompting perform
strongly in few-shot learning with pretrained language models (PLMs). In this
paper, we show that discrete and soft prompting perform better than finetuning
in multilingual cases: Crosslingual transfer and in-language training of
multilingual natural language inference. For example, with 48 English training
examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely
surpassing the majority baseline (33.33%). In contrast, discrete and soft
prompting outperform finetuning, achieving 36.43% and 38.79%. We also
demonstrate good performance of prompting with training data in multiple
languages other than English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. (arXiv:2109.03645v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03645">
<div class="article-summary-box-inner">
<span><p>In the context of neural machine translation, data augmentation (DA)
techniques may be used for generating additional training samples when the
available parallel data are scarce. Many DA approaches aim at expanding the
support of the empirical data distribution by generating new sentence pairs
that contain infrequent words, thus making it closer to the true data
distribution of parallel sentences. In this paper, we propose to follow a
completely different approach and present a multi-task DA approach in which we
generate new sentence pairs with transformations, such as reversing the order
of the target sentence, which produce unfluent target sentences. During
training, these augmented sentences are used as auxiliary tasks in a multi-task
framework with the aim of providing new contexts where the target prefix is not
informative enough to predict the next word. This strengthens the encoder and
forces the decoder to pay more attention to the source representations of the
encoder. Experiments carried out on six low-resource translation tasks show
consistent improvements over the baseline and over DA methods aiming at
extending the support of the empirical data distribution. The systems trained
with our approach rely more on the source tokens, are more robust against
domain shift and suffer less hallucinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sustainable Modular Debiasing of Language Models. (arXiv:2109.03646v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03646">
<div class="article-summary-box-inner">
<span><p>Unfair stereotypical biases (e.g., gender, racial, or religious biases)
encoded in modern pretrained language models (PLMs) have negative ethical
implications for widespread adoption of state-of-the-art language technology.
To remedy for this, a wide range of debiasing techniques have recently been
introduced to remove such stereotypical biases from PLMs. Existing debiasing
methods, however, directly modify all of the PLMs parameters, which -- besides
being computationally expensive -- comes with the inherent risk of
(catastrophic) forgetting of useful language knowledge acquired in pretraining.
In this work, we propose a more sustainable modular debiasing approach based on
dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter
modules into the original PLM layers and (2) update only the adapters (i.e., we
keep the original PLM parameters frozen) via language modeling training on a
counterfactually augmented corpus. We showcase ADELE, in gender debiasing of
BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic
bias measures, renders ADELE, very effective in bias mitigation. We further
show that -- due to its modular nature -- ADELE, coupled with task adapters,
retains fairness even after large-scale downstream training. Finally, by means
of multilingual BERT, we successfully transfer ADELE, to six target languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction. (arXiv:2109.03659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03659">
<div class="article-summary-box-inner">
<span><p>Relation extraction systems require large amounts of labeled examples which
are costly to annotate. In this work we reformulate relation extraction as an
entailment task, with simple, hand-made, verbalizations of relations produced
in less than 15 min per relation. The system relies on a pretrained textual
entailment engine which is run as-is (no training examples, zero-shot) or
further fine-tuned on labeled examples (few-shot or fully trained). In our
experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per
relation (17% points better than the best supervised system on the same
conditions), and only 4 points short to the state-of-the-art (which uses 20
times more training data). We also show that the performance can be improved
significantly with larger entailment models, up to 12 points in zero-shot,
allowing to report the best results to date on TACRED when fully trained. The
analysis shows that our few-shot systems are specially effective when
discriminating between relations, and that the performance difference in low
data regimes comes mainly from identifying no-relation cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Aspect Target Sentiment Classification with Natural Language Prompts. (arXiv:2109.03685v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03685">
<div class="article-summary-box-inner">
<span><p>For many business applications, we often seek to analyze sentiments
associated with any arbitrary aspects of commercial products, despite having a
very limited amount of labels or even without any labels at all. However,
existing aspect target sentiment classification (ATSC) models are not trainable
if annotated datasets are not available. Even with labeled data, they fall
short of reaching satisfactory performance. To address this, we propose simple
approaches that better solve ATSC with natural language prompts, enabling the
task under zero-shot cases and enhancing supervised settings, especially for
few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop
domain, our method of reformulating ATSC as an NLI task outperforms supervised
SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.
Moreover, we demonstrate that our prompts could handle implicitly stated
aspects as well: our models reach about 77% accuracy on detecting sentiments
for aspect categories (e.g., food), which do not necessarily appear within the
text, even though we trained the models only with explicitly mentioned aspect
terms (e.g., fajitas) from just 16 reviews - while the accuracy of the
no-prompt baseline is only around 65%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continuous Entailment Patterns for Lexical Inference in Context. (arXiv:2109.03695v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03695">
<div class="article-summary-box-inner">
<span><p>Combining a pretrained language model (PLM) with textual patterns has been
shown to help in both zero- and few-shot settings. For zero-shot performance,
it makes sense to design patterns that closely resemble the text seen during
self-supervised pretraining because the model has never seen anything else.
Supervised training allows for more flexibility. If we allow for tokens outside
the PLM's vocabulary, patterns can be adapted more flexibly to a PLM's
idiosyncrasies. Contrasting patterns where a "token" can be any continuous
vector vs. those where a discrete choice between vocabulary elements has to be
made, we call our method CONtinuous pAtterNs (CONAN). We evaluate CONAN on two
established benchmarks for lexical inference in context (LIiC) a.k.a. predicate
entailment, a challenging natural language understanding task with relatively
small training sets. In a direct comparison with discrete patterns, CONAN
consistently leads to improved performance, setting a new state of the art. Our
experiments give valuable insights into the kind of pattern that enhances a
PLM's performance on LIiC and raise important questions regarding our
understanding of PLMs using text patterns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Policy Compliance Detection via Question Answering. (arXiv:2109.03731v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03731">
<div class="article-summary-box-inner">
<span><p>Policy compliance detection is the task of ensuring that a scenario conforms
to a policy (e.g. a claim is valid according to government rules or a post in
an online platform conforms to community guidelines). This task has been
previously instantiated as a form of textual entailment, which results in poor
accuracy due to the complexity of the policies. In this paper we propose to
address policy compliance detection via decomposing it into question answering,
where questions check whether the conditions stated in the policy apply to the
scenario, and an expression tree combines the answers to obtain the label.
Despite the initial upfront annotation cost, we demonstrate that this approach
results in better accuracy, especially in the cross-policy setup where the
policies during testing are unseen in training. In addition, it allows us to
use existing question answering models pre-trained on existing large datasets.
Finally, it explicitly identifies the information missing from a scenario in
case policy compliance cannot be determined. We conduct our experiments using a
recent dataset consisting of government policies, which we augment with expert
annotations and find that the cost of annotating question answering
decomposition is largely offset by improved inter-annotator agreement and
speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03754">
<div class="article-summary-box-inner">
<span><p>Measuring event salience is essential in the understanding of stories. This
paper takes a recent unsupervised method for salience detection derived from
Barthes Cardinal Functions and theories of surprise and applies it to longer
narrative forms. We improve the standard transformer language model by
incorporating an external knowledgebase (derived from Retrieval Augmented
Generation) and adding a memory mechanism to enhance performance on longer
works. We use a novel approach to derive salience annotation using
chapter-aligned summaries from the Shmoop corpus for classic literary works.
Our evaluation against this data demonstrates that our salience detection model
improves performance over and above a non-knowledgebase and memory augmented
language model, both of which are crucial to this improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning by Acquiring Contrastive Examples. (arXiv:2109.03764v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03764">
<div class="article-summary-box-inner">
<span><p>Common acquisition functions for active learning use either uncertainty or
diversity sampling, aiming to select difficult and diverse data points from the
pool of unlabeled data, respectively. In this work, leveraging the best of both
worlds, we propose an acquisition function that opts for selecting
\textit{contrastive examples}, i.e. data points that are similar in the model
feature space and yet the model outputs maximally different predictive
likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a
diverse set of acquisition functions in four natural language understanding
tasks and seven datasets. Our experiments show that CAL performs consistently
better or equal than the best performing baseline across all tasks, on both
in-domain and out-of-domain data. We also conduct an extensive ablation study
of our method and we further analyze all actively acquired datasets showing
that CAL achieves a better trade-off between uncertainty and diversity compared
to other strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03772">
<div class="article-summary-box-inner">
<span><p>Multi-party dialogue machine reading comprehension (MRC) brings tremendous
challenge since it involves multiple speakers at one dialogue, resulting in
intricate speaker information flows and noisy dialogue contexts. To alleviate
such difficulties, previous models focus on how to incorporate these
information using complex graph-based modules and additional manually labeled
data, which is usually rare in real scenarios. In this paper, we design two
labour-free self- and pseudo-self-supervised prediction tasks on speaker and
key-utterance to implicitly model the speaker information flows, and capture
salient clues in a long dialogue. Experimental results on two benchmark
datasets have justified the effectiveness of our method over competitive
baselines and current state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03777">
<div class="article-summary-box-inner">
<span><p>Graph neural networks have triggered a resurgence of graph-based text
classification. We show that already a simple MLP baseline achieves comparable
performance on benchmark datasets, questioning the importance of synthetic
graph structures. When considering an inductive scenario, i. e., when adding
new documents to a corpus, a simple MLP even outperforms most graph-based
models. We further fine-tune DistilBERT for comparison and find that it
outperforms all state-of-the-art models. We suggest that future studies use at
least an MLP baseline to contextualize the results. We provide recommendations
for the design and training of such a baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Highly Parallel Autoregressive Entity Linking with Discriminative Correction. (arXiv:2109.03792v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03792">
<div class="article-summary-box-inner">
<span><p>Generative approaches have been recently shown to be effective for both
Entity Disambiguation and Entity Linking (i.e., joint mention detection and
disambiguation). However, the previously proposed autoregressive formulation
for EL suffers from i) high computational cost due to a complex (deep) decoder,
ii) non-parallelizable decoding that scales with the source sequence length,
and iii) the need for training on a large amount of data. In this work, we
propose a very efficient approach that parallelizes autoregressive linking
across all potential mentions and relies on a shallow and efficient decoder.
Moreover, we augment the generative objective with an extra discriminative
component, i.e., a correction term which lets us directly optimize the
generator's ranking. When taken together, these techniques tackle all the above
issues: our model is &gt;70 times faster and more accurate than the previous
generative method, outperforming state-of-the-art approaches on the standard
English dataset AIDA-CoNLL. Source code available at
https://github.com/nicola-decao/efficient-autoregressive-EL
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation. (arXiv:2109.03808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03808">
<div class="article-summary-box-inner">
<span><p>Recent work on multilingual AMR-to-text generation has exclusively focused on
data augmentation strategies that utilize silver AMR. However, this assumes a
high quality of generated AMRs, potentially limiting the transferability to the
target task. In this paper, we investigate different techniques for
automatically generating AMR annotations, where we aim to study which source of
information yields better multilingual results. Our models trained on gold AMR
with silver (machine translated) sentences outperform approaches which leverage
generated silver AMR. We find that combining both complementary sources of
information further improves multilingual AMR-to-text generation. Our models
surpass the previous state of the art for German, Italian, Spanish, and Chinese
by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Common Semantic Space for Monolingual and Cross-Lingual Meta-Embeddings. (arXiv:2001.06381v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.06381">
<div class="article-summary-box-inner">
<span><p>This paper presents a new technique for creating monolingual and
cross-lingual meta-embeddings. Our method integrates multiple word embeddings
created from complementary techniques, textual sources, knowledge bases and
languages. Existing word vectors are projected to a common semantic space using
linear transformations and averaging. With our method the resulting
meta-embeddings maintain the dimensionality of the original embeddings without
losing information while dealing with the out-of-vocabulary problem. An
extensive empirical evaluation demonstrates the effectiveness of our technique
with respect to previous work on various intrinsic and extrinsic multilingual
evaluations, obtaining competitive results for Semantic Textual Similarity and
state-of-the-art performance for word similarity and POS tagging (English and
Spanish). The resulting cross-lingual meta-embeddings also exhibit excellent
cross-lingual transfer learning capabilities. In other words, we can leverage
pre-trained source embeddings from a resource-rich language in order to improve
the word representations for under-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2004.13805v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13805">
<div class="article-summary-box-inner">
<span><p>As it has been unveiled that pre-trained language models (PLMs) are to some
extent capable of recognizing syntactic concepts in natural language, much
effort has been made to develop a method for extracting complete (binary)
parses from PLMs without training separate parsers. We improve upon this
paradigm by proposing a novel chart-based method and an effective top-K
ensemble technique. Moreover, we demonstrate that we can broaden the scope of
application of the approach into multilingual settings. Specifically, we show
that by applying our method on multilingual PLMs, it becomes possible to induce
non-trivial parses for sentences from nine languages in an integrated and
language-agnostic manner, attaining performance superior or comparable to that
of unsupervised PCFGs. We also verify that our approach is robust to
cross-lingual transfer. Finally, we provide analyses on the inner workings of
our method. For instance, we discover universal attention heads which are
consistently sensitive to syntactic information irrespective of the input
language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13847">
<div class="article-summary-box-inner">
<span><p>Word embeddings are a powerful natural lan-guage processing technique, but
they are ex-tremely difficult to interpret. To enable inter-pretable NLP
models, we create vectors whereeach dimension isinherently interpretable.
Byinherently interpretable, we mean a systemwhere each dimension is associated
with somehuman-understandablehintthat can describethe meaning of that
dimension. In order tocreate more interpretable word embeddings,we transform
pretrained dense word embed-dings into sparse embeddings. These new em-beddings
are inherently interpretable: each oftheir dimensions is created from and
repre-sents a natural language word or specific gram-matical concept. We
construct these embed-dings through sparse coding, where each vec-tor in the
basis set is itself a word embedding.Therefore, each dimension of our sparse
vec-tors corresponds to a natural language word.We also show that models
trained using thesesparse embeddings can achieve good perfor-mance and are more
interpretable in practice,including through human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2010.09313v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09313">
<div class="article-summary-box-inner">
<span><p>Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
paper, we probe BERT specifically to understand and measure the relational
knowledge it captures. We utilize knowledge base completion tasks to probe
every layer of pre-trained as well as fine-tuned BERT (ranking, question
answering, NER). Our findings show that knowledge is not just contained in
BERT's final layers. Intermediate layers contribute a significant amount
(17-60%) to the total knowledge found. Probing intermediate layers also reveals
how different types of knowledge emerge at varying rates. When BERT is
fine-tuned, relational knowledge is forgotten but the extent of forgetting is
impacted by the fine-tuning objective but not the size of the dataset. We found
that ranking models forget the least and retain more knowledge in their final
layer. We release our code on github to repeat the experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00234">
<div class="article-summary-box-inner">
<span><p>Transformers have shown improved performance when compared to previous
architectures for sequence processing such as RNNs. Despite their sizeable
performance gains, as recently suggested, the model is computationally
expensive to train and with a high parameter budget. In light of this, we
explore parameter-sharing methods in Transformers with a specific focus on
generative models. We perform an analysis of different parameter
sharing/reduction methods and develop the Subformer. Our model combines
sandwich-style parameter sharing, which overcomes naive cross-layer parameter
sharing in generative models, and self-attentive embedding factorization
(SAFE). Experiments on machine translation, abstractive summarization and
language modeling show that the Subformer can outperform the Transformer even
when using significantly fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v4 [cs.DS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03277">
<div class="article-summary-box-inner">
<span><p>The Minimum Linear Arrangement problem (MLA) consists of finding a mapping
$\pi$ from vertices of a graph to distinct integers that minimizes
$\sum_{\{u,v\}\in E}|\pi(u) - \pi(v)|$. In that setting, vertices are often
assumed to lie on a horizontal line and edges are drawn as semicircles above
said line. For trees, various algorithms are available to solve the problem in
polynomial time in $n=|V|$. There exist variants of the MLA in which the
arrangements are constrained. Iordanskii, and later Hochberg and Stallmann
(HS), put forward $O(n)$-time algorithms that solve the problem when
arrangements are constrained to be planar (also known as one-page book
embeddings). We also consider linear arrangements of rooted trees that are
constrained to be projective (planar embeddings where the root is not covered
by any edge). Gildea and Temperley (GT) sketched an algorithm for projective
arrangements which they claimed runs in $O(n)$ but did not provide any
justification of its cost. In contrast, Park and Levy claimed that GT's
algorithm runs in $O(n \log d_{max})$ where $d_{max}$ is the maximum degree but
did not provide sufficient detail. Here we correct an error in HS's algorithm
for the planar case, show its relationship with the projective case, and derive
simple algorithms for the projective and planar cases that run without a doubt
in $O(n)$ time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structural Adapters in Pretrained Language Models for AMR-to-text Generation. (arXiv:2103.09120v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09120">
<div class="article-summary-box-inner">
<span><p>Pretrained language models (PLM) have recently advanced graph-to-text
generation, where the input graph is linearized into a sequence and fed into
the PLM to obtain its representation. However, efficiently encoding the graph
structure in PLMs is challenging because such models were pretrained on natural
language, and modeling structured data may lead to catastrophic forgetting of
distributional knowledge. In this paper, we propose StructAdapt, an adapter
method to encode graph structure into PLMs. Contrary to prior work, StructAdapt
effectively models interactions among the nodes based on the graph
connectivity, only training graph structure-aware adapter parameters. In this
way, we incorporate task-specific knowledge while maintaining the topological
structure of the graph. We empirically show the benefits of explicitly encoding
graph structure into PLMs using StructAdapt, outperforming the state of the art
on two AMR-to-text datasets, training only 5.1% of the PLM parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers. (arXiv:2103.12279v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12279">
<div class="article-summary-box-inner">
<span><p>We introduce SelfExplain, a novel self-explaining model that explains a text
classifier's predictions using phrase-based concepts. SelfExplain augments
existing neural classifiers by adding (1) a globally interpretable layer that
identifies the most influential concepts in the training set for a given sample
and (2) a locally interpretable layer that quantifies the contribution of each
local input concept by computing a relevance score relative to the predicted
label. Experiments across five text-classification datasets show that
SelfExplain facilitates interpretability without sacrificing performance. Most
importantly, explanations from SelfExplain show sufficiency for model
predictions and are perceived as adequate, trustworthy and understandable by
human judges compared to existing widely-used baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. (arXiv:2104.01027v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01027">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning of speech representations has been a very active
research area but most work is focused on a single domain such as read audio
books for which there exist large quantities of labeled and unlabeled data. In
this paper, we explore more general setups where the domain of the unlabeled
data for pre-training data differs from the domain of the labeled data for
fine-tuning, which in turn may differ from the test data domain. Our
experiments show that using target domain data during pre-training leads to
large performance improvements across a variety of setups. On a large-scale
competitive setup, we show that pre-training on unlabeled in-domain data
reduces the gap between models trained on in-domain and out-of-domain labeled
data by 66%-73%. This has obvious practical implications since it is much
easier to obtain unlabeled target domain data than labeled data. Moreover, we
find that pre-training on multiple domains improves generalization performance
on domains not seen during training. Code and models will be made available at
https://github.com/pytorch/fairseq.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04670">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (LMs) such as GPT-3 have acquired a
surprising ability to perform zero-shot learning. For example, to classify
sentiment without any training examples, we can "prompt" the LM with the review
and the label description "Does the user like this movie?", and ask whether the
next word is "yes" or "no". However, the next word prediction training
objective is still misaligned with the target zero-shot learning objective. To
address this weakness, we propose meta-tuning, which directly optimizes the
zero-shot learning objective by fine-tuning pre-trained language models on a
collection of datasets. We focus on classification tasks, and construct the
meta-dataset by aggregating 43 existing datasets and annotating 441 label
descriptions in a question-answering (QA) format. When evaluated on unseen
tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA
zero-shot learning system based on natural language inference. Additionally,
increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,
and we forecast that even larger models would perform better. Therefore,
measuring zero-shot learning performance on language models out-of-the-box
might underestimate their true potential, and community-wide efforts on
aggregating datasets and unifying their formats can help build models that
answer prompts better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media. (arXiv:2104.08116v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08116">
<div class="article-summary-box-inner">
<span><p>Language use differs between domains and even within a domain, language use
changes over time. For pre-trained language models like BERT, domain adaptation
through continued pre-training has been shown to improve performance on
in-domain downstream tasks. In this article, we investigate whether temporal
adaptation can bring additional benefits. For this purpose, we introduce a
corpus of social media comments sampled over three years. It contains
unlabelled data for adaptation and evaluation on an upstream masked language
modelling task as well as labelled data for fine-tuning and evaluation on a
downstream document classification task. We find that temporality matters for
both tasks: temporal adaptation improves upstream and temporal fine-tuning
downstream task performance. Time-specific models generally perform better on
past than on future test sets, which matches evidence on the bursty usage of
topical words. However, adapting BERT to time and domain does not improve
performance on the downstream task over only adapting to domain. Token-level
analysis shows that temporal adaptation captures event-driven changes in
language use in the downstream task, but not those changes that are actually
relevant to task performance. Based on our findings, we discuss when temporal
adaptation may be more effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Noisy Labels for Entity-Centric Information Extraction. (arXiv:2104.08656v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08656">
<div class="article-summary-box-inner">
<span><p>Recent information extraction approaches have relied on training deep neural
models. However, such models can easily overfit noisy labels and suffer from
performance degradation. While it is very costly to filter noisy labels in
large learning resources, recent studies show that such labels take more
training steps to be memorized and are more frequently forgotten than clean
labels, therefore are identifiable in training. Motivated by such properties,
we propose a simple co-regularization framework for entity-centric information
extraction, which consists of several neural models with identical structures
but different parameter initialization. These models are jointly optimized with
the task-specific losses and are regularized to generate similar predictions
based on an agreement loss, which prevents overfitting on noisy labels.
Extensive experiments on two widely used but noisy benchmarks for information
extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.
We release our code to the community for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v3 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08663">
<div class="article-summary-box-inner">
<span><p>Existing neural information retrieval (IR) models have often been studied in
homogeneous and narrow settings, which has considerably limited insights into
their out-of-distribution (OOD) generalization capabilities. To address this,
and to facilitate researchers to broadly evaluate the effectiveness of their
models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous
evaluation benchmark for information retrieval. We leverage a careful selection
of 18 publicly available datasets from diverse text retrieval tasks and domains
and evaluate 10 state-of-the-art retrieval systems including lexical, sparse,
dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our
results show BM25 is a robust baseline and re-ranking and
late-interaction-based models on average achieve the best zero-shot
performances, however, at high computational costs. In contrast, dense and
sparse-retrieval models are computationally more efficient but often
underperform other approaches, highlighting the considerable room for
improvement in their generalization capabilities. We hope this framework allows
us to better evaluate and understand existing retrieval systems, and
contributes to accelerating progress towards better robust and generalizable
systems in the future. BEIR is publicly available at
https://github.com/UKPLab/beir.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Out-of-Distribution Detection for Pretrained Transformers. (arXiv:2104.08812v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08812">
<div class="article-summary-box-inner">
<span><p>Pretrained Transformers achieve remarkable performance when training and test
data are from the same distribution. However, in real-world scenarios, the
model often faces out-of-distribution (OOD) instances that can cause severe
semantic shift problems at inference time. Therefore, in practice, a reliable
model should identify such instances, and then either reject them during
inference or pass them over to models that handle another distribution. In this
paper, we develop an unsupervised OOD detection method, in which only the
in-distribution (ID) data are used in training. We propose to fine-tune the
Transformers with a contrastive loss, which improves the compactness of
representations, such that OOD instances can be better differentiated from ID
ones. These OOD instances can then be accurately detected using the Mahalanobis
distance in the model's penultimate layer. We experiment with comprehensive
settings and achieve near-perfect OOD detection performance, outperforming
baselines drastically. We further investigate the rationales behind the
improvement, finding that more compact representations through margin-based
contrastive learning bring the improvement. We release our code to the
community for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stream-level Latency Evaluation for Simultaneous Machine Translation. (arXiv:2104.08817v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08817">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation has recently gained traction thanks to
significant quality improvements and the advent of streaming applications.
Simultaneous translation systems need to find a trade-off between translation
quality and response time, and with this purpose multiple latency measures have
been proposed. However, latency evaluations for simultaneous translation are
estimated at the sentence level, not taking into account the sequential nature
of a streaming scenario. Indeed, these sentence-level latency measures are not
well suited for continuous stream translation resulting in figures that are not
coherent with the simultaneous translation policy of the system being assessed.
This work proposes a stream-level adaptation of the current latency measures
based on a re-segmentation approach applied to the output translation, that is
successfully evaluated on streaming conditions for a reference IWSLT task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02902">
<div class="article-summary-box-inner">
<span><p>Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
article, we probe BERT specifically to understand and measure the relational
knowledge it captures in its parametric memory. While probing for linguistic
understanding is commonly applied to all layers of BERT as well as fine-tuned
models, this has not been done for factual knowledge. We utilize existing
knowledge base completion tasks (LAMA) to probe every layer of pre-trained as
well as fine-tuned BERT models(ranking, question answering, NER). Our findings
show that knowledge is not just contained in BERT's final layers. Intermediate
layers contribute a significant amount (17-60%) to the total knowledge found.
Probing intermediate layers also reveals how different types of knowledge
emerge at varying rates. When BERT is fine-tuned, relational knowledge is
forgotten. The extent of forgetting is impacted by the fine-tuning objective
and the training data. We found that ranking models forget the least and retain
more knowledge in their final layer compared to masked language modeling and
question-answering. However, masked language modeling performed the best at
acquiring new knowledge from the training data. When it comes to learning
facts, we found that capacity and fact density are key factors. We hope this
initial work will spur further research into understanding the parametric
memory of language models and the effect of training objectives on factual
knowledge. The code to repeat the experiments is publicly available on GitHub.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13405">
<div class="article-summary-box-inner">
<span><p>COLIEE is an annual competition in automatic computerized legal text
processing. Automatic legal document processing is an ambitious goal, and the
structure and semantics of the law are often far more complex than everyday
language. In this article, we survey and report our methods and experimental
results in using deep learning in legal document processing. The results show
the difficulties as well as potentials in this family of approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04539">
<div class="article-summary-box-inner">
<span><p>Understanding documents from their visual snapshots is an emerging problem
that requires both advanced computer vision and NLP methods. The recent advance
in OCR enables the accurate recognition of text blocks, yet it is still
challenging to extract key information from documents due to the diversity of
their layouts. Although recent studies on pre-trained language models show the
importance of incorporating layout information on this task, the conjugation of
texts and their layouts still follows the style of BERT optimized for
understanding the 1D text. This implies there is room for further improvement
considering the 2D nature of text layouts. This paper introduces a pre-trained
language model, BERT Relying On Spatiality (BROS), which effectively utilizes
the information included in individual text blocks and their layouts.
Specifically, BROS encodes spatial information by utilizing relative positions
and learns spatial dependencies between OCR blocks with a novel area-masking
strategy. These two novel approaches lead to an efficient encoding of spatial
layout information highlighted by the robust performance of BROS under
low-resource environments. We also introduce a general-purpose parser that can
be combined with BROS to extract key information even when there is no order
information between text blocks. BROS shows its superiority on four public
benchmarks -- FUNSD, SROIE*, CORD, and SciTSR -- and its robustness in
practical cases where order information of text blocks is not available.
Further experiments with a varying number of training examples demonstrate the
high training efficiency of our approach. Our code will be open to the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v6 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on six public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work has claimed, our auxiliary experiments suggest that
relation prediction is contributory to named entity prediction in a
non-negligible way. The source code can be found at
https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications since the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current algorithms can tackle such
problem reliably in a realistic scenario where zero OOD training data is
available. In this study, we propose ProtoInfoMax, a new architecture that
extends Prototypical Networks to simultaneously process In-Domain (ID) and OOD
sentences via Mutual Information Maximization (InfoMax) objective. Experimental
results show that our proposed method can substantially improve performance up
to 20% for OOD detection in low resource settings of text classification. We
also show that ProtoInfoMax is less prone to typical over-confidence Error of
Neural Networks, leading to more reliable ID and OOD prediction outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. (arXiv:2108.13134v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13134">
<div class="article-summary-box-inner">
<span><p>Despite significant progress has been achieved in text summarization, factual
inconsistency in generated summaries still severely limits its practical
applications. Among the key factors to ensure factual consistency, a reliable
automatic evaluation metric is the first and the most crucial one. However,
existing metrics either neglect the intrinsic cause of the factual
inconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation
with human judgments or increasing the inconvenience of usage in practice. In
light of these challenges, we propose a novel metric to evaluate the factual
consistency in text summarization via counterfactual estimation, which
formulates the causal relationship among the source document, the generated
summary, and the language prior. We remove the effect of language prior, which
can cause factual inconsistency, from the total causal effect on the generated
summary, and provides a simple yet effective way to evaluate consistency
without relying on other auxiliary tasks. We conduct a series of experiments on
three public abstractive text summarization datasets, and demonstrate the
advantages of the proposed metric in both improving the correlation with human
judgments and the convenience of usage. The source code is available at
https://github.com/xieyxclack/factual_coco.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13990">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,
but how to properly use them for dialogue state tracking has not been
systematically investigated. In this paper, we study this problem from the
perspectives of pre-training objectives as well as the formats of context
representations. We demonstrate that the choice of pre-training objective makes
a significant difference to the state tracking quality. In particular, we find
that masked span prediction is more effective than auto-regressive language
modeling. We also explore using Pegasus, a span prediction-based pre-training
objective for text summarization, for the state tracking model. We found that
pre-training for the seemingly distant summarization task works surprisingly
well for dialogue state tracking. In addition, we found that while recurrent
state context representation works also reasonably well, the model may have a
hard time recovering from earlier mistakes. We conducted experiments on the
MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00084">
<div class="article-summary-box-inner">
<span><p>Collaborative software development is an integral part of the modern software
development life cycle, essential to the success of large-scale software
projects. When multiple developers make concurrent changes around the same
lines of code, a merge conflict may occur. Such conflicts stall pull requests
and continuous integration pipelines for hours to several days, seriously
hurting developer productivity.
</p>
<p>In this paper, we introduce MergeBERT, a novel neural program merge framework
based on the token-level three-way differencing and a transformer encoder
model. Exploiting restricted nature of merge conflict resolutions, we
reformulate the task of generating the resolution sequence as a classification
task over a set of primitive merge patterns extracted from real-world merge
commit data.
</p>
<p>Our model achieves 64--69% precision of merge resolution synthesis, yielding
nearly a 2x performance improvement over existing structured and neural program
merge tools. Finally, we demonstrate versatility of our model, which is able to
perform program merge in a multilingual setting with Java, JavaScript,
TypeScript, and C# programming languages, generalizing zero-shot to unseen
languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00430">
<div class="article-summary-box-inner">
<span><p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a
range of professional medical services, i.e., diagnosis, consultation, and
treatment. However, one-stop MDS is still unexplored because: (1) no dataset
has so large-scale dialogues contains both multiple medical services and
fine-grained medical labels (i.e., intents, slots, values); (2) no model has
addressed a MDS based on multiple-service conversations in a unified framework.
In this work, we first build a Multiple-domain Multiple-service medical
dialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between
doctors and patients, covering 276 types of diseases, 2,468 medical entities,
and 3 specialties of medical services. To the best of our knowledge, it is the
only medical dialogue dataset that includes both multiple medical services and
fine-grained medical labels. Then, we formulate a one-stop MDS as a
sequence-to-sequence generation problem. We unify a MDS with causal language
modeling and conditional causal language modeling, respectively. Specifically,
we employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)
and their variants to get benchmarks on M^2-MedDialog dataset. We also propose
pseudo labeling and natural perturbation methods to expand M2-MedDialog dataset
and enhance the state-of-the-art pretrained models. We demonstrate the results
achieved by the benchmarks so far through extensive experiments on
M2-MedDialog. We release the dataset, the code, as well as the evaluation
scripts to facilitate future research in this important research direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PermuteFormer: Efficient Relative Position Encoding for Long Sequences. (arXiv:2109.02377v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02377">
<div class="article-summary-box-inner">
<span><p>A recent variation of Transformer, Performer, scales Transformer to longer
sequences with a linear attention mechanism. However, it is not compatible with
relative position encoding, which has advantages over absolute position
encoding. In this paper, we discuss possible ways to add relative position
encoding to Performer. Based on the analysis, we propose PermuteFormer, a
Performer-based model with relative position encoding that scales linearly on
long sequences. PermuteFormer applies position-dependent transformation on
queries and keys to encode positional information into the attention module.
This transformation is carefully crafted so that the final output of
self-attention is not affected by absolute positions of tokens. PermuteFormer
introduces negligible computational overhead by design that it runs as fast as
Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long
sequences, as well as WikiText-103, a language modeling dataset. The
experiments show that PermuteFormer uniformly improves the performance of
Performer with almost no computational overhead and outperforms vanilla
Transformer on most of the tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02401">
<div class="article-summary-box-inner">
<span><p>Multimodal abstractive summarization (MAS) models that summarize videos
(vision modality) and their corresponding transcripts (text modality) are able
to extract the essential information from massive multimodal data on the
Internet. Recently, large-scale generative pre-trained language models (GPLMs)
have been shown to be effective in text generation tasks. However, existing MAS
models cannot leverage GPLMs' powerful generation ability. To fill this
research gap, we aim to study two research questions: 1) how to inject visual
information into GPLMs without hurting their generation ability; and 2) where
is the optimal place in GPLMs to inject the visual information? In this paper,
we present a simple yet effective method to construct vision guided (VG) GPLMs
for the MAS task using attention-based add-on layers to incorporate visual
information while maintaining their original text generation ability. Results
show that our best model significantly surpasses the prior state-of-the-art
model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,
and our visual guidance method contributes 83.6% of the overall improvement.
Furthermore, we conduct thorough ablation studies to analyze the effectiveness
of various modality fusion methods and fusion locations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition. (arXiv:2109.01163v2 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01163">
<div class="article-summary-box-inner">
<span><p>The recently proposed Conformer architecture has shown state-of-the-art
performances in Automatic Speech Recognition by combining convolution with
attention to model both local and global dependencies. In this paper, we study
how to reduce the Conformer architecture complexity with a limited computing
budget, leading to a more efficient architecture design that we call Efficient
Conformer. We introduce progressive downsampling to the Conformer encoder and
propose a novel attention mechanism named grouped attention, allowing us to
reduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence
length $n$, hidden dimension $d$ and group size parameter $g$. We also
experiment the use of strided multi-head self-attention as a global
downsampling operation. Our experiments are performed on the LibriSpeech
dataset with CTC and RNN-Transducer losses. We show that within the same
computing budget, the proposed architecture achieves better performances with
faster training and decoding compared to the Conformer. Our 13M parameters CTC
model achieves competitive WERs of 3.6%/9.0% without using a language model and
2.7%/6.7% with an external n-gram language model on the test-clean/test-other
sets while being 29% faster than our CTC Conformer baseline at inference and
36% faster to train.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-09 23:08:46.158934190 UTC">2021-09-09 23:08:46 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>