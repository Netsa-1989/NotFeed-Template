<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-15T01:30:00Z">10-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modelling via Learning to Rank. (arXiv:2110.06961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06961">
<div class="article-summary-box-inner">
<span><p>We consider language modelling (LM) as a multi-label structured prediction
task by re-framing training from solely predicting a single ground-truth word
to ranking a set of words which could continue a given context. To avoid
annotating top-$k$ ranks, we generate them using pre-trained LMs: GPT-2, BERT,
and Born-Again models. This leads to a rank-based form of knowledge
distillation (KD). We also develop a method using $N$-grams to create a
non-probabilistic teacher which generates the ranks without the need of a
pre-trained LM.
</p>
<p>We confirm the hypotheses that we can treat LMing as a ranking task and that
we can do so without the use of a pre-trained LM. We show that rank-based KD
generally improves perplexity (PPL), often with statistical significance, when
compared to Kullback-Leibler-based KD. Surprisingly, given the simplicity of
the method, $N$-grams act as competitive teachers and achieve similar
performance as using either BERT or a Born-Again model teachers. GPT-2 always
acts as the best teacher, though, and using it and a Transformer-XL student on
Wiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and
against a KL-based KD of 56.70.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Domain Question-Answering for COVID-19 and Other Emergent Domains. (arXiv:2110.06962v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06962">
<div class="article-summary-box-inner">
<span><p>Since late 2019, COVID-19 has quickly emerged as the newest biomedical
domain, resulting in a surge of new information. As with other emergent
domains, the discussion surrounding the topic has been rapidly changing,
leading to the spread of misinformation. This has created the need for a public
space for users to ask questions and receive credible, scientific answers. To
fulfill this need, we turn to the task of open-domain question-answering, which
we can use to efficiently find answers to free-text questions from a large set
of documents. In this work, we present such a system for the emergent domain of
COVID-19. Despite the small data size available, we are able to successfully
train the system to retrieve answers from a large-scale corpus of published
COVID-19 scientific papers. Furthermore, we incorporate effective re-ranking
and question-answering techniques, such as document diversity and multiple
answer spans. Our open-domain question-answering system can further act as a
model for the quick development of similar systems that can be adapted and
modified for other developing emergent domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FlexiTerm: A more efficient implementation of flexible multi-word term recognition. (arXiv:2110.06981v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06981">
<div class="article-summary-box-inner">
<span><p>Terms are linguistic signifiers of domain-specific concepts. Automated
recognition of multi-word terms (MWT) in free text is a sequence labelling
problem, which is commonly addressed using supervised machine learning methods.
Their need for manual annotation of training data makes it difficult to port
such methods across domains. FlexiTerm, on the other hand, is a fully
unsupervised method for MWT recognition from domain-specific corpora.
Originally implemented in Java as a proof of concept, it did not scale well,
thus offering little practical value in the context of big data. In this paper,
we describe its re-implementation in Python and compare the performance of
these two implementations. The results demonstrated major improvements in terms
of efficiency, which allow FlexiTerm to transition from the proof of concept to
the production-grade application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits. (arXiv:2110.06997v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06997">
<div class="article-summary-box-inner">
<span><p>Training data for machine translation (MT) is often sourced from a multitude
of large corpora that are multi-faceted in nature, e.g. containing contents
from multiple domains or different levels of quality or complexity. Naturally,
these facets do not occur with equal frequency, nor are they equally important
for the test scenario at hand. In this work, we propose to optimize this
balance jointly with MT model parameters to relieve system developers from
manual schedule design. A multi-armed bandit is trained to dynamically choose
between facets in a way that is most beneficial for the MT system. We evaluate
it on three different multi-facet applications: balancing translationese and
natural training data, or data from multiple domains or multiple language
pairs. We find that bandit learning leads to competitive MT systems across
tasks, and our analysis provides insights into its learned strategies and the
underlying data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation. (arXiv:2110.07002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07002">
<div class="article-summary-box-inner">
<span><p>Text autoencoders are often used for unsupervised conditional text generation
by applying mappings in the latent space to change attributes to the desired
values. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these
mappings in the embedding space of an autoencoder. However, their method is
restricted to autoencoders with a single-vector embedding, which limits how
much information can be retained. We address this issue by extending their
method to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a
variable-size bag of vectors that grows with the size of the text, as in
attention-based models. This allows to encode and reconstruct much longer texts
than standard autoencoders. Analogous to conventional autoencoders, we propose
regularization techniques that facilitate learning meaningful operations in the
latent space. Finally, we adapt for a training scheme that learns to map an
input bag to an output bag, including a novel loss function and neural
architecture. Our experimental evaluations on unsupervised sentiment transfer
and sentence summarization show that our method performs substantially better
than a standard autoencoder.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparison of SVD and factorized TDNN approaches for speech to text. (arXiv:2110.07027v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07027">
<div class="article-summary-box-inner">
<span><p>This work concentrates on reducing the RTF and word error rate of a hybrid
HMM-DNN. Our baseline system uses an architecture with TDNN and LSTM layers. We
find this architecture particularly useful for lightly reverberated
environments. However, these models tend to demand more computation than is
desirable. In this work, we explore alternate architectures employing singular
value decomposition (SVD) is applied to the TDNN layers to reduce the RTF, as
well as to the affine transforms of every LSTM cell. We compare this approach
with specifying bottleneck layers similar to those introduced by SVD before
training. Additionally, we reduced the search space of the decoding graph to
make it a better fit to operate in real-time applications. We report -61.57%
relative reduction in RTF and almost 1% relative decrease in WER for our
architecture trained on Fisher data along with reverberated versions of this
dataset in order to match one of our target test distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving the Robustness to Variations of Objects and Instructions with a Neuro-Symbolic Approach for Interactive Instruction Following. (arXiv:2110.07031v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07031">
<div class="article-summary-box-inner">
<span><p>An interactive instruction following task has been proposed as a benchmark
for learning to map natural language instructions and first-person vision into
sequences of actions to interact with objects in a 3D simulated environment. We
find that an existing end-to-end neural model for this task is not robust to
variations of objects and language instructions. We assume that this problem is
due to the high sensitiveness of neural feature extraction to small changes in
vision and language inputs. To mitigate this problem, we propose a
neuro-symbolic approach that performs reasoning over high-level symbolic
representations that are robust to small changes in raw inputs. Our experiments
on the ALFRED dataset show that our approach significantly outperforms the
existing model by 18, 52, and 73 points in the success rate on the
ToggleObject, PickupObject, and SliceObject subtasks in unseen environments
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Efficient NLP: A Standard Evaluation and A Strong Baseline. (arXiv:2110.07038v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07038">
<div class="article-summary-box-inner">
<span><p>Supersized pre-trained language models have pushed the accuracy of various
NLP tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless
SOTA accuracy, most works are pursuing improvement on other dimensions such as
efficiency, leading to "Pareto SOTA". Different from accuracy, the metric for
efficiency varies across different studies, making them hard to be fairly
compared. To that end, this work presents ELUE (Efficient Language
Understanding Evaluation), a standard evaluation, and a public leaderboard for
efficient NLP models. ELUE is dedicated to depicting the Pareto Front for
various language understanding tasks, such that it can tell whether and how
much a method achieves Pareto improvement. Along with the benchmark, we also
pre-train and release a strong baseline, ElasticBERT, whose elasticity is both
static and dynamic. ElasticBERT is static in that it allows reducing model
layers on demand. ElasticBERT is dynamic in that it selectively executes parts
of model layers conditioned on the input. We demonstrate the ElasticBERT,
despite its simplicity, outperforms or performs on par with SOTA compressed and
early exiting models. The ELUE benchmark is publicly available at
<a href="http://eluebenchmark.fastnlp.top/.">this http URL</a>
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Continual learning using lattice-free MMI for speech recognition. (arXiv:2110.07055v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07055">
<div class="article-summary-box-inner">
<span><p>Continual learning (CL), or domain expansion, recently became a popular topic
for automatic speech recognition (ASR) acoustic modeling because practical
systems have to be updated frequently in order to work robustly on types of
speech not observed during initial training. While sequential adaptation allows
tuning a system to a new domain, it may result in performance degradation on
the old domains due to catastrophic forgetting. In this work we explore
regularization-based CL for neural network acoustic models trained with the
lattice-free maximum mutual information (LF-MMI) criterion. We simulate domain
expansion by incrementally adapting the acoustic model on different public
datasets that include several accents and speaking styles. We investigate two
well-known CL techniques, elastic weight consolidation (EWC) and learning
without forgetting (LWF), which aim to reduce forgetting by preserving model
weights or network outputs. We additionally introduce a sequence-level LWF
regularization, which exploits posteriors from the denominator graph of LF-MMI
to further reduce forgetting. Empirical results show that the proposed
sequence-level LWF can improve the best average word error rate across all
domains by up to 9.4% relative compared with using regular LWF.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MIMICause : Defining, identifying and predicting types of causal relationships between biomedical concepts from clinical notes. (arXiv:2110.07090v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07090">
<div class="article-summary-box-inner">
<span><p>Understanding of causal narratives communicated in clinical notes can help
make strides towards personalized healthcare. In this work, MIMICause, we
propose annotation guidelines, develop an annotated corpus and provide baseline
scores to identify types and direction of causal relations between a pair of
biomedical concepts in clinical notes; communicated implicitly or explicitly,
identified either in a single sentence or across multiple sentences.
</p>
<p>We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2
shared task dataset and train four different language model based
architectures. Annotation based on our guidelines achieved a high
inter-annotator agreement i.e. Fleiss' kappa score of 0.72 and our model for
identification of causal relation achieved a macro F1 score of 0.56 on test
data. The high inter-annotator agreement for clinical text shows the quality of
our annotation guidelines while the provided baseline F1 score sets the
direction for future research towards understanding narratives in clinical
texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Introductions in Podcast Episodes from Automatically Generated Transcripts. (arXiv:2110.07096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07096">
<div class="article-summary-box-inner">
<span><p>As the volume of long-form spoken-word content such as podcasts explodes,
many platforms desire to present short, meaningful, and logically coherent
segments extracted from the full content. Such segments can be consumed by
users to sample content before diving in, as well as used by the platform to
promote and recommend content. However, little published work is focused on the
segmentation of spoken-word content, where the errors (noise) in transcripts
generated by automatic speech recognition (ASR) services poses many challenges.
Here we build a novel dataset of complete transcriptions of over 400 podcast
episodes, in which we label the position of introductions in each episode.
These introductions contain information about the episodes' topics, hosts, and
guests, providing a valuable summary of the episode content, as it is created
by the authors. We further augment our dataset with word substitutions to
increase the amount of available training data. We train three Transformer
models based on the pre-trained BERT and different augmentation strategies,
which achieve significantly better performance compared with a static embedding
model, showing that it is possible to capture generalized, larger-scale
structural information from noisy, loosely-organized speech data. This is
further demonstrated through an analysis of the models' inner architecture. Our
methods and dataset can be used to facilitate future work on the
structure-based segmentation of spoken-word content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A CLIP-Enhanced Method for Video-Language Understanding. (arXiv:2110.07137v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07137">
<div class="article-summary-box-inner">
<span><p>This technical report summarizes our method for the Video-And-Language
Understanding Evaluation (VALUE) challenge
(https://value-benchmark.github.io/challenge\_2021.html). We propose a
CLIP-Enhanced method to incorporate the image-text pretrained knowledge into
downstream video-text tasks. Combined with several other improved designs, our
method outperforms the state-of-the-art by $2.4\%$ ($57.58$ to $60.00$)
Meta-Ave score on VALUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer. (arXiv:2110.07139v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07139">
<div class="article-summary-box-inner">
<span><p>Adversarial attacks and backdoor attacks are two common security threats that
hang over deep learning. Both of them harness task-irrelevant features of data
in their implementation. Text style is a feature that is naturally irrelevant
to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In
this paper, we make the first attempt to conduct adversarial and backdoor
attacks based on text style transfer, which is aimed at altering the style of a
sentence while preserving its meaning. We design an adversarial attack method
and a backdoor attack method, and conduct extensive experiments to evaluate
them. Experimental results show that popular NLP models are vulnerable to both
adversarial and backdoor attacks based on text style transfer -- the attack
success rates can exceed 90% without much effort. It reflects the limited
ability of NLP models to handle the feature of text style that has not been
widely realized. In addition, the style transfer-based adversarial and backdoor
attack methods show superiority to baselines in many aspects. All the code and
data of this paper can be obtained at https://github.com/thunlp/StyleAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">bert2BERT: Towards Reusable Pretrained Language Models. (arXiv:2110.07143v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07143">
<div class="article-summary-box-inner">
<span><p>In recent years, researchers tend to pre-train ever-larger language models to
explore the upper limit of deep models. However, large language model
pre-training costs intensive computational resources and most of the models are
trained from scratch without reusing the existing pre-trained models, which is
wasteful. In this paper, we propose bert2BERT, which can effectively transfer
the knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a
large model (e.g., BERT_LARGE) through parameter initialization and
significantly improve the pre-training efficiency of the large model.
Specifically, we extend the previous function-preserving on Transformer-based
language model, and further improve it by proposing advanced knowledge for
large model's initialization. In addition, a two-stage pre-training method is
proposed to further accelerate the training process. We did extensive
experiments on representative PLMs (e.g., BERT and GPT) and demonstrate that
(1) our method can save a significant amount of training cost compared with
baselines including learning from scratch, StackBERT and MSLT; (2) our method
is generic and applicable to different types of pre-trained models. In
particular, bert2BERT saves about 45% and 47% computational cost of
pre-training BERT_BASE and GPT_BASE by reusing the models of almost their half
sizes. The source code will be publicly available upon publication.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual GenQA: A Language-Agnostic Generative Question Answering Approach for Open-Domain Question Answering. (arXiv:2110.07150v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07150">
<div class="article-summary-box-inner">
<span><p>Open-Retrieval Generative Question Answering (GenQA) is proven to deliver
high-quality, natural-sounding answers in English. In this paper, we present
the first generalization of the GenQA approach for the multilingual
environment. To this end, we present the GenTyDiQA dataset, which extends the
TyDiQA evaluation data (Clark et al., 2020) with natural-sounding, well-formed
answers in Arabic, Bengali, English, Japanese, and Russian. For all these
languages, we show that a GenQA sequence-to-sequence-based model outperforms a
state-of-the-art Answer Sentence Selection model. We also show that a
multilingually-trained model competes with, and in some cases outperforms, its
monolingual counterparts. Finally, we show that our system can even compete
with strong baselines, even when fed with information from a variety of
languages. Essentially, our system is able to answer a question in any language
of our language set using information from many languages, making it the first
Language-Agnostic GenQA system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causally Estimating the Sensitivity of Neural NLP Models to Spurious Features. (arXiv:2110.07159v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07159">
<div class="article-summary-box-inner">
<span><p>Recent work finds modern natural language processing (NLP) models relying on
spurious features for prediction. Mitigating such effects is thus important.
Despite this need, there is no quantitative measure to evaluate or compare the
effects of different forms of spurious features in NLP. We address this gap in
the literature by quantifying model sensitivity to spurious features with a
causal estimand, dubbed CENT, which draws on the concept of average treatment
effect from the causality literature. By conducting simulations with four
prominent NLP models -- TextRNN, BERT, RoBERTa and XLNet -- we rank the models
against their sensitivity to artificial injections of eight spurious features.
We further hypothesize and validate that models that are more sensitive to a
spurious feature will be less robust against perturbations with this feature
during inference. Conversely, data augmentation with this feature improves
robustness to similar perturbations. We find statistically significant inverse
correlations between sensitivity and robustness, providing empirical support
for our hypothesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence. (arXiv:2110.07160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07160">
<div class="article-summary-box-inner">
<span><p>This paper proposes a transformer over transformer framework, called
Transformer$^2$, to perform neural text segmentation. It consists of two
components: bottom-level sentence encoders using pre-trained transformers, and
an upper-level transformer-based segmentation model based on the sentence
embeddings. The bottom-level component transfers the pre-trained knowledge
learned from large external corpora under both single and pair-wise supervised
NLP tasks to model the sentence embeddings for the documents. Given the
sentence embeddings, the upper-level transformer is trained to recover the
segmentation boundaries as well as the topic labels of each sentence. Equipped
with a multi-task loss and the pre-trained knowledge, Transformer$^2$ can
better capture the semantic coherence within the same segments. Our experiments
show that (1) Transformer$^2$ manages to surpass state-of-the-art text
segmentation models in terms of a commonly-used semantic coherence measure; (2)
in most cases, both single and pair-wise pre-trained knowledge contribute to
the model performance; (3) bottom-level sentence encoders pre-trained on
specific languages yield better performance than those pre-trained on specific
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Attention-Aware Hierarchical Topic Model. (arXiv:2110.07161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07161">
<div class="article-summary-box-inner">
<span><p>Neural topic models (NTMs) apply deep neural networks to topic modelling.
Despite their success, NTMs generally ignore two important aspects: (1) only
document-level word count information is utilized for the training, while more
fine-grained sentence-level information is ignored, and (2) external semantic
knowledge regarding documents, sentences and words are not exploited for the
training. To address these issues, we propose a variational autoencoder (VAE)
NTM model that jointly reconstructs the sentence and document word counts using
combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic
embeddings. The pre-trained embeddings are first transformed into a common
latent topical space to align their semantics with the BoW embeddings. Our
model also features hierarchical KL divergence to leverage embeddings of each
document to regularize those of their sentences, thereby paying more attention
to semantically relevant sentences. Both quantitative and qualitative
experiments have shown the efficacy of our model in 1) lowering the
reconstruction errors at both the sentence and document levels, and 2)
discovering more coherent topics from real-world datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07165">
<div class="article-summary-box-inner">
<span><p>Analysis of vision-and-language models has revealed their brittleness under
linguistic phenomena such as paraphrasing, negation, textual entailment, and
word substitutions with synonyms or antonyms. While data augmentation
techniques have been designed to mitigate against these failure modes, methods
that can integrate this knowledge into the training pipeline remain
under-explored. In this paper, we present \textbf{SDRO}, a model-agnostic
method that utilizes a set linguistic transformations in a distributed robust
optimization setting, along with an ensembling technique to leverage these
transformations during inference. Experiments on benchmark datasets with images
(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as
robustness to adversarial attacks. Experiments on binary VQA explore the
generalizability of this method to other V\&amp;L tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization. (arXiv:2110.07166v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07166">
<div class="article-summary-box-inner">
<span><p>Neural abstractive summarization models are susceptible to generating
factually inconsistent content, a phenomenon known as hallucination. This
limits the usability and adoption of these systems in real-world applications.
To reduce the presence of hallucination, we propose the Mixture of Factual
Experts (MoFE) model, which combines multiple summarization experts that each
target a specific type of error. We train our experts using reinforcement
learning (RL) to minimize the error defined by two factual consistency metrics:
entity overlap and dependency arc entailment. We construct MoFE by combining
the experts using two ensembling strategies (weights and logits) and evaluate
them on two summarization datasets (XSUM and CNN/DM). Our experiments on BART
models show that the MoFE improves performance according to both entity overlap
and dependency arc entailment, without a significant performance drop on
standard ROUGE metrics. The performance improvement also transfers to unseen
factual consistency metrics, such as question answer-based factuality
evaluation metric and BERTScore precision with respect to the source document.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-gloss Augmentation for Improving Word Sense Disambiguation. (arXiv:2110.07174v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07174">
<div class="article-summary-box-inner">
<span><p>The goal of Word Sense Disambiguation (WSD) is to identify the sense of a
polysemous word in a specific context. Deep-learning techniques using BERT have
achieved very promising results in the field and different methods have been
proposed to integrate structured knowledge to enhance performance. At the same
time, an increasing number of data augmentation techniques have been proven to
be useful for NLP tasks. Building upon previous works leveraging BERT and
WordNet knowledge, we explore different data augmentation techniques on
context-gloss pairs to improve the performance of WSD. In our experiment, we
show that both sentence-level and word-level augmentation methods are effective
strategies for WSD. Also, we find out that performance can be improved by
adding hypernyms' glosses obtained from a lexical knowledge base. We compare
and analyze different context-gloss augmentation techniques, and the results
show that applying back translation on gloss performs the best.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models. (arXiv:2110.07178v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07178">
<div class="article-summary-box-inner">
<span><p>The common practice for training commonsense models has gone
from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in
order to train commonsense models. In this work, we investigate an alternative,
from-machine-to-corpus-to-machine: general language models author these
commonsense knowledge graphs to train commonsense models. Our study leads to a
new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge
Distillation (Hinton et al., 2015), our approach uses larger models to teach
smaller models. A key difference is that we distill knowledge symbolically-as
text-in addition to the neural model. We also distill only one aspect-the
commonsense of a general language model teacher, allowing the student to be a
different type, a commonsense model. Altogether, we show that careful prompt
engineering and a separately trained critic model allow us to selectively
distill high-quality causal commonsense from GPT-3, a general language model.
Empirical results demonstrate that, for the first time, a human-authored
commonsense knowledge graph is surpassed by our automatically distilled variant
in all three criteria: quantity, quality, and diversity. In addition, it
results in a neural commonsense model that surpasses the teacher model's
commonsense capabilities despite its 100x smaller size. We apply this to the
ATOMIC resource, and share our new symbolic knowledge graph and commonsense
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting IPA-based Cross-lingual Text-to-speech. (arXiv:2110.07187v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07187">
<div class="article-summary-box-inner">
<span><p>International Phonetic Alphabet (IPA) has been widely used in cross-lingual
text-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,
IPA itself has been understudied in cross-lingual TTS. In this paper, we report
some empirical findings of building a cross-lingual TTS model using IPA as
inputs. Experiments show that the way to process the IPA and suprasegmental
sequence has a negligible impact on the CL VC performance. Furthermore, we find
that using a dataset including one speaker per language to build an IPA-based
TTS system would fail CL VC since the language-unique IPA and tone/stress
symbols could leak the speaker information. In addition, we experiment with
different combinations of speakers in the training dataset to further
investigate the effect of the number of speakers on the CL VC performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling. (arXiv:2110.07198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07198">
<div class="article-summary-box-inner">
<span><p>Although large-scale pre-trained neural models have shown impressive
performances in a variety of tasks, their ability to generate coherent text
that appropriately models discourse phenomena is harder to evaluate and less
understood. Given the claims of improved text generation quality across various
systems, we consider the coherence evaluation of machine generated text to be
one of the principal applications of coherence models that needs to be
investigated. We explore training data and self-supervision objectives that
result in a model that generalizes well across tasks and can be used
off-the-shelf to perform such evaluations. Prior work in neural coherence
modeling has primarily focused on devising new architectures, and trained the
model to distinguish coherent and incoherent text through pairwise
self-supervision on the permuted documents task. We instead use a basic model
architecture and show significant improvements over state of the art within the
same training regime. We then design a harder self-supervision objective by
increasing the ratio of negative samples within a contrastive learning setup,
and enhance the model further through automatic hard negative mining coupled
with a large global negative queue encoded by a momentum encoder. We show
empirically that increasing the density of negative samples improves the basic
model, and using a global negative queue further improves and stabilizes the
model while training with hard negative samples. We evaluate the coherence
model on task-independent test sets that resemble real-world use cases and show
significant improvements in coherence evaluations of downstream applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechT5: Unified-Modal Encoder-Decoder Pre-training for Spoken Language Processing. (arXiv:2110.07205v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07205">
<div class="article-summary-box-inner">
<span><p>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in
pre-training natural language processing models, we propose a unified-modal
SpeechT5 framework that explores the encoder-decoder pre-training for
self-supervised speech/text representation learning. The SpeechT5 framework
consists of a shared encoder-decoder network and six modal-specific
(speech/text) pre/post-nets. After preprocessing the speech/text input through
the pre-nets, the shared encoder-decoder network models the sequence to
sequence transformation, and then the post-nets generate the output in the
speech/text modality based on the decoder output. Particularly, SpeechT5 can
pre-train on a large scale of unlabeled speech and text data to improve the
capability of the speech and textual modeling. To align the textual and speech
information into a unified semantic space, we propose a cross-modal vector
quantization method with random mixing-up to bridge speech and text. Extensive
evaluations on a wide variety of spoken language processing tasks, including
voice conversion, automatic speech recognition, text to speech, and speaker
identification, show the superiority of the proposed SpeechT5 framework.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dual-Attention Neural Network for Pun Location and Using Pun-Gloss Pairs for Interpretation. (arXiv:2110.07209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07209">
<div class="article-summary-box-inner">
<span><p>Pun location is to identify the punning word (usually a word or a phrase that
makes the text ambiguous) in a given short text, and pun interpretation is to
find out two different meanings of the punning word. Most previous studies
adopt limited word senses obtained by WSD(Word Sense Disambiguation) technique
or pronunciation information in isolation to address pun location. For the task
of pun interpretation, related work pays attention to various WSD algorithms.
In this paper, a model called DANN (Dual-Attentive Neural Network) is proposed
for pun location, effectively integrates word senses and pronunciation with
context information to address two kinds of pun at the same time. Furthermore,
we treat pun interpretation as a classification task and construct pungloss
pairs as processing data to solve this task. Experiments on the two benchmark
datasets show that our proposed methods achieve new state-of-the-art results.
Our source code is available in the public code repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data. (arXiv:2110.07210v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07210">
<div class="article-summary-box-inner">
<span><p>Recently, sequence-to-sequence (seq-to-seq) models have been successfully
applied in text-to-speech (TTS) to synthesize speech for single-language text.
To synthesize speech for multiple languages usually requires multi-lingual
speech from the target speaker. However, it is both laborious and expensive to
collect high-quality multi-lingual TTS data for the target speakers. In this
paper, we proposed to use low-quality code-switched found data from the
non-target speakers to achieve cross-lingual voice cloning for the target
speakers. Experiments show that our proposed method can generate high-quality
code-switched speech in the target voices in terms of both naturalness and
speaker consistency. More importantly, we find that our method can achieve a
comparable result to the state-of-the-art (SOTA) performance in cross-lingual
voice cloning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Causal Transformers Perform Below Chance on Recursive Nested Constructions, Unlike Humans. (arXiv:2110.07240v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07240">
<div class="article-summary-box-inner">
<span><p>Recursive processing is considered a hallmark of human linguistic abilities.
A recent study evaluated recursive processing in recurrent neural language
models (RNN-LMs) and showed that such models perform below chance level on
embedded dependencies within nested constructions -- a prototypical example of
recursion in natural language. Here, we study if state-of-the-art Transformer
LMs do any better. We test four different Transformer LMs on two different
types of nested constructions, which differ in whether the embedded (inner)
dependency is short or long range. We find that Transformers achieve
near-perfect performance on short-range embedded dependencies, significantly
better than previous results reported for RNN-LMs and humans. However, on
long-range embedded dependencies, Transformers' performance sharply drops below
chance level. Remarkably, the addition of only three words to the embedded
dependency caused Transformers to fall from near-perfect to below-chance
performance. Taken together, our results reveal Transformers' shortcoming when
it comes to recursive, structure-based, processing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Building Chinese Biomedical Language Models via Multi-Level Text Discrimination. (arXiv:2110.07244v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07244">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized
the field of NLP, not only in the general domain but also in the biomedical
domain. Most prior efforts in building biomedical PLMs have resorted simply to
domain adaptation and focused mainly on English. In this work we introduce
eHealth, a biomedical PLM in Chinese built with a new pre-training framework.
This new framework trains eHealth as a discriminator through both token-level
and sequence-level discrimination. The former is to detect input tokens
corrupted by a generator and select their original signals from plausible
candidates, while the latter is to further distinguish corruptions of a same
original sequence from those of the others. As such, eHealth can learn language
semantics at both the token and sequence levels. Extensive experiments on 11
Chinese biomedical language understanding tasks of various forms verify the
effectiveness and superiority of our approach. The pre-trained model is
available to the public at
\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and the
code will also be released later.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Approach to Mispronunciation Detection and Diagnosis with Acoustic, Phonetic and Linguistic (APL) Embeddings. (arXiv:2110.07274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07274">
<div class="article-summary-box-inner">
<span><p>Many mispronunciation detection and diagnosis (MD&amp;D) research approaches try
to exploit both the acoustic and linguistic features as input. Yet the
improvement of the performance is limited, partially due to the shortage of
large amount annotated training data at the phoneme level. Phonetic embeddings,
extracted from ASR models trained with huge amount of word level annotations,
can serve as a good representation of the content of input speech, in a
noise-robust and speaker-independent manner. These embeddings, when used as
implicit phonetic supplementary information, can alleviate the data shortage of
explicit phoneme annotations. We propose to utilize Acoustic, Phonetic and
Linguistic (APL) embedding features jointly for building a more powerful MD\&amp;D
system. Experimental results obtained on the L2-ARCTIC database show the
proposed approach outperforms the baseline by 9.93%, 10.13% and 6.17% on the
detection accuracy, diagnosis error rate and the F-measure, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts. (arXiv:2110.07280v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07280">
<div class="article-summary-box-inner">
<span><p>Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of
the factual information extracted from Large Language Models (LLMs) depends on
the prompts used to query them. This inconsistency is problematic because
different users will query LLMs for the same information using different
wording, but should receive the same, accurate responses regardless. In this
work we aim to address this shortcoming by introducing P-Adapters: lightweight
models that sit between the embedding layer and first attention layer of LLMs.
They take LLM embeddings as input and output continuous prompts that are used
to query the LLM. Additionally, we investigate Mixture of Experts (MoE) models
that learn a set of continuous prompts ("experts") and select one to query the
LLM. They require a separate classifier trained on human-annotated data to map
natural language prompts to the continuous ones. P-Adapters perform comparably
to the more complex MoE models in extracting factual information from BERT and
RoBERTa while eliminating the need for additional annotations. P-Adapters show
between 12-26% absolute improvement in precision and 36-50% absolute
improvement in consistency over a baseline of only using natural language
queries. Finally, we investigate what makes a P-adapter successful and conclude
that access to the LLM's embeddings of the original natural language prompt,
particularly the subject of the entity pair being asked about, is a significant
factor.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. (arXiv:2110.07298v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07298">
<div class="article-summary-box-inner">
<span><p>Existing approaches to lifelong language learning rely on plenty of labeled
data for learning a new task, which is hard to obtain in most real scenarios.
Considering that humans can continually learn new tasks from a handful of
examples, we expect the models also to be able to generalize well on new
few-shot tasks without forgetting the previous ones. In this work, we define
this more challenging yet practical problem as Lifelong Few-shot Language
Learning (LFLL) and propose a unified framework for it based on prompt tuning
of T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot
learning ability, and simultaneously trains the model as a task solver and a
data generator. Before learning a new domain of the same task type, LFPT5
generates pseudo (labeled) samples of previously learned domains, and later
gets trained on those samples to alleviate forgetting of previous knowledge as
it learns the new domain. In addition, a KL divergence loss is minimized to
achieve label consistency between the previous and the current model. While
adapting to a new task type, LFPT5 includes and tunes additional prompt
embeddings for the new task. With extensive experiments, we demonstrate that
LFPT5 can be applied to various different types of tasks and significantly
outperform previous methods in different LFLL settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-Sentiment-Multiple-Opinion Triplet Extraction. (arXiv:2110.07303v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07303">
<div class="article-summary-box-inner">
<span><p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term
(aspect), sentiment and opinion term (opinion) triplets from sentences and can
tell a complete story, i.e., the discussed aspect, the sentiment toward the
aspect, and the cause of the sentiment. ASTE is a charming task, however, one
triplet extracted by ASTE only includes one opinion of the aspect, but an
aspect in a sentence may have multiple corresponding opinions and one opinion
only provides part of the reason why the aspect has this sentiment, as a
consequence, some triplets extracted by ASTE are hard to understand, and
provide erroneous information for downstream tasks. In this paper, we introduce
a new task, named Aspect Sentiment Multiple Opinions Triplet Extraction
(ASMOTE). ASMOTE aims to extract aspect, sentiment and multiple opinions
triplets. Specifically, one triplet extracted by ASMOTE contains all opinions
about the aspect and can tell the exact reason that the aspect has the
sentiment. We propose an Aspect-Guided Framework (AGF) to address this task.
AGF first extracts aspects, then predicts their opinions and sentiments.
Moreover, with the help of the proposed Sequence Labeling Attention(SLA), AGF
improves the performance of the sentiment classification using the extracted
opinions. Experimental results on multiple datasets demonstrate the
effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Investigation of Multi-bridge Multilingual NMT models. (arXiv:2110.07304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07304">
<div class="article-summary-box-inner">
<span><p>In this paper, we present an extensive investigation of multi-bridge,
many-to-many multilingual NMT models (MB-M2M) ie., models trained on
non-English language pairs in addition to English-centric language pairs. In
addition to validating previous work which shows that MB-M2M models can
overcome zeroshot translation problems, our analysis reveals the following
results about multibridge models: (1) it is possible to extract a reasonable
amount of parallel corpora between non-English languages for low-resource
languages (2) with limited non-English centric data, MB-M2M models are
competitive with or outperform pivot models, (3) MB-M2M models can outperform
English-Any models and perform at par with Any-English models, so a single
multilingual NMT system can serve all translation directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solving Aspect Category Sentiment Analysis as a Text Generation Task. (arXiv:2110.07310v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07310">
<div class="article-summary-box-inner">
<span><p>Aspect category sentiment analysis has attracted increasing research
attention. The dominant methods make use of pre-trained language models by
learning effective aspect category-specific representations, and adding
specific output layers to its pre-trained representation. We consider a more
direct way of making use of pre-trained language models, by casting the ACSA
tasks into natural language generation tasks, using natural language sentences
to represent the output. Our method allows more direct use of pre-trained
knowledge in seq2seq language models by directly following the task setting
during pre-training. Experiments on several benchmarks show that our method
gives the best reported results, having large advantages in few-shot and
zero-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WMDecompose: A Framework for Leveraging the Interpretable Properties of Word Mover's Distance in Sociocultural Analysis. (arXiv:2110.07330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07330">
<div class="article-summary-box-inner">
<span><p>Despite the increasing popularity of NLP in the humanities and social
sciences, advances in model performance and complexity have been accompanied by
concerns about interpretability and explanatory power for sociocultural
analysis. One popular model that balances complexity and legibility is Word
Mover's Distance (WMD). Ostensibly adapted for its interpretability, WMD has
nonetheless been used and further developed in ways which frequently discard
its most interpretable aspect: namely, the word-level distances required for
translating a set of words into another set of words. To address this apparent
gap, we introduce WMDecompose: a model and Python library that 1) decomposes
document-level distances into their constituent word-level distances, and 2)
subsequently clusters words to induce thematic elements, such that useful
lexical information is retained and summarized for analysis. To illustrate its
potential in a social scientific context, we apply it to a longitudinal social
media corpus to explore the interrelationship between conspiracy theories and
conservative American discourses. Finally, because of the full WMD model's high
time-complexity, we additionally suggest a method of sampling document pairs
from large datasets in a reproducible way, with tight bounds that prevent
extrapolation of unreliable results due to poor sampling practices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plug-Tagger: A Pluggable Sequence Labeling Framework Using Language Models. (arXiv:2110.07331v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07331">
<div class="article-summary-box-inner">
<span><p>Plug-and-play functionality allows deep learning models to adapt well to
different tasks without requiring any parameters modified. Recently,
prefix-tuning was shown to be a plug-and-play method on various text generation
tasks by simply inserting corresponding continuous vectors into the inputs.
However, sequence labeling tasks invalidate existing plug-and-play methods
since different label sets demand changes to the architecture of the model
classifier. In this work, we propose the use of label word prediction instead
of classification to totally reuse the architecture of pre-trained models for
sequence labeling tasks. Specifically, for each task, a label word set is first
constructed by selecting a high-frequency word for each class respectively, and
then, task-specific vectors are inserted into the inputs and optimized to
manipulate the model predictions towards the corresponding label words. As a
result, by simply switching the plugin vectors on the input, a frozen
pre-trained language model is allowed to perform different tasks. Experimental
results on three sequence labeling tasks show that the performance of the
proposed method can achieve comparable performance with standard fine-tuning
with only 0.1\% task-specific parameters. In addition, our method is up to 70
times faster than non-plug-and-play methods while switching different tasks
under the resource-constrained scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Legal Question Answering Systems. (arXiv:2110.07333v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07333">
<div class="article-summary-box-inner">
<span><p>Many legal professionals think that the explosion of information about local,
regional, national, and international legislation makes their practice more
costly, time-consuming, and even error-prone. The two main reasons for this are
that most legislation is usually unstructured, and the tremendous amount and
pace with which laws are released causes information overload in their daily
tasks. In the case of the legal domain, the research community agrees that a
system allowing to generate automatic responses to legal questions could
substantially impact many practical implications in daily activities. The
degree of usefulness is such that even a semi-automatic solution could
significantly help to reduce the workload to be faced. This is mainly because a
Question Answering system could be able to automatically process a massive
amount of legal resources to answer a question or doubt in seconds, which means
that it could save resources in the form of effort, money, and time to many
professionals in the legal sector. In this work, we quantitatively and
qualitatively survey the solutions that currently exist to meet this challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07342">
<div class="article-summary-box-inner">
<span><p>Recent methods for embodied instruction following are typically trained
end-to-end using imitation learning. This requires the use of expert
trajectories and low-level language instructions. Such approaches assume
learned hidden states will simultaneously integrate semantics from the language
and vision to perform state tracking, spatial memory, exploration, and
long-term planning. In contrast, we propose a modular method with structured
representations that (1) builds a semantic map of the scene, and (2) performs
exploration with a semantic search policy, to achieve the natural language
goal. Our modular method achieves SOTA performance (24.46%) with a substantial
(8.17 % absolute) gap from previous work while using less data by eschewing
both expert trajectories and low-level instructions. Leveraging low-level
language, however, can further increase our performance (26.49%). Our findings
suggest that an explicit spatial memory and a semantic search policy can
provide a stronger and more general representation for state-tracking and
guidance, even in the absence of expert trajectories or low-level instructions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Music Playlist Title Generation: A Machine-Translation Approach. (arXiv:2110.07354v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07354">
<div class="article-summary-box-inner">
<span><p>We propose a machine-translation approach to automatically generate a
playlist title from a set of music tracks. We take a sequence of track IDs as
input and a sequence of words in a playlist title as output, adapting the
sequence-to-sequence framework based on Recurrent Neural Network (RNN) and
Transformer to the music data. Considering the orderless nature of music tracks
in a playlist, we propose two techniques that remove the order of the input
sequence. One is data augmentation by shuffling and the other is deleting the
positional encoding. We also reorganize the existing music playlist datasets to
generate phrase-level playlist titles. The result shows that the Transformer
models generally outperform the RNN model. Also, removing the order of input
sequence improves the performance further.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization. (arXiv:2110.07356v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07356">
<div class="article-summary-box-inner">
<span><p>In medical dialogue summarization, summaries must be coherent and must
capture all the medically relevant information in the dialogue. However,
learning effective models for summarization require large amounts of labeled
data which is especially hard to obtain. We present an algorithm to create
synthetic training data with an explicit focus on capturing medically relevant
information. We utilize GPT-3 as the backbone of our algorithm and scale 210
human labeled examples to yield results comparable to using 6400 human labeled
examples (~30x) leveraging low-shot learning and an ensemble method. In
detailed experiments, we show that this approach produces high quality training
data that can further be combined with human labeled data to get summaries that
are strongly preferable to those produced by models trained on human data alone
both in terms of medical accuracy and coherency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Text Mining of COVID-19 Records. (arXiv:2110.07357v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07357">
<div class="article-summary-box-inner">
<span><p>Since the beginning of coronavirus, the disease has spread worldwide and
drastically changed many aspects of the human's lifestyle. Twitter as a
powerful tool can help researchers measure public health in response to
COVID-19. According to the high volume of data production on social networks,
automated text mining approaches can help search, read and summarize helpful
information. This paper preprocessed the existing medical dataset regarding
COVID-19 named CORD-19 and annotated the dataset for supervised classification
tasks. At this time of the COVID-19 pandemic, we made a preprocessed dataset
for the research community. This may contribute towards finding new solutions
for some social interventions that COVID-19 has made. The preprocessed version
of the mentioned dataset is publicly available through Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory-Based Semantic Parsing. (arXiv:2110.07358v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07358">
<div class="article-summary-box-inner">
<span><p>We present a memory-based model for context-dependent semantic parsing.
Previous approaches focus on enabling the decoder to copy or modify the parse
from the previous utterance, assuming there is a dependency between the current
and previous parses. In this work, we propose to represent contextual
information using an external memory. We learn a context memory controller that
manages the memory by maintaining the cumulative meaning of sequential user
utterances. We evaluate our approach on three semantic parsing benchmarks.
Experimental results show that our model can better process context-dependent
information and demonstrates improved performance without using task-specific
decoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. (arXiv:2110.07367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07367">
<div class="article-summary-box-inner">
<span><p>In various natural language processing tasks, passage retrieval and passage
re-ranking are two key procedures in finding and ranking relevant information.
Since both the two procedures contribute to the final performance, it is
important to jointly optimize them in order to achieve mutual improvement. In
this paper, we propose a novel joint training approach for dense passage
retrieval and passage re-ranking. A major contribution is that we introduce the
dynamic listwise distillation, where we design a unified listwise training
approach for both the retriever and the re-ranker. During the dynamic
distillation, the retriever and the re-ranker can be adaptively improved
according to each other's relevance information. We also propose a hybrid data
augmentation strategy to construct diverse training instances for listwise
training approach. Extensive experiments show the effectiveness of our approach
on both MSMARCO and Natural Questions datasets. Our code is available at
https://github.com/PaddlePaddle/RocketQA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Semantic Knowledge Into Language Encoders. (arXiv:2110.07382v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07382">
<div class="article-summary-box-inner">
<span><p>We introduce semantic form mid-tuning, an approach for transferring semantic
knowledge from semantic meaning representations into transformer-based language
encoders. In mid-tuning, we learn to align the text of general sentences -- not
tied to any particular inference task -- and structured semantic
representations of those sentences. Our approach does not require gold
annotated semantic representations. Instead, it makes use of automatically
generated semantic representations, such as from off-the-shelf PropBank and
FrameNet semantic parsers. We show that this alignment can be learned
implicitly via classification or directly via triplet loss. Our method yields
language encoders that demonstrate improved predictive performance across
inference, reading comprehension, textual similarity, and other semantic tasks
drawn from the GLUE, SuperGLUE, and SentEval benchmarks. We evaluate our
approach on three popular baseline models, where our experimental results and
analysis concludes that current pre-trained language models can further benefit
from structured semantic frames with the proposed mid-tuning method, as they
inject additional task-agnostic knowledge to the encoder, improving the
generated embeddings as well as the linguistic properties of the given model,
as evident from improvements on a popular sentence embedding toolkit and a
variety of probing tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Neglected Sibling: Isotropic Gaussian Posterior for VAE. (arXiv:2110.07383v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07383">
<div class="article-summary-box-inner">
<span><p>Deep generative models have been widely used in several areas of NLP, and
various techniques have been proposed to augment them or address their training
challenges. In this paper, we propose a simple modification to Variational
Autoencoders (VAEs) by using an Isotropic Gaussian Posterior (IGP) that allows
for better utilisation of their latent representation space. This model avoids
the sub-optimal behavior of VAEs related to inactive dimensions in the
representation space. We provide both theoretical analysis, and empirical
evidence on various datasets and tasks that show IGP leads to consistent
improvement on several quantitative and qualitative grounds, from downstream
task performance and sample efficiency to robustness. Additionally, we give
insights about the representational properties encouraged by IGP and also show
that its gain generalises to image domain as well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages. (arXiv:2110.07385v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07385">
<div class="article-summary-box-inner">
<span><p>Style transfer is the task of rewriting an input sentence into a target style
while approximately preserving its content. While most prior literature assumes
access to large style-labelled corpora, recent work (Riley et al. 2021) has
attempted "few-shot" style transfer using only 3-10 sentences at inference for
extracting the target style. In this work we consider one such low resource
setting where no datasets are available: style transfer for Indian languages.
We find that existing few-shot methods perform this task poorly, with a strong
tendency to copy inputs verbatim. We push the state-of-the-art for few-shot
style transfer with a new method modeling the stylistic difference between
paraphrases. When compared to prior work using automatic and human evaluations,
our model achieves 2-3x better performance and output diversity in formality
transfer and code-mixing addition across five Indian languages. Moreover, our
method is better able to control the amount of style transfer using an input
scalar knob. We report promising qualitative results for several attribute
transfer directions, including sentiment transfer, text simplification, gender
neutralization and text anonymization, all without retraining the model.
Finally we found model evaluation to be difficult due to the lack of evaluation
datasets and metrics for Indian languages. To facilitate further research in
formality transfer for Indic languages, we crowdsource annotations for 4000
sentence pairs in four languages, and use this dataset to design our automatic
evaluation suite.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning. (arXiv:2110.07410v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07410">
<div class="article-summary-box-inner">
<span><p>Automated audio captioning (AAC) is the task of automatically generating
textual descriptions for general audio signals. A captioning system has to
identify various information from the input signal and express it with natural
language. Existing works mainly focus on investigating new methods and try to
improve their performance measured on existing datasets. Having attracted
attention only recently, very few works on AAC study the performance of
existing pre-trained audio and natural language processing resources. In this
paper, we evaluate the performance of off-the-shelf models with a
Transformer-based captioning approach. We utilize the freely available Clotho
dataset to compare four different pre-trained machine listening models, four
word embedding models, and their combinations in many different settings. Our
evaluation suggests that YAMNet combined with BERT embeddings produces the best
captions. Moreover, in general, fine-tuning pre-trained word embeddings can
lead to better performance. Finally, we show that sequences of audio embeddings
can be processed using a Transformer encoder to produce higher-quality
captions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple, Strong and Robust Baseline for Distantly Supervised Relation Extraction. (arXiv:2110.07415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07415">
<div class="article-summary-box-inner">
<span><p>Distantly supervised relation extraction (DS-RE) is generally framed as a
multi-instance multi-label (MI-ML) task, where the optimal aggregation of
information from multiple instances is of key importance. Intra-bag attention
(Lin et al., 2016) is an example of a popularly used aggregation scheme for
this framework. Apart from this scheme, however, there is not much to choose
from in the DS-RE literature as most of the advances in this field are focused
on improving the instance-encoding step rather than the instance-aggregation
step. With recent works leveraging large pre-trained language models as
encoders, the increased capacity of models might allow for more flexibility in
the instance-aggregation step. In this work, we explore this hypothesis and
come up with a novel aggregation scheme which we call Passage-Att. Under this
aggregation scheme, we combine all instances mentioning an entity pair into a
"passage of instances", which is summarized independently for each relation
class. These summaries are used to predict the validity of a potential triple.
We show that our Passage-Att with BERT as passage encoder achieves
state-of-the-art performance in three different settings (monolingual DS,
monolingual DS with manually-annotated test set, multilingual DS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal Frames. (arXiv:2110.07420v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07420">
<div class="article-summary-box-inner">
<span><p>Social concepts referring to non-physical objects--such as revolution,
violence, or friendship--are powerful tools to describe, index, and query the
content of visual data, including ever-growing collections of art images from
the Cultural Heritage (CH) field. While much progress has been made towards
complete image understanding in computer vision, automatic detection of social
concepts evoked by images is still a challenge. This is partly due to the
well-known semantic gap problem, worsened for social concepts given their lack
of unique physical features, and reliance on more unspecific features than
concrete concepts. In this paper, we propose the translation of recent
cognitive theories about social concept representation into a software approach
to represent them as multimodal frames, by integrating multisensory data. Our
method focuses on the extraction, analysis, and integration of multimodal
features from visual art material tagged with the concepts of interest. We
define a conceptual model and present a novel ontology for formally
representing social concepts as multimodal frames. Taking the Tate Gallery's
collection as an empirical basis, we experiment our method on a corpus of art
images to provide a proof of concept of its potential. We discuss further
directions of research, and provide all software, data sources, and results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Model Robustness to User-generated Noisy Texts. (arXiv:2110.07428v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07428">
<div class="article-summary-box-inner">
<span><p>Sensitivity of deep-neural models to input noise is known to be a challenging
problem. In NLP, model performance often deteriorates with naturally occurring
noise, such as spelling errors. To mitigate this issue, models may leverage
artificially noised data. However, the amount and type of generated noise has
so far been determined arbitrarily. We therefore propose to model the errors
statistically from grammatical-error-correction corpora. We present a thorough
evaluation of several state-of-the-art NLP systems' robustness in multiple
languages, with tasks including morpho-syntactic analysis, named entity
recognition, neural machine translation, a subset of the GLUE benchmark and
reading comprehension. We also compare two approaches to address the
performance drop: a) training the NLP models with noised data generated by our
framework; and b) reducing the input noise with external system for natural
language correction. The code is released at https://github.com/ufal/kazitext.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards More Effective and Economic Sparsely-Activated Model. (arXiv:2110.07431v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07431">
<div class="article-summary-box-inner">
<span><p>The sparsely-activated models have achieved great success in natural language
processing through large-scale parameters and relatively low computational
cost, and gradually become a feasible technique for training and implementing
extremely large models. Due to the limit of communication cost, activating
multiple experts is hardly affordable during training and inference. Therefore,
previous work usually activate just one expert at a time to alleviate
additional communication cost. Such routing mechanism limits the upper bound of
model performance. In this paper, we first investigate a phenomenon that
increasing the number of activated experts can boost the model performance with
higher sparse ratio. To increase the number of activated experts without an
increase in computational cost, we propose SAM (Switch and Mixture) routing, an
efficient hierarchical routing mechanism that activates multiple experts in a
same device (GPU). Our methods shed light on the training of extremely large
sparse models and experiments prove that our models can achieve significant
performance gain with great efficiency improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Designing Language Technologies for Social Good: The Road not Taken. (arXiv:2110.07444v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07444">
<div class="article-summary-box-inner">
<span><p>Development of speech and language technology for social good (LT4SG),
especially those targeted at the welfare of marginalized communities and
speakers of low-resource and under-served languages, has been a prominent theme
of research within NLP, Speech, and the AI communities. Researchers have mostly
relied on their individual expertise, experiences or ad hoc surveys for
prioritization of language technologies that provide social good to the
end-users. This has been criticized by several scholars who argue that work on
LT4SG must include the target linguistic communities during the design and
development process. However, none of the LT4SG work and their critiques
suggest principled techniques for prioritization of the technologies and
methods for inclusion of the end-user during the development cycle. Drawing
inspiration from the fields of Economics, Ethics, Psychology, and Participatory
Design, here we chart out a set of methodologies for prioritizing LT4SG that
are aligned with the end-user preferences. We then analyze several LT4SG
efforts in light of the proposed methodologies and bring out their hidden
assumptions and potential pitfalls. While the current study is limited to
language technologies, we believe that the principles and prioritization
techniques highlighted here are applicable more broadly to AI for Social Good.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MReD: A Meta-Review Dataset for Controllable Text Generation. (arXiv:2110.07474v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07474">
<div class="article-summary-box-inner">
<span><p>When directly using existing text generation datasets for controllable
generation, we are facing the problem of not having the domain knowledge and
thus the aspects that could be controlled are limited.A typical example is when
using CNN/Daily Mail dataset for controllable text summarization, there is no
guided information on the emphasis of summary sentences. A more useful text
generator should leverage both the input text and control variables to guide
the generation, which can only be built with deep understanding of the domain
knowledge. Motivated by this vi-sion, our paper introduces a new text
generation dataset, named MReD. Our new dataset consists of 7,089 meta-reviews
and all its 45k meta-review sentences are manually annotated as one of the
carefully defined 9 categories, including abstract, strength, decision, etc. We
present experimental results on start-of-the-art summarization models, and
propose methods for controlled generation on both extractive and abstractive
models using our annotated data. By exploring various settings and analaysing
the model behavior with respect to the control inputs, we demonstrate the
challenges and values of our dataset. MReD allows us to have a better
understanding of the meta-review corpora and enlarge the research room for
controllable text generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding. (arXiv:2110.07476v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07476">
<div class="article-summary-box-inner">
<span><p>Event extraction is typically modeled as a multi-class classification problem
where both event types and argument roles are treated as atomic symbols. These
approaches are usually limited to a set of pre-defined types. We propose a
novel event extraction framework that takes event types and argument roles as
natural language queries to extract candidate triggers and arguments from the
input text. With the rich semantics in the queries, our framework benefits from
the attention mechanisms to better capture the semantic correlation between the
event types or argument roles and the input text. Furthermore, the
query-and-extract formulation allows our approach to leverage all available
event annotations from various ontologies as a unified model. Experiments on
two public benchmarks, ACE and ERE, demonstrate that our approach achieves
state-of-the-art performance on each dataset and significantly outperforms
existing methods on zero-shot event extraction. We will make all the programs
publicly available once the paper is accepted.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning Large-Scale Pre-trained Language Models for Conversational Recommendation with Knowledge Graph. (arXiv:2110.07477v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07477">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a pre-trained language model (PLM) based framework
called RID for conversational recommender system (CRS). RID finetunes the
large-scale PLMs such as DialoGPT, together with a pre-trained Relational Graph
Convolutional Network (RGCN) to encode the node representations of an
item-oriented knowledge graph. The former aims to generate fluent and diverse
dialogue responses based on the strong language generation ability of PLMs,
while the latter is to facilitate the item recommendation by learning better
node embeddings on the structural knowledge base. To unify two modules of
dialogue generation and item recommendation into a PLMs-based framework, we
expand the generation vocabulary of PLMs to include an extra item vocabulary,
and introduces a vocabulary pointer to control when to recommend target items
in the generation process. Extensive experiments on the benchmark dataset
ReDial show RID significantly outperforms the state-of-the-art methods on both
evaluations of dialogue and recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition. (arXiv:2110.07480v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07480">
<div class="article-summary-box-inner">
<span><p>Nested entities are observed in many domains due to their compositionality,
which cannot be easily recognized by the widely-used sequence labeling
framework. A natural solution is to treat the task as a span classification
problem. To increase performance on span representation and classification, it
is crucial to effectively integrate all useful information of different
formats, which we refer to heterogeneous factors including tokens, labels,
boundaries, and related spans. To fuse these heterogeneous factors, we propose
a novel triaffine mechanism including triaffine attention and scoring, which
interacts with multiple factors in both the stages of representation and
classification. Experiments results show that our proposed method achieves the
state-of-the-art F1 scores on four nested NER datasets: ACE2004, ACE2005,
GENIA, and KBP2017.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Pitfalls of Analyzing Individual Neurons in Language Models. (arXiv:2110.07483v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07483">
<div class="article-summary-box-inner">
<span><p>While many studies have shown that linguistic information is encoded in
hidden word representations, few have studied individual neurons, to show how
and in which neurons it is encoded. Among these, the common approach is to use
an external probe to rank neurons according to their relevance to some
linguistic attribute, and to evaluate the obtained ranking using the same probe
that produced it. We show two pitfalls in this methodology: 1. It confounds
distinct factors: probe quality and ranking quality. We separate them and draw
conclusions on each. 2. It focuses on encoded information, rather than
information that is used by the model. We show that these are not the same. We
compare two recent ranking methods and a simple one we introduce, and evaluate
them with regard to both of these aspects.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Keyword Spotting using Xception-1d. (arXiv:2110.07498v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07498">
<div class="article-summary-box-inner">
<span><p>The field of conversational agents is growing fast and there is an increasing
need for algorithms that enhance natural interaction. In this work we show how
we achieved state of the art results in the Keyword Spotting field by adapting
and tweaking the Xception algorithm, which achieved outstanding results in
several computer vision tasks. We obtained about 96\% accuracy when classifying
audio clips belonging to 35 different categories, beating human annotation at
the most complex tasks proposed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision. (arXiv:2110.07515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07515">
<div class="article-summary-box-inner">
<span><p>How do we perform efficient inference while retaining high translation
quality? Existing neural machine translation models, such as Transformer,
achieve high performance, but they decode words one by one, which is
inefficient. Recent non-autoregressive translation models speed up the
inference, but their quality is still inferior. In this work, we propose DSLP,
a highly efficient and high-performance model for machine translation. The key
insight is to train a non-autoregressive Transformer with Deep Supervision and
feed additional Layer-wise Predictions. We conducted extensive experiments on
four translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO).
Results show that our approach consistently improves the BLEU scores compared
with respective base models. Specifically, our best variant outperforms the
autoregressive model on three translation tasks, while being 14.8 times more
efficient in inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures. (arXiv:2110.07518v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07518">
<div class="article-summary-box-inner">
<span><p>Current open-domain conversational models can easily be made to talk in
inadequate ways. Online learning from conversational feedback given by the
conversation partner is a promising avenue for a model to improve and adapt, so
as to generate fewer of these safety failures. However, current
state-of-the-art models tend to react to feedback with defensive or oblivious
responses. This makes for an unpleasant experience and may discourage
conversation partners from giving feedback in the future. This work proposes
SaFeRDialogues, a task and dataset of graceful responses to conversational
feedback about safety failures. We collect a dataset of 10k dialogues
demonstrating safety failures, feedback signaling them, and a response
acknowledging the feedback. We show how fine-tuning on this dataset results in
conversations that human raters deem considerably more likely to lead to a
civil conversation, without sacrificing engagingness or general conversational
ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparative Opinion Summarization via Collaborative Decoding. (arXiv:2110.07520v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07520">
<div class="article-summary-box-inner">
<span><p>Opinion summarization focuses on generating summaries that reflect popular
opinions of multiple reviews for a single entity (e.g., a hotel or a product.)
While generated summaries offer general and concise information about a
particular entity, the information may be insufficient to help the user compare
multiple entities. Thus, the user may still struggle with the question "Which
one should I pick?" In this paper, we propose a {\em comparative opinion
summarization} task, which is to generate two contrastive summaries and one
common summary from two given sets of reviews from different entities. We
develop a comparative summarization framework CoCoSum, which consists of two
few-shot summarization models that are jointly used to generate contrastive and
common summaries. Experimental results on a newly created benchmark CoCoTrip
show that CoCoSum can produce high-quality contrastive and common summaries
than state-of-the-art opinion summarization models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Decoupling for Open-Domain Passage Retrieval. (arXiv:2110.07524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07524">
<div class="article-summary-box-inner">
<span><p>Training dense passage representations via contrastive learning (CL) has been
shown effective for Open-Domain Passage Retrieval (ODPR). Recent studies mainly
focus on optimizing this CL framework by improving the sampling strategy or
extra pretraining. Different from previous studies, this work devotes itself to
investigating the influence of conflicts in the widely used CL strategy in
ODPR, motivated by our observation that a passage can be organized by multiple
semantically different sentences, thus modeling such a passage as a unified
dense vector is not optimal. We call such conflicts Contrastive Conflicts. In
this work, we propose to solve it with a representation decoupling method, by
decoupling the passage representations into contextual sentence-level ones, and
design specific CL strategies to mediate these conflicts. Experiments on widely
used datasets including Natural Questions, Trivia QA, and SQuAD verify the
effectiveness of our method, especially on the dataset where the conflicting
problem is severe. Our method also presents good transferability across the
datasets, which further supports our idea of mediating Contrastive Conflicts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Irrationality of Neural Rationale Models. (arXiv:2110.07550v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07550">
<div class="article-summary-box-inner">
<span><p>Neural rationale models are popular for interpretable predictions of NLP
tasks. In these, a selector extracts segments of the input text, called
rationales, and passes these segments to a classifier for prediction. Since the
rationale is the only information accessible to the classifier, it is plausibly
defined as the explanation. Is such a characterization unconditionally correct?
In this paper, we argue to the contrary, with both philosophical perspectives
and empirical evidence suggesting that rationale models are, perhaps, less
rational and interpretable than expected. We call for more rigorous and
comprehensive evaluations of these models to ensure desired properties of
interpretability are indeed achieved. The code can be found at
https://github.com/yimingz89/Neural-Rationale-Analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BI-RADS BERT & Using Section Tokenization to Understand Radiology Reports. (arXiv:2110.07552v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07552">
<div class="article-summary-box-inner">
<span><p>Radiology reports are the main form of communication between radiologists and
other clinicians, and contain important information for patient care. However
in order to use this information for research it is necessary to convert the
raw text into structured data suitable for analysis. Domain specific contextual
word embeddings have been shown to achieve impressive accuracy at such natural
language processing tasks in medicine. In this work we pre-trained a contextual
embedding BERT model using breast radiology reports and developed a classifier
that incorporated the embedding with auxiliary global textual features in order
to perform a section tokenization task. This model achieved a 98% accuracy at
segregating free text reports into sections of information outlined in the
Breast Imaging Reporting and Data System (BI-RADS) lexicon, a significant
improvement over the Classic BERT model without auxiliary information. We then
evaluated whether using section tokenization improved the downstream extraction
of the following fields: modality/procedure, previous cancer, menopausal
status, purpose of exam, breast density and background parenchymal enhancement.
Using the BERT model pre-trained on breast radiology reports combined with
section tokenization resulted in an overall accuracy of 95.9% in field
extraction. This is a 17% improvement compared to an overall accuracy of 78.9%
for field extraction for models without section tokenization and with Classic
BERT embeddings. Our work shows the strength of using BERT in radiology report
analysis and the advantages of section tokenization in identifying key features
of patient factors recorded in breast radiology reports.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Composable Sparse Fine-Tuning for Cross-Lingual Transfer. (arXiv:2110.07560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07560">
<div class="article-summary-box-inner">
<span><p>Fine-tuning all parameters of a pre-trained model has become the mainstream
approach for transfer learning. To increase its efficiency and prevent
catastrophic forgetting and interference, techniques like adapters and sparse
fine-tuning have been developed. Adapters are modular, as they can be combined
to adapt a model towards different facets of knowledge (e.g., dedicated
language and/or task adapters). Sparse fine-tuning is expressive, as it
controls the behavior of all model components. In this work, we introduce a new
fine-tuning method with both these desirable properties. In particular, we
learn sparse, real-valued masks based on a simple variant of the Lottery Ticket
Hypothesis. Task-specific masks are obtained from annotated data in a source
language, and language-specific masks from masked language modeling in a target
language. Both these masks can then be composed with the pre-trained model.
Unlike adapter-based fine-tuning, this method neither increases the number of
parameters at inference time nor alters the original model architecture. Most
importantly, it outperforms adapters in zero-shot cross-lingual transfer by a
large margin in a series of multilingual benchmarks, including Universal
Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we
additionally find that sparsity is crucial to prevent both 1) interference
between the fine-tunings to be composed and 2) overfitting. We release the code
and models at https://github.com/cambridgeltl/composable-sft.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Practical Benefits of Feature Feedback Under Distribution Shift. (arXiv:2110.07566v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07566">
<div class="article-summary-box-inner">
<span><p>In attempts to develop sample-efficient algorithms, researcher have explored
myriad mechanisms for collecting and exploiting feature feedback, auxiliary
annotations provided for training (but not test) instances that highlight
salient evidence. Examples include bounding boxes around objects and salient
spans in text. Despite its intuitive appeal, feature feedback has not delivered
significant gains in practical problems as assessed on iid holdout sets.
However, recent works on counterfactually augmented data suggest an alternative
benefit of supplemental annotations: lessening sensitivity to spurious patterns
and consequently delivering gains in out-of-domain evaluations. Inspired by
these findings, we hypothesize that while the numerous existing methods for
incorporating feature feedback have delivered negligible in-sample gains, they
may nevertheless generalize better out-of-domain. In experiments addressing
sentiment analysis, we show that feature feedback methods perform significantly
better on various natural out-of-domain datasets even absent differences on
in-domain evaluation. By contrast, on natural language inference tasks,
performance remains comparable. Finally, we compare those tasks where feature
feedback does (and does not) help.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing. (arXiv:2110.07572v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07572">
<div class="article-summary-box-inner">
<span><p>Semantic parsing is the task of producing a structured meaning representation
for natural language utterances or questions. Recent research has pointed out
that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle
to generalize systematically, i.e. to handle examples that require recombining
known knowledge in novel settings. In this work, we show that better systematic
generalization can be achieved by producing the meaning representation (MR)
directly as a graph and not as a sequence. To this end we propose LAGr, the
Labeling Aligned Graphs algorithm that produces semantic parses by predicting
node and edge labels for a complete multi-layer input-aligned graph. The
strongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas
weakly-supervised LAGr infers alignments for originally unaligned target graphs
using an approximate MAP inference procedure. On the COGS and CFQ compositional
generalization benchmarks the strongly- and weakly- supervised LAGr algorithms
achieve significant improvements upon the baseline seq2seq parsers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Delphi: Towards Machine Ethics and Norms. (arXiv:2110.07574v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07574">
<div class="article-summary-box-inner">
<span><p>What would it take to teach a machine to behave ethically? While broad
ethical rules may seem straightforward to state ("thou shalt not kill"),
applying such rules to real-world situations is far more complex. For example,
while "helping a friend" is generally a good thing to do, "helping a friend
spread fake news" is not. We identify four underlying challenges towards
machine ethics and norms: (1) an understanding of moral precepts and social
norms; (2) the ability to perceive real-world situations visually or by reading
natural language descriptions; (3) commonsense reasoning to anticipate the
outcome of alternative actions in different contexts; (4) most importantly, the
ability to make ethical judgments given the interplay between competing values
and their grounding in different contexts (e.g., the right to freedom of
expression vs. preventing the spread of fake news).
</p>
<p>Our paper begins to address these questions within the deep learning
paradigm. Our prototype model, Delphi, demonstrates strong promise of
language-based commonsense moral reasoning, with up to 92.1% accuracy vetted by
humans. This is in stark contrast to the zero-shot performance of GPT-3 of
52.3%, which suggests that massive scale alone does not endow pre-trained
neural language models with human values. Thus, we present Commonsense Norm
Bank, a moral textbook customized for machines, which compiles 1.7M examples of
people's ethical judgments on a broad spectrum of everyday situations. In
addition to the new resources and baseline performances for future research,
our study provides new insights that lead to several important open research
questions: differentiating between universal human values and personal values,
modeling different moral frameworks, and explainable, consistent approaches to
machine ethics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset. (arXiv:2110.07575v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07575">
<div class="article-summary-box-inner">
<span><p>Visually-grounded spoken language datasets can enable models to learn
cross-modal correspondences with very weak supervision. However, modern
audio-visual datasets contain biases that undermine the real-world performance
of models trained on that data. We introduce Spoken ObjectNet, which is
designed to remove some of these biases and provide a way to better evaluate
how effectively models will perform in real-world scenarios. This dataset
expands upon ObjectNet, which is a bias-controlled image dataset that features
similar image classes to those present in ImageNet. We detail our data
collection pipeline, which features several methods to improve caption quality,
including automated language model checks. Lastly, we show baseline results on
image retrieval and audio retrieval tasks. These results show that models
trained on other datasets and then evaluated on Spoken ObjectNet tend to
perform poorly due to biases in other datasets that the models have learned. We
also show evidence that the performance decrease is due to the dataset
controls, and not the transfer setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. (arXiv:2110.07577v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07577">
<div class="article-summary-box-inner">
<span><p>Conventional fine-tuning of pre-trained language models tunes all model
parameters and stores a full model copy for each downstream task, which has
become increasingly infeasible as the model size grows larger. Recent
parameter-efficient language model tuning (PELT) methods manage to match the
performance of fine-tuning with much fewer trainable parameters and perform
especially well when the training data is limited. However, different PELT
methods may perform rather differently on the same task, making it nontrivial
to select the most appropriate method for a specific task, especially
considering the fast-growing number of new PELT methods and downstream tasks.
In light of model diversity and the difficulty of model selection, we propose a
unified framework, UniPELT, which incorporates different PELT methods as
submodules and learns to activate the ones that best suit the current data or
task setup. Remarkably, on the GLUE benchmark, UniPELT consistently achieves
1~3pt gains compared to the best individual PELT method that it incorporates
and even outperforms fine-tuning under different setups. Moreover, UniPELT
often surpasses the upper bound when taking the best performance of all its
submodules used individually on each task, indicating that a mixture of
multiple PELT methods may be inherently more effective than single methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations. (arXiv:2110.07581v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07581">
<div class="article-summary-box-inner">
<span><p>Dense retrieval (DR) methods conduct text retrieval by first encoding texts
in the embedding space and then matching them by nearest neighbor search. This
requires strong locality properties from the representation space, i.e, the
close allocations of each small group of relevant texts, which are hard to
generalize to domains without sufficient training data. In this paper, we aim
to improve the generalization ability of DR models from source training domains
with rich supervision signals to target domains without any relevant labels, in
the zero-shot setting. To achieve that, we propose Momentum adversarial Domain
Invariant Representation learning (MoDIR), which introduces a momentum method
in the DR training process to train a domain classifier distinguishing source
versus target, and then adversarially updates the DR encoder to learn domain
invariant representations. Our experiments show that MoDIR robustly outperforms
its baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot
setup, with more than 10% relative gains on datasets with enough sensitivity
for DR models' evaluation. Source code of this paper will be released.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Explanations Be Useful for Calibrating Black Box Models?. (arXiv:2110.07586v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07586">
<div class="article-summary-box-inner">
<span><p>One often wants to take an existing, trained NLP model and use it on data
from a new domain. While fine-tuning or few-shot learning can be used to adapt
the base model, there is no one simple recipe to getting these working;
moreover, one may not have access to the original model weights if it is
deployed as a black box. To this end, we study how to improve a black box
model's performance on a new domain given examples from the new domain by
leveraging explanations of the model's behavior. Our approach first extracts a
set of features combining human intuition about the task with model
attributions generated by black box interpretation techniques, and then uses a
simple model to calibrate or rerank the model's predictions based on the
features. We experiment with our method on two tasks, extractive question
answering and natural language inference, covering adaptation from several
pairs of domains. The experimental results across all the domain pairs show
that explanations are useful for calibrating these models. We show that the
calibration features transfer to some extent between tasks and shed light on
how to effectively use them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Toxicity Analysis: A New Spoken Language Processing Task. (arXiv:2110.07592v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07592">
<div class="article-summary-box-inner">
<span><p>Toxic speech, also known as hate speech, is regarded as one of the crucial
issues plaguing online social media today. Most recent work on toxic speech
detection is constrained to the modality of text with no existing work on
toxicity detection from spoken utterances. In this paper, we propose a new
Spoken Language Processing task of detecting toxicity from spoken speech. We
introduce DeToxy, the first publicly available toxicity annotated dataset for
English speech, sourced from various openly available speech databases,
consisting of over 2 million utterances. Finally, we also provide analysis on
how a spoken speech corpus annotated for toxicity can help facilitate the
development of E2E models which better capture various prosodic cues in speech,
thereby boosting toxicity classification on spoken utterances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compressibility of Distributed Document Representations. (arXiv:2110.07595v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07595">
<div class="article-summary-box-inner">
<span><p>Contemporary natural language processing (NLP) revolves around learning from
latent document representations, generated either implicitly by neural language
models or explicitly by methods such as doc2vec or similar. One of the key
properties of the obtained representations is their dimension. Whilst the
commonly adopted dimensions of 256 and 768 offer sufficient performance on many
tasks, it is many times unclear whether the default dimension is the most
suitable choice for the subsequent downstream learning tasks. Furthermore,
representation dimensions are seldom subject to hyperparameter tuning due to
computational constraints. The purpose of this paper is to demonstrate that a
surprisingly simple and efficient recursive compression procedure can be
sufficient to both significantly compress the initial representation, but also
potentially improve its performance when considering the task of text
classification. Having smaller and less noisy representations is the desired
property during deployment, as orders of magnitude smaller models can
significantly reduce the computational overload and with it the deployment
costs. We propose CoRe, a straightforward, representation learner-agnostic
framework suitable for representation compression. The CoRe's performance is
showcased and studied on a collection of 17 real-life corpora from biomedical,
news, social media, and literary domains. We explored CoRe's behavior when
considering contextual and non-contextual document representations, different
compression levels, and 9 different compression algorithms. Current results
based on more than 100,000 compression experiments indicate that recursive
Singular Value Decomposition offers a very good trade-off between the
compression efficiency and performance, making CoRe useful in many existing,
representation-dependent NLP pipelines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-guided Counterfactual Generation for QA. (arXiv:2110.07596v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07596">
<div class="article-summary-box-inner">
<span><p>Deep NLP models have been shown to learn spurious correlations, leaving them
brittle to input perturbations. Recent work has shown that counterfactual or
contrastive data -- i.e. minimally perturbed inputs -- can reveal these
weaknesses, and that data augmentation using counterfactuals can help
ameliorate them. Proposed techniques for generating counterfactuals rely on
human annotations, perturbations based on simple heuristics, and meaning
representation frameworks. We focus on the task of creating counterfactuals for
question answering, which presents unique challenges related to world
knowledge, semantic diversity, and answerability. To address these challenges,
we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual
evaluation and training data with minimal human supervision. Using an
open-domain QA framework and question generation model trained on original task
data, we create counterfactuals that are fluent, semantically diverse, and
automatically labeled. Data augmentation with RGF counterfactuals improves
performance on out-of-domain and challenging evaluation sets over and above
existing methods, in both the reading comprehension and open-domain QA
settings. Moreover, we find that RGF data leads to significant improvements in
a model's robustness to local perturbations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (arXiv:2110.07602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07602">
<div class="article-summary-box-inner">
<span><p>Prompt tuning, which only tunes continuous prompts with a frozen language
model, substantially reduces per-task storage and memory usage at training.
However, in the context of NLU, prior work and our results reveal that existing
methods of prompt tuning do not perform well for normal-sized pre-trained
models and for hard sequence tasks, indicating lack of universality. We present
a novel empirical finding that properly-optimized prompt tuning can be
universally effective across a wide range of model scales and NLU tasks, where
it matches the performance of fine-tuning while having only 0.1\%-3\% tuned
parameters. Our method P-Tuning v2 is not a new method but a version of
prefix-tuning \cite{li2021prefix} optimized and adapted for NLU. Given the
universality and simplicity of P-Tuning v2, we believe it can serve as an
alternative for fine-tuning and a strong baseline for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sub-word Level Lip Reading With Visual Attention. (arXiv:2110.07603v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.07603">
<div class="article-summary-box-inner">
<span><p>The goal of this paper is to learn strong lip reading models that can
recognise speech in silent videos. Most prior works deal with the open-set
visual speech recognition problem by adapting existing automatic speech
recognition techniques on top of trivially pooled visual features. Instead, in
this paper we focus on the unique challenges encountered in lip reading and
propose tailored solutions. To that end we make the following contributions:
(1) we propose an attention-based pooling mechanism to aggregate visual speech
representations; (2) we use sub-word units for lip reading for the first time
and show that this allows us to better model the ambiguities of the task; (3)
we propose a training pipeline that balances the lip reading performance with
other key factors such as data and compute efficiency. Following the above, we
obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks
when training on public datasets, and even surpass models trained on
large-scale industrial datasets by using an order of magnitude less data. Our
best model achieves 22.6% word error rate on the LRS2 dataset, a performance
unprecedented for lip reading models, significantly reducing the performance
gap between lip reading and automatic speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Political Text Scaling Meets Computational Semantics. (arXiv:1904.06217v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.06217">
<div class="article-summary-box-inner">
<span><p>During the last fifteen years, automatic text scaling has become one of the
key tools of the Text as Data community in political science. Prominent text
scaling algorithms, however, rely on the assumption that latent positions can
be captured just by leveraging the information about word frequencies in
documents under study. We challenge this traditional view and present a new,
semantically aware text scaling algorithm, SemScale, which combines recent
developments in the area of computational linguistics with unsupervised
graph-based clustering. We conduct an extensive quantitative analysis over a
collection of speeches from the European Parliament in five different languages
and from two different legislative terms, and show that a scaling approach
relying on semantic document representations is often better at capturing known
underlying political dimensions than the established frequency-based (i.e.,
symbolic) scaling method. We further validate our findings through a series of
experiments focused on text preprocessing and feature selection, document
representation, scaling of party manifestos, and a supervised extension of our
algorithm. To catalyze further research on this new branch of text scaling
methods, we release a Python implementation of SemScale with all included data
sets and evaluation procedures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BAS: An Answer Selection Method Using BERT Language Model. (arXiv:1911.01528v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.01528">
<div class="article-summary-box-inner">
<span><p>In recent years, Question Answering systems have become more popular and
widely used by users. Despite the increasing popularity of these systems, the
their performance is not even sufficient for textual data and requires further
research. These systems consist of several parts that one of them is the Answer
Selection component. This component detects the most relevant answer from a
list of candidate answers. The methods presented in previous researches have
attempted to provide an independent model to undertake the answer-selection
task. An independent model cannot comprehend the syntactic and semantic
features of questions and answers with a small training dataset. To fill this
gap, language models can be employed in implementing the answer selection part.
This action enables the model to have a better understanding of the language in
order to understand questions and answers better than previous works. In this
research, we will present the "BAS" (BERT Answer Selection) that uses the BERT
language model to comprehend language. The empirical results of applying the
model on the TrecQA Raw, TrecQA Clean, and WikiQA datasets demonstrate that
using a robust language model such as BERT can enhance the performance. Using a
more robust classifier also enhances the effect of the language model on the
answer selection component. The results demonstrate that language comprehension
is an essential requirement in natural language processing tasks such as
answer-selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Deep Neural Networks. (arXiv:2010.01496v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01496">
<div class="article-summary-box-inner">
<span><p>Deep neural networks are becoming more and more popular due to their
revolutionary success in diverse areas, such as computer vision, natural
language processing, and speech recognition. However, the decision-making
processes of these models are generally not interpretable to users. In various
domains, such as healthcare, finance, or law, it is critical to know the
reasons behind a decision made by an artificial intelligence system. Therefore,
several directions for explaining neural models have recently been explored. In
this thesis, I investigate two major directions for explaining deep neural
networks. The first direction consists of feature-based post-hoc explanatory
methods, that is, methods that aim to explain an already trained and fixed
model (post-hoc), and that provide explanations in terms of input features,
such as tokens for text and superpixels for images (feature-based). The second
direction consists of self-explanatory neural models that generate natural
language explanations, that is, models that have a built-in module that
generates explanations for the predictions of the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering. (arXiv:2010.12643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12643">
<div class="article-summary-box-inner">
<span><p>Coupled with the availability of large scale datasets, deep learning
architectures have enabled rapid progress on the Question Answering task.
However, most of those datasets are in English, and the performances of
state-of-the-art multilingual models are significantly lower when evaluated on
non-English data. Due to high data collection costs, it is not realistic to
obtain annotated data for each language one desires to support.
</p>
<p>We propose a method to improve the Cross-lingual Question Answering
performance without requiring additional annotated data, leveraging Question
Generation models to produce synthetic samples in a cross-lingual fashion. We
show that the proposed method allows to significantly outperform the baselines
trained on English data only. We report a new state-of-the-art on four
multilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-the-Fly Attention Modulation for Neural Generation. (arXiv:2101.00371v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00371">
<div class="article-summary-box-inner">
<span><p>Despite considerable advancements with deep neural language models (LMs),
neural text generation still suffers from degeneration: the generated text is
repetitive, generic, self-contradictory, and often lacks commonsense. Our
analyses on sentence-level attention patterns in LMs reveal that neural
degeneration may be associated with insufficient learning of task-specific
characteristics by the attention mechanism. This finding motivates on-the-fly
attention modulation -- a simple but effective method that enables the
injection of priors into attention computation during inference. Automatic and
human evaluation results on three text generation benchmarks demonstrate that
attention modulation helps LMs generate text with enhanced fluency, creativity,
and commonsense reasoning, in addition to significantly reduce sentence-level
repetition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Characterizing Partisan Political Narrative Frameworks about COVID-19 on Twitter. (arXiv:2103.06960v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06960">
<div class="article-summary-box-inner">
<span><p>The COVID-19 pandemic is a global crisis that has been testing every society
and exposing the critical role of local politics in crisis response. In the
United States, there has been a strong partisan divide between the Democratic
and Republican party's narratives about the pandemic which resulted in
polarization of individual behaviors and divergent policy adoption across
regions. As shown in this case, as well as in most major social issues,
strongly polarized narrative frameworks facilitate such narratives. To
understand polarization and other social chasms, it is critical to dissect
these diverging narratives. Here, taking the Democratic and Republican
political social media posts about the pandemic as a case study, we demonstrate
that a combination of computational methods can provide useful insights into
the different contexts, framing, and characters and relationships that
construct their narrative frameworks which individual posts source from.
Leveraging a dataset of tweets from elite politicians in the U.S., we found
that the Democrats' narrative tends to be more concerned with the pandemic as
well as financial and social support, while the Republicans discuss more about
other political entities such as China. We then perform an automatic framing
analysis to characterize the ways in which they frame their narratives, where
we found that the Democrats emphasize the government's role in responding to
the pandemic, and the Republicans emphasize the roles of individuals and
support for small businesses. Finally, we present a semantic role analysis that
uncovers the important characters and relationships in their narratives as well
as how they facilitate a membership categorization process. Our findings
concretely expose the gaps in the "elusive consensus" between the two parties.
Our methodologies may be applied to computationally study narratives in various
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines. (arXiv:2104.08790v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08790">
<div class="article-summary-box-inner">
<span><p>Even to a simple and short news headline, readers react in a multitude of
ways: cognitively (e.g., inferring the writer's intent), emotionally (e.g.,
feeling distrust), and behaviorally (e.g., sharing the news with their
friends). Such reactions are instantaneous and yet complex, as they rely on
factors that go beyond interpreting the factual content the news headline.
Instead, understanding reactions require pragmatic understanding of the news
headline, including broader background knowledge about contentious news topics
as well as commonsense reasoning about people's intents and emotional
reactions. We propose Misinfo Reaction Frames, a pragmatic formalism for
modeling how readers might react to a news headline cognitively, emotionally,
and behaviorally. We also introduce a Misinfo Reaction Frames corpus, a dataset
of over 200k news headline annotated with crowdsourced reactions focusing on
global crises: the Covid-19 pandemic, climate change, and cancer. Empirical
results confirm that it is indeed possible to learn the prominent patterns of
readers' reactions to news headlines. We also find a potentially positive use
case of our model; When we present our model generated inferences to people, we
find that the machine inferences can increase readers' trust in real news while
decreasing their trust in misinformation. Our work demonstrates the feasibility
and the importance of pragmatic inferences of news to help enhance AI-guided
misinformation detection and mitigation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct Speech Translation. (arXiv:2104.11710v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.11710">
<div class="article-summary-box-inner">
<span><p>The audio segmentation mismatch between training data and those seen at
run-time is a major problem in direct speech translation. Indeed, while systems
are usually trained on manually segmented corpora, in real use cases they are
often presented with continuous audio requiring automatic (and sub-optimal)
segmentation. After comparing existing techniques (VAD-based, fixed-length and
hybrid segmentation methods), in this paper we propose enhanced hybrid
solutions to produce better results without sacrificing latency. Through
experiments on different domains and language pairs, we show that our methods
outperform all the other techniques, reducing by at least 30% the gap between
the traditional VAD-based approach and optimal manual segmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detection of Emotions in Hindi-English Code Mixed Text Data. (arXiv:2105.09226v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09226">
<div class="article-summary-box-inner">
<span><p>In recent times, we have seen an increased use of text chat for communication
on social networks and smartphones. This particularly involves the use of
Hindi-English code-mixed text which contains words which are not recognized in
English vocabulary. We have worked on detecting emotions in these mixed data
and classify the sentences in human emotions which are angry, fear, happy or
sad. We have used state of the art natural language processing models and
compared their performance on the dataset comprising sentences in this mixed
data. The dataset was collected and annotated from sources and then used to
train the models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model. (arXiv:2105.11314v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11314">
<div class="article-summary-box-inner">
<span><p>We present RobeCzech, a monolingual RoBERTa language representation model
trained on Czech data. RoBERTa is a robustly optimized Transformer-based
pretraining approach. We show that RobeCzech considerably outperforms
equally-sized multilingual and Czech-trained contextualized language
representation models, surpasses current state of the art in all five evaluated
NLP tasks and reaches state-of-the-art results in four of them. The RobeCzech
model is released publicly at https://hdl.handle.net/11234/1-3691 and
https://huggingface.co/ufal/robeczech-base.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07847">
<div class="article-summary-box-inner">
<span><p>While unbiased machine learning models are essential for many applications,
bias is a human-defined concept that can vary across tasks. Given only
input-label pairs, algorithms may lack sufficient information to distinguish
stable (causal) features from unstable (spurious) features. However, related
tasks often share similar biases -- an observation we may leverage to develop
stable classifiers in the transfer setting. In this work, we explicitly inform
the target classifier about unstable features in the source tasks.
Specifically, we derive a representation that encodes the unstable features by
contrasting different data environments in the source task. We achieve
robustness by clustering data of the target task according to this
representation and minimizing the worst-case risk across these clusters. We
evaluate our method on both text and image classifications. Empirical results
demonstrate that our algorithm is able to maintain robustness on the target
task, outperforming the best baseline by 22.9% in absolute accuracy across 12
transfer settings. Our code is available at https://github.com/YujiaBao/Tofu.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConveRT for FAQ Answering. (arXiv:2108.00719v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00719">
<div class="article-summary-box-inner">
<span><p>Knowledgeable FAQ chatbots are a valuable resource to any organization. While
powerful and efficient retrieval-based models exist for English, it is rarely
the case for other languages for which the same amount of training data is not
available. In this paper, we propose a novel pre-training procedure to adapt
ConveRT, an English conversational retriever model, to other languages with
less training data available. We apply it for the first time to the task of
Dutch FAQ answering related to the COVID-19 vaccine. We show it performs better
than an open-source alternative in both a low-data regime and a high-data
regime.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13741">
<div class="article-summary-box-inner">
<span><p>Recent researches have demonstrated that BERT shows potential in a wide range
of natural language processing tasks. It is adopted as an encoder for many
state-of-the-art automatic summarizing systems, which achieve excellent
performance. However, so far, there is not much work done for Vietnamese. In
this paper, we showcase how BERT can be implemented for extractive text
summarization in Vietnamese on multi-document. We introduce a novel comparison
between different multilingual and monolingual BERT models. The experiment
results indicate that monolingual models produce promising results compared to
other multilingual models and previous text summarizing models for Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The VoicePrivacy 2020 Challenge: Results and findings. (arXiv:2109.00648v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00648">
<div class="article-summary-box-inner">
<span><p>This paper presents the results and analyses stemming from the first
VoicePrivacy 2020 Challenge which focuses on developing anonymization solutions
for speech technology. We provide a systematic overview of the challenge design
with an analysis of submitted systems and evaluation results. In particular, we
describe the voice anonymization task and datasets used for system development
and evaluation. Also, we present different attack models and the associated
objective and subjective evaluation metrics. We introduce two anonymization
baselines and provide a summary description of the anonymization systems
developed by the challenge participants. We report objective and subjective
evaluation results for baseline and submitted systems. In addition, we present
experimental results for alternative privacy metrics and attack models
developed as a part of the post-evaluation analysis. Finally, we summarize our
insights and observations that will influence the design of the next
VoicePrivacy challenge edition and some directions for future voice
anonymization research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration. (arXiv:2109.06304v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06304">
<div class="article-summary-box-inner">
<span><p>Phrase representations derived from BERT often do not exhibit complex phrasal
compositionality, as the model relies instead on lexical similarity to
determine semantic relatedness. In this paper, we propose a contrastive
fine-tuning objective that enables BERT to produce more powerful phrase
embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal
paraphrases, which is automatically generated using a paraphrase generation
model, as well as a large-scale dataset of phrases in context mined from the
Books3 corpus. Phrase-BERT outperforms baselines across a variety of
phrase-level similarity tasks, while also demonstrating increased lexical
diversity between nearest neighbors in the vector space. Finally, as a case
study, we show that Phrase-BERT embeddings can be easily integrated with a
simple autoencoder to build a phrase-based neural topic model that interprets
topics as mixtures of words and phrases by performing a nearest neighbor search
in the embedding space. Crowdsourced evaluations demonstrate that this
phrase-based topic model produces more coherent and meaningful topics than
baseline word and phrase-level topic models, further validating the utility of
Phrase-BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. (arXiv:2109.10604v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10604">
<div class="article-summary-box-inner">
<span><p>While diverse question answering (QA) datasets have been proposed and
contributed significantly to the development of deep learning models for QA
tasks, the existing datasets fall short in two aspects. First, we lack QA
datasets covering complex questions that involve answers as well as the
reasoning processes to get the answers. As a result, the state-of-the-art QA
research on numerical reasoning still focuses on simple calculations and does
not provide the mathematical expressions or evidences justifying the answers.
Second, the QA community has contributed much effort to improving the
interpretability of QA models. However, these models fail to explicitly show
the reasoning process, such as the evidence order for reasoning and the
interactions between different pieces of evidence. To address the above
shortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset
with questions requiring numerical reasoning with compound mathematical
expressions. With NOAHQA, we develop an interpretable reasoning graph as well
as the appropriate evaluation metric to measure the answer quality. We evaluate
the state-of-the-art QA models trained using existing QA datasets on NOAHQA and
show that the best among them can only achieve 55.5 exact match scores, while
the human performance is 89.7. We also present a new QA model for generating a
reasoning graph where the reasoning graph metric still has a large gap compared
with that of humans, e.g., 28 scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking BERT: Understanding its Vulnerabilities for Biomedical Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11308">
<div class="article-summary-box-inner">
<span><p>Biomedical named entity recognition (NER) is a key task in the extraction of
information from biomedical literature and electronic health records. For this
task, both generic and biomedical BERT models are widely used. Robustness of
these models is vital for medical applications, such as automated medical
decision making. In this paper we investigate the vulnerability of BERT models
to variation in input data for NER through adversarial attack. Since
adversarial attack methods for NER are sparse, we propose two black-box methods
for NER based on existing methods for classification tasks. Experimental
results show that the original as well as the biomedical BERT models are highly
vulnerable to entity replacement: They can be fooled in 89.2 to 99.4% of the
cases to mislabel previously correct entities. BERT models are also vulnerable
to variation in the entity context with 20.2 to 45.0% of entities predicted
completely wrong and another 29.3 to 53.3% of entities predicted wrong
partially. Often a single change is sufficient to fool the model. BERT models
seem most vulnerable to changes in the local context of entities. Of the
biomedical BERT models, the vulnerability of BioBERT is comparable to the
original BERT model whereas SciBERT is even more vulnerable. Our results chart
the vulnerabilities of BERT models for biomedical NER and emphasize the
importance of further research into uncovering and reducing these weaknesses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11728">
<div class="article-summary-box-inner">
<span><p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively
used by states and language testing agencies alike to evaluate millions of
candidates for life-changing decisions ranging from college applications to
visa approvals. However, little research has been put to understand and
interpret the black-box nature of deep-learning based scoring algorithms.
Previous studies indicate that scoring models can be easily fooled. In this
paper, we explore the reason behind their surprising adversarial brittleness.
We utilize recent advances in interpretability to find the extent to which
features such as coherence, content, vocabulary, and relevance are important
for automated scoring mechanisms. We use this to investigate the
oversensitivity i.e., large change in output score with a little change in
input essay content) and overstability i.e., little change in output scores
with large changes in input essay content) of AES. Our results indicate that
autoscoring models, despite getting trained as "end-to-end" models with rich
contextual embeddings such as BERT, behave like bag-of-words models. A few
words determine the essay score without the requirement of any context making
the model largely overstable. This is in stark contrast to recent probing
studies on pre-trained representation learning models, which show that rich
linguistic features such as parts-of-speech and morphology are encoded by them.
Further, we also find that the models have learnt dataset biases, making them
oversensitive. To deal with these issues, we propose detection-based protection
models that can detect oversensitivity and overstability causing samples with
high accuracies. We find that our proposed models are able to detect unusual
attribution patterns and flag adversarial samples successfully.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual AMR Parsing with Noisy Knowledge Distillation. (arXiv:2109.15196v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15196">
<div class="article-summary-box-inner">
<span><p>We study multilingual AMR parsing from the perspective of knowledge
distillation, where the aim is to learn and improve a multilingual AMR parser
by using an existing English parser as its teacher. We constrain our
exploration in a strict multilingual setting: there is but one model to parse
all different languages including English. We identify that noisy input and
precise output are the key to successful distillation. Together with extensive
pre-training, we obtain an AMR parser whose performances surpass all previously
published results on four different foreign languages, including German,
Spanish, Italian, and Chinese, by large margins (up to 18.8 \textsc{Smatch}
points on Chinese and on average 11.3 \textsc{Smatch} points). Our parser also
achieves comparable performance on English to the latest state-of-the-art
English-only parser.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v4 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00165">
<div class="article-summary-box-inner">
<span><p>Self- and semi-supervised learning methods have been actively investigated to
reduce labeled training data or enhance the model performance. However, the
approach mostly focus on in-domain performance for public datasets. In this
study, we utilize the combination of self- and semi-supervised learning methods
to solve unseen domain adaptation problem in a large-scale production setting
for online ASR model. This approach demonstrates that using the source domain
data with a small fraction of the target domain data (3%) can recover the
performance gap compared to a full data baseline: relative 13.5% WER
improvement for target domain data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning. (arXiv:2110.00678v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00678">
<div class="article-summary-box-inner">
<span><p>To address the performance gap of English ASR models on L2 English speakers,
we evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;
Xu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,
2018) under different training settings. We compare \textbf{(a)} models trained
with a combination of diverse accents to ones trained with only specific
accents and \textbf{(b)} results from different single-accent models. Our
experiments demonstrate the promise of developing ASR models for non-native
English speakers, even with small amounts of L2 training data and even without
a language model. Our models also excel in the zero-shot setting where we train
on multiple L2 datasets and test on a blind L2 test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05896">
<div class="article-summary-box-inner">
<span><p>Trained on the large corpus, pre-trained language models (PLMs) can capture
different levels of concepts in context and hence generate universal language
representations. They can benefit multiple downstream natural language
processing (NLP) tasks. Although PTMs have been widely used in most NLP
applications, especially for high-resource languages such as English, it is
under-represented in Lao NLP research. Previous work on Lao has been hampered
by the lack of annotated datasets and the sparsity of language resources. In
this work, we construct a text classification dataset to alleviate the
resource-scare situation of the Lao language. We additionally present the first
transformer-based PTMs for Lao with four versions: BERT-small, BERT-base,
ELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:
part-of-speech tagging and text classification. Experiments demonstrate the
effectiveness of our Lao models. We will release our models and datasets to the
community, hoping to facilitate the future development of Lao NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time Masking for Temporal Language Models. (arXiv:2110.06366v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06366">
<div class="article-summary-box-inner">
<span><p>Our world is constantly evolving, and so is the content on the web.
Consequently, our languages, often said to mirror the world, are dynamic in
nature. However, most current contextual language models are static and cannot
adapt to changes over time. In this work, we propose a temporal contextual
language model called TempoBERT, which uses time as an additional context of
texts. Our technique is based on modifying texts with temporal information and
performing time masking - specific masking for the supplementary time
information. We leverage our approach for the tasks of semantic change
detection and sentence time prediction, experimenting on diverse datasets in
terms of time, size, genre, and language. Our extensive evaluation shows that
both tasks benefit from exploiting time masking.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese. (arXiv:2110.06696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06696">
<div class="article-summary-box-inner">
<span><p>Although pre-trained models (PLMs) have achieved remarkable improvements in a
wide range of NLP tasks, they are expensive in terms of time and resources.
This calls for the study of training more efficient models with less
computation but still ensures impressive performance. Instead of pursuing a
larger scale, we are committed to developing lightweight yet more powerful
models trained with equal or less computation and friendly to rapid deployment.
This technical report releases our pre-trained model called Mengzi, which
stands for a family of discriminative, generative, domain-specific, and
multimodal pre-trained model variants, capable of a wide range of language and
vision tasks. Compared with public Chinese PLMs, Mengzi is simple but more
powerful. Our lightweight model has achieved new state-of-the-art results on
the widely-used CLUE benchmark with our optimized pre-training and fine-tuning
techniques. Without modifying the model architecture, our model can be easily
employed as an alternative to existing PLMs. Our sources are available at
https://github.com/Langboat/Mengzi.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-15 23:09:52.827375737 UTC">2021-10-15 23:09:52 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>