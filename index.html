<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-06T01:30:00Z">10-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Privacy enabled Financial Text Classification using Differential Privacy and Federated Learning. (arXiv:2110.01643v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01643">
<div class="article-summary-box-inner">
<span><p>Privacy is important considering the financial Domain as such data is highly
confidential and sensitive. Natural Language Processing (NLP) techniques can be
applied for text classification and entity detection purposes in financial
domains such as customer feedback sentiment analysis, invoice entity detection,
categorisation of financial documents by type etc. Due to the sensitive nature
of such data, privacy measures need to be taken for handling and training large
models with such data. In this work, we propose a contextualized transformer
(BERT and RoBERTa) based text classification model integrated with privacy
features such as Differential Privacy (DP) and Federated Learning (FL). We
present how to privately train NLP models and desirable privacy-utility
tradeoffs and evaluate them on the Financial Phrase Bank dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01661">
<div class="article-summary-box-inner">
<span><p>Iterating with new and improved OCR solutions enforces decisions to be taken
when it comes to targeting the right reprocessing candidates. This especially
applies when the underlying data collection is of considerable size and rather
diverse in terms of fonts, languages, periods of publication and consequently
OCR quality. This article captures the efforts of the National Library of
Luxembourg to support those exact decisions. They are crucial in order to
guarantee low computational overhead and reduced quality degradation risks,
combined with a more quantifiable OCR improvement. In particular, this work
explains the methodology of the library with respect to text block level
quality assessment. As an extension of this technique, another contribution
comes in the form of a regression model that takes the enhancement potential of
a new OCR engine into account. They both mark promising approaches, especially
for cultural institutions dealing with historic data of lower quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. (arXiv:2110.01691v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01691">
<div class="article-summary-box-inner">
<span><p>Although large language models (LLMs) have demonstrated impressive potential
on simple tasks, their breadth of scope, lack of transparency, and insufficient
controllability can make them less effective when assisting humans on more
complex tasks. In response, we introduce the concept of Chaining LLM steps
together, where the output of one step becomes the input for the next, thus
aggregating the gains per step. We first define a set of LLM primitive
operations useful for Chain construction, then present an interactive system
where users can modify these Chains, along with their intermediate results, in
a modular way. In a 20-person user study, we found that Chaining not only
improved the quality of task outcomes, but also significantly enhanced system
transparency, controllability, and sense of collaboration. Additionally, we saw
that users developed new ways of interacting with LLMs through Chains: they
leveraged sub-tasks to calibrate model expectations, compared and contrasted
alternative strategies by observing parallel downstream effects, and debugged
unexpected model outputs by "unit-testing" sub-components of a Chain. In two
case studies, we further explore how LLM Chains may be used in future
applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MoEfication: Conditional Computation of Transformer Models for Efficient Inference. (arXiv:2110.01786v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01786">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained language models can achieve superior
performance on most NLP tasks due to large parameter capacity, but also lead to
huge computation cost. Fortunately, we find by empirical study that, most
inputs only activate a tiny ratio of neurons during inference. Hence, we
explore to accelerate large-model inference by conditional computation based on
the sparse activation phenomenon. We propose to transform a large model into
its mixture-of-experts (MoE) version with equal model size, namely MoEfication.
Model MoEfication consists of two steps: (1) splitting the parameters of
feed-forward neural networks (FFNs) into multiple parts as experts, and (2)
building expert routers to decide which experts will be used for each input. To
further improve the performance of MoEfied models, we can also fine-tune the
models on downstream tasks, namely parameter calibration. Experimental results
show that the MoEfied models can significantly reduce computation cost, e.g.,
only activating 20% FFN parameters of a 700-million-parameter model without
performance degradation on several downstream tasks including text
classification and reading comprehension.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts. (arXiv:2110.01799v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01799">
<div class="article-summary-box-inner">
<span><p>Reviewing contracts is a time-consuming procedure that incurs large expenses
to companies and social inequality to those who cannot afford it. In this work,
we propose "document-level natural language inference (NLI) for contracts", a
novel, real-world application of NLI that addresses such problems. In this
task, a system is given a set of hypotheses (such as "Some obligations of
Agreement may survive termination.") and a contract, and it is asked to
classify whether each hypothesis is "entailed by", "contradicting to" or "not
mentioned by" (neutral to) the contract as well as identifying "evidence" for
the decision as spans in the contract. We annotated and release the largest
corpus to date consisting of 607 annotated contracts. We then show that
existing models fail badly on our task and introduce a strong baseline, which
(1) models evidence identification as multi-label classification over spans
instead of trying to predict start and end tokens, and (2) employs more
sophisticated context segmentation for dealing with long documents. We also
show that linguistic characteristics of contracts, such as negations by
exceptions, are contributing to the difficulty of this task and that there is
much room for improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey On Neural Word Embeddings. (arXiv:2110.01804v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01804">
<div class="article-summary-box-inner">
<span><p>Understanding human language has been a sub-challenge on the way of
intelligent machines. The study of meaning in natural language processing (NLP)
relies on the distributional hypothesis where language elements get meaning
from the words that co-occur within contexts. The revolutionary idea of
distributed representation for a concept is close to the working of a human
mind in that the meaning of a word is spread across several neurons, and a loss
of activation will only slightly affect the memory retrieval process.
</p>
<p>Neural word embeddings transformed the whole field of NLP by introducing
substantial improvements in all NLP tasks. In this survey, we provide a
comprehensive literature review on neural word embeddings. We give theoretical
foundations and describe existing work by an interplay between word embeddings
and language modelling. We provide broad coverage on neural word embeddings,
including early word embeddings, embeddings targeting specific semantic
relations, sense embeddings, morpheme embeddings, and finally, contextual
representations. Finally, we describe benchmark datasets in word embeddings'
performance evaluation and downstream tasks along with the performance results
of/due to word embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation. (arXiv:2110.01811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01811">
<div class="article-summary-box-inner">
<span><p>Pre-training (PT) and back-translation (BT) are two simple and powerful
methods to utilize monolingual data for improving the model performance of
neural machine translation (NMT). This paper takes the first step to
investigate the complementarity between PT and BT. We introduce two probing
tasks for PT and BT respectively and find that PT mainly contributes to the
encoder module while BT brings more benefits to the decoder. Experimental
results show that PT and BT are nicely complementary to each other,
establishing state-of-the-art performances on the WMT16 English-Romanian and
English-Russian benchmarks. Through extensive analyses on sentence originality
and word frequency, we also demonstrate that combining Tagged BT with PT is
more helpful to their complementarity, leading to better translation quality.
Source code is freely available at https://github.com/SunbowLiu/PTvsBT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Truth-Conditional Captioning of Time Series Data. (arXiv:2110.01839v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01839">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the task of automatically generating natural
language descriptions of salient patterns in a time series, such as stock
prices of a company over a week. A model for this task should be able to
extract high-level patterns such as presence of a peak or a dip. While typical
contemporary neural models with attention mechanisms can generate fluent output
descriptions for this task, they often generate factually incorrect
descriptions. We propose a computational model with a truth-conditional
architecture which first runs small learned programs on the input time series,
then identifies the programs/patterns which hold true for the given input, and
finally conditions on only the chosen valid program (rather than the input time
series) to generate the output text description. A program in our model is
constructed from modules, which are small neural networks that are designed to
capture numerical patterns and temporal information. The modules are shared
across multiple programs, enabling compositionality as well as efficient
learning of module parameters. The modules, as well as the composition of the
modules, are unobserved in data, and we learn them in an end-to-end fashion
with the only training signal coming from the accompanying natural language
text descriptions. We find that the proposed model is able to generate
high-precision captions even though we consider a small and simple space of
module types.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation Approaches in Natural Language Processing: A Survey. (arXiv:2110.01852v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01852">
<div class="article-summary-box-inner">
<span><p>As an effective strategy, data augmentation (DA) alleviates data scarcity
scenarios where deep learning techniques may fail. It is widely applied in
computer vision then introduced to natural language processing and achieves
improvements in many tasks. One of the main focuses of the DA methods is to
improve the diversity of training data, thereby helping the model to better
generalize to unseen testing data. In this survey, we frame DA methods into
three categories based on the diversity of augmented data, including
paraphrasing, noising, and sampling. Our paper sets out to analyze DA methods
in detail according to the above categories. Further, we also introduce their
applications in NLP tasks as well as the challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ASR Rescoring and Confidence Estimation with ELECTRA. (arXiv:2110.01857v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01857">
<div class="article-summary-box-inner">
<span><p>In automatic speech recognition (ASR) rescoring, the hypothesis with the
fewest errors should be selected from the n-best list using a language model
(LM). However, LMs are usually trained to maximize the likelihood of correct
word sequences, not to detect ASR errors. We propose an ASR rescoring method
for directly detecting errors with ELECTRA, which is originally a pre-training
method for NLP tasks. ELECTRA is pre-trained to predict whether each word is
replaced by BERT or not, which can simulate ASR error detection on large text
corpora. To make this pre-training closer to ASR error detection, we further
propose an extended version of ELECTRA called phone-attentive ELECTRA
(P-ELECTRA). In the pre-training of P-ELECTRA, each word is replaced by a
phone-to-word conversion model, which leverages phone information to generate
acoustically similar words. Since our rescoring method is optimized for
detecting errors, it can also be used for word-level confidence estimation.
Experimental evaluations on the Librispeech and TED-LIUM2 corpora show that our
rescoring method with ELECTRA is competitive with conventional rescoring
methods with faster inference. ELECTRA also performs better in confidence
estimation than BERT because it can learn to detect inappropriate words not
only in fine-tuning but also in pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating the Impact of Pre-trained Language Models on Dialog Evaluation. (arXiv:2110.01895v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01895">
<div class="article-summary-box-inner">
<span><p>Recently, there is a surge of interest in applying pre-trained language
models (Pr-LM) in automatic open-domain dialog evaluation. Pr-LMs offer a
promising direction for addressing the multi-domain evaluation challenge. Yet,
the impact of different Pr-LMs on the performance of automatic metrics is not
well-understood. This paper examines 8 different Pr-LMs and studies their
impact on three typical automatic dialog evaluation metrics across three
different dialog evaluation benchmarks. Specifically, we analyze how the choice
of Pr-LMs affects the performance of automatic metrics. Extensive correlation
analyses on each of the metrics are performed to assess the effects of
different Pr-LMs along various axes, including pre-training objectives, dialog
evaluation criteria, model size, and cross-dataset robustness. This study
serves as the first comprehensive assessment of the effects of different Pr-LMs
on automatic dialog evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01900">
<div class="article-summary-box-inner">
<span><p>Self-supervised speech representation learning methods like wav2vec 2.0 and
Hidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and
offer good representations for numerous speech processing tasks. Despite the
success of these methods, they require large memory and high pre-training
costs, making them inaccessible for researchers in academia and small
companies. Therefore, this paper introduces DistilHuBERT, a novel multi-task
learning framework to distill hidden representations from a HuBERT model
directly. This method reduces HuBERT's size by 75% and 73% faster while
retaining most performance in ten different tasks. Moreover, DistilHuBERT
required little training time and data, opening the possibilities of
pre-training personal and on-device SSL models for speech.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sicilian Translator: A Recipe for Low-Resource NMT. (arXiv:2110.01938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01938">
<div class="article-summary-box-inner">
<span><p>With 17,000 pairs of Sicilian-English translated sentences, Arba Sicula
developed the first neural machine translator for the Sicilian language. Using
small subword vocabularies, we trained small Transformer models with high
dropout parameters and achieved BLEU scores in the upper 20s. Then we
supplemented our dataset with backtranslation and multilingual translation and
pushed our scores into the mid 30s. We also attribute our success to
incorporating theoretical information in our dataset. Prior to training, we
biased the subword vocabulary towards the desinences one finds in a textbook.
And we included textbook exercises in our dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AraCOVID19-SSD: Arabic COVID-19 Sentiment and Sarcasm Detection Dataset. (arXiv:2110.01948v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01948">
<div class="article-summary-box-inner">
<span><p>Coronavirus disease (COVID-19) is an infectious respiratory disease that was
first discovered in late December 2019, in Wuhan, China, and then spread
worldwide causing a lot of panic and death. Users of social networking sites
such as Facebook and Twitter have been focused on reading, publishing, and
sharing novelties, tweets, and articles regarding the newly emerging pandemic.
A lot of these users often employ sarcasm to convey their intended meaning in a
humorous, funny, and indirect way making it hard for computer-based
applications to automatically understand and identify their goal and the harm
level that they can inflect. Motivated by the emerging need for annotated
datasets that tackle these kinds of problems in the context of COVID-19, this
paper builds and releases AraCOVID19-SSD a manually annotated Arabic COVID-19
sarcasm and sentiment detection dataset containing 5,162 tweets. To confirm the
practical utility of the built dataset, it has been carefully analyzed and
tested using several classification models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Objective Few-shot Learning for Fair Classification. (arXiv:2110.01951v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01951">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a general framework for mitigating the disparities
of the predicted classes with respect to secondary attributes within the data
(e.g., race, gender etc.). Our proposed method involves learning a
multi-objective function that in addition to learning the primary objective of
predicting the primary class labels from the data, also employs a
clustering-based heuristic to minimize the disparities of the class label
distribution with respect to the cluster memberships, with the assumption that
each cluster should ideally map to a distinct combination of attribute values.
Experiments demonstrate effective mitigation of cognitive biases on a benchmark
dataset without the use of annotations of secondary attribute values (the
zero-shot case) or with the use of a small number of attribute value
annotations (the few-shot case).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Transition System for End-to-End Opinion Role Labeling. (arXiv:2110.02001v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02001">
<div class="article-summary-box-inner">
<span><p>Unified opinion role labeling (ORL) aims to detect all possible opinion
structures of `opinion-holder-target' in one shot, given a text. The existing
transition-based unified method, unfortunately, is subject to longer opinion
terms and fails to solve the term overlap issue. Current top performance has
been achieved by employing the span-based graph model, which however still
suffers from both high model complexity and insufficient interaction among
opinions and roles. In this work, we investigate a novel solution by revisiting
the transition architecture, and augment it with a pointer network (PointNet).
The framework parses out all opinion structures in linear-time complexity,
meanwhile breaks through the limitation of any length of terms with PointNet.
To achieve the explicit opinion-role interactions, we further propose a unified
dependency-opinion graph (UDOG), co-modeling the syntactic dependency structure
and the partial opinion-role structure. We then devise a relation-centered
graph aggregator (RCGA) to encode the multi-relational UDOG, where the
resulting high-order representations are used to promote the predictions in the
vanilla transition system. Our model achieves new state-of-the-art results on
the MPQA benchmark. Analyses further demonstrate the superiority of our methods
on both efficacy and efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FoodChem: A food-chemical relation extraction model. (arXiv:2110.02019v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02019">
<div class="article-summary-box-inner">
<span><p>In this paper, we present FoodChem, a new Relation Extraction (RE) model for
identifying chemicals present in the composition of food entities, based on
textual information provided in biomedical peer-reviewed scientific literature.
The RE task is treated as a binary classification problem, aimed at identifying
whether the contains relation exists between a food-chemical entity pair. This
is accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models.
For evaluation purposes, a novel dataset with annotated contains relations in
food-chemical entity pairs is generated, in a golden and silver version. The
models are integrated into a voting scheme in order to produce the silver
version of the dataset which we use for augmenting the individual models, while
the manually annotated golden version is used for their evaluation. Out of the
three evaluated models, the BioBERT model achieves the best results, with a
macro averaged F1 score of 0.902 in the unbalanced augmentation setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings. (arXiv:2110.02030v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02030">
<div class="article-summary-box-inner">
<span><p>Semantic sentence embeddings are usually supervisedly built minimizing
distances between pairs of embeddings of sentences labelled as semantically
similar by annotators. Since big labelled datasets are rare, in particular for
non-English languages, and expensive, recent studies focus on unsupervised
approaches that require not-paired input sentences. We instead propose a
language-independent approach to build large datasets of pairs of informal
texts weakly similar, without manual human effort, exploiting Twitter's
intrinsic powerful signals of relatedness: replies and quotes of tweets. We use
the collected pairs to train a Transformer model with triplet-like structures,
and we test the generated embeddings on Twitter NLP similarity tasks (PIT and
TURL) and STSb. We also introduce four new sentence ranking evaluation
benchmarks of informal texts, carefully extracted from the initial collections
of tweets, proving not only that our best model learns classical Semantic
Textual Similarity, but also excels on tasks where pairs of sentences are not
exact paraphrases. Ablation studies reveal how increasing the corpus size
influences positively the results, even at 2M samples, suggesting that bigger
collections of Tweets still do not contain redundant information about semantic
similarities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions. (arXiv:2110.02035v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02035">
<div class="article-summary-box-inner">
<span><p>In this paper we introduce the Food Drinks and groceries Images Multi Lingual
(FooDI-ML) dataset. This dataset contains over 1.5M unique images and over 9.5M
store names, product names descriptions, and collection sections gathered from
the Glovo application. The data made available corresponds to food, drinks and
groceries products from 37 countries in Europe, the Middle East, Africa and
Latin America. The dataset comprehends 33 languages, including 870K samples of
languages of countries from Eastern Europe and Western Asia such as Ukrainian
and Kazakh, which have been so far underrepresented in publicly available
visio-linguistic datasets. The dataset also includes widely spoken languages
such as Spanish and English. To assist further research, we include a benchmark
over the text-image retrieval task using ADAPT, a SotA existing technique.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ur-iw-hnt at GermEval 2021: An Ensembling Strategy with Multiple BERT Models. (arXiv:2110.02042v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02042">
<div class="article-summary-box-inner">
<span><p>This paper describes our approach (ur-iw-hnt) for the Shared Task of
GermEval2021 to identify toxic, engaging, and fact-claiming comments. We
submitted three runs using an ensembling strategy by majority (hard) voting
with multiple different BERT models of three different types: German-based,
Twitter-based, and multilingual models. All ensemble models outperform single
models, while BERTweet is the winner of all individual models in every subtask.
Twitter-based models perform better than GermanBERT models, and multilingual
models perform worse but by a small margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TENT: Text Classification Based on ENcoding Tree Learning. (arXiv:2110.02047v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02047">
<div class="article-summary-box-inner">
<span><p>Text classification is a primary task in natural language processing (NLP).
Recently, graph neural networks (GNNs) have developed rapidly and been applied
to text classification tasks. Although more complex models tend to achieve
better performance, research highly depends on the computing power of the
device used. In this article, we propose TENT (https://github.com/Daisean/TENT)
to obtain better text classification performance and reduce the reliance on
computing power. Specifically, we first establish a dependency analysis graph
for each text and then convert each graph into its corresponding encoding tree.
The representation of the entire graph is obtained by updating the
representation of the non-leaf nodes in the encoding tree. Experimental results
show that our method outperforms other baselines on several datasets while
having a simple structure and few parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transfer Learning for Multi-lingual Tasks -- a Survey. (arXiv:2110.02052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02052">
<div class="article-summary-box-inner">
<span><p>These days different platforms such as social media provide their clients
from different backgrounds and languages the possibility to connect and
exchange information. It is not surprising anymore to see comments from
different languages in posts published by international celebrities or data
providers. In this era, understanding cross languages content and
multilingualism in natural language processing (NLP) are hot topics, and
multiple efforts have tried to leverage existing technologies in NLP to tackle
this challenging research problem. In this survey, we provide a comprehensive
overview of the existing literature with a focus on transfer learning
techniques in multilingual tasks. We also identify potential opportunities for
further research in this domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NoiER: An Approach for Training more Reliable Fine-TunedDownstream Task Models. (arXiv:2110.02054v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02054">
<div class="article-summary-box-inner">
<span><p>The recent development in pretrained language models trained in a
self-supervised fashion, such as BERT, is driving rapid progress in the field
of NLP. However, their brilliant performance is based on leveraging syntactic
artifacts of the training data rather than fully understanding the intrinsic
meaning of language. The excessive exploitation of spurious artifacts causes a
problematic issue: The distribution collapse problem, which is the phenomenon
that the model fine-tuned on downstream tasks is unable to distinguish
out-of-distribution (OOD) sentences while producing a high confidence score. In
this paper, we argue that distribution collapse is a prevalent issue in
pretrained language models and propose noise entropy regularisation (NoiER) as
an efficient learning paradigm that solves the problem without auxiliary models
and additional~data. The proposed approach improved traditional OOD detection
evaluation metrics by 55% on average compared to the original fine-tuned
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Training Resources Insufficient? Predict First Then Explain!. (arXiv:2110.02056v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02056">
<div class="article-summary-box-inner">
<span><p>Natural language free-text explanation generation is an efficient approach to
train explainable language processing models for
commonsense-knowledge-requiring tasks. The most predominant form of these
models is the explain-then-predict (EtP) structure, which first generates
explanations and uses them for making decisions. The performance of EtP models
is highly dependent on that of the explainer by the nature of their structure.
Therefore, large-sized explanation data are required to train a good explainer
model. However, annotating explanations is expensive. Also, recent works reveal
that free-text explanations might not convey sufficient information for
decision making. These facts cast doubts on the effectiveness of EtP models. In
this paper, we argue that the predict-then-explain (PtE) architecture is a more
efficient approach in terms of the modelling perspective. Our main contribution
is twofold. First, we show that the PtE structure is the most data-efficient
approach when explanation data are lacking. Second, we reveal that the PtE
structure is always more training-efficient than the EtP structure. We also
provide experimental results that confirm the theoretical advantages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Prediction in NLP -- A survey. (arXiv:2110.02057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02057">
<div class="article-summary-box-inner">
<span><p>Over the last several years, the field of Structured prediction in NLP has
had seen huge advancements with sophisticated probabilistic graphical models,
energy-based networks, and its combination with deep learning-based approaches.
This survey provides a brief of major techniques in structured prediction and
its applications in the NLP domains like parsing, sequence labeling, text
generation, and sequence to sequence tasks. We also deep-dived into
energy-based and attention-based techniques in structured prediction,
identified some relevant open issues and gaps in the current state-of-the-art
research, and have come up with some detailed ideas for future research in
these fields.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Interactively Generating Explanations for Transformer-based Language Models. (arXiv:2110.02058v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02058">
<div class="article-summary-box-inner">
<span><p>Transformer language models are state-of-the-art in a multitude of NLP tasks.
Despite these successes, their opaqueness remains problematic. Recent methods
aiming to provide interpretability and explainability to black-box models
primarily focus on post-hoc explanations of (sometimes spurious) input-output
correlations. Instead, we emphasize using prototype networks directly
incorporated into the model architecture and hence explain the reasoning
process behind the network's decisions. Moreover, while our architecture
performs on par with several language models, it enables one to learn from user
interactions. This not only offers a better understanding of language models,
but uses human capabilities to incorporate knowledge outside of the rigid range
of purely data-driven approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Relational Graph based Heterogeneous Multi-Task Learning in Community Question Answering. (arXiv:2110.02059v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02059">
<div class="article-summary-box-inner">
<span><p>Various data mining tasks have been proposed to study Community Question
Answering (CQA) platforms like Stack Overflow. The relatedness between some of
these tasks provides useful learning signals to each other via Multi-Task
Learning (MTL). However, due to the high heterogeneity of these tasks, few
existing works manage to jointly solve them in a unified framework. To tackle
this challenge, we develop a multi-relational graph based MTL model called
Heterogeneous Multi-Task Graph Isomorphism Network (HMTGIN) which efficiently
solves heterogeneous CQA tasks. In each training forward pass, HMTGIN embeds
the input CQA forum graph by an extension of Graph Isomorphism Network and skip
connections. The embeddings are then shared across all task-specific output
layers to compute respective losses. Moreover, two cross-task constraints based
on the domain knowledge about tasks' relationships are used to regularize the
joint learning. In the evaluation, the embeddings are shared among different
task-specific output layers to make corresponding predictions. To the best of
our knowledge, HMTGIN is the first MTL model capable of tackling CQA tasks from
the aspect of multi-relational graphs. To evaluate HMTGIN's effectiveness, we
build a novel large-scale multi-relational graph CQA dataset with over two
million nodes from Stack Overflow. Extensive experiments show that: $(1)$
HMTGIN is superior to all baselines on five tasks; $(2)$ The proposed MTL
strategy and cross-task constraints have substantial advantages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teach Me What to Say and I Will Learn What to Pick: Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models. (arXiv:2110.02067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02067">
<div class="article-summary-box-inner">
<span><p>Knowledge Grounded Conversation Models (KGCM) are usually based on a
selection/retrieval module and a generation module, trained separately or
simultaneously, with or without having access to a gold knowledge option. With
the introduction of large pre-trained generative models, the selection and
generation part have become more and more entangled, shifting the focus towards
enhancing knowledge incorporation (from multiple sources) instead of trying to
pick the best knowledge option. These approaches however depend on knowledge
labels and/or a separate dense retriever for their best performance. In this
work we study the unsupervised selection abilities of pre-trained generative
models (e.g. BART) and show that by adding a score-and-aggregate module between
encoder and decoder, they are capable of learning to pick the proper knowledge
through minimising the language modelling loss (i.e. without having access to
knowledge labels). Trained as such, our model - K-Mine - shows competitive
selection and generation performance against models that benefit from knowledge
labels and/or separate dense retriever.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OPAD: An Optimized Policy-based Active Learning Framework for Document Content Analysis. (arXiv:2110.02069v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02069">
<div class="article-summary-box-inner">
<span><p>Documents are central to many business systems, and include forms, reports,
contracts, invoices or purchase orders. The information in documents is
typically in natural language, but can be organized in various layouts and
formats. There have been recent spurt of interest in understanding document
content with novel deep learning architectures. However, document understanding
tasks need dense information annotations, which are costly to scale and
generalize. Several active learning techniques have been proposed to reduce the
overall budget of annotation while maintaining the performance of the
underlying deep learning model. However, most of these techniques work only for
classification problems. But content detection is a more complex task, and has
been scarcely explored in active learning literature. In this paper, we propose
\textit{OPAD}, a novel framework using reinforcement policy for active learning
in content detection tasks for documents. The proposed framework learns the
acquisition function to decide the samples to be selected while optimizing
performance metrics that the tasks typically have. Furthermore, we extend to
weak labelling scenarios to further reduce the cost of annotation
significantly. We propose novel rewards to account for class imbalance and user
feedback in the annotation interface, to improve the active learning method. We
show superior performance of the proposed \textit{OPAD} framework for active
learning for various tasks related to document understanding like layout
parsing, object detection and named entity recognition. Ablation studies for
human feedback and class imbalance rewards are presented, along with a
comparison of annotation times for different approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NaRLE: Natural Language Models using Reinforcement Learning with Emotion Feedback. (arXiv:2110.02148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02148">
<div class="article-summary-box-inner">
<span><p>Current research in dialogue systems is focused on conversational assistants
working on short conversations in either task-oriented or open domain settings.
In this paper, we focus on improving task-based conversational assistants
online, primarily those working on document-type conversations (e.g., emails)
whose contents may or may not be completely related to the assistant's task. We
propose "NARLE" a deep reinforcement learning (RL) framework for improving the
natural language understanding (NLU) component of dialogue systems online
without the need to collect human labels for customer data. The proposed
solution associates user emotion with the assistant's action and uses that to
improve NLU models using policy gradients. For two intent classification
problems, we empirically show that using reinforcement learning to fine tune
the pre-trained supervised learning models improves performance up to 43%.
Furthermore, we demonstrate the robustness of the method to partial and noisy
implicit feedback.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Impact of COVID-19 on Economy from the Perspective of Users Reviews. (arXiv:2110.02198v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02198">
<div class="article-summary-box-inner">
<span><p>One of the most important incidents in the world in 2020 is the outbreak of
the Coronavirus. Users on social networks publish a large number of comments
about this event. These comments contain important hidden information of public
opinion regarding this pandemic. In this research, a large number of
Coronavirus-related tweets are considered and analyzed using natural language
processing and information retrieval science. Initially, the location of the
tweets is determined using a dictionary prepared through the Geo-Names
geographic database, which contains detailed and complete information of places
such as city names, streets, and postal codes. Then, using a large dictionary
prepared from the terms of economics, related tweets are extracted and
sentiments corresponded to tweets are analyzed with the help of the RoBERTa
language-based model, which has high accuracy and good performance. Finally,
the frequency chart of tweets related to the economy and their sentiment scores
(positive and negative tweets) is plotted over time for the entire world and
the top 10 economies. From the analysis of the charts, we learn that the reason
for publishing economic tweets is not only the increase in the number of people
infected with the Coronavirus but also imposed restrictions and lockdowns in
countries. The consequences of these restrictions include the loss of millions
of jobs and the economic downturn.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Psuedolabels for training Sentiment Classifiers makes the model generalize better across datasets. (arXiv:2110.02200v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02200">
<div class="article-summary-box-inner">
<span><p>The problem statement addressed in this work is : For a public sentiment
classification API, how can we set up a classifier that works well on different
types of data, having limited ability to annotate data from across domains. We
show that given a large amount of unannotated data from across different
domains and pseudolabels on this dataset generated by a classifier trained on a
small annotated dataset from one domain, we can train a sentiment classifier
that generalizes better across different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Sense-Specific Static Embeddings using Contextualised Word Embeddings as a Proxy. (arXiv:2110.02204v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02204">
<div class="article-summary-box-inner">
<span><p>Contextualised word embeddings generated from Neural Language Models (NLMs),
such as BERT, represent a word with a vector that considers the semantics of
the target word as well its context. On the other hand, static word embeddings
such as GloVe represent words by relatively low-dimensional, memory- and
compute-efficient vectors but are not sensitive to the different senses of the
word. We propose Context Derived Embeddings of Senses (CDES), a method that
extracts sense related information from contextualised embeddings and injects
it into static embeddings to create sense-specific static embeddings.
Experimental results on multiple benchmarks for word sense disambiguation and
sense discrimination tasks show that CDES can accurately learn sense-specific
static embeddings reporting comparable performance to the current
state-of-the-art sense embeddings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Waypoint Models for Instruction-guided Navigation in Continuous Environments. (arXiv:2110.02207v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02207">
<div class="article-summary-box-inner">
<span><p>Little inquiry has explicitly addressed the role of action spaces in
language-guided visual navigation -- either in terms of its effect on
navigation success or the efficiency with which a robotic agent could execute
the resulting trajectory. Building on the recently released VLN-CE setting for
instruction following in continuous environments, we develop a class of
language-conditioned waypoint prediction networks to examine this question. We
vary the expressivity of these models to explore a spectrum between low-level
actions and continuous waypoint prediction. We measure task performance and
estimated execution time on a profiled LoCoBot robot. We find more expressive
models result in simpler, faster to execute trajectories, but lower-level
actions can achieve better navigation metrics by approximating shortest paths
better. Further, our models outperform prior work in VLN-CE and set a new
state-of-the-art on the public leaderboard -- increasing success rate by 4%
with our best model on this challenging task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Legal Approach to Hate Speech: Operationalizing the EU's Legal Framework against the Expression of Hatred as an NLP Task. (arXiv:2004.03422v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03422">
<div class="article-summary-box-inner">
<span><p>We propose a 'legal approach' to hate speech detection by operationalization
of the decision as to whether a post is subject to criminal law into an NLP
task. Comparing existing regulatory regimes for hate speech, we base our
investigation on the European Union's framework as it provides a widely
applicable legal minimum standard. Accurately judging whether a post is
punishable or not usually requires legal training. We show that, by breaking
the legal assessment down into a series of simpler sub-decisions, even
laypersons can annotate consistently. Based on a newly annotated dataset, our
experiments show that directly learning an automated model of punishable
content is challenging. However, learning the two sub-tasks of `target group'
and `targeting conduct' instead of an end-to-end approach to punishability
yields better results. Overall, our method also provides decisions that are
more transparent than those of end-to-end models, which is a crucial point in
legal decision-making.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enriched Pre-trained Transformers for Joint Slot Filling and Intent Detection. (arXiv:2004.14848v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14848">
<div class="article-summary-box-inner">
<span><p>Detecting the user's intent and finding the corresponding slots among the
utterance's words are important tasks in natural language understanding. Their
interconnected nature makes their joint modeling a standard part of training
such models. Moreover, data scarceness and specialized vocabularies pose
additional challenges. Recently, the advances in pre-trained language models,
namely contextualized models such as ELMo and BERT have revolutionized the
field by tapping the potential of training very large models with just a few
steps of fine-tuning on a task-specific dataset. Here, we leverage such models,
namely BERT and RoBERTa, and we design a novel architecture on top of them.
Moreover, we propose an intent pooling attention mechanism, and we reinforce
the slot filling task by fusing intent distributions, word features, and token
representations. The experimental results on standard datasets show that our
model outperforms both the current non-BERT state of the art as well as some
stronger BERT-based baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dataset for Automatic Summarization of Russian News. (arXiv:2006.11063v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.11063">
<div class="article-summary-box-inner">
<span><p>Automatic text summarization has been studied in a variety of domains and
languages. However, this does not hold for the Russian language. To overcome
this issue, we present Gazeta, the first dataset for summarization of Russian
news. We describe the properties of this dataset and benchmark several
extractive and abstractive models. We demonstrate that the dataset is a valid
task for methods of text summarization for Russian. Additionally, we prove the
pretrained mBART model to be useful for Russian text summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Book Success Prediction with Pretrained Sentence Embeddings and Readability Scores. (arXiv:2007.11073v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.11073">
<div class="article-summary-box-inner">
<span><p>Predicting the potential success of a book in advance is vital in many
applications. This could help both publishers and readers in their
decision-making process whether or not a book is worth publishing and reading,
respectively. In this paper, we propose a model that leverages pretrained
sentence embeddings along with various readability scores for book success
prediction. Unlike previous methods, the proposed method requires no
count-based, lexical, or syntactic features. Instead, we use a convolutional
neural network over pretrained sentence embeddings and leverage different
readability scores through a simple concatenation operation. Our proposed model
outperforms strong baselines for this task by as large as 6.4\% F1-score
points. Moreover, our experiments show that according to our model, only the
first 1K sentences are good enough to predict the potential success of books.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The optimality of syntactic dependency distances. (arXiv:2007.15342v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15342">
<div class="article-summary-box-inner">
<span><p>It is often stated that human languages, as other biological systems, are
shaped by cost-cutting pressures but, to what extent? Attempts to quantify the
degree of optimality of languages by means of an optimality score have been
scarce and focused mostly on English. Here we recast the problem of the
optimality of the word order of a sentence as an optimization problem on a
spatial network where the vertices are words, arcs indicate syntactic
dependencies and the space is defined by the linear order of the words in the
sentence. We introduce a new score to quantify the cognitive pressure to reduce
the distance between linked words in a sentence. The analysis of sentences from
93 languages representing 19 linguistic families reveals that half of languages
are optimized to a 70% or more. The score indicates that distances are not
significantly reduced in a few languages and confirms two theoretical
predictions, i.e. that longer sentences are more optimized and that distances
are more likely to be longer than expected by chance in short sentences. We
present a new hierarchical ranking of languages by their degree of
optimization. The new score has implications for various fields of language
research (dependency linguistics, typology, historical linguistics, clinical
linguistics and cognitive science). Finally, the principles behind the design
of the score have implications for network science.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Quality Assessment of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations. (arXiv:2102.11573v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11573">
<div class="article-summary-box-inner">
<span><p>During a psychotherapy session, the counselor typically adopts techniques
which are codified along specific dimensions (e.g., 'displays warmth and
confidence', or 'attempts to set up collaboration') to facilitate the
evaluation of the session. Those constructs, traditionally scored by trained
human raters, reflect the complex nature of psychotherapy and highly depend on
the context of the interaction. Recent advances in deep contextualized language
models offer an avenue for accurate in-domain linguistic representations which
can lead to robust recognition and scoring of such psychotherapy-relevant
behavioral constructs, and support quality assurance and supervision. In this
work, we propose a BERT-based model for automatic behavioral scoring of a
specific type of psychotherapy, called Cognitive Behavioral Therapy (CBT),
where prior work is limited to frequency-based language features and/or short
text excerpts which do not capture the unique elements involved in a
spontaneous long conversational interaction. The model focuses on the
classification of therapy sessions with respect to the overall score achieved
on the widely-used Cognitive Therapy Rating Scale (CTRS), but is trained in a
multi-task manner in order to achieve higher interpretability. BERT-based
representations are further augmented with available therapy metadata,
providing relevant non-linguistic context and leading to consistent performance
improvements. We train and evaluate our models on a set of 1,118 real-world
therapy sessions, recorded and automatically transcribed. Our best model
achieves an F1 score equal to 72.61% on the binary classification task of low
vs. high total CTRS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability. (arXiv:2103.07162v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07162">
<div class="article-summary-box-inner">
<span><p>This paper investigates whether the power of the models pre-trained on text
data, such as BERT, can be transferred to general token sequence classification
applications. To verify pre-trained models' transferability, we test the
pre-trained models on text classification tasks with meanings of tokens
mismatches, and real-world non-text token sequence classification data,
including amino acid, DNA, and music. We find that even on non-text data, the
models pre-trained on text converge faster, perform better than the randomly
initialized models, and only slightly worse than the models using task-specific
knowledge. We also find that the representations of the text and non-text
pre-trained models share non-trivial similarities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integer-only Zero-shot Quantization for Efficient Speech Recognition. (arXiv:2103.16827v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16827">
<div class="article-summary-box-inner">
<span><p>End-to-end neural network models achieve improved performance on various
automatic speech recognition (ASR) tasks. However, these models perform poorly
on edge hardware due to large memory and computation requirements. While
quantizing model weights and/or activations to low-precision can be a promising
solution, previous research on quantizing ASR models is limited. In particular,
the previous approaches use floating-point arithmetic during inference and thus
they cannot fully exploit efficient integer processing units. Moreover, they
require training/validation data during quantization, which may not be
available due to security/privacy concerns. To address these limitations, we
propose an integer-only, zero shot quantization scheme for ASR models. In
particular, we generate synthetic data whose runtime statistics resemble the
real data, and we use it to calibrate models during quantization. We apply our
method to quantize QuartzNet, Jasper, and Conformer and show negligible WER
change as compared to the full-precision baseline models, even without using
any training data. Moreover, we achieve up to 2.35x speedup on a T4 GPU and 4x
compression rate, with a modest WER degradation of &lt;1% with INT8 quantization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Architectures and Training for Raw Waveform Feature Extraction in ASR. (arXiv:2104.04298v3 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04298">
<div class="article-summary-box-inner">
<span><p>With the success of neural network based modeling in automatic speech
recognition (ASR), many studies investigated acoustic modeling and learning of
feature extractors directly based on the raw waveform. Recently, one line of
research has focused on unsupervised pre-training of feature extractors on
audio-only data to improve downstream ASR performance. In this work, we
investigate the usefulness of one of these front-end frameworks, namely
wav2vec, in a setting without additional untranscribed data for hybrid ASR
systems. We compare this framework both to the manually defined standard
Gammatone feature set, as well as to features extracted as part of the acoustic
model of an ASR system trained supervised. We study the benefits of using the
pre-trained feature extractor and explore how to additionally exploit an
existing acoustic model trained with different features. Finally, we
systematically examine combinations of the described features in order to
further advance the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Dialog Generation with Fine-Grained Intents. (arXiv:2105.06829v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06829">
<div class="article-summary-box-inner">
<span><p>Empathetic dialog generation aims at generating coherent responses following
previous dialog turns and, more importantly, showing a sense of caring and a
desire to help. Existing models either rely on pre-defined emotion labels to
guide the response generation, or use deterministic rules to decide the emotion
of the response. With the advent of advanced language models, it is possible to
learn subtle interactions directly from the dataset, providing that the emotion
categories offer sufficient nuances and other non-emotional but emotional
regulating intents are included. In this paper, we describe how to incorporate
a taxonomy of 32 emotion categories and 8 additional emotion regulating intents
to succeed the task of empathetic response generation. To facilitate the
training, we also curated a large-scale emotional dialog dataset from movie
subtitles. Through a carefully designed crowdsourcing experiment, we evaluated
and demonstrated how our model produces more empathetic dialogs compared with
its baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence. (arXiv:2107.02173v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02173">
<div class="article-summary-box-inner">
<span><p>Topic model evaluation, like evaluation of other unsupervised methods, can be
contentious. However, the field has coalesced around automated estimates of
topic coherence, which rely on the frequency of word co-occurrences in a
reference corpus. Recent models relying on neural components surpass classical
topic models according to these metrics. At the same time, unlike classical
models, the practice of neural topic model evaluation suffers from a validation
gap: automatic coherence for neural models has not been validated using human
experimentation. In addition, as we show via a meta-analysis of topic modeling
literature, there is a substantial standardization gap in the use of automated
topic modeling benchmarks. We address both the standardization gap and the
validation gap. Using two of the most widely used topic model evaluation
datasets, we assess a dominant classical model and two state-of-the-art neural
models in a systematic, clearly documented, reproducible way. We use automatic
coherence along with the two most widely accepted human judgment tasks, namely,
topic rating and word intrusion. Automated evaluation will declare one model
significantly different from another when corresponding human evaluations do
not, calling into question the validity of fully automatic evaluations
independent of human judgments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuned Language Models Are Zero-Shot Learners. (arXiv:2109.01652v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01652">
<div class="article-summary-box-inner">
<span><p>This paper explores a simple method for improving the zero-shot learning
abilities of language models. We show that instruction tuning -- finetuning
language models on a collection of tasks described via instructions --
substantially boosts zero-shot performance on unseen tasks.
</p>
<p>We take a 137B parameter pretrained language model and instruction-tune it on
over 60 NLP tasks verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task
types. FLAN substantially improves the performance of its unmodified
counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we
evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,
BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number
of tasks and model scale are key components to the success of instruction
tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12584">
<div class="article-summary-box-inner">
<span><p>In recent times, there has been definitive progress in the field of NLP, with
its applications growing as the utility of our language models increases with
advances in their performance. However, these models require a large amount of
computational power and data to train, consequently leading to large carbon
footprints. Therefore, is it imperative that we study the carbon efficiency and
look for alternatives to reduce the overall environmental impact of training
models, in particular large language models. In our work, we assess the
performance of models for machine translation, across multiple language pairs
to assess the difference in computational power required to train these models
for each of these language pairs and examine the various components of these
models to analyze aspects of our pipeline that can be optimized to reduce these
carbon emissions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MFAQ: a Multilingual FAQ Dataset. (arXiv:2109.12870v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12870">
<div class="article-summary-box-inner">
<span><p>In this paper, we present the first multilingual FAQ dataset publicly
available. We collected around 6M FAQ pairs from the web, in 21 different
languages. Although this is significantly larger than existing FAQ retrieval
datasets, it comes with its own challenges: duplication of content and uneven
distribution of topics. We adopt a similar setup as Dense Passage Retrieval
(DPR) and test various bi-encoders on this dataset. Our experiments reveal that
a multilingual model based on XLM-RoBERTa achieves the best results, except for
English. Lower resources languages seem to learn from one another as a
multilingual model achieves a higher MRR than language-specific ones. Our
qualitative analysis reveals the brittleness of the model on simple word
changes. We publicly release our dataset, model and training script.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems. (arXiv:2109.14895v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.14895">
<div class="article-summary-box-inner">
<span><p>In translating text where sentiment is the main message, human translators
give particular attention to sentiment-carrying words. The reason is that an
incorrect translation of such words would miss the fundamental aspect of the
source text, i.e. the author's sentiment. In the online world, MT systems are
extensively used to translate User-Generated Content (UGC) such as reviews,
tweets, and social media posts, where the main message is often the author's
positive or negative attitude towards the topic of the text. It is important in
such scenarios to accurately measure how far an MT system can be a reliable
real-life utility in transferring the correct affect message. This paper
tackles an under-recognised problem in the field of machine translation
evaluation which is judging to what extent automatic metrics concur with the
gold standard of human evaluation for a correct translation of sentiment. We
evaluate the efficacy of conventional quality metrics in spotting a
mistranslation of sentiment, especially when it is the sole error in the MT
output. We propose a numerical `sentiment-closeness' measure appropriate for
assessing the accuracy of a translated affect message in UGC text by an MT
system. We will show that incorporating this sentiment-aware measure can
significantly enhance the correlation of some available quality metrics with
the human judgement of an accurate translation of sentiment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional generalization in semantic parsing with pretrained transformers. (arXiv:2109.15101v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.15101">
<div class="article-summary-box-inner">
<span><p>Large-scale pretraining instills large amounts of knowledge in deep neural
networks. This, in turn, improves the generalization behavior of these models
in downstream tasks. What exactly are the limits to the generalization benefits
of large-scale pretraining? Here, we report observations from some simple
experiments aimed at addressing this question in the context of two semantic
parsing tasks involving natural language, SCAN and COGS. We show that language
models pretrained exclusively with non-English corpora, or even with
programming language corpora, significantly improve out-of-distribution
generalization in these benchmarks, compared with models trained from scratch,
even though both benchmarks are English-based. This demonstrates the
surprisingly broad transferability of pretrained representations and knowledge.
Pretraining with a large-scale protein sequence prediction task, on the other
hand, mostly deteriorates the generalization performance in SCAN and COGS,
suggesting that pretrained representations do not transfer universally and that
there are constraints on the similarity between the pretraining and downstream
domains for successful transfer. Finally, we show that larger models are harder
to train from scratch and their generalization accuracy is lower when trained
up to convergence on the relatively small SCAN and COGS datasets, but the
benefits of large-scale pretraining become much clearer with larger models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts. (arXiv:2110.01159v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01159">
<div class="article-summary-box-inner">
<span><p>Recent models in developing summarization systems consist of millions of
parameters and the model performance is highly dependent on the abundance of
training data. While most existing summarization corpora contain data in the
order of thousands to one million, generation of large-scale summarization
datasets in order of couple of millions is yet to be explored. Practically,
more data is better at generalizing the training patterns to unseen data. In
this paper, we introduce TLDR9+ -- a large-scale summarization dataset --
containing over 9 million training instances extracted from Reddit discussion
forum (https://github.com/sajastu/reddit_collector). This dataset is
specifically gathered to perform extreme summarization (i.e., generating
one-sentence summary in high compression and abstraction) and is more than
twice larger than the previously proposed dataset. We go one step further and
with the help of human annotations, we distill a more fine-grained dataset by
sampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We
further pinpoint different state-of-the-art summarization models on our
proposed datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LawSum: A weakly supervised approach for Indian Legal Document Summarization. (arXiv:2110.01188v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01188">
<div class="article-summary-box-inner">
<span><p>Unlike the courts in western countries, public records of Indian judiciary
are completely unstructured and noisy. No large scale publicly available
annotated datasets of Indian legal documents exist till date. This limits the
scope for legal analytics research. In this work, we propose a new dataset
consisting of over 10,000 judgements delivered by the supreme court of India
and their corresponding hand written summaries. The proposed dataset is
pre-processed by normalising common legal abbreviations, handling spelling
variations in named entities, handling bad punctuations and accurate sentence
tokenization. Each sentence is tagged with their rhetorical roles. We also
annotate each judgement with several attributes like date, names of the
plaintiffs, defendants and the people representing them, judges who delivered
the judgement, acts/statutes that are cited and the most common citations used
to refer the judgement. Further, we propose an automatic labelling technique
for identifying sentences which have summary worthy information. We demonstrate
that this auto labeled data can be used effectively to train a weakly
supervised sentence extractor with high accuracy. Some possible applications of
this dataset besides legal document summarization can be in retrieval, citation
analysis and prediction of decisions by a particular judge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01500">
<div class="article-summary-box-inner">
<span><p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)
systems have achieved great success due to their simplicity and promising
performance. Neural Transducer based models are increasingly popular in
streaming E2E based ASR systems and have been reported to outperform the
traditional hybrid system in some scenarios. However, the joint optimization of
acoustic model, lexicon and language model in neural Transducer also brings
about challenges to utilize pure text for language model adaptation. This
drawback might prevent their potential applications in practice. In order to
address this issue, in this paper, we propose a novel model, factorized neural
Transducer, by factorizing the blank and vocabulary prediction, and adopting a
standalone language model for the vocabulary prediction. It is expected that
this factorization can transfer the improvement of the standalone language
model to the Transducer for speech recognition, which allows various language
model adaptation techniques to be applied. We demonstrate that the proposed
factorized neural Transducer yields 15% to 20% WER improvements when
out-of-domain text data is used for language model adaptation, at the cost of a
minor degradation in WER on a general test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v2 [eess.AS] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.00165">
<div class="article-summary-box-inner">
<span><p>Self- and semi-supervised learning methods have been actively investigated to
reduce labeled training data or enhance the model performance. However, the
approach mostly focus on in-domain performance for public datasets. In this
study, we utilize the combination of self- and semi-supervised learning methods
to solve unseen domain adaptation problem in a large-scale production setting
for online ASR model. This approach demonstrates that using the source domain
data with a small fraction of the target domain data (3%) can recover the
performance gap compared to a full data baseline: relative 13.5% WER
improvement for target domain data.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-06 23:09:22.723797085 UTC">2021-10-06 23:09:22 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>