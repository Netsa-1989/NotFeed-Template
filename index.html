<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-27T01:30:00Z">09-27</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">CSAGN: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling. (arXiv:2109.11541v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11541">
<div class="article-summary-box-inner">
<span><p>Conversational semantic role labeling (CSRL) is believed to be a crucial step
towards dialogue understanding. However, it remains a major challenge for
existing CSRL parser to handle conversational structural information. In this
paper, we present a simple and effective architecture for CSRL which aims to
address this problem. Our model is based on a conversational structure-aware
graph network which explicitly encodes the speaker dependent information. We
also propose a multi-task learning method to further improve the model.
Experimental results on benchmark datasets show that our model with our
proposed training objectives significantly outperforms previous baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document Automation Architectures and Technologies: A Survey. (arXiv:2109.11603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11603">
<div class="article-summary-box-inner">
<span><p>This paper surveys the current state of the art in document automation (DA).
The objective of DA is to reduce the manual effort during the generation of
documents by automatically integrating input from different sources and
assembling documents conforming to defined templates. There have been reviews
of commercial solutions of DA, particularly in the legal domain, but to date
there has been no comprehensive review of the academic research on DA
architectures and technologies. The current survey of DA reviews the academic
literature and provides a clearer definition and characterization of DA and its
features, identifies state-of-the-art DA architectures and technologies in
academic research, and provides ideas that can lead to new research
opportunities within the DA field in light of recent advances in artificial
intelligence and deep neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration. (arXiv:2109.11621v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11621">
<div class="article-summary-box-inner">
<span><p>We introduce iFacetSum, a web application for exploring topical document
sets. iFacetSum integrates interactive summarization together with faceted
search, by providing a novel faceted navigation scheme that yields abstractive
summaries for the user's selections. This approach offers both a comprehensive
overview as well as concise details regarding subtopics of choice. Fine-grained
facets are automatically produced based on cross-document coreference
pipelines, rendering generic concepts, entities and statements surfacing in the
source texts. We analyze the effectiveness of our application through
small-scale user studies, which suggest the usefulness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting the Uniform Information Density Hypothesis. (arXiv:2109.11635v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11635">
<div class="article-summary-box-inner">
<span><p>The uniform information density (UID) hypothesis posits a preference among
language users for utterances structured such that information is distributed
uniformly across a signal. While its implications on language production have
been well explored, the hypothesis potentially makes predictions about language
comprehension and linguistic acceptability as well. Further, it is unclear how
uniformity in a linguistic signal -- or lack thereof -- should be measured, and
over which linguistic unit, e.g., the sentence or language level, this
uniformity should hold. Here we investigate these facets of the UID hypothesis
using reading time and acceptability data. While our reading time results are
generally consistent with previous work, they are also consistent with a weakly
super-linear effect of surprisal, which would be compatible with UID's
predictions. For acceptability judgments, we find clearer evidence that
non-uniformity in information density is predictive of lower acceptability. We
then explore multiple operationalizations of UID, motivated by different
interpretations of the original hypothesis, and analyze the scope over which
the pressure towards uniformity is exerted. The explanatory power of a subset
of the proposed operationalizations suggests that the strongest trend may be a
regression towards a mean surprisal across the language, rather than the
phrase, sentence, or document -- a finding that supports a typical
interpretation of UID, namely that it is the byproduct of language users
maximizing the use of a (hypothetical) communication channel.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Simple and Effective Zero-shot Cross-lingual Phoneme Recognition. (arXiv:2109.11680v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11680">
<div class="article-summary-box-inner">
<span><p>Recent progress in self-training, self-supervised pretraining and
unsupervised learning enabled well performing speech recognition systems
without any labeled data. However, in many cases there is labeled data
available for related languages which is not utilized by these methods. This
paper extends previous work on zero-shot cross-lingual transfer learning by
fine-tuning a multilingually pretrained wav2vec 2.0 model to transcribe unseen
languages. This is done by mapping phonemes of the training languages to the
target language using articulatory features. Experiments show that this simple
method significantly outperforms prior work which introduced task-specific
architectures and used only part of a monolingually pretrained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding. (arXiv:2109.11708v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11708">
<div class="article-summary-box-inner">
<span><p>Written language carries explicit and implicit biases that can distract from
meaningful signals. For example, letters of reference may describe male and
female candidates differently, or their writing style may indirectly reveal
demographic characteristics. At best, such biases distract from the meaningful
content of the text; at worst they can lead to unfair outcomes. We investigate
the challenge of re-generating input sentences to 'neutralize' sensitive
attributes while maintaining the semantic meaning of the original text (e.g. is
the candidate qualified?). We propose a gradient-based rewriting framework,
Detect and Perturb to Neutralize (DEPEN), that first detects sensitive
components and masks them for regeneration, then perturbs the generation model
at decoding time under a neutralizing constraint that pushes the (predicted)
distribution of sensitive attributes towards a uniform distribution. Our
experiments in two different scenarios show that DEPEN can regenerate fluent
alternatives that are neutral in the sensitive attribute while maintaining the
semantics of other attributes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AES Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11728">
<div class="article-summary-box-inner">
<span><p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively
used by states and language testing agencies alike to evaluate millions of
candidates for life-changing decisions ranging from college applications to
visa approvals. However, little research has been put to understand and
interpret the black-box nature of deep-learning based scoring algorithms.
Previous studies indicate that scoring models can be easily fooled. In this
paper, we explore the reason behind their surprising adversarial brittleness.
We utilize recent advances in interpretability to find the extent to which
features such as coherence, content, vocabulary, and relevance are important
for automated scoring mechanisms. We use this to investigate the
oversensitivity i.e., large change in output score with a little change in
input essay content) and overstability i.e., little change in output scores
with large changes in input essay content) of AES. Our results indicate that
autoscoring models, despite getting trained as "end-to-end" models with rich
contextual embeddings such as BERT, behave like bag-of-words models. A few
words determine the essay score without the requirement of any context making
the model largely overstable. This is in stark contrast to recent probing
studies on pre-trained representation learning models, which show that rich
linguistic features such as parts-of-speech and morphology are encoded by them.
Further, we also find that the models have learnt dataset biases, making them
oversensitive. To deal with these issues, we propose detection-based protection
models that can detect oversensitivity and overstability causing samples with
high accuracies. We find that our proposed models are able to detect unusual
attribution patterns and flag adversarial samples successfully.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference. (arXiv:2109.11745v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11745">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have shown remarkable results in
diverse NLP applications. Unfortunately, these performance gains have been
accompanied by a significant increase in computation time and model size,
stressing the need to develop new or complementary strategies to increase the
efficiency of these models. In this paper we propose DACT-BERT, a
differentiable adaptive computation time strategy for BERT-like models.
DACT-BERT adds an adaptive computational mechanism to BERT's regular processing
pipeline, which controls the number of Transformer blocks that need to be
executed at inference time. By doing this, the model learns to combine the most
appropriate intermediate representations for the task at hand. Our experiments
demonstrate that our approach, when compared to the baselines, excels on a
reduced computational regime and is competitive in other less restrictive ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lacking the embedding of a word? Look it up into a traditional dictionary. (arXiv:2109.11763v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11763">
<div class="article-summary-box-inner">
<span><p>Word embeddings are powerful dictionaries, which may easily capture language
variations. However, these dictionaries fail to give sense to rare words, which
are surprisingly often covered by traditional dictionaries. In this paper, we
propose to use definitions retrieved in traditional dictionaries to produce
word embeddings for rare words. For this purpose, we introduce two methods:
Definition Neural Network (DefiNNet) and Define BERT (DefBERT). In our
experiments, DefiNNet and DefBERT significantly outperform state-of-the-art as
well as baseline methods devised for producing embeddings of unknown words. In
fact, DefiNNet significantly outperforms FastText, which implements a method
for the same task-based on n-grams, and DefBERT significantly outperforms the
BERT method for OOV words. Then, definitions in traditional dictionaries are
useful to build word embeddings for rare words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dense Contrastive Visual-Linguistic Pretraining. (arXiv:2109.11778v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11778">
<div class="article-summary-box-inner">
<span><p>Inspired by the success of BERT, several multimodal representation learning
approaches have been proposed that jointly represent image and text. These
approaches achieve superior performance by capturing high-level semantic
information from large-scale multimodal pretraining. In particular, LXMERT and
UNITER adopt visual region feature regression and label classification as
pretext tasks. However, they tend to suffer from the problems of noisy labels
and sparse semantic annotations, based on the visual features having been
pretrained on a crowdsourced dataset with limited and inconsistent semantic
labeling. To overcome these issues, we propose unbiased Dense Contrastive
Visual-Linguistic Pretraining (DCVLP), which replaces the region regression and
classification with cross-modality region contrastive learning that requires no
annotations. Two data augmentation strategies (Mask Perturbation and
Intra-/Inter-Adversarial Perturbation) are developed to improve the quality of
negative samples used in contrastive learning. Overall, DCVLP allows
cross-modality dense region contrastive learning in a self-supervised setting
independent of any object annotations. We compare our method against prior
visual-linguistic pretraining frameworks to validate the superiority of dense
contrastive learning on multimodal representation learning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11797">
<div class="article-summary-box-inner">
<span><p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising
capabilities in grounding natural language in image data, facilitating a broad
variety of cross-modal tasks. However, we note that there exists a significant
gap between the objective forms of model pre-training and fine-tuning,
resulting in a need for quantities of labeled data to stimulate the visual
grounding capability of VL-PTMs for downstream tasks. To address the challenge,
we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt
Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual
grounding into a fill-in-the-blank problem with color-based co-referential
markers in image and text, maximally mitigating the gap. In this way, our
prompt tuning approach enables strong few-shot and even zero-shot visual
grounding capabilities of VL-PTMs. Comprehensive experimental results show that
prompt tuned VL-PTMs outperform their fine-tuned counterparts by a large margin
(e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard
deviation reduction on average with one shot in RefCOCO evaluation). All the
data and code will be available to facilitate future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View. (arXiv:2109.11800v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11800">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph Embedding (KGE) aims to learn representations for entities
and relations. Most KGE models have gained great success, especially on
extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a
trained model can still correctly predict t from (h, r, ?), or h from (?, r,
t), such extrapolation ability is impressive. However, most existing KGE works
focus on the design of delicate triple modeling function, which mainly tell us
how to measure the plausibility of observed triples, but we have limited
understanding of why the methods can extrapolate to unseen data, and what are
the important factors to help KGE extrapolate. Therefore in this work, we
attempt to, from a data relevant view, study KGE extrapolation of two problems:
1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with
better extrapolation ability? For the problem 1, we first discuss the impact
factors for extrapolation and from relation, entity and triple level
respectively, propose three Semantic Evidences (SEs), which can be observed
from training set and provide important semantic information for extrapolation
to unseen data. Then we verify the effectiveness of SEs through extensive
experiments on several typical KGE methods, and demonstrate that SEs serve as
an important role for understanding the extrapolation ability of KGE. For the
problem 2, to make better use of the SE information for more extrapolative
knowledge representation, we propose a novel GNN-based KGE model, called
Semantic Evidence aware Graph Neural Network (SE-GNN). Finally, through
extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN
achieves state-of-the-art performance on Knowledge Graph Completion task and
perform a better extrapolation ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Diversity-Enhanced and Constraints-Relaxed Augmentation for Low-Resource Classification. (arXiv:2109.11834v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11834">
<div class="article-summary-box-inner">
<span><p>Data augmentation (DA) aims to generate constrained and diversified data to
improve classifiers in Low-Resource Classification (LRC). Previous studies
mostly use a fine-tuned Language Model (LM) to strengthen the constraints but
ignore the fact that the potential of diversity could improve the effectiveness
of generated data. In LRC, strong constraints but weak diversity in DA result
in the poor generalization ability of classifiers. To address this dilemma, we
propose a {D}iversity-{E}nhanced and {C}onstraints-\{R}elaxed {A}ugmentation
(DECRA). Our DECRA has two essential components on top of a transformer-based
backbone model. 1) A k-beta augmentation, an essential component of DECRA, is
proposed to enhance the diversity in generating constrained data. It expands
the changing scope and improves the degree of complexity of the generated data.
2) A masked language model loss, instead of fine-tuning, is used as a
regularization. It relaxes constraints so that the classifier can be trained
with more scattered generated data. The combination of these two components
generates data that can reach or approach category boundaries and hence help
the classifier generalize better. We evaluate our DECRA on three public
benchmark datasets under low-resource settings. Extensive experiments
demonstrate that our DECRA outperforms state-of-the-art approaches by 3.8% in
the overall score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robustness and Sensitivity of BERT Models Predicting Alzheimer's Disease from Text. (arXiv:2109.11888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11888">
<div class="article-summary-box-inner">
<span><p>Understanding robustness and sensitivity of BERT models predicting
Alzheimer's disease from text is important for both developing better
classification models and for understanding their capabilities and limitations.
In this paper, we analyze how a controlled amount of desired and undesired text
alterations impacts performance of BERT. We show that BERT is robust to natural
linguistic variations in text. On the other hand, we show that BERT is not
sensitive to removing clinically important information from text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Crowd Sourcing for Semantic Similarity. (arXiv:2109.11969v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11969">
<div class="article-summary-box-inner">
<span><p>Estimation of semantic similarity is crucial for a variety of natural
language processing (NLP) tasks. In the absence of a general theory of semantic
information, many papers rely on human annotators as the source of ground truth
for semantic similarity estimation. This paper investigates the ambiguities
inherent in crowd-sourced semantic labeling. It shows that annotators that
treat semantic similarity as a binary category (two sentences are either
similar or not similar and there is no middle ground) play the most important
role in the labeling. The paper offers heuristics to filter out unreliable
annotators and stimulates further discussions on human perception of semantic
similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction. (arXiv:2109.12008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12008">
<div class="article-summary-box-inner">
<span><p>State-of-the-art NLP models can adopt shallow heuristics that limit their
generalization capability (McCoy et al., 2019). Such heuristics include lexical
overlap with the training set in Named-Entity Recognition (Taill\'e et al.,
2020) and Event or Type heuristics in Relation Extraction (Rosenman et al.,
2020). In the more realistic end-to-end RE setting, we can expect yet another
heuristic: the mere retention of training relation triples. In this paper, we
propose several experiments confirming that retention of known facts is a key
factor of performance on standard benchmarks. Furthermore, one experiment
suggests that a pipeline model able to use intermediate type representations is
less prone to over-rely on retention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Translation of German--Lower Sorbian: Exploring Training and Novel Transfer Methods on a Low-Resource Language. (arXiv:2109.12012v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12012">
<div class="article-summary-box-inner">
<span><p>This paper describes the methods behind the systems submitted by the
University of Groningen for the WMT 2021 Unsupervised Machine Translation task
for German--Lower Sorbian (DE--DSB): a high-resource language to a low-resource
one. Our system uses a transformer encoder-decoder architecture in which we
make three changes to the standard training procedure. First, our training
focuses on two languages at a time, contrasting with a wealth of research on
multilingual systems. Second, we introduce a novel method for initializing the
vocabulary of an unseen language, achieving improvements of 3.2 BLEU for
DE$\rightarrow$DSB and 4.0 BLEU for DSB$\rightarrow$DE. Lastly, we experiment
with the order in which offline and online back-translation are used to train
an unsupervised system, finding that using online back-translation first works
better for DE$\rightarrow$DSB by 2.76 BLEU. Our submissions ranked first (tied
with another team) for DSB$\rightarrow$DE and third for DE$\rightarrow$DSB.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indirectly Supervised English Sentence Break Prediction Using Paragraph Break Probability Estimates. (arXiv:2109.12023v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12023">
<div class="article-summary-box-inner">
<span><p>This report explores the use of paragraph break probability estimates to help
predict the location of sentence breaks in English natural language text. We
show that a sentence break predictor based almost solely on paragraph break
probability estimates can achieve high accuracy on this task. This sentence
break predictor is trained almost entirely on a large amount of naturally
occurring text without sentence break annotations, with only a small amount of
annotated data needed to tune two hyperparameters. We also show that even
better results can be achieved across in-domain and out-of-domain test data, if
paragraph break probability signals are combined with a support vector machine
classifier trained on a somewhat larger amount of sentence-break-annotated
data. Numerous related issues are addressed along the way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Post-pretraining Representation Alignment for Cross-Lingual Question Answering. (arXiv:2109.12028v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12028">
<div class="article-summary-box-inner">
<span><p>Human knowledge is collectively encoded in the roughly 6500 languages spoken
around the world, but it is not distributed equally across languages. Hence,
for information-seeking question answering (QA) systems to adequately serve
speakers of all languages, they need to operate cross-lingually. In this work
we investigate the capabilities of multilingually pre-trained language models
on cross-lingual QA. We find that explicitly aligning the representations
across languages with a post-hoc fine-tuning step generally leads to improved
performance. We additionally investigate the effect of data size as well as the
language choice in this fine-tuning step, also releasing a dataset for
evaluating cross-lingual QA systems. Code and dataset are publicly available
here: https://github.com/ffaisal93/aligned_qa
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers Generalize Linearly. (arXiv:2109.12036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12036">
<div class="article-summary-box-inner">
<span><p>Natural language exhibits patterns of hierarchically governed dependencies,
in which relations between words are sensitive to syntactic structure rather
than linear ordering. While re-current network models often fail to generalize
in a hierarchically sensitive way (McCoy et al.,2020) when trained on ambiguous
data, the improvement in performance of newer Trans-former language models
(Vaswani et al., 2017)on a range of syntactic benchmarks trained on large data
sets (Goldberg, 2019; Warstadtet al., 2019) opens the question of whether these
models might exhibit hierarchical generalization in the face of impoverished
data.In this paper we examine patterns of structural generalization for
Transformer sequence-to-sequence models and find that not only do Transformers
fail to generalize hierarchically across a wide variety of grammatical mapping
tasks, but they exhibit an even stronger preference for linear generalization
than comparable recurrent networks
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual and Cross-Lingual Acceptability Judgments with the Italian CoLA corpus. (arXiv:2109.12053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12053">
<div class="article-summary-box-inner">
<span><p>The development of automated approaches to linguistic acceptability has been
greatly fostered by the availability of the English CoLA corpus, which has also
been included in the widely used GLUE benchmark. However, this kind of research
for languages other than English, as well as the analysis of cross-lingual
approaches, has been hindered by the lack of resources with a comparable size
in other languages. We have therefore developed the ItaCoLA corpus, containing
almost 10,000 sentences with acceptability judgments, which has been created
following the same approach and the same steps as the English one. In this
paper we describe the corpus creation, we detail its content, and we present
the first experiments on this new resource. We compare in-domain and
out-of-domain classification, and perform a specific evaluation of nine
linguistic phenomena. We also present the first cross-lingual experiments,
aimed at assessing whether multilingual transformerbased approaches can benefit
from using sentences in two languages during fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. (arXiv:2109.12068v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12068">
<div class="article-summary-box-inner">
<span><p>Transfer learning with a unified Transformer framework (T5) that converts all
language problems into a text-to-text format has recently been proposed as a
simple, yet effective, transfer learning approach. Although a multilingual
version of the T5 model (mT5) has been introduced, it is not clear how well it
can fare on non-English tasks involving diverse data. To investigate this
question, we apply mT5 on a language with a wide variety of dialects--Arabic.
For evaluation, we use an existing benchmark for Arabic language understanding
and introduce a new benchmark for Arabic language generation (ARGEN). We also
pre-train three powerful Arabic-specific text-to-text Transformer based models
and evaluate them on the two benchmarks. Our new models perform significantly
better than mT5 and exceed MARBERT, the current state-of-the-art Arabic
BERT-based model, on Arabic language understanding. The models also set new
SOTA on the generation benchmark. Our new models and are publicly released at
https://github.com/UBC-NLP/araT5 and ARLGE will be released through the same
repository.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SD-QA: Spoken Dialectal Question Answering for the Real World. (arXiv:2109.12072v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12072">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) systems are now available through numerous commercial
applications for a wide variety of domains, serving millions of users that
interact with them via speech interfaces. However, current benchmarks in QA
research do not account for the errors that speech recognition models might
introduce, nor do they consider the language variations (dialects) of the
users. To address this gap, we augment an existing QA dataset to construct a
multi-dialect, spoken QA benchmark on five languages (Arabic, Bengali, English,
Kiswahili, Korean) with more than 68k audio prompts in 24 dialects from 255
speakers. We provide baseline results showcasing the real-world performance of
QA systems and analyze the effect of language variety and other sensitive
speaker attributes on downstream performance. Last, we study the fairness of
the ASR and QA models with respect to the underlying user populations. The
dataset, model outputs, and code for reproducing all our experiments are
available: https://github.com/ffaisal93/SD-QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Adversarial Learning for Bootstrapping: A Case Study on Entity Set Expansion. (arXiv:2109.12082v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12082">
<div class="article-summary-box-inner">
<span><p>Bootstrapping has become the mainstream method for entity set expansion.
Conventional bootstrapping methods mostly define the expansion boundary using
seed-based distance metrics, which heavily depend on the quality of selected
seeds and are hard to be adjusted due to the extremely sparse supervision. In
this paper, we propose BootstrapGAN, a new learning method for bootstrapping
which jointly models the bootstrapping process and the boundary learning
process in a GAN framework. Specifically, the expansion boundaries of different
bootstrapping iterations are learned via different discriminator networks; the
bootstrapping network is the generator to generate new positive entities, and
the discriminator networks identify the expansion boundaries by trying to
distinguish the generated entities from known positive entities. By iteratively
performing the above adversarial learning, the generator and the discriminators
can reinforce each other and be progressively refined along the whole
bootstrapping process. Experiments show that BootstrapGAN achieves the new
state-of-the-art entity set expansion performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-based NP Enrichment. (arXiv:2109.12085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12085">
<div class="article-summary-box-inner">
<span><p>Understanding the relations between entities denoted by NPs in text is a
critical part of human-like natural language understanding. However, only a
fraction of such relations is covered by NLP tasks and models nowadays. In this
work, we establish the task of text-based NP enrichment (TNE), that is,
enriching each NP with all the preposition-mediated relations that hold between
this and the other NPs in the text. The relations are represented as triplets,
each denoting two NPs linked via a preposition. Humans recover such relations
seamlessly, while current state-of-the-art models struggle with them due to the
implicit nature of the problem. We build the first large-scale dataset for the
problem, provide the formal framing and scope of annotation, analyze the data,
and report the result of fine-tuned neural language models on the task,
demonstrating the challenge it poses to current technology. We created a
webpage with the data, data-exploration UI, code, models, and demo to foster
further research into this challenging text understanding problem at
yanaiela.github.io/TNE/.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction. (arXiv:2109.12093v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12093">
<div class="article-summary-box-inner">
<span><p>Stepping from sentence-level to document-level relation extraction, the
research community confronts increasing text length and more complicated entity
interactions. Consequently, it is more challenging to encode the key sources of
information--relevant contexts and entity types. However, existing methods only
implicitly learn to model these critical information sources while being
trained for relation extraction. As a result, they suffer the problems of
ineffective supervision and uninterpretable model predictions. In contrast, we
propose to explicitly teach the model to capture relevant contexts and entity
types by supervising and augmenting intermediate steps (SAIS) for relation
extraction. Based on a broad spectrum of carefully designed tasks, our proposed
SAIS method not only extracts relations of better quality due to more effective
supervision, but also retrieves the corresponding supporting evidence more
accurately so as to enhance interpretability. By assessing model uncertainty,
SAIS further boosts the performance via evidence-based data augmentation and
ensemble inference while reducing the computational cost. Eventually, SAIS
delivers state-of-the-art relation extraction results on three benchmarks
(DocRED, CDR, and GDA) and achieves 5.04% relative gains in F1 score compared
to the runner-up in evidence retrieval on DocRED.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPort: What and Where Pathways for Robotic Manipulation. (arXiv:2109.12098v1 [cs.RO])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12098">
<div class="article-summary-box-inner">
<span><p>How can we imbue robots with the ability to manipulate objects precisely but
also to reason about them in terms of abstract concepts? Recent works in
manipulation have shown that end-to-end networks can learn dexterous skills
that require precise spatial reasoning, but these methods often fail to
generalize to new goals or quickly learn transferable concepts across tasks. In
parallel, there has been great progress in learning generalizable semantic
representations for vision and language by training on large-scale internet
data, however these representations lack the spatial understanding necessary
for fine-grained manipulation. To this end, we propose a framework that
combines the best of both worlds: a two-stream architecture with semantic and
spatial pathways for vision-based manipulation. Specifically, we present
CLIPort, a language-conditioned imitation-learning agent that combines the
broad semantic understanding (what) of CLIP [1] with the spatial precision
(where) of Transporter [2]. Our end-to-end framework is capable of solving a
variety of language-specified tabletop tasks from packing unseen objects to
folding cloths, all without any explicit representations of object poses,
instance segmentations, memory, symbolic states, or syntactic structures.
Experiments in simulated and real-world settings show that our approach is data
efficient in few-shot settings and generalizes effectively to seen and unseen
semantic concepts. We even learn one multi-task policy for 10 simulated and 9
real-world tasks that is better or comparable to single-task policies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GERNERMED -- An Open German Medical NER Model. (arXiv:2109.12104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12104">
<div class="article-summary-box-inner">
<span><p>The current state of adoption of well-structured electronic health records
and integration of digital methods for storing medical patient data in
structured formats can often considered as inferior compared to the use of
traditional, unstructured text based patient data documentation. Data mining in
the field of medical data analysis often needs to rely solely on processing of
unstructured data to retrieve relevant data. In natural language processing
(NLP), statistical models have been shown successful in various tasks like
part-of-speech tagging, relation extraction (RE) and named entity recognition
(NER). In this work, we present GERNERMED, the first open, neural NLP model for
NER tasks dedicated to detect medical entity types in German text data. Here,
we avoid the conflicting goals of protection of sensitive patient data from
training data extraction and the publication of the statistical model weights
by training our model on a custom dataset that was translated from publicly
available datasets in foreign language by a pretrained neural machine
translation model. The sample code and the statistical model is available at:
https://github.com/frankkramer-lab/GERNERMED
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful Target Attribute Prediction in Neural Machine Translation. (arXiv:2109.12105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12105">
<div class="article-summary-box-inner">
<span><p>The training data used in NMT is rarely controlled with respect to specific
attributes, such as word casing or gender, which can cause errors in
translations. We argue that predicting the target word and attributes
simultaneously is an effective way to ensure that translations are more
faithful to the training data distribution with respect to these attributes.
Experimental results on two tasks, uppercased input translation and gender
prediction, show that this strategy helps mirror the training data distribution
in testing. It also facilitates data augmentation on the task of uppercased
input translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Source Code Search: A Study of the Past and a Glimpse at the Future. (arXiv:1908.06738v2 [cs.SE] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.06738">
<div class="article-summary-box-inner">
<span><p>With the recent explosion in the size and complexity of source codebases and
software projects, the need for efficient source code search engines has
increased dramatically. Unfortunately, existing information retrieval-based
methods fail to capture the query semantics and perform well only when the
query contains syntax-based keywords. Consequently, such methods will perform
poorly when given high-level natural language queries. In this paper, we review
existing methods for building code search engines. We also outline the open
research directions and the various obstacles that stand in the way of having a
universal source code search engine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SOLID: A Large-Scale Semi-Supervised Dataset for Offensive Language Identification. (arXiv:2004.14454v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14454">
<div class="article-summary-box-inner">
<span><p>The widespread use of offensive content in social media has led to an
abundance of research in detecting language such as hate speech, cyberbullying,
and cyber-aggression. Recent work presented the OLID dataset, which follows a
taxonomy for offensive language identification that provides meaningful
information for understanding the type and the target of offensive messages.
However, it is limited in size and it might be biased towards offensive
language as it was collected using keywords. In this work, we present SOLID, an
expanded dataset, where the tweets were collected in a more principled manner.
SOLID contains over nine million English tweets labeled in a semi-supervised
fashion. We demonstrate that using SOLID along with OLID yields sizable
performance gains on the OLID test set for two different models, especially for
the lower levels of the taxonomy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ValNorm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries. (arXiv:2006.03950v4 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03950">
<div class="article-summary-box-inner">
<span><p>Word embeddings learn implicit biases from linguistic regularities captured
by word co-occurrence statistics. By extending methods that quantify human-like
biases in word embeddings, we introduceValNorm, a novel intrinsic evaluation
task and method to quantify the valence dimension of affect in human-rated word
sets from social psychology. We apply ValNorm on static word embeddings from
seven languages (Chinese, English, German, Polish, Portuguese, Spanish, and
Turkish) and from historical English text spanning 200 years. ValNorm achieves
consistently high accuracy in quantifying the valence of non-discriminatory,
non-social group word sets. Specifically, ValNorm achieves a Pearson
correlation of r=0.88 for human judgment scores of valence for 399 words
collected to establish pleasantness norms in English. In contrast, we measure
gender stereotypes using the same set of word embeddings and find that social
biases vary across languages. Our results indicate that valence associations of
non-discriminatory, non-social group words represent widely-shared
associations, in seven languages and over 200 years.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Calling Out Bluff: Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring Systems. (arXiv:2007.06796v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.06796">
<div class="article-summary-box-inner">
<span><p>Automatic scoring engines have been used for scoring approximately fifteen
million test-takers in just the last three years. This number is increasing
further due to COVID-19 and the associated automation of education and testing.
Despite such wide usage, the AI-based testing literature of these "intelligent"
models is highly lacking. Most of the papers proposing new models rely only on
quadratic weighted kappa (QWK) based agreement with human raters for showing
model efficacy. However, this effectively ignores the highly multi-feature
nature of essay scoring. Essay scoring depends on features like coherence,
grammar, relevance, sufficiency and, vocabulary. To date, there has been no
study testing Automated Essay Scoring: AES systems holistically on all these
features. With this motivation, we propose a model agnostic adversarial
evaluation scheme and associated metrics for AES systems to test their natural
language understanding capabilities and overall robustness. We evaluate the
current state-of-the-art AES models using the proposed scheme and report the
results on five recent models. These models range from
feature-engineering-based approaches to the latest deep learning algorithms. We
find that AES models are highly overstable. Even heavy modifications(as much as
25%) with content unrelated to the topic of the questions do not decrease the
score produced by the models. On the other hand, irrelevant content, on
average, increases the scores, thus showing that the model evaluation strategy
and rubrics should be reconsidered. We also ask 200 human raters to score both
an original and adversarial response to seeing if humans can detect differences
between the two and whether they agree with the scores assigned by auto scores.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenAttack: An Open-source Textual Adversarial Attack Toolkit. (arXiv:2009.09191v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09191">
<div class="article-summary-box-inner">
<span><p>Textual adversarial attacking has received wide and increasing attention in
recent years. Various attack models have been proposed, which are enormously
distinct and implemented with different programming frameworks and settings.
These facts hinder quick utilization and fair comparison of attack models. In
this paper, we present an open-source textual adversarial attack toolkit named
OpenAttack to solve these issues. Compared with existing other textual
adversarial attack toolkits, OpenAttack has its unique strengths in support for
all attack types, multilinguality, and parallel processing. Currently,
OpenAttack includes 15 typical attack models that cover all attack types. Its
highly inclusive modular design not only supports quick utilization of existing
attack models, but also enables great flexibility and extensibility. OpenAttack
has broad uses including comparing and evaluating attack models, measuring
robustness of a model, assisting in developing new attack models, and
adversarial training. Source code and documentation can be obtained at
https://github.com/thunlp/OpenAttack.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Char2Subword: Extending the Subword Embedding Space Using Robust Character Compositionality. (arXiv:2010.12730v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12730">
<div class="article-summary-box-inner">
<span><p>Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword
tokenization process of language models as it provides multiple benefits.
However, this process is solely based on pre-training data statistics, making
it hard for the tokenizer to handle infrequent spellings. On the other hand,
though robust to misspellings, pure character-level models often lead to
unreasonably long sequences and make it harder for the model to learn
meaningful words. To alleviate these challenges, we propose a character-based
subword module (char2subword) that learns the subword embedding table in
pre-trained models like BERT. Our char2subword module builds representations
from characters out of the subword vocabulary, and it can be used as a drop-in
replacement of the subword embedding table. The module is robust to
character-level alterations such as misspellings, word inflection, casing, and
punctuation. We integrate it further with BERT through pre-training while
keeping BERT transformer parameters fixed--and thus, providing a practical
method. Finally, we show that incorporating our module to mBERT significantly
improves the performance on the social media linguistic code-switching
evaluation (LinCE) benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting. (arXiv:2101.00416v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00416">
<div class="article-summary-box-inner">
<span><p>In this paper, we generalize text infilling (e.g., masked language models) by
proposing Sequence Span Rewriting (SSR) as a self-supervised
sequence-to-sequence (seq2seq) pre-training objective. SSR provides more
fine-grained learning signals for text representations by supervising the model
to rewrite imperfect spans to ground truth, and it is more consistent than text
infilling with many downstream seq2seq tasks that rewrite a source sentences
into a target sentence. Our experiments with T5 models on various seq2seq tasks
show that SSR can substantially improve seq2seq pre-training. Moreover, we
observe SSR is especially helpful to improve pre-training a small-size seq2seq
model with a powerful imperfect span generator, which indicates a new
perspective of transferring knowledge from a large model to a smaller model for
seq2seq pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advances and Challenges in Conversational Recommender Systems: A Survey. (arXiv:2101.09459v7 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09459">
<div class="article-summary-box-inner">
<span><p>Recommender systems exploit interaction history to estimate user preference,
having been heavily used in a wide range of industry applications. However,
static recommendation models are difficult to answer two important questions
well due to inherent shortcomings: (a) What exactly does a user like? (b) Why
does a user like an item? The shortcomings are due to the way that static
models learn user preference, i.e., without explicit instructions and active
feedback from users. The recent rise of conversational recommender systems
(CRSs) changes this situation fundamentally. In a CRS, users and the system can
dynamically communicate through natural language interactions, which provide
unprecedented opportunities to explicitly obtain the exact preference of users.
</p>
<p>Considerable efforts, spread across disparate settings and applications, have
been put into developing CRSs. Existing models, technologies, and evaluation
methods for CRSs are far from mature. In this paper, we provide a systematic
review of the techniques used in current CRSs. We summarize the key challenges
of developing CRSs in five directions: (1) Question-based user preference
elicitation. (2) Multi-turn conversational recommendation strategies. (3)
Dialogue understanding and generation. (4) Exploitation-exploration trade-offs.
(5) Evaluation and user simulation. These research directions involve multiple
research fields like information retrieval (IR), natural language processing
(NLP), and human-computer interaction (HCI). Based on these research
directions, we discuss some future challenges and opportunities. We provide a
road map for researchers from multiple communities to get started in this area.
We hope this survey can help to identify and address challenges in CRSs and
inspire future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Statistically significant detection of semantic shifts using contextual word embeddings. (arXiv:2104.03776v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03776">
<div class="article-summary-box-inner">
<span><p>Detecting lexical semantic change in smaller data sets, e.g. in historical
linguistics and digital humanities, is challenging due to a lack of statistical
power. This issue is exacerbated by non-contextual embedding models that
produce one embedding per word and, therefore, mask the variability present in
the data. In this article, we propose an approach to estimate semantic shift by
combining contextual word embeddings with permutation-based statistical tests.
We use the false discovery rate procedure to address the large number of
hypothesis tests being conducted simultaneously. We demonstrate the performance
of this approach in simulation where it achieves consistently high precision by
suppressing false positives. We additionally analyze real-world data from
SemEval-2020 Task 1 and the Liverpool FC subreddit corpus. We show that by
taking sample variation into account, we can improve the robustness of
individual semantic shift estimates without degrading overall performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer. (arXiv:2106.01732v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01732">
<div class="article-summary-box-inner">
<span><p>Multilingual pre-trained models have achieved remarkable performance on
cross-lingual transfer learning. Some multilingual models such as mBERT, have
been pre-trained on unlabeled corpora, therefore the embeddings of different
languages in the models may not be aligned very well. In this paper, we aim to
improve the zero-shot cross-lingual transfer performance by proposing a
pre-training task named Word-Exchange Aligning Model (WEAM), which uses the
statistical alignment information as the prior knowledge to guide cross-lingual
word prediction. We evaluate our model on multilingual machine reading
comprehension task MLQA and natural language interface task XNLI. The results
show that WEAM can significantly improve the zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Token Pruning for Transformers. (arXiv:2107.00910v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00910">
<div class="article-summary-box-inner">
<span><p>Deploying transformer models in practice is challenging due to their
inference cost, which scales quadratically with input sequence length. To
address this, we present a novel Learned Token Pruning (LTP) method which
adaptively removes unimportant tokens as an input sequence passes through
transformer layers. In particular, LTP prunes tokens with an attention score
below a threshold value which is learned for each layer during training. Our
threshold-based method allows the length of the pruned sequence to vary
adaptively based on the input sequence, and avoids algorithmically expensive
operations such as top-k token selection. We extensively test the performance
of LTP on GLUE tasks and show that our method outperforms the prior
state-of-the-art token pruning methods by up to ~2.5% higher accuracy with the
same amount of FLOPs. In particular, LTP achieves up to 2.1x FLOPs reduction
with less than 1% accuracy drop, which results in up to 1.9x and 2.0x
throughput improvement on Intel Haswell CPUs and NVIDIA V100 GPUs,
respectively. Furthermore, we demonstrate that LTP is more robust than prior
methods to variations on input sentence lengths. Our code has been developed in
PyTorch and has been open-sourced.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agent. (arXiv:2107.05541v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05541">
<div class="article-summary-box-inner">
<span><p>Chatbots are intelligent software built to be used as a replacement for human
interaction. Existing studies typically do not provide enough support for
low-resource languages like Bangla. Due to the increasing popularity of social
media, we can also see the rise of interactions in Bangla transliteration
(mostly in English) among the native Bangla speakers. In this paper, we propose
a novel approach to build a Bangla chatbot aimed to be used as a business
assistant which can communicate in Bangla and Bangla Transliteration in English
with high confidence consistently. Since annotated data was not available for
this purpose, we had to work on the whole machine learning life cycle (data
preparation, machine learning modeling, and model deployment) using Rasa Open
Source Framework, fastText embeddings, Polyglot embeddings, Flask, and other
systems as building blocks. While working with the skewed annotated dataset, we
try out different setups and pipelines to evaluate which works best and provide
possible reasoning behind the observed results. Finally, we present a pipeline
for intent classification and entity extraction which achieves reasonable
performance (accuracy: 83.02%, precision: 80.82%, recall: 83.02%, F1-score:
80%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary Explorer: Visualizing the State of the Art in Text Summarization. (arXiv:2108.01879v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01879">
<div class="article-summary-box-inner">
<span><p>This paper introduces Summary Explorer, a new tool to support the manual
inspection of text summarization systems by compiling the outputs of
55~state-of-the-art single document summarization approaches on three benchmark
datasets, and visually exploring them during a qualitative assessment. The
underlying design of the tool considers three well-known summary quality
criteria (coverage, faithfulness, and position bias), encapsulated in a guided
assessment based on tailored visualizations. The tool complements existing
approaches for locally debugging summarization models and improves upon them.
The tool is available at https://tldr.webis.de/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03481">
<div class="article-summary-box-inner">
<span><p>Contrastive learning models have achieved great success in unsupervised
visual representation learning, which maximize the similarities between feature
representations of different views of the same image, while minimize the
similarities between feature representations of views of different images. In
text summarization, the output summary is a shorter form of the input document
and they have similar meanings. In this paper, we propose a contrastive
learning model for supervised abstractive text summarization, where we view a
document, its gold summary and its model generated summaries as different views
of the same mean representation and maximize the similarities between them
during training. We improve over a strong sequence-to-sequence text generation
model (i.e., BART) on three different summarization datasets. Human evaluation
also shows that our model achieves better faithfulness ratings compared to its
counterpart without contrastive objectives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03777">
<div class="article-summary-box-inner">
<span><p>Graph neural networks have triggered a resurgence of graph-based text
classification. We show that already a simple MLP baseline achieves comparable
performance on benchmark datasets, questioning the importance of synthetic
graph structures. When considering an inductive scenario, i. e., when adding
new documents to a corpus, a simple MLP even outperforms the recent graph-based
models TextGCN and HeteGCN and is comparable with HyperGAT. We further
fine-tune DistilBERT and find that it outperforms all state-of-the-art models.
We suggest that future studies use at least an MLP baseline to contextualize
the results. We provide recommendations for the design and training of such a
baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations. (arXiv:2109.10080v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10080">
<div class="article-summary-box-inner">
<span><p>Adverse Drug Event (ADE) extraction models can rapidly examine large
collections of social media texts, detecting mentions of drug-related adverse
reactions and trigger medical investigations. However, despite the recent
advances in NLP, it is currently unknown if such models are robust in face of
negation, which is pervasive across language varieties.
</p>
<p>In this paper we evaluate three state-of-the-art systems, showing their
fragility against negation, and then we introduce two possible strategies to
increase the robustness of these models: a pipeline approach, relying on a
specific component for negation detection; an augmentation of an ADE extraction
dataset to artificially create negated samples and further train the models.
</p>
<p>We show that both strategies bring significant increases in performance,
lowering the number of spurious entities predicted by the models. Our dataset
and code will be publicly released to encourage research on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Diarisation using location tracking with agglomerative clustering. (arXiv:2109.10598v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10598">
<div class="article-summary-box-inner">
<span><p>Previous works have shown that spatial location information can be
complementary to speaker embeddings for a speaker diarisation task. However,
the models used often assume that speakers are fairly stationary throughout a
meeting. This paper proposes to relax this assumption, by explicitly modelling
the movements of speakers within an Agglomerative Hierarchical Clustering (AHC)
diarisation framework. Kalman filters, which track the locations of speakers,
are used to compute log-likelihood ratios that contribute to the cluster
affinity computations for the AHC merging and stopping decisions. Experiments
show that the proposed approach is able to yield improvements on a Microsoft
rich meeting transcription task, compared to methods that do not use location
information or that make stationarity assumptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT. (arXiv:2109.11010v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11010">
<div class="article-summary-box-inner">
<span><p>Alzheimers disease is a fatal progressive brain disorder that worsens with
time. It is high time we have inexpensive and quick clinical diagnostic
techniques for early detection and care. In previous studies, various Machine
Learning techniques and Pre-trained Deep Learning models have been used in
conjunction with the extraction of various acoustic and linguistic features.
Our study focuses on three models for the classification task in the ADReSS
(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021
Challenge. We use the well-balanced dataset provided by the ADReSS Challenge
for training and validating our models. Model 1 uses various acoustic features
from the eGeMAPs feature-set, Model 2 uses various linguistic features that we
generated from auto-generated transcripts and Model 3 uses the auto-generated
transcripts directly to extract features using a Pre-trained BERT and TF-IDF.
These models are described in detail in the models section.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21. (arXiv:2109.11247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11247">
<div class="article-summary-box-inner">
<span><p>This paper describes the Volctrans' submission to the WMT21 news translation
shared task for German-&gt;English translation. We build a parallel (i.e.,
non-autoregressive) translation system using the Glancing Transformer, which
enables fast and accurate parallel decoding in contrast to the currently
prevailing autoregressive models. To the best of our knowledge, this is the
first parallel translation system that can be scaled to such a practical
scenario like WMT competition. More importantly, our parallel translation
system achieves the best BLEU score (35.0) on German-&gt;English translation task,
outperforming all strong autoregressive counterparts.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-27 23:09:02.772837099 UTC">2021-09-27 23:09:02 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>