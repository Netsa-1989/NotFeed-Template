<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-06T01:30:00Z">09-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Generalization in Open Domain Question Answering. (arXiv:2109.01156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01156">
<div class="article-summary-box-inner">
<span><p>Recent work on Open Domain Question Answering has shown that there is a large
discrepancy in model performance between novel test questions and those that
largely overlap with training questions. However, it is as of yet unclear which
aspects of novel questions that make them challenging. Drawing upon studies on
systematic generalization, we introduce and annotate questions according to
three categories that measure different levels and kinds of generalization:
training set overlap, compositional generalization (comp-gen), and novel entity
generalization (novel-entity). When evaluating six popular parametric and
non-parametric models, we find that for the established Natural Questions and
TriviaQA datasets, even the strongest model performance for
comp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the
full test set -- indicating the challenge posed by these types of questions.
Furthermore, we show that whilst non-parametric models can handle questions
containing novel entities, they struggle with those requiring compositional
generalization. Through thorough analysis we find that key question difficulty
factors are: cascading errors from the retrieval component, frequency of
question pattern, and frequency of the entity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Conformer: Progressive Downsampling and Grouped Attention for Automatic Speech Recognition. (arXiv:2109.01163v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01163">
<div class="article-summary-box-inner">
<span><p>The recently proposed Conformer architecture has shown state-of-the-art
performances in Automatic Speech Recognition by combining convolution with
attention to model both local and global dependencies. In this paper, we study
how to reduce the Conformer architecture complexity with a limited computing
budget, leading to a more efficient architecture design that we call Efficient
Conformer. We introduce progressive downsampling to the Conformer encoder and
propose a novel attention mechanism named grouped attention, allowing us to
reduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence
length $n$, hidden dimension $d$ and group size parameter $g$. We also
experiment the use of strided multi-head self-attention as a global
downsampling operation. Our experiments are performed on the LibriSpeech
dataset with CTC and RNN-Transducer losses. We show that within the same
computing budget, the proposed architecture achieves better performances with
faster training and decoding compared to the Conformer. Our 13M parameters CTC
model achieves competitive WERs of 3.6\%/9.0\% without using a language model
and 2.7\%/6.7\% with an external n-gram language model on the
test-clean/test-other sets while being 29\% faster than our CTC Conformer
baseline at inference and 36\% faster to train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ranking Scientific Papers Using Preference Learning. (arXiv:2109.01190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01190">
<div class="article-summary-box-inner">
<span><p>Peer review is the main quality control mechanism in academia. Quality of
scientific work has many dimensions; coupled with the subjective nature of the
reviewing task, this makes final decision making based on the reviews and
scores therein very difficult and time-consuming. To assist with this important
task, we cast it as a paper ranking problem based on peer review texts and
reviewer scores. We introduce a novel, multi-faceted generic evaluation
framework for making final decisions based on peer reviews that takes into
account effectiveness, efficiency and fairness of the evaluated system. We
propose a novel approach to paper ranking based on Gaussian Process Preference
Learning (GPPL) and evaluate it on peer review data from the ACL-2018
conference. Our experiments demonstrate the superiority of our GPPL-based
approach over prior work, while highlighting the importance of using both texts
and review scores for paper ranking during peer review aggregation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Establishing Interlingua in Multilingual Language Models. (arXiv:2109.01207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01207">
<div class="article-summary-box-inner">
<span><p>Large multilingual language models show remarkable zero-shot cross-lingual
transfer performance on a range of tasks. Follow-up works hypothesized that
these models internally project representations of different languages into a
shared interlingual space. However, they produced contradictory results. In
this paper, we correct %one of the previous works the famous prior work
claiming that "BERT is not an Interlingua" and show that with the proper choice
of sentence representation different languages actually do converge to a shared
space in such language models. Furthermore, we demonstrate that this
convergence pattern is robust across four measures of correlation similarity
and six mBERT-like models. We then extend our analysis to 28 diverse languages
and find that the interlingual space exhibits a particular structure similar to
the linguistic relatedness of languages. We also highlight a few outlier
languages that seem to fail to converge to the shared space. The code for
replicating our results is available at the following URL:
https://github.com/maksym-del/interlingua.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Reproducibility in NLP and ML. (arXiv:2109.01211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01211">
<div class="article-summary-box-inner">
<span><p>Reproducibility has become an intensely debated topic in NLP and ML over
recent years, but no commonly accepted way of assessing reproducibility, let
alone quantifying it, has so far emerged. The assumption has been that wider
scientific reproducibility terminology and definitions are not applicable to
NLP/ML, with the result that many different terms and definitions have been
proposed, some diametrically opposed. In this paper, we test this assumption,
by taking the standard terminology and definitions from metrology and applying
them directly to NLP/ML. We find that we are able to straightforwardly derive a
practical framework for assessing reproducibility which has the desirable
property of yielding a quantified degree of reproducibility that is comparable
across different reproduction studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements. (arXiv:2109.01226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01226">
<div class="article-summary-box-inner">
<span><p>More predictable words are easier to process - they are read faster and
elicit smaller neural signals associated with processing difficulty, most
notably, the N400 component of the event-related brain potential. Thus, it has
been argued that prediction of upcoming words is a key component of language
comprehension, and that studying the amplitude of the N400 is a valuable way to
investigate the predictions that we make. In this study, we investigate whether
the linguistic predictions of computational language models or humans better
reflect the way in which natural language stimuli modulate the amplitude of the
N400. One important difference in the linguistic predictions of humans versus
computational language models is that while language models base their
predictions exclusively on the preceding linguistic context, humans may rely on
other factors. We find that the predictions of three top-of-the-line
contemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more
closely than human predictions. This suggests that the predictive processes
underlying the N400 may be more sensitive to the surface-level statistics of
language than previously thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Conditionality for Natural Language Generation. (arXiv:2109.01229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01229">
<div class="article-summary-box-inner">
<span><p>Large scale pretrained language models have demonstrated state-of-the-art
performance in language understanding tasks. Their application has recently
expanded into multimodality learning, leading to improved representations
combining vision and language. However, progress in adapting language models
towards conditional Natural Language Generation (NLG) has been limited to a
single modality, generally text. We propose MAnTiS, Multimodal Adaptation for
Text Synthesis, a general approach for multimodal conditionality in
transformer-based NLG models. In this method, we pass inputs from each modality
through modality-specific encoders, project to textual token space, and finally
join to form a conditionality prefix. We fine-tune the pretrained language
model and encoders with the conditionality prefix guiding the generation. We
apply MAnTiS to the task of product description generation, conditioning a
network on both product images and titles to generate descriptive text. We
demonstrate that MAnTiS outperforms strong baseline approaches on standard NLG
scoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS
can generate human quality descriptions consistent with given multimodal
inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction. (arXiv:2109.01238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01238">
<div class="article-summary-box-inner">
<span><p>Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new
subtask of target-oriented sentiment analysis that aims to extract opinion
words for a given aspect in text. Current state-of-the-art methods leverage
position embeddings to capture the relative position of a word to the target.
However, the performance of these methods depends on the ability to incorporate
this information into word representations. In this paper, we explore a variety
of text encoders based on pretrained word embeddings or language models that
leverage part-of-speech and position embeddings, aiming to examine the actual
contribution of each component in TOWE. We also adapt a graph convolutional
network (GCN) to enhance word representations by incorporating syntactic
information. Our experimental results demonstrate that BiLSTM-based models can
effectively encode position information into word representations while using a
GCN only achieves marginal gains. Interestingly, our simple methods outperform
several state-of-the-art complex neural structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Linking and Discovery via Arborescence-based Supervised Clustering. (arXiv:2109.01242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01242">
<div class="article-summary-box-inner">
<span><p>Previous work has shown promising results in performing entity linking by
measuring not only the affinities between mentions and entities but also those
amongst mentions. In this paper, we present novel training and inference
procedures that fully utilize mention-to-mention affinities by building minimum
arborescences (i.e., directed spanning trees) over mentions and entities across
documents in order to make linking decisions. We also show that this method
gracefully extends to entity discovery, enabling the clustering of mentions
that do not have an associated entity in the knowledge base. We evaluate our
approach on the Zero-Shot Entity Linking dataset and MedMentions, the largest
publicly available biomedical dataset, and show significant improvements in
performance for both entity linking and discovery compared to identically
parameterized models. We further show significant efficiency improvements with
only a small loss in accuracy over previous work, which use more
computationally expensive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Prompt-Based Models Really Understand the Meaning of their Prompts?. (arXiv:2109.01247v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01247">
<div class="article-summary-box-inner">
<span><p>Recently, a boom of papers have shown extraordinary progress in few-shot
learning with various prompt-based models. Such success can give the impression
that prompts help models to learn faster in the same way that humans learn
faster when provided with task instructions expressed in natural language. In
this study, we experiment with over 30 prompts manually written for natural
language inference (NLI). We find that models learn just as fast with many
prompts that are intentionally irrelevant or even pathologically misleading as
they do with instructively "good" prompts. Additionally, we find that model
performance is more dependent on the choice of the LM target words (a.k.a. the
"verbalizer" that converts LM vocabulary prediction to class labels) than on
the text of the prompt itself. In sum, we find little evidence that suggests
existing prompt-based models truly understand the meaning of their given
prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Context-Aware Hierarchical BERT Fusion Network for Multi-turn Dialog Act Detection. (arXiv:2109.01267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01267">
<div class="article-summary-box-inner">
<span><p>The success of interactive dialog systems is usually associated with the
quality of the spoken language understanding (SLU) task, which mainly
identifies the corresponding dialog acts and slot values in each turn. By
treating utterances in isolation, most SLU systems often overlook the semantic
context in which a dialog act is expected. The act dependency between turns is
non-trivial and yet critical to the identification of the correct semantic
representations. Previous works with limited context awareness have exposed the
inadequacy of dealing with complexity in multiproned user intents, which are
subject to spontaneous change during turn transitions. In this work, we propose
to enhance SLU in multi-turn dialogs, employing a context-aware hierarchical
BERT fusion Network (CaBERT-SLU) to not only discern context information within
a dialog but also jointly identify multiple dialog acts and slots in each
utterance. Experimental results show that our approach reaches new
state-of-the-art (SOTA) performances in two complicated multi-turn dialogue
datasets with considerable improvements compared with previous methods, which
only consider single utterances for multiple intents and slot filling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Open-Source Dataset and A Multi-Task Model for Malay Named Entity Recognition. (arXiv:2109.01293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01293">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is a fundamental task of natural language
processing (NLP). However, most state-of-the-art research is mainly oriented to
high-resource languages such as English and has not been widely applied to
low-resource languages. In Malay language, relevant NER resources are limited.
In this work, we propose a dataset construction framework, which is based on
labeled datasets of homologous languages and iterative optimization, to build a
Malay NER dataset (MYNER) comprising 28,991 sentences (over 384 thousand
tokens). Additionally, to better integrate boundary information for NER, we
propose a multi-task (MT) model with a bidirectional revision (Bi-revision)
mechanism for Malay NER task. Specifically, an auxiliary task, boundary
detection, is introduced to improve NER training in both explicit and implicit
ways. Furthermore, a gated ignoring mechanism is proposed to conduct
conditional label transfer and alleviate error propagation by the auxiliary
task. Experimental results demonstrate that our model achieves comparable
results over baselines on MYNER. The dataset and the model in this paper would
be publicly released as a benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Symmetry Matters: A Modal-Alternating Propagation Network for Few-Shot Learning. (arXiv:2109.01295v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01295">
<div class="article-summary-box-inner">
<span><p>Semantic information provides intra-class consistency and inter-class
discriminability beyond visual concepts, which has been employed in Few-Shot
Learning (FSL) to achieve further gains. However, semantic information is only
available for labeled samples but absent for unlabeled samples, in which the
embeddings are rectified unilaterally by guiding the few labeled samples with
semantics. Therefore, it is inevitable to bring a cross-modal bias between
semantic-guided samples and nonsemantic-guided samples, which results in an
information asymmetry problem. To address this problem, we propose a
Modal-Alternating Propagation Network (MAP-Net) to supplement the absent
semantic information of unlabeled samples, which builds information symmetry
among all samples in both visual and semantic modalities. Specifically, the
MAP-Net transfers the neighbor information by the graph propagation to generate
the pseudo-semantics for unlabeled samples guided by the completed visual
relationships and rectify the feature embeddings. In addition, due to the large
discrepancy between visual and semantic modalities, we design a Relation
Guidance (RG) strategy to guide the visual relation vectors via semantics so
that the propagated information is more beneficial. Extensive experimental
results on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011,
SUN Attribute Database, and Oxford 102 Flower, have demonstrated that our
proposed method achieves promising performance and outperforms the
state-of-the-art approaches, which indicates the necessity of information
symmetry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indexing Context-Sensitive Reachability. (arXiv:2109.01321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01321">
<div class="article-summary-box-inner">
<span><p>Many context-sensitive data flow analyses can be formulated as a variant of
the all-pairs Dyck-CFL reachability problem, which, in general, is of sub-cubic
time complexity and quadratic space complexity. Such high complexity
significantly limits the scalability of context-sensitive data flow analysis
and is not affordable for analyzing large-scale software. This paper presents
\textsc{Flare}, a reduction from the CFL reachability problem to the
conventional graph reachability problem for context-sensitive data flow
analysis. This reduction allows us to benefit from recent advances in
reachability indexing schemes, which often consume almost linear space for
answering reachability queries in almost constant time. We have applied our
reduction to a context-sensitive alias analysis and a context-sensitive
information-flow analysis for C/C++ programs. Experimental results on standard
benchmarks and open-source software demonstrate that we can achieve orders of
magnitude speedup at the cost of only moderate space to store the indexes. The
implementation of our approach is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Speaker Personas from Conversational Texts. (arXiv:2109.01330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01330">
<div class="article-summary-box-inner">
<span><p>Personas are useful for dialogue response prediction. However, the personas
used in current studies are pre-defined and hard to obtain before a
conversation. To tackle this issue, we study a new task, named Speaker Persona
Detection (SPD), which aims to detect speaker personas based on the plain
conversational text. In this task, a best-matched persona is searched out from
candidates given the conversational text. This is a many-to-many semantic
matching task because both contexts and personas in SPD are composed of
multiple sentences. The long-term dependency and the dynamic redundancy among
these sentences increase the difficulty of this task. We build a dataset for
SPD, dubbed as Persona Match on Persona-Chat (PMPC). Furthermore, we evaluate
several baseline models and propose utterance-to-profile (U2P) matching
networks for this task. The U2P models operate at a fine granularity which
treat both contexts and personas as sets of multiple sequences. Then, each
sequence pair is scored and an interpretable overall score is obtained for a
context-persona pair through aggregation. Evaluation results show that the U2P
models outperform their baseline counterparts significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT. (arXiv:2109.01396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01396">
<div class="article-summary-box-inner">
<span><p>Differently from the traditional statistical MT that decomposes the
translation task into distinct separately learned components, neural machine
translation uses a single neural network to model the entire translation
process. Despite neural machine translation being de-facto standard, it is
still not clear how NMT models acquire different competences over the course of
training, and how this mirrors the different models in traditional SMT. In this
work, we look at the competences related to three core SMT components and find
that during training, NMT first focuses on learning target-side language
modeling, then improves translation quality approaching word-by-word
translation, and finally learns more complicated reordering patterns. We show
that this behavior holds for several models and language pairs. Additionally,
we explain how such an understanding of the training process can be useful in
practice and, as an example, show how it can be used to improve vanilla
non-autoregressive neural machine translation by guiding teacher model
selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining. (arXiv:2109.01411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01411">
<div class="article-summary-box-inner">
<span><p>The Linked Open Data practice has led to a significant growth of structured
data on the Web in the last decade. Such structured data describe real-world
entities in a machine-readable way, and have created an unprecedented
opportunity for research in the field of Natural Language Processing. However,
there is a lack of studies on how such data can be used, for what kind of
tasks, and to what extent they can be useful for these tasks. This work focuses
on the e-commerce domain to explore methods of utilising such structured data
to create language resources that may be used for product classification and
linking. We process billions of structured data points in the form of RDF
n-quads, to create multi-million words of product-related corpora that are
later used in three different ways for creating of language resources: training
word embedding models, continued pre-training of BERT-like language models, and
training Machine Translation models that are used as a proxy to generate
product-related keywords. Our evaluation on an extensive set of benchmarks
shows word embeddings to be the most reliable and consistent method to improve
the accuracy on both tasks (with up to 6.9 percentage points in macro-average
F1 on some datasets). The other two methods however, are not as useful. Our
analysis shows that this could be due to a number of reasons, including the
biased domain representation in the structured data and lack of vocabulary
coverage. We share our datasets and discuss how our lessons learned could be
taken forward to inform future research in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LG4AV: Combining Language Models and Graph Neural Networks for Author Verification. (arXiv:2109.01479v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01479">
<div class="article-summary-box-inner">
<span><p>The automatic verification of document authorships is important in various
settings. Researchers are for example judged and compared by the amount and
impact of their publications and public figures are confronted by their posts
on social media platforms. Therefore, it is important that authorship
information in frequently used web services and platforms is correct. The
question whether a given document is written by a given author is commonly
referred to as authorship verification (AV). While AV is a widely investigated
problem in general, only few works consider settings where the documents are
short and written in a rather uniform style. This makes most approaches
unpractical for online databases and knowledge graphs in the scholarly domain.
Here, authorships of scientific publications have to be verified, often with
just abstracts and titles available. To this point, we present our novel
approach LG4AV which combines language models and graph neural networks for
authorship verification. By directly feeding the available texts in a
pre-trained transformer architecture, our model does not need any hand-crafted
stylometric features that are not meaningful in scenarios where the writing
style is, at least to some extent, standardized. By the incorporation of a
graph neural network structure, our model can benefit from relations between
authors that are meaningful with respect to the verification process. For
example, scientific authors are more likely to write about topics that are
addressed by their co-authors and twitter users tend to post about the same
subjects as people they follow. We experimentally evaluate our model and study
to which extent the inclusion of co-authorships enhances verification decisions
in bibliometric environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation. (arXiv:2109.01484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01484">
<div class="article-summary-box-inner">
<span><p>Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target
sentence which conforms to the style of the given exemplar while encapsulating
the content information of the source sentence. In this paper, we propose a new
method with the goal of learning a better representation of the style andthe
content. This method is mainly motivated by the recent success of contrastive
learning which has demonstrated its power in unsupervised feature extraction
tasks. The idea is to design two contrastive losses with respect to the content
and the style by considering two problem characteristics during training. One
characteristic is that the target sentence shares the same content with the
source sentence, and the second characteristic is that the target sentence
shares the same style with the exemplar. These two contrastive losses are
incorporated into the general encoder-decoder paradigm. Experiments on two
datasets, namely QQP-Pos and ParaNMT, demonstrate the effectiveness of our
proposed constrastive losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical Data-to-Text Generation via Fine-Tuning Transformers. (arXiv:2109.01518v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01518">
<div class="article-summary-box-inner">
<span><p>Data-to-text (D2T) generation in the biomedical domain is a promising - yet
mostly unexplored - field of research. Here, we apply neural models for D2T
generation to a real-world dataset consisting of package leaflets of European
medicines. We show that fine-tuned transformers are able to generate realistic,
multisentence text from data in the biomedical domain, yet have important
limitations. We also release a new dataset (BioLeaflets) for benchmarking D2T
generation models in the biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01537">
<div class="article-summary-box-inner">
<span><p>Dementia is a family of neurogenerative conditions affecting memory and
cognition in an increasing number of individuals in our globally aging
population. Automated analysis of language, speech and paralinguistic
indicators have been gaining popularity as potential indicators of cognitive
decline. Here we propose a novel longitudinal multi-modal dataset collected
from people with mild dementia and age matched controls over a period of
several months in a natural setting. The multi-modal data consists of spoken
conversations, a subset of which are transcribed, as well as typed and written
thoughts and associated extra-linguistic information such as pen strokes and
keystrokes. We describe the dataset in detail and proceed to focus on a task
using the speech modality. The latter involves distinguishing controls from
people with dementia by exploiting the longitudinal nature of the data. Our
experiments showed significant differences in how the speech varied from
session to session in the control and dementia groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Neural Models for Natural Language Processing in the Face of Distributional Shift. (arXiv:2109.01558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01558">
<div class="article-summary-box-inner">
<span><p>The dominating NLP paradigm of training a strong neural predictor to perform
one task on a specific dataset has led to state-of-the-art performance in a
variety of applications (eg. sentiment classification, span-prediction based
question answering or machine translation). However, it builds upon the
assumption that the data distribution is stationary, ie. that the data is
sampled from a fixed distribution both at training and test time. This way of
training is inconsistent with how we as humans are able to learn from and
operate within a constantly changing stream of information. Moreover, it is
ill-adapted to real-world use cases where the data distribution is expected to
shift over the course of a model's lifetime.
</p>
<p>The first goal of this thesis is to characterize the different forms this
shift can take in the context of natural language processing, and propose
benchmarks and evaluation metrics to measure its effect on current deep
learning architectures. We then proceed to take steps to mitigate the effect of
distributional shift on NLP models. To this end, we develop methods based on
parametric reformulations of the distributionally robust optimization
framework. Empirically, we demonstrate that these approaches yield more robust
models as demonstrated on a selection of realistic problems. In the third and
final part of this thesis, we explore ways of efficiently adapting existing
models to new domains or tasks. Our contribution to this topic takes
inspiration from information geometry to derive a new gradient update rule
which alleviate catastrophic forgetting issues during adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Embeddings based Convolutional Neural Networks for Duplicate Question Identification. (arXiv:2109.01560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01560">
<div class="article-summary-box-inner">
<span><p>Question Paraphrase Identification (QPI) is a critical task for large-scale
Question-Answering forums. The purpose of QPI is to determine whether a given
pair of questions are semantically identical or not. Previous approaches for
this task have yielded promising results, but have often relied on complex
recurrence mechanisms that are expensive and time-consuming in nature. In this
paper, we propose a novel architecture combining a Bidirectional Transformer
Encoder with Convolutional Neural Networks for the QPI task. We produce the
predictions from the proposed architecture using two different inference
setups: Siamese and Matched Aggregation. Experimental results demonstrate that
our model achieves state-of-the-art performance on the Quora Question Pairs
dataset. We empirically prove that the addition of convolution layers to the
model architecture improves the results in both inference setups. We also
investigate the impact of partial and complete fine-tuning and analyze the
trade-off between computational power and accuracy in the process. Based on the
obtained results, we conclude that the Matched-Aggregation setup consistently
outperforms the Siamese setup. Our work provides insights into what
architecture combinations and setups are likely to produce better results for
the QPI task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding. (arXiv:2109.01583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01583">
<div class="article-summary-box-inner">
<span><p>Lack of training data presents a grand challenge to scaling out spoken
language understanding (SLU) to low-resource languages. Although various data
augmentation approaches have been proposed to synthesize training data in
low-resource target languages, the augmented data sets are often noisy, and
thus impede the performance of SLU models. In this paper we focus on mitigating
noise in augmented data. We develop a denoising training approach. Multiple
models are trained with data produced by various augmented methods. Those
models provide supervision signals to each other. The experimental results show
that our method outperforms the existing state of the art by 3.05 and 4.24
percentage points on two benchmark datasets, respectively. The code will be
made open sourced on github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Training with Dense Retrieval for Document Retrieval. (arXiv:2109.01628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01628">
<div class="article-summary-box-inner">
<span><p>Dense retrieval has shown great success in passage ranking in English.
However, its effectiveness in document retrieval for non-English languages
remains unexplored due to the limitation in training resources. In this work,
we explore different transfer techniques for document ranking from English
annotations to multiple non-English languages. Our experiments on the test
collections in six languages (Chinese, Arabic, French, Hindi, Bengali, Spanish)
from diverse language families reveal that zero-shot model-based transfer using
mBERT improves the search quality in non-English mono-lingual retrieval. Also,
we find that weakly-supervised target language transfer yields competitive
performances against the generation-based target language transfer that
requires external translators and query generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01636">
<div class="article-summary-box-inner">
<span><p>With the fast development of Deep Learning techniques, Named Entity
Recognition (NER) is becoming more and more important in the information
extraction task. The greatest difficulty that the NER task faces is to keep the
detectability even when types of NE and documents are unfamiliar. Realizing
that the specificity information may contain potential meanings of a word and
generate semantic-related features for word embedding, we develop a
distribution-aware word embedding and implement three different methods to make
use of the distribution information in a NER framework. And the result shows
that the performance of NER will be improved if the word specificity is
incorporated into existing NER methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuned Language Models Are Zero-Shot Learners. (arXiv:2109.01652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01652">
<div class="article-summary-box-inner">
<span><p>This paper explores a simple method for improving the zero-shot learning
abilities of language models. We show that instruction tuning -- finetuning
language models on a collection of tasks described via instructions --
substantially boosts zero-shot performance on unseen tasks.
</p>
<p>We take a 137B parameter pretrained language model and instruction-tune it on
over 60 NLP tasks verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task
types. FLAN substantially improves the performance of its unmodified
counterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 tasks that we
evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,
BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number
of tasks and model scale are key components to the success of instruction
tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge. (arXiv:2109.01653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01653">
<div class="article-summary-box-inner">
<span><p>Most benchmark datasets targeting commonsense reasoning focus on everyday
scenarios: physical knowledge like knowing that you could fill a cup under a
waterfall [Talmor et al., 2019], social knowledge like bumping into someone is
awkward [Sap et al., 2019], and other generic situations. However, there is a
rich space of commonsense inferences anchored to knowledge about specific
entities: for example, deciding the truthfulness of a claim "Harry Potter can
teach classes on how to fly on a broomstick." Can models learn to combine
entity knowledge with commonsense reasoning in this fashion? We introduce
CREAK, a testbed for commonsense reasoning about entity knowledge, bridging
fact-checking about entities (Harry Potter is a wizard and is skilled at riding
a broomstick) with commonsense inferences (if you're good at a skill you can
teach others how to do it). Our dataset consists of 13k human-authored English
claims about entities that are either true or false, in addition to a small
contrast set. Crowdworkers can easily come up with these statements and human
performance on the dataset is high (high 90s); we argue that models should be
able to blend entity knowledge and commonsense reasoning to do well here. In
our experiments, we focus on the closed-book setting and observe that a
baseline model finetuned on existing fact verification benchmark struggles on
CREAK. Training a model on CREAK improves accuracy by a substantial margin, but
still falls short of human performance. Our benchmark provides a unique probe
into natural language understanding models, testing both its ability to
retrieve facts (e.g., who teaches at the University of Chicago?) and unstated
commonsense knowledge (e.g., butlers do not yell at guests).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?. (arXiv:1905.10617v10 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.10617">
<div class="article-summary-box-inner">
<span><p>Exposure bias has been regarded as a central problem for auto-regressive
language models (LM). It claims that teacher forcing would cause the test-time
generation to be incrementally distorted due to the training-generation
discrepancy. Although a lot of algorithms have been proposed to avoid teacher
forcing and therefore alleviate exposure bias, there is little work showing how
serious the exposure bias problem actually is. In this work, we focus on the
task of open-ended language generation, propose metrics to quantify the impact
of exposure bias in the aspects of quality, diversity, and consistency. Our key
intuition is that if we feed ground-truth data prefixes (instead of prefixes
generated by the model itself) into the model and ask it to continue the
generation, the performance should become much better because the
training-generation discrepancy in the prefix is removed. Both automatic and
human evaluations are conducted in our experiments. On the contrary to the
popular belief in exposure bias, we find that the the distortion induced by the
prefix discrepancy is limited, and does not seem to be incremental during the
generation. Moreover, our analysis reveals an interesting self-recovery ability
of the LM, which we hypothesize to be countering the harmful effects from
exposure bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v10 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02358">
<div class="article-summary-box-inner">
<span><p>Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adjusting for Confounders with Text: Challenges and an Empirical Evaluation Framework for Causal Inference. (arXiv:2009.09961v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09961">
<div class="article-summary-box-inner">
<span><p>Leveraging text, such as social media posts, for causal inferences requires
the use of NLP models to 'learn' and adjust for confounders, which could
otherwise impart bias. However, evaluating such models is challenging, as
ground truth is almost never available. We demonstrate the need for empirical
evaluation frameworks for causal inference in natural language by showing that
existing, commonly used models regularly disagree with one another on real
world tasks. We contribute the first such framework, generalizing several
challenges across these real world tasks. Using this framework, we evaluate a
large set of commonly used causal inference models based on propensity scores
and identify their strengths and weaknesses to inform future improvements. We
make all tasks, data, and models public to inform applications and encourage
additional research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation. (arXiv:2010.10333v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.10333">
<div class="article-summary-box-inner">
<span><p>Growing interests have been attracted in Conversational Recommender Systems
(CRS), which explore user preference through conversational interactions in
order to make appropriate recommendation. However, there is still a lack of
ability in existing CRS to (1) traverse multiple reasoning paths over
background knowledge to introduce relevant items and attributes, and (2)
arrange selected entities appropriately under current system intents to control
response generation. To address these issues, we propose CR-Walker in this
paper, a model that performs tree-structured reasoning on a knowledge graph,
and generates informative dialog acts to guide language generation. The unique
scheme of tree-structured reasoning views the traversed entity at each hop as
part of dialog acts to facilitate language generation, which links how entities
are selected and expressed. Automatic and human evaluations show that CR-Walker
can arrive at more accurate recommendation, and generate more informative and
engaging responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where Are You? Localization from Embodied Dialog. (arXiv:2011.08277v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08277">
<div class="article-summary-box-inner">
<span><p>We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans
-- an Observer and a Locator -- complete a cooperative localization task. The
Observer is spawned at random in a 3D environment and can navigate from
first-person views while answering questions from the Locator. The Locator must
localize the Observer in a detailed top-down map by asking questions and giving
instructions. Based on this dataset, we define three challenging tasks:
Localization from Embodied Dialog or LED (localizing the Observer from dialog
history), Embodied Visual Dialog (modeling the Observer), and Cooperative
Localization (modeling both agents). In this paper, we focus on the LED task --
providing a strong baseline model with detailed ablations characterizing both
dataset biases and the importance of various modeling choices. Our best model
achieves 32.7% success at identifying the Observer's location within 3m in
unseen buildings, vs. 70.4% for human Locators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade. (arXiv:2012.14682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14682">
<div class="article-summary-box-inner">
<span><p>Dynamic early exiting aims to accelerate the inference of pre-trained
language models (PLMs) by emitting predictions in internal layers without
passing through the entire model. In this paper, we empirically analyze the
working mechanism of dynamic early exiting and find that it faces a performance
bottleneck under high speed-up ratios. On one hand, the PLMs' representations
in shallow layers lack high-level semantic information and thus are not
sufficient for accurate predictions. On the other hand, the exiting decisions
made by internal classifiers are unreliable, leading to wrongly emitted early
predictions. We instead propose a new framework for accelerating the inference
of PLMs, CascadeBERT, which dynamically selects proper-sized and complete
models in a cascading manner, providing comprehensive representations for
predictions. We further devise a difficulty-aware objective, encouraging the
model to output the class probability that reflects the real difficulty of each
instance for a more reliable cascading mechanism. Experimental results show
that CascadeBERT can achieve an overall 15\% improvement under 4$\times$
speed-up compared with existing dynamic early exiting methods on six
classification tasks, yielding more calibrated and accurate predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering. (arXiv:2101.00391v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00391">
<div class="article-summary-box-inner">
<span><p>Many Question-Answering (QA) datasets contain unanswerable questions, but
their treatment in QA systems remains primitive. Our analysis of the Natural
Questions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion
of unanswerable questions ($\sim$21%) can be explained based on the presence of
unverifiable presuppositions. We discuss the shortcomings of current models in
handling such questions, and describe how an improved system could handle them.
Through a user preference study, we demonstrate that the oracle behavior of our
proposed system that provides responses based on presupposition failure is
preferred over the oracle behavior of existing QA systems. Then we discuss how
our proposed system could be implemented, presenting a novel framework that
breaks down the problem into three steps: presupposition generation,
presupposition verification and explanation generation. We report our progress
in tackling each subproblem, and present a preliminary approach to integrating
these steps into an existing QA system. We find that adding presuppositions and
their verifiability to an existing model yields modest gains in downstream
performance and unanswerability detection. The biggest bottleneck is the
verification component, which needs to be substantially improved for the
integrated system to approach ideal behavior -- even transfer from the best
entailment models currently falls short.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDLM: Cross-Document Language Modeling. (arXiv:2101.00406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00406">
<div class="article-summary-box-inner">
<span><p>We introduce a new pretraining approach geared for multi-document language
modeling, incorporating two key ideas into the masked language modeling
self-supervised objective. First, instead of considering documents in
isolation, we pretrain over sets of multiple related documents, encouraging the
model to learn cross-document relationships. Second, we improve over recent
long-range transformers by introducing dynamic global attention that has access
to the entire input to predict masked tokens. We release CDLM (Cross-Document
Language Model), a new general language model for multi-document setting that
can be easily applied to downstream tasks. Our extensive analysis shows that
both ideas are essential for the success of CDLM, and work in synergy to set
new state-of-the-art results for several multi-text tasks. Code and models are
available at https://github.com/aviclu/CDLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04670">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (LMs) such as GPT-3 have acquired a
surprising ability to perform zero-shot learning. For example, to classify
sentiment without any training examples, we can "prompt" the LM with the review
and the label description "Does the user like this movie?", and ask whether the
next word is "yes" or "no". However, the next word prediction training
objective is still misaligned with the target zero-shot learning objective. To
address this weakness, we propose meta-tuning, which directly optimizes the
zero-shot learning objective by fine-tuning pre-trained language models on a
collection of datasets. We focus on classification tasks, and construct the
meta-dataset by aggregating 43 existing datasets and annotating 441 label
descriptions in a question-answering (QA) format. When evaluated on unseen
tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA
zero-shot learning system based on natural language inference. Additionally,
increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,
and we forecast that even larger models would perform better. Therefore,
measuring zero-shot learning performance on language models out-of-the-box
might underestimate their true potential, and community-wide efforts on
aggregating datasets and unifying their formats can help build models that
answer prompts better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has achieved promising results on some few-shot
classification tasks. The core idea of prompt-tuning is to insert text pieces,
i.e., templates, into the input and transform a classification task into a
masked language modeling problem. However, as for relation extraction,
determining the appropriate prompt template requires domain expertise. Single
label word handcrafted or auto-searched is cumbersome and time-consuming to
verify their effectiveness in non-few-shot scenarios. Further, there exist
abundant semantic knowledge among the entities and relation labels which cannot
be ignored. To this end, we focus on incorporating knowledge into prompt-tuning
for relation extraction and propose a knowledge-aware prompt-tuning with
synergistic optimization (KnowPrompt) approach. Specifically, we inject entity
and relation knowledge into prompt construction with learnable virtual template
words and answer words and jointly optimize their representation with knowledge
constraints. Extensive experimental results on five datasets with standard and
low-resource settings demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sublanguage: A Serious Issue Affects Pretrained Models in Legal Domain. (arXiv:2104.07782v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07782">
<div class="article-summary-box-inner">
<span><p>Legal English is a sublanguage that is important for everyone but not for
everyone to understand. Pretrained models have become best practices among
current deep learning approaches for different problems. It would be a waste or
even a danger if these models were applied in practice without knowledge of the
sublanguage of the law. In this paper, we raise the issue and propose a trivial
solution by introducing BERTLaw a legal sublanguage pretrained model. The
paper's experiments demonstrate the superior effectiveness of the method
compared to the baseline pretrained model
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding. (arXiv:2104.08145v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08145">
<div class="article-summary-box-inner">
<span><p>Contextualized entity representations learned by state-of-the-art
transformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the
attention mechanism to learn the data context from training data corpus.
However, these models do not use the knowledge context. Knowledge context can
be understood as semantics about entities and their relationship with
neighboring entities in knowledge graphs. We propose a novel and effective
technique to infuse knowledge context from multiple knowledge graphs for
conceptual and ambiguous entities into TLMs during fine-tuning. It projects
knowledge graph embeddings in the homogeneous vector-space, introduces new
token-types for entities, aligns entity position ids, and a selective attention
mechanism. We take BERT as a baseline model and implement the
"Knowledge-Infused BERT" by infusing knowledge context from ConceptNet and
WordNet, which significantly outperforms BERT and other recent knowledge-aware
BERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks
of GLUE benchmark. The KI-BERT-base model even significantly outperforms
BERT-large for domain-specific tasks like SciTail and academic subsets of QQP,
QNLI, and MNLI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation. (arXiv:2104.08724v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08724">
<div class="article-summary-box-inner">
<span><p>Prior studies on text-to-text generation typically assume that the model
could figure out what to attend to in the input and what to include in the
output via seq2seq learning, with only the parallel training data and no
additional guidance. However, it remains unclear whether current models can
preserve important concepts in the source input, as seq2seq learning does not
have explicit focus on the concepts and commonly used evaluation metrics also
treat concepts equally important as other tokens. In this paper, we present a
systematic analysis that studies whether current seq2seq models, especially
pre-trained language models, are good enough for preserving important input
concepts and to what extent explicitly guiding generation with the concepts as
lexical constraints is beneficial. We answer the above questions by conducting
extensive analytical experiments on four representative text-to-text generation
tasks. Based on the observations, we then propose a simple yet effective
framework to automatically extract, denoise, and enforce important input
concepts as lexical constraints. This new method performs comparably or better
than its unconstrained counterpart on automatic metrics, demonstrates higher
coverage for concept preservation, and receives better ratings in the human
evaluation. Our code is available at https://github.com/morningmoni/EDE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Verdi: Quality Estimation and Error Detection for Bilingual Corpora. (arXiv:2105.14878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14878">
<div class="article-summary-box-inner">
<span><p>Translation Quality Estimation is critical to reducing post-editing efforts
in machine translation and to cross-lingual corpus cleaning. As a research
problem, quality estimation (QE) aims to directly estimate the quality of
translation in a given pair of source and target sentences, and highlight the
words that need corrections, without referencing to golden translations. In
this paper, we propose Verdi, a novel framework for word-level and
sentence-level post-editing effort estimation for bilingual corpora. Verdi
adopts two word predictors to enable diverse features to be extracted from a
pair of sentences for subsequent quality estimation, including a
transformer-based neural machine translation (NMT) model and a pre-trained
cross-lingual language model (XLM). We exploit the symmetric nature of
bilingual corpora and apply model-level dual learning in the NMT predictor,
which handles a primal task and a dual task simultaneously with weight sharing,
leading to stronger context prediction ability than single-direction NMT
models. By taking advantage of the dual learning scheme, we further design a
novel feature to directly encode the translated target information without
relying on the source context. Extensive experiments conducted on WMT20 QE
tasks demonstrate that our method beats the winner of the competition and
outperforms other baseline methods by a great margin. We further use the
sentence-level scores provided by Verdi to clean a parallel corpus and observe
benefits on both model performance and training efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Coreference Resolution with Harmonized Annotations. (arXiv:2107.12088v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12088">
<div class="article-summary-box-inner">
<span><p>In this paper, we present coreference resolution experiments with a newly
created multilingual corpus CorefUD. We focus on the following languages:
Czech, Russian, Polish, German, Spanish, and Catalan. In addition to
monolingual experiments, we combine the training data in multilingual
experiments and train two joined models -- for Slavic languages and for all the
languages together. We rely on an end-to-end deep learning model that we
slightly adapted for the CorefUD corpus. Our results show that we can profit
from harmonized annotations, and using joined models helps significantly for
the languages with smaller training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06314">
<div class="article-summary-box-inner">
<span><p>Time is an important dimension in our physical world. Lots of facts can
evolve with respect to time. For example, the U.S. President might change every
four years. Therefore, it is important to consider the time dimension and
empower the existing QA models to reason over time. However, the existing QA
datasets contain rather few time-sensitive questions, hence not suitable for
diagnosing or benchmarking the model's temporal reasoning capability. In order
to promote research in this direction, we propose to construct a time-sensitive
QA dataset. The dataset is constructed by 1) mining time-evolving facts from
WikiData and align them to their corresponding Wikipedia page, 2) employing
crowd workers to verify and calibrate these noisy facts, 3) generating
question-answer pairs based on the annotated time-sensitive facts. Our dataset
poses challenges in the aspect of both temporal understanding and temporal
reasoning. We evaluate different SoTA long-document QA systems like BigBird and
FiD on our dataset. The best-performing model FiD can only achieve 46\%
accuracy, still far behind the human performance of 87\%. We demonstrate that
these models are still lacking the ability to perform consistent temporal
reasoning. Therefore, we believe that our dataset could serve as a benchmark to
develop NLP models more sensitive to temporal shift. The dataset and code are
released in~\url{https://github.com/wenhuchen/Time-Sensitive-QA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affective Decoding for Empathetic Response Generation. (arXiv:2108.08102v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08102">
<div class="article-summary-box-inner">
<span><p>Understanding speaker's feelings and producing appropriate responses with
emotion connection is a key communicative skill for empathetic dialogue
systems. In this paper, we propose a simple technique called Affective Decoding
for empathetic response generation. Our method can effectively incorporate
emotion signals during each decoding step, and can additionally be augmented
with an auxiliary dual emotion encoder, which learns separate embeddings for
the speaker and listener given the emotion base of the dialogue. Extensive
empirical studies show that our models are perceived to be more empathetic by
human evaluations, in comparison to several strong mainstream methods for
empathetic responding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on five public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work claims, our auxiliary experiments suggest that relation
prediction is contributory to named entity prediction in a non-negligible way.
The source code can be found at https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications since the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current algorithms can tackle such
problem reliably in a realistic scenario where zero OOD training data is
available. In this study, we propose ProtoInfoMax, a new architecture that
extends Prototypical Networks to simultaneously process In-Domain (ID) and OOD
sentences via Mutual Information Maximization (InfoMax) objective. Experimental
results show that our proposed method can substantially improve performance up
to 20% for OOD detection in low resource settings of text classification. We
also show that ProtoInfoMax is less prone to typical over-confidence Error of
Neural Networks, leading to more reliable ID and OOD prediction outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NEREL: A Russian Dataset with Nested Named Entities, Relations and Events. (arXiv:2108.13112v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13112">
<div class="article-summary-box-inner">
<span><p>In this paper, we present NEREL, a Russian dataset for named entity
recognition and relation extraction. NEREL is significantly larger than
existing Russian datasets: to date it contains 56K annotated named entities and
39K annotated relations. Its important difference from previous datasets is
annotation of nested named entities, as well as relations within nested
entities and at the discourse level. NEREL can facilitate development of novel
models that can extract relations between nested named entities, as well as
relations on both sentence and document levels. NEREL also contains the
annotation of events involving named entities and their roles in the events.
The NEREL collection is available via https://github.com/nerel-ds/NEREL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00627">
<div class="article-summary-box-inner">
<span><p>Contextual knowledge is important for real-world automatic speech recognition
(ASR) applications. In this paper, a novel tree-constrained pointer generator
(TCPGen) component is proposed that incorporates such knowledge as a list of
biasing words into both attention-based encoder-decoder and transducer
end-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing
words into an efficient prefix tree to serve as its symbolic input and creates
a neural shortcut between the tree and the final ASR output distribution to
facilitate recognising biasing words during decoding. Systems were trained and
evaluated on the Librispeech corpus where biasing words were extracted at the
scales of an utterance, a chapter, or a book to simulate different application
scenarios. Experimental results showed that TCPGen consistently improved word
error rates (WERs) compared to the baselines, and in particular, achieved
significant WER reductions on the biasing words. TCPGen is highly efficient: it
can handle 5,000 biasing words and distractors and only add a small overhead to
memory use and computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training. (arXiv:2109.00993v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00993">
<div class="article-summary-box-inner">
<span><p>Large Transformer-based language models such as BERT have led to broad
performance improvements on many NLP tasks. Domain-specific variants of these
models have demonstrated excellent performance on a variety of specialised
tasks. In legal NLP, BERT-based models have led to new state-of-the-art results
on multiple tasks. The exploration of these models has demonstrated the
importance of capturing the specificity of the legal language and its
vocabulary. However, such approaches suffer from high computational costs,
leading to a higher ecological impact and lower accessibility. Our findings,
focusing on English language legal text, show that lightweight LSTM-based
Language Models are able to capture enough information from a small legal text
pretraining corpus and achieve excellent performance on short legal text
classification tasks. This is achieved with a significantly reduced
computational overhead compared to BERT-based models. However, our method also
shows degraded performance on a more complex task, multi-label classification
of longer documents, highlighting the limitations of this lightweight approach.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-06 23:08:50.163717610 UTC">2021-09-06 23:08:50 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>