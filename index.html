<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-21T01:30:00Z">09-21</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">When a Computer Cracks a Joke: Automated Generation of Humorous Headlines. (arXiv:2109.08702v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08702">
<div class="article-summary-box-inner">
<span><p>Automated news generation has become a major interest for new agencies in the
past. Oftentimes headlines for such automatically generated news articles are
unimaginative as they have been generated with ready-made templates. We present
a computationally creative approach for headline generation that can generate
humorous versions of existing headlines. We evaluate our system with human
judges and compare the results to human authored humorous titles. The headlines
produced by the system are considered funny 36\% of the time by human
evaluators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relating Neural Text Degeneration to Exposure Bias. (arXiv:2109.08705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08705">
<div class="article-summary-box-inner">
<span><p>This work focuses on relating two mysteries in neural-based text generation:
exposure bias, and text degeneration. Despite the long time since exposure bias
was mentioned and the numerous studies for its remedy, to our knowledge, its
impact on text generation has not yet been verified. Text degeneration is a
problem that the widely-used pre-trained language model GPT-2 was recently
found to suffer from (Holtzman et al., 2020). Motivated by the unknown
causation of the text degeneration, in this paper we attempt to relate these
two mysteries. Specifically, we first qualitatively quantitatively identify
mistakes made before text degeneration occurs. Then we investigate the
significance of the mistakes by inspecting the hidden states in GPT-2. Our
results show that text degeneration is likely to be partly caused by exposure
bias. We also study the self-reinforcing mechanism of text degeneration,
explaining why the mistakes amplify. In sum, our study provides a more concrete
foundation for further investigation on exposure bias and text degeneration
problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On-device neural speech synthesis. (arXiv:2109.08710v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08710">
<div class="article-summary-box-inner">
<span><p>Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and
WaveRNN, have made it possible to construct a fully neural network based TTS
system, by coupling the two components together. Such a system is conceptually
simple as it only takes grapheme or phoneme input, uses Mel-spectrogram as an
intermediate feature, and directly generates speech samples. The system
achieves quality equal or close to natural speech. However, the high
computational cost of the system and issues with robustness have limited their
usage in real-world speech synthesis applications and products. In this paper,
we present key modeling improvements and optimization strategies that enable
deploying these models, not only on GPU servers, but also on mobile devices.
The proposed system can generate high-quality 24 kHz speech at 5x faster than
real time on server and 3x faster than real time on mobile devices.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back-translation for Large-Scale Multilingual Machine Translation. (arXiv:2109.08712v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08712">
<div class="article-summary-box-inner">
<span><p>This paper illustrates our approach to the shared task on large-scale
multilingual machine translation in the sixth conference on machine translation
(WMT-21). This work aims to build a single multilingual translation system with
a hypothesis that a universal cross-language representation leads to better
multilingual translation performance. We extend the exploration of different
back-translation methods from bilingual translation to multilingual
translation. Better performance is obtained by the constrained sampling method,
which is different from the finding of the bilingual translation. Besides, we
also explore the effect of vocabularies and the amount of synthetic data.
Surprisingly, the smaller size of vocabularies perform better, and the
extensive monolingual English data offers a modest improvement. We submitted to
both the small tasks and achieved the second place.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08722">
<div class="article-summary-box-inner">
<span><p>Prerequisite chain learning helps people acquire new knowledge efficiently.
While people may quickly determine learning paths over concepts in a domain,
finding such paths in other domains can be challenging. We introduce
Domain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this
cross-domain prerequisite chain learning task efficiently. Our novel model
consists of a variational graph autoencoder (VGAE) and a domain discriminator.
The VGAE is trained to predict concept relations through link prediction, while
the domain discriminator takes both source and target domain data as input and
is trained to predict domain labels. Most importantly, this method only needs
simple homogeneous graphs as input, compared with the current state-of-the-art
model. We evaluate our model on the LectureBankCD dataset, and results show
that our model outperforms recent graph-based benchmarks while using only 1/10
of graph scale and 1/3 computation time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The JHU-Microsoft Submission for WMT21 Quality Estimation Shared Task. (arXiv:2109.08724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08724">
<div class="article-summary-box-inner">
<span><p>This paper presents the JHU-Microsoft joint submission for WMT 2021 quality
estimation shared task. We only participate in Task 2 (post-editing effort
estimation) of the shared task, focusing on the target-side word-level quality
estimation. The techniques we experimented with include Levenshtein Transformer
training and data augmentation with a combination of forward, backward,
round-trip translation, and pseudo post-editing of the MT output. We
demonstrate the competitiveness of our system compared to the widely adopted
OpenKiwi-XLM baseline. Our system is also the top-ranking system on the MT MCC
metric for the English-German language pair.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Few-Shot Intent Classification and Slot Filling. (arXiv:2109.08754v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08754">
<div class="article-summary-box-inner">
<span><p>Intent classification (IC) and slot filling (SF) are two fundamental tasks in
modern Natural Language Understanding (NLU) systems. Collecting and annotating
large amounts of data to train deep learning models for such systems is not
scalable. This problem can be addressed by learning from few examples using
fast supervised meta-learning techniques such as prototypical networks. In this
work, we systematically investigate how contrastive learning and unsupervised
data augmentation methods can benefit these existing supervised meta-learning
pipelines for jointly modelled IC/SF tasks. Through extensive experiments
across standard IC/SF benchmarks (SNIPS and ATIS), we show that our proposed
semi-supervised approaches outperform standard supervised meta-learning
methods: contrastive losses in conjunction with prototypical networks
consistently outperform the existing state-of-the-art for both IC and SF tasks,
while data augmentation strategies primarily improve few-shot IC by a
significant margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Solar cell patent classification method based on keyword extraction and deep neural network. (arXiv:2109.08796v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08796">
<div class="article-summary-box-inner">
<span><p>With the growing impact of ESG on businesses, research related to renewable
energy is receiving great attention. Solar cells are one of them, and
accordingly, it can be said that the research value of solar cell patent
analysis is very high. Patent documents have high research value. Being able to
accurately analyze and classify patent documents can reveal several important
technical relationships. It can also describe the business trends in that
technology. And when it comes to investment, new industrial solutions will also
be inspired and proposed to make important decisions. Therefore, we must
carefully analyze patent documents and utilize the value of patents. To solve
the solar cell patent classification problem, we propose a keyword extraction
method and a deep neural network-based solar cell patent classification method.
First, solar cell patents are analyzed for pretreatment. It then uses the
KeyBERT algorithm to extract keywords and key phrases from the patent abstract
to construct a lexical dictionary. We then build a solar cell patent
classification model according to the deep neural network. Finally, we use a
deep neural network-based solar cell patent classification model to classify
power patents, and the training accuracy is greater than 95%. Also, the
validation accuracy is about 87.5%. It can be seen that the deep neural network
method can not only realize the classification of complex and difficult solar
cell patents, but also have a good classification effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT-Beta: A Proactive Probabilistic Approach to Text Moderation. (arXiv:2109.08805v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08805">
<div class="article-summary-box-inner">
<span><p>Text moderation for user generated content, which helps to promote healthy
interaction among users, has been widely studied and many machine learning
models have been proposed. In this work, we explore an alternative perspective
by augmenting reactive reviews with proactive forecasting. Specifically, we
propose a new concept {\it text toxicity propensity} to characterize the extent
to which a text tends to attract toxic comments. Beta regression is then
introduced to do the probabilistic modeling, which is demonstrated to function
well in comprehensive experiments. We also propose an explanation method to
communicate the model decision clearly. Both propensity scoring and
interpretation benefit text moderation in a novel manner. Finally, the proposed
scaling mechanism for the linear model offers useful insights beyond this work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Structured Pattern Pruning Using Regularization. (arXiv:2109.08814v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08814">
<div class="article-summary-box-inner">
<span><p>Iterative Magnitude Pruning (IMP) is a network pruning method that repeats
the process of removing weights with the least magnitudes and retraining the
model. When visualizing the weight matrices of language models pruned by IMP,
previous research has shown that a structured pattern emerges, wherein the
resulting surviving weights tend to prominently cluster in a select few rows
and columns of the matrix. Though the need for further research in utilizing
these structured patterns for potential performance gains has previously been
indicated, it has yet to be thoroughly studied. We propose SPUR (Structured
Pattern pruning Using Regularization), a novel pruning mechanism that
preemptively induces structured patterns in compression by adding a
regularization term to the objective function in the IMP. Our results show that
SPUR can significantly preserve model performance under high sparsity settings
regardless of the language or the task. Our contributions are as follows: (i)
We propose SPUR, a network pruning mechanism that improves upon IMP regardless
of the language or the task. (ii) We are the first to empirically verify the
efficacy of "structured patterns" observed previously in pruning research.
(iii) SPUR is a resource-efficient mechanism in that it does not require
significant additional computations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DyLex: Incoporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08818">
<div class="article-summary-box-inner">
<span><p>Incorporating lexical knowledge into deep learning models has been proved to
be very effective for sequence labeling tasks. However, previous works commonly
have difficulty dealing with large-scale dynamic lexicons which often cause
excessive matching noise and problems of frequent updates. In this paper, we
propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence
labeling tasks. Instead of leveraging embeddings of words in the lexicon as in
conventional methods, we adopt word-agnostic tag embeddings to avoid
re-training the representation while updating the lexicon. Moreover, we employ
an effective supervised lexical knowledge denoising method to smooth out
matching noise. Finally, we introduce a col-wise attention based knowledge
fusion mechanism to guarantee the pluggability of the proposed framework.
Experiments on ten datasets of three tasks show that the proposed framework
achieves new SOTA, even with very large scale lexicons.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems. (arXiv:2109.08820v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08820">
<div class="article-summary-box-inner">
<span><p>Most prior work on task-oriented dialogue systems is restricted to supporting
domain APIs. However, users may have requests that are out of the scope of
these APIs. This work focuses on identifying such user requests. Existing
methods for this task mainly rely on fine-tuning pre-trained models on large
annotated data. We propose a novel method, REDE, based on adaptive
representation learning and density estimation. REDE can be applied to
zero-shot cases, and quickly learns a high-performing detector with only a few
shots by updating less than 3K parameters. We demonstrate REDE's competitive
performance on DSTC9 data and our newly collected test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08828">
<div class="article-summary-box-inner">
<span><p>Empathy is a complex cognitive ability based on the reasoning of others'
affective states. In order to better understand others and express stronger
empathy in dialogues, we argue that two issues must be tackled at the same
time: (i) identifying which word is the cause for the other's emotion from his
or her utterance and (ii) reflecting those specific words in the response
generation. However, previous approaches for recognizing emotion cause words in
text require sub-utterance level annotations, which can be demanding. Taking
inspiration from social cognition, we leverage a generative estimator to infer
emotion cause words from utterances with no word-level label. Also, we
introduce a novel method based on pragmatics to make dialogue models focus on
targeted words in the input during generation. Our method is applicable to any
dialogue models with no additional training on the fly. We show our approach
improves multiple best-performing dialogue agents on generating more focused
empathetic responses in terms of both automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MM-Deacon: Multimodal molecular domain embedding analysis via contrastive learning. (arXiv:2109.08830v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08830">
<div class="article-summary-box-inner">
<span><p>Molecular representation learning plays an essential role in cheminformatics.
Recently, language model-based approaches have been popular as an alternative
to traditional expert-designed features to encode molecules. However, these
approaches only utilize a single modality for representing molecules. Driven by
the fact that a given molecule can be described through different modalities
such as Simplified Molecular Line Entry System (SMILES), The International
Union of Pure and Applied Chemistry (IUPAC), and The IUPAC International
Chemical Identifier (InChI), we propose a multimodal molecular embedding
generation approach called MM-Deacon (multimodal molecular domain embedding
analysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC
molecule representations as two different modalities. First, SMILES and IUPAC
strings are encoded by using two different transformer-based language models
independently, then the contrastive loss is utilized to bring these encoded
representations from different modalities closer to each other if they belong
to the same molecule, and to push embeddings farther from each other if they
belong to different molecules. We evaluate the robustness of our molecule
embeddings on molecule clustering, cross-modal molecule search, drug similarity
assessment and drug-drug interaction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TVRecap: A Dataset for Generating Stories with Character Descriptions. (arXiv:2109.08833v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08833">
<div class="article-summary-box-inner">
<span><p>We introduce TVRecap, a story generation dataset that requires generating
detailed TV show episode recaps from a brief summary and a set of documents
describing the characters involved. Unlike other story generation datasets,
TVRecap contains stories that are authored by professional screenwriters and
that feature complex interactions among multiple characters. Generating stories
in TVRecap requires drawing relevant information from the lengthy provided
documents about characters based on the brief summary. In addition, by swapping
the input and output, TVRecap can serve as a challenging testbed for
abstractive summarization. We create TVRecap from fan-contributed websites,
which allows us to collect 26k episode recaps with 1868.7 tokens on average.
Empirically, we take a hierarchical story generation approach and find that the
neural model that uses oracle content selectors for character descriptions
demonstrates the best performance on automatic metrics, showing the potential
of our dataset to inspire future research on story generation with constraints.
Qualitative analysis shows that the best-performing model sometimes generates
content that is unfaithful to the short summaries, suggesting promising
directions for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification. (arXiv:2109.08839v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08839">
<div class="article-summary-box-inner">
<span><p>Recently, x-vector has been a successful and popular approach for speaker
verification, which employs a time delay neural network (TDNN) and statistics
pooling to extract speaker characterizing embedding from variable-length
utterances. Improvement upon the x-vector has been an active research area, and
enormous neural networks have been elaborately designed based on the x-vector,
eg, extended TDNN (E-TDNN), factorized TDNN (F-TDNN), and densely connected
TDNN (D-TDNN). In this work, we try to identify the optimal architectures from
a TDNN based search space employing neural architecture search (NAS), named
SpeechNAS. Leveraging the recent advances in the speaker recognition, such as
high-order statistics pooling, multi-branch mechanism, D-TDNN and angular
additive margin softmax (AAM) loss with a minimum hyper-spherical energy (MHE),
SpeechNAS automatically discovers five network architectures, from SpeechNAS-1
to SpeechNAS-5, of various numbers of parameters and GFLOPs on the large-scale
text-independent speaker recognition dataset VoxCeleb1. Our derived best neural
network achieves an equal error rate (EER) of 1.02% on the standard test set of
VoxCeleb1, which surpasses previous TDNN based state-of-the-art approaches by a
large margin. Code and trained weights are in
https://github.com/wentaozhu/speechnas.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Feature Engineering for US State Legislative Hearings: Stance, Affiliation, Engagement and Absentees. (arXiv:2109.08855v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08855">
<div class="article-summary-box-inner">
<span><p>In US State government legislatures, most of the activity occurs in
committees made up of lawmakers discussing bills. When analyzing, classifying
or summarizing these committee proceedings, some important features become
broadly interesting. In this paper, we engineer four useful features, two
applying to lawmakers (engagement and absence), and two to non-lawmakers
(stance and affiliation). We propose a system to automatically track the
affiliation of organizations in public comments and whether the organizational
representative supports or opposes the bill. The model tracking affiliation
achieves an F1 of 0.872 while the support determination has an F1 of 0.979.
Additionally, a metric to compute legislator engagement and absenteeism is also
proposed and as proof-of-concept, a list of the most and least engaged
legislators over one full California legislative session is presented.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Emily: Developing An Emotion-affective Open-Domain Chatbot with Knowledge Graph-based Persona. (arXiv:2109.08875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08875">
<div class="article-summary-box-inner">
<span><p>In this paper, we describe approaches for developing Emily, an
emotion-affective open-domain chatbot. Emily can perceive a user's negative
emotion state and offer supports by positively converting the user's emotion
states. This is done by finetuning a pretrained dialogue model upon data
capturing dialogue contexts and desirable emotion states transition across
turns. Emily can differentiate a general open-domain dialogue utterance with
questions relating to personal information. By leveraging a question-answering
approach based on knowledge graphs to handle personal information, Emily
maintains personality consistency. We evaluate Emily against a few
state-of-the-art open-domain chatbots and show the effects of the proposed
approaches in emotion affecting and addressing personality inconsistency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation. (arXiv:2109.08877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08877">
<div class="article-summary-box-inner">
<span><p>In this paper, we provide a bilingual parallel human-to-human recommendation
dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging
task of multilingual and cross-lingual conversational recommendation. The
difference between DuRecDial 2.0 and existing conversational recommendation
datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in
DuRecDial 2.0 is annotated in two languages, both English and Chinese, while
other datasets are built with the setting of a single language. We collect 8.2k
dialogs aligned across English and Chinese languages (16.5k dialogs and 255k
utterances in total) that are annotated by crowdsourced workers with strict
quality control procedure. We then build monolingual, multilingual, and
cross-lingual conversational recommendation baselines on DuRecDial 2.0.
Experiment results show that the use of additional English data can bring
performance improvement for Chinese conversational recommendation, indicating
the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging
testbed for future studies of monolingual, multilingual, and cross-lingual
conversational recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Joint Intent Detection and Slot Filling via Higher-order Attention. (arXiv:2109.08890v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08890">
<div class="article-summary-box-inner">
<span><p>Intent detection (ID) and Slot filling (SF) are two major tasks in spoken
language understanding (SLU). Recently, attention mechanism has been shown to
be effective in jointly optimizing these two tasks in an interactive manner.
However, latest attention-based works concentrated only on the first-order
attention design, while ignoring the exploration of higher-order attention
mechanisms. In this paper, we propose a BiLinear attention block, which
leverages bilinear pooling to simultaneously exploit both the contextual and
channel-wise bilinear attention distributions to capture the second-order
interactions between the input intent or slot features. Higher and even
infinity order interactions are built by stacking numerous blocks and assigning
Exponential Linear Unit (ELU) to blocks. Before the decoding stage, we
introduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot
information in a more effective way. Technically, instead of simply
concatenating intent and slot features, we first compute two correlation
matrices to weight on two features. Furthermore, we present Higher-order
Attention Network for the SLU tasks. Experiments on two benchmark datasets show
that our approach yields improvements compared with the state-of-the-art
approach. We also provide discussion to demonstrate the effectiveness of the
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency distance minimization predicts compression. (arXiv:2109.08900v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08900">
<div class="article-summary-box-inner">
<span><p>Dependency distance minimization (DDm) is a well-established principle of
word order. It has been predicted theoretically that DDm implies compression,
namely the minimization of word lengths. This is a second order prediction
because it links a principle with another principle, rather than a principle
and a manifestation as in a first order prediction. Here we test that second
order prediction with a parallel collection of treebanks controlling for
annotation style with Universal Dependencies and Surface-Syntactic Universal
Dependencies. To test it, we use a recently introduced score that has many
mathematical and statistical advantages with respect to the widely used sum of
dependency distances. We find that the prediction is confirmed by the new score
when word lengths are measured in phonemes, independently of the annotation
style, but not when word lengths are measured in syllables. In contrast, one of
the most widely used scores, i.e. the sum of dependency distances, fails to
confirm that prediction, showing the weakness of raw dependency distances for
research on word order. Finally, our findings expand the theory of natural
communication by linking two distinct levels of organization, namely syntax
(word order) and word internal structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text Detoxification using Large Pre-trained Neural Models. (arXiv:2109.08914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08914">
<div class="article-summary-box-inner">
<span><p>We present two novel unsupervised methods for eliminating toxicity in text.
Our first method combines two recent ideas: (1) guidance of the generation
process with small style-conditional language models and (2) use of
paraphrasing models to perform style transfer. We use a well-performing
paraphraser guided by style-trained language models to keep the text content
and remove toxicity. Our second method uses BERT to replace toxic words with
their non-offensive synonyms. We make the method more flexible by enabling BERT
to replace mask tokens with a variable number of words. Finally, we present the
first large-scale comparative study of style transfer models on the task of
toxicity removal. We compare our models with a number of methods for style
transfer. The models are evaluated in a reference-free way using a combination
of unsupervised style transfer metrics. Both methods we suggest yield new SOTA
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs. (arXiv:2109.08925v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08925">
<div class="article-summary-box-inner">
<span><p>Complex Query Answering (CQA) is an important reasoning task on knowledge
graphs. Current CQA learning models have been shown to be able to generalize
from atomic operators to more complex formulas, which can be regarded as the
combinatorial generalizability. In this paper, we present EFO-1-QA, a new
dataset to benchmark the combinatorial generalizability of CQA models by
including 301 different queries types, which is 20 times larger than existing
datasets. Besides, our work, for the first time, provides a benchmark to
evaluate and analyze the impact of different operators and normal forms by
using (a) 7 choices of the operator systems and (b) 9 forms of complex queries.
Specifically, we provide the detailed study of the combinatorial
generalizability of two commonly used operators, i.e., projection and
intersection, and justify the impact of the forms of queries given the
canonical choice of operators. Our code and data can provide an effective
pipeline to benchmark CQA models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic. (arXiv:2109.08927v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08927">
<div class="article-summary-box-inner">
<span><p>Natural language inference (NLI) aims to determine the logical relationship
between two sentences among the target labels Entailment, Contradiction, and
Neutral. In recent years, deep learning models have become a prevailing
approach to NLI, but they lack interpretability and explainability. In this
work, we address the explainability for NLI by weakly supervised logical
reasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our
model first detects phrases as the semantic unit and aligns corresponding
phrases. Then, the model predicts the NLI label for the aligned phrases, and
induces the sentence label by fuzzy logic formulas. Our EPR is almost
everywhere differentiable and thus the system can be trained end-to-end in a
weakly supervised manner. We annotated a corpus and developed a set of metrics
to evaluate phrasal reasoning. Results show that our EPR yields much more
meaningful explanations in terms of F scores than previous studies. To the best
of our knowledge, we are the first to develop a weakly supervised phrasal
reasoning model for the NLI task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Complex Temporal Question Answering on Knowledge Graphs. (arXiv:2109.08935v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08935">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge graphs (KG-QA) is a vital topic in IR.
Questions with temporal intent are a special class of practical importance, but
have not received much attention in research. This work presents EXAQT, the
first end-to-end system for answering complex temporal questions that have
multiple entities and predicates, and associated temporal conditions. EXAQT
answers natural language questions over KGs in two stages, one geared towards
high recall, the other towards precision at top ranks. The first step computes
question-relevant compact subgraphs within the KG, and judiciously enhances
them with pertinent temporal facts, using Group Steiner Trees and fine-tuned
BERT models. The second step constructs relational graph convolutional networks
(R-GCNs) from the first step's output, and enhances the R-GCNs with time-aware
entity embeddings and attention over temporal relations. We evaluate EXAQT on
TimeQuestions, a large dataset of 16k temporal questions we compiled from a
variety of general purpose KG-QA benchmarks. Results show that EXAQT
outperforms three state-of-the-art systems for answering complex questions over
KGs, thereby justifying specialized treatment of temporal QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ReaSCAN: Compositional Reasoning in Language Grounding. (arXiv:2109.08994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08994">
<div class="article-summary-box-inner">
<span><p>The ability to compositionally map language to referents, relations, and
actions is an essential component of language understanding. The recent gSCAN
dataset (Ruis et al. 2020, NeurIPS) is an inspiring attempt to assess the
capacity of models to learn this kind of grounding in scenarios involving
navigational instructions. However, we show that gSCAN's highly constrained
design means that it does not require compositional interpretation and that
many details of its instructions and scenarios are not required for task
success. To address these limitations, we propose ReaSCAN, a benchmark dataset
that builds off gSCAN but requires compositional language interpretation and
reasoning about entities and relations. We assess two models on ReaSCAN: a
multi-modal baseline and a state-of-the-art graph convolutional neural model.
These experiments show that ReaSCAN is substantially harder than gSCAN for both
neural architectures. This suggests that ReaSCAN can serve as a valuable
benchmark for advancing our understanding of models' compositional
generalization and reasoning capabilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Augmenting semantic lexicons using word embeddings and transfer learning. (arXiv:2109.09010v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09010">
<div class="article-summary-box-inner">
<span><p>Sentiment-aware intelligent systems are essential to a wide array of
applications including marketing, political campaigns, recommender systems,
behavioral economics, social psychology, and national security. These
sentiment-aware intelligent systems are driven by language models which broadly
fall into two paradigms: 1. Lexicon-based and 2. Contextual. Although recent
contextual models are increasingly dominant, we still see demand for
lexicon-based models because of their interpretability and ease of use. For
example, lexicon-based models allow researchers to readily determine which
words and phrases contribute most to a change in measured sentiment. A
challenge for any lexicon-based approach is that the lexicon needs to be
routinely expanded with new words and expressions. Crowdsourcing annotations
for semantic dictionaries may be an expensive and time-consuming task. Here, we
propose two models for predicting sentiment scores to augment semantic lexicons
at a relatively low cost using word embeddings and transfer learning. Our first
model establishes a baseline employing a simple and shallow neural network
initialized with pre-trained word embeddings using a non-contextual approach.
Our second model improves upon our baseline, featuring a deep Transformer-based
network that brings to bear word definitions to estimate their lexical
polarity. Our evaluation shows that both models are able to score new words
with a similar accuracy to reviewers from Amazon Mechanical Turk, but at a
fraction of the cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Machine Learning Pipeline to Examine Political Bias with Congressional Speeches. (arXiv:2109.09014v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09014">
<div class="article-summary-box-inner">
<span><p>Computational methods to model political bias in social media involve several
challenges due to heterogeneity, high-dimensional, multiple modalities, and the
scale of the data. Political bias in social media has been studied in multiple
viewpoints like media bias, political ideology, echo chambers, and
controversies using machine learning pipelines. Most of the current methods
rely heavily on the manually-labeled ground-truth data for the underlying
political bias prediction tasks. Limitations of such methods include
human-intensive labeling, labels related to only a specific problem, and the
inability to determine the near future bias state of a social media
conversation. In this work, we address such problems and give machine learning
approaches to study political bias in two ideologically diverse social media
forums: Gab and Twitter without the availability of human-annotated data. Our
proposed methods exploit the use of transcripts collected from political
speeches in US congress to label the data and achieve the highest accuracy of
70.5% and 65.1% in Twitter and Gab data respectively to predict political bias.
We also present a machine learning approach that combines features from
cascades and text to forecast cascade's political bias with an accuracy of
about 85%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hierarchical Relation-Guided Type-Sentence Alignment for Long-Tail Relation Extraction with Distant Supervision. (arXiv:2109.09036v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09036">
<div class="article-summary-box-inner">
<span><p>Distant supervision uses triple facts in knowledge graphs to label a corpus
for relation extraction, leading to wrong labeling and long-tail problems. Some
works use the hierarchy of relations for knowledge transfer to long-tail
relations. However, a coarse-grained relation often implies only an attribute
(e.g., domain or topic) of the distant fact, making it hard to discriminate
relations based solely on sentence semantics. One solution is resorting to
entity types, but open questions remain about how to fully leverage the
information of entity types and how to align multi-granular entity types with
sentences. In this work, we propose a novel model to enrich
distantly-supervised sentences with entity types. It consists of (1) a pairwise
type-enriched sentence encoding module injecting both context-free and -related
backgrounds to alleviate sentence-level wrong labeling, and (2) a hierarchical
type-sentence alignment module enriching a sentence with the triple fact's
basic attributes to support long-tail relations. Our model achieves new
state-of-the-art results in overall and long-tail performance on benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Enhanced Evidence Retrieval for Counterargument Generation. (arXiv:2109.09057v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09057">
<div class="article-summary-box-inner">
<span><p>Finding counterevidence to statements is key to many tasks, including
counterargument generation. We build a system that, given a statement,
retrieves counterevidence from diverse sources on the Web. At the core of this
system is a natural language inference (NLI) model that determines whether a
candidate sentence is valid counterevidence or not. Most NLI models to date,
however, lack proper reasoning abilities necessary to find counterevidence that
involves complex inference. Thus, we present a knowledge-enhanced NLI model
that aims to handle causality- and example-based inference by incorporating
knowledge graphs. Our NLI model outperforms baselines for NLI tasks, especially
for instances that require the targeted inference. In addition, this NLI model
further improves the counterevidence retrieval system, notably finding complex
counterevidence better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Training with Contrastive Learning in NLP. (arXiv:2109.09075v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09075">
<div class="article-summary-box-inner">
<span><p>For years, adversarial training has been extensively studied in natural
language processing (NLP) settings. The main goal is to make models robust so
that similar inputs derive in semantically similar outcomes, which is not a
trivial problem since there is no objective measure of semantic similarity in
language. Previous works use an external pre-trained NLP model to tackle this
challenge, introducing an extra training stage with huge memory consumption
during training. However, the recent popular approach of contrastive learning
in language processing hints a convenient way of obtaining such similarity
restrictions. The main advantage of the contrastive learning approach is that
it aims for similar data points to be mapped close to each other and further
from different ones in the representation space. In this work, we propose
adversarial training with contrastive learning (ATCL) to adversarially train a
language processing task using the benefits of contrastive learning. The core
idea is to make linear perturbations in the embedding space of the input via
fast gradient methods (FGM) and train the model to keep the original and
perturbed representations close via contrastive learning. In NLP experiments,
we applied ATCL to language modeling and neural machine translation tasks. The
results show not only an improvement in the quantitative (perplexity and BLEU)
scores when compared to the baselines, but ATCL also achieves good qualitative
results in the semantic level for both tasks without using a pre-trained model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09105">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) have been ubiquitously leveraged in various tasks
including spoken language understanding (SLU). Spoken language requires careful
understanding of speaker interactions, dialog states and speech induced
multimodal behaviors to generate a meaningful representation of the
conversation.In this work, we propose to dissect SLU into three representative
properties:conversational(disfluency, pause, overtalk), channel(speaker-type,
turn-tasks) andASR(insertion, deletion,substitution). We probe BERT based
language models (BERT, RoBERTa) trained on spoken transcripts to investigate
its ability to understand multifarious properties in absence of any speech
cues. Empirical results indicate that LM is surprisingly good at capturing
conversational properties such as pause prediction and overtalk detection from
lexical tokens. On the downsides, the LM scores low on turn-tasks and ASR
errors predictions. Additionally, pre-training the LM on spoken transcripts
restrain its linguistic understanding. Finally,we establish the efficacy and
transferability of the mentioned properties on two benchmark datasets:
Switchboard Dialog Act and Disfluency datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Long-Range Language Models Actually Use Long-Range Context?. (arXiv:2109.09115v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09115">
<div class="article-summary-box-inner">
<span><p>Language models are generally trained on short, truncated input sequences,
which limits their ability to use discourse-level information present in
long-range context to improve their predictions. Recent efforts to improve the
efficiency of self-attention have led to a proliferation of long-range
Transformer language models, which can process much longer sequences than
models of the past. However, the ways in which such models take advantage of
the long-range context remain unclear. In this paper, we perform a fine-grained
analysis of two long-range Transformer language models (including the
\emph{Routing Transformer}, which achieves state-of-the-art perplexity on the
PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to
8K tokens. Our results reveal that providing long-range context (i.e., beyond
the previous 2K tokens) to these models only improves their predictions on a
small set of tokens (e.g., those that can be copied from the distant context)
and does not help at all for sentence-level prediction tasks. Finally, we
discover that PG-19 contains a variety of different document types and domains,
and that long-range context helps most for literary novels (as opposed to
textbooks or magazines).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Preventing Author Profiling through Zero-Shot Multilingual Back-Translation. (arXiv:2109.09133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09133">
<div class="article-summary-box-inner">
<span><p>Documents as short as a single sentence may inadvertently reveal sensitive
information about their authors, including e.g. their gender or ethnicity.
Style transfer is an effective way of transforming texts in order to remove any
information that enables author profiling. However, for a number of current
state-of-the-art approaches the improved privacy is accompanied by an
undesirable drop in the down-stream utility of the transformed data. In this
paper, we propose a simple, zero-shot way to effectively lower the risk of
author profiling through multilingual back-translation using off-the-shelf
translation models. We compare our models with five representative text style
transfer models on three datasets across different domains. Results from both
an automatic and a human evaluation show that our approach achieves the best
overall performance while requiring no training data. We are able to lower the
adversarial prediction of gender and race by up to $22\%$ while retaining
$95\%$ of the original utility on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition. (arXiv:2109.09161v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09161">
<div class="article-summary-box-inner">
<span><p>Unifying acoustic and linguistic representation learning has become
increasingly crucial to transfer the knowledge learned on the abundance of
high-resource language data for low-resource speech recognition. Existing
approaches simply cascade pre-trained acoustic and language models to learn the
transfer from speech to text. However, how to solve the representation
discrepancy of speech and text is unexplored, which hinders the utilization of
acoustic and linguistic information. Moreover, previous works simply replace
the embedding layer of the pre-trained language model with the acoustic
features, which may cause the catastrophic forgetting problem. In this work, we
introduce Wav-BERT, a cooperative acoustic and linguistic representation
learning method to fuse and utilize the contextual information of speech and
text. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a
language model (BERT) into an end-to-end trainable framework. A Representation
Aggregation Module is designed to aggregate acoustic and linguistic
representation, and an Embedding Attention Module is introduced to incorporate
acoustic information into BERT, which can effectively facilitate the
cooperation of two pre-trained models and thus boost the representation
learning. Extensive experiments show that our Wav-BERT significantly
outperforms the existing approaches and achieves state-of-the-art performance
on low-resource speech recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FST Morphological Analyser and Generator for Mapud\"ungun. (arXiv:2109.09176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09176">
<div class="article-summary-box-inner">
<span><p>Following the Mapuche grammar by Smeets, this article describes the main
morphophonological aspects of Mapud\"ungun, explaining what triggers them and
the contexts where they arise. We present a computational approach producing a
finite state morphological analyser (and generator) capable of classifying and
appropriately tagging all the components (roots and suffixes) that interact in
a Mapuche word form. The bulk of the article focuses on presenting details
about the morphology of Mapud\"ungun verb and its formalisation using FOMA. A
system evaluation process and its results are also present in this article.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Training Dynamic based data filtering may not work for NLP datasets. (arXiv:2109.09191v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09191">
<div class="article-summary-box-inner">
<span><p>The recent increase in dataset size has brought about significant advances in
natural language understanding. These large datasets are usually collected
through automation (search engines or web crawlers) or crowdsourcing which
inherently introduces incorrectly labeled data. Training on these datasets
leads to memorization and poor generalization. Thus, it is pertinent to develop
techniques that help in the identification and isolation of mislabelled data.
In this paper, we study the applicability of the Area Under the Margin (AUM)
metric to identify and remove/rectify mislabelled examples in NLP datasets. We
find that mislabelled samples can be filtered using the AUM metric in NLP
datasets but it also removes a significant number of correctly labeled points
and leads to the loss of a large amount of relevant language information. We
show that models rely on the distributional information instead of relying on
syntactic and semantic representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Zero-Label Language Learning. (arXiv:2109.09193v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09193">
<div class="article-summary-box-inner">
<span><p>This paper explores zero-label learning in Natural Language Processing (NLP),
whereby no human-annotated data is used anywhere during training and models are
trained purely on synthetic data. At the core of our framework is a novel
approach for better leveraging the powerful pretrained language models.
Specifically, inspired by the recent success of few-shot inference on GPT-3, we
present a training data creation procedure named Unsupervised Data Generation
(UDG), which leverages few-shot prompts to synthesize high-quality training
data without real human annotations. Our method enables zero-label learning as
we train task-specific models solely on the synthetic data, yet we achieve
better or comparable results from strong baseline models trained on
human-labeled data. Furthermore, when mixed with labeled data, our approach
serves as a highly effective data augmentation procedure, achieving new
state-of-the-art results on the SuperGLUE benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Crowdsourcing Protocols for Evaluatingthe Factual Consistency of Summaries. (arXiv:2109.09195v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09195">
<div class="article-summary-box-inner">
<span><p>Current pre-trained models applied to summarization are prone to factual
inconsistencies which either misrepresent the source text or introduce
extraneous information. Thus, comparing the factual consistency of summaries is
necessary as we develop improved models. However, the optimal human evaluation
setup for factual consistency has not been standardized. To address this issue,
we crowdsourced evaluations for factual consistency using the rating-based
Likert scale and ranking-based Best-Worst Scaling protocols, on 100 articles
from each of the CNN-Daily Mail and XSum datasets over four state-of-the-art
models, to determine the most reliable evaluation framework. We find that
ranking-based protocols offer a more reliable measure of summary quality across
datasets, while the reliability of Likert ratings depends on the target dataset
and the evaluation design. Our crowdsourcing templates and summary evaluations
will be publicly available to facilitate future research on factual consistency
in summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization. (arXiv:2109.09209v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09209">
<div class="article-summary-box-inner">
<span><p>We study generating abstractive summaries that are faithful and factually
consistent with the given articles. A novel contrastive learning formulation is
presented, which leverages both reference summaries, as positive training data,
and automatically generated erroneous summaries, as negative training data, to
train summarization systems that are better at distinguishing between them. We
further design four types of strategies for creating negative samples, to
resemble errors made commonly by two state-of-the-art models, BART and PEGASUS,
found in our new human annotations of summary errors. Experiments on XSum and
CNN/Daily Mail show that our contrastive learning framework is robust across
datasets and models. It consistently produces more factual summaries than
strong comparisons with post error correction, entailment-based reranking, and
unlikelihood training, according to QA-based factuality evaluation. Human
judges echo the observation and find that our model summaries correct more
errors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UPV at CheckThat! 2021: Mitigating Cultural Differences for Identifying Multilingual Check-worthy Claims. (arXiv:2109.09232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09232">
<div class="article-summary-box-inner">
<span><p>Identifying check-worthy claims is often the first step of automated
fact-checking systems. Tackling this task in a multilingual setting has been
understudied. Encoding inputs with multilingual text representations could be
one approach to solve the multilingual check-worthiness detection. However,
this approach could suffer if cultural bias exists within the communities on
determining what is check-worthy.In this paper, we propose a language
identification task as an auxiliary task to mitigate unintended bias.With this
purpose, we experiment joint training by using the datasets from CLEF-2021
CheckThat!, that contain tweets in English, Arabic, Bulgarian, Spanish and
Turkish. Our results show that joint training of language identification and
check-worthy claim detection tasks can provide performance gains for some of
the selected languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified and Multilingual Author Profiling for Detecting Haters. (arXiv:2109.09233v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09233">
<div class="article-summary-box-inner">
<span><p>This paper presents a unified user profiling framework to identify hate
speech spreaders by processing their tweets regardless of the language. The
framework encodes the tweets with sentence transformers and applies an
attention mechanism to select important tweets for learning user profiles.
Furthermore, the attention layer helps to explain why a user is a hate speech
spreader by producing attention weights at both token and post level. Our
proposed model outperformed the state-of-the-art multilingual transformer
models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional probing: measuring usable information beyond a baseline. (arXiv:2109.09234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09234">
<div class="article-summary-box-inner">
<span><p>Probing experiments investigate the extent to which neural representations
make properties -- like part-of-speech -- predictable. One suggests that a
representation encodes a property if probing that representation produces
higher accuracy than probing a baseline representation like non-contextual word
embeddings. Instead of using baselines as a point of comparison, we're
interested in measuring information that is contained in the representation but
not in the baseline. For example, current methods can detect when a
representation is more useful than the word identity (a baseline) for
predicting part-of-speech; however, they cannot detect when the representation
is predictive of just the aspects of part-of-speech not explainable by the word
identity. In this work, we extend a theory of usable information called
$\mathcal{V}$-information and propose conditional probing, which explicitly
conditions on the information in the baseline. In a case study, we find that
after conditioning on non-contextual word embeddings, properties like
part-of-speech are accessible at deeper layers of a network than previously
thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models. (arXiv:2109.09237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09237">
<div class="article-summary-box-inner">
<span><p>Recent work indicated that pretrained language models (PLMs) such as BERT and
RoBERTa can be transformed into effective sentence and word encoders even via
simple self-supervised techniques. Inspired by this line of work, in this paper
we propose a fully unsupervised approach to improving word-in-context (WiC)
representations in PLMs, achieved via a simple and efficient WiC-targeted
fine-tuning procedure: MirrorWiC. The proposed method leverages only raw texts
sampled from Wikipedia, assuming no sense-annotated data, and learns
context-aware word representations within a standard contrastive learning
setup. We experiment with a series of standard and comprehensive WiC benchmarks
across multiple languages. Our proposed fully unsupervised MirrorWiC models
obtain substantial gains over off-the-shelf PLMs across all monolingual,
multilingual and cross-lingual setups. Moreover, on some standard WiC
benchmarks, MirrorWiC is even on-par with supervised models fine-tuned with
in-task data and sense labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Measuring the Volatility of the Political agenda in Public Opinion and News Media. (arXiv:1808.09037v2 [cs.CY] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1808.09037">
<div class="article-summary-box-inner">
<span><p>Recent election surprises, regime changes, and political shocks indicate that
political agendas have become more fast-moving and volatile. The ability to
measure the complex dynamics of agenda change and capture the nature and extent
of volatility in political systems is therefore more crucial than ever before.
This study proposes a definition and operationalization of volatility that
combines insights from political science, communications, information theory,
and computational techniques. The proposed measures of fractionalization and
agenda change encompass the shifting salience of issues in the agenda as a
whole and allow the study of agendas across different domains. We evaluate
these metrics and compare them to other measures such as issue-level survival
rates and the Pedersen Index, which uses public-opinion poll data to measure
public agendas, as well as traditional media content to measure media agendas
in the UK and Germany. We show how these measures complement existing
approaches and could be employed in future agenda-setting research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NatCat: Weakly Supervised Text Classification with Naturally Annotated Resources. (arXiv:2009.14335v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14335">
<div class="article-summary-box-inner">
<span><p>We describe NatCat, a large-scale resource for text classification
constructed from three data sources: Wikipedia, Stack Exchange, and Reddit.
NatCat consists of document-category pairs derived from manual curation that
occurs naturally within online communities. To demonstrate its usefulness, we
build general purpose text classifiers by training on NatCat and evaluate them
on a suite of 11 text classification tasks (CatEval), reporting large
improvements compared to prior work. We benchmark different modeling choices
and resource combinations and show how tasks benefit from particular NatCat
data sources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exemplar-Controllable Paraphrasing and Translation using Bitext. (arXiv:2010.05856v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05856">
<div class="article-summary-box-inner">
<span><p>Most prior work on exemplar-based syntactically controlled paraphrase
generation relies on automatically-constructed large-scale paraphrase datasets,
which are costly to create. We sidestep this prerequisite by adapting models
from prior work to be able to learn solely from bilingual text (bitext).
Despite only using bitext for training, and in near zero-shot conditions, our
single proposed model can perform four tasks: controlled paraphrase generation
in both languages and controlled machine translation in both language
directions. To evaluate these tasks quantitatively, we create three novel
evaluation datasets. Our experimental results show that our models achieve
competitive results on controlled paraphrase generation and strong performance
on controlled machine translation. Analysis shows that our models learn to
disentangle semantics and syntax in their latent representations, but still
suffer from semantic drift.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can images help recognize entities? A study of the role of images for Multimodal NER. (arXiv:2010.12712v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12712">
<div class="article-summary-box-inner">
<span><p>Multimodal named entity recognition (MNER) requires to bridge the gap between
language understanding and visual context. While many multimodal neural
techniques have been proposed to incorporate images into the MNER task, the
model's ability to leverage multimodal interactions remains poorly understood.
In this work, we conduct in-depth analyses of existing multimodal fusion
techniques from different perspectives and describe the scenarios where adding
information from the image does not always boost performance. We also study the
use of captions as a way to enrich the context for MNER. Experiments on three
datasets from popular social platforms expose the bottleneck of existing
multimodal models and the situations where using captions is beneficial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08091">
<div class="article-summary-box-inner">
<span><p>Sentiment quantification is the task of training, by means of supervised
learning, estimators of the relative frequency (also called ``prevalence'') of
sentiment-related classes (such as \textsf{Positive}, \textsf{Neutral},
\textsf{Negative}) in a sample of unlabelled texts. This task is especially
important when these texts are tweets, since the final goal of most sentiment
classification efforts carried out on Twitter data is actually quantification
(and not the classification of individual tweets). It is well-known that
solving quantification by means of ``classify and count'' (i.e., by classifying
all unlabelled items by means of a standard classifier and counting the items
that have been assigned to a given class) is less than optimal in terms of
accuracy, and that more accurate quantification methods exist. Gao and
Sebastiani (2016) carried out a systematic comparison of quantification methods
on the task of tweet sentiment quantification. In hindsight, we observe that
the experimental protocol followed in that work was weak, and that the
reliability of the conclusions that were drawn from the results is thus
questionable. We now re-evaluate those quantification methods (plus a few more
modern ones) on exactly the same same datasets, this time following a now
consolidated and much more robust experimental protocol (which also involves
simulating the presence, in the test data, of class prevalence values very
different from those of the training set). This experimental protocol (even
without counting the newly added methods) involves a number of experiments
5,775 times larger than that of the original study. The results of our
experiments are dramatically different from those obtained by Gao and
Sebastiani, and they provide a different, much more solid understanding of the
relative strengths and weaknesses of different sentiment quantification
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned. (arXiv:2101.00133v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00133">
<div class="article-summary-box-inner">
<span><p>We review the EfficientQA competition from NeurIPS 2020. The competition
focused on open-domain question answering (QA), where systems take natural
language questions as input and return natural language answers. The aim of the
competition was to build systems that can predict correct answers while also
satisfying strict on-disk memory budgets. These memory budgets were designed to
encourage contestants to explore the trade-off between storing retrieval
corpora or the parameters of learned models. In this report, we describe the
motivation and organization of the competition, review the best submissions,
and analyze system predictions to inform a discussion of evaluation for
open-domain QA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Size Does Not Fit All: Finding the Optimal Subword Sizes for FastText Models across Languages. (arXiv:2102.02585v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02585">
<div class="article-summary-box-inner">
<span><p>Unsupervised representation learning of words from large multilingual corpora
is useful for downstream tasks such as word sense disambiguation, semantic text
similarity, and information retrieval. The representation precision of
log-bilinear fastText models is mostly due to their use of subword information.
In previous work, the optimization of fastText's subword sizes has not been
fully explored, and non-English fastText models were trained using subword
sizes optimized for English and German word analogy tasks. In our work, we find
the optimal subword sizes on the English, German, Czech, Italian, Spanish,
French, Hindi, Turkish, and Russian word analogy tasks. We then propose a
simple n-gram coverage model and we show that it predicts better-than-default
subword sizes on the Spanish, French, Hindi, Turkish, and Russian word analogy
tasks. We show that the optimization of fastText's subword sizes matters and
results in a 14% improvement on the Czech word analogy task. We also show that
expensive parameter optimization can be replaced by a simple n-gram coverage
model that consistently improves the accuracy of fastText models on the word
analogy tasks by up to 3% compared to the default subword sizes, and that it is
within 1% accuracy of the optimal subword sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Post-Processing Technique for Improving Readability Assessment of Texts using Word Mover's Distance. (arXiv:2103.07277v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07277">
<div class="article-summary-box-inner">
<span><p>Assessing the proper difficulty levels of reading materials or texts in
general is the first step towards effective comprehension and learning. In this
study, we improve the conventional methodology of automatic readability
assessment by incorporating the Word Mover's Distance (WMD) of ranked texts as
an additional post-processing technique to further ground the difficulty level
given by a model. Results of our experiments on three multilingual datasets in
Filipino, German, and English show that the post-processing technique
outperforms previous vanilla and ranking-based models using SVM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning Pretrained Transformers into RNNs. (arXiv:2103.13076v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13076">
<div class="article-summary-box-inner">
<span><p>Transformers have outperformed recurrent neural networks (RNNs) in natural
language generation. But this comes with a significant computational cost, as
the attention mechanism's complexity scales quadratically with sequence length.
Efficient transformer variants have received increasing interest in recent
works. Among them, a linear-complexity recurrent variant has proven well suited
for autoregressive generation. It approximates the softmax attention with
randomized or heuristic feature maps, but can be difficult to train and may
yield suboptimal accuracy. This work aims to convert a pretrained transformer
into its efficient recurrent counterpart, improving efficiency while
maintaining accuracy. Specifically, we propose a swap-then-finetune procedure:
in an off-the-shelf pretrained transformer, we replace the softmax attention
with its linear-complexity recurrent alternative and then finetune. With a
learned feature map, our approach provides an improved tradeoff between
efficiency and accuracy over the standard transformer and other recurrent
variants. We also show that the finetuning process has lower training cost
relative to training these recurrent variants from scratch. As many models for
natural language tasks are increasingly dependent on large-scale pretrained
transformers, this work presents a viable approach to improving inference
efficiency without repeating the expensive pretraining process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">No Keyword is an Island: In search of covert associations. (arXiv:2103.17114v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17114">
<div class="article-summary-box-inner">
<span><p>This paper describes how corpus-assisted discourse analysis based on keyword
(KW) identification and interpretation can benefit from employing Market basket
analysis (MBA) after KW extraction. MBA is a data mining technique used
originally in marketing that can reveal consistent associations between items
in a shopping cart, but also between keywords in a corpus of many texts. By
identifying recurring associations between KWs we can compensate for the lack
of wider context which is a major issue impeding the interpretation of isolated
KWs (esp. when analyzing large data). To showcase the advantages of MBA in
"re-contextualizing" keywords within the discourse, a pilot study on the topic
of migration was conducted contrasting anti-system and center-right Czech
internet media. was conducted. The results show that MBA is useful in
identifying the dominant strategy of anti-system news portals: to weave in a
confounding ideological undercurrent and connect the concept of migrants to a
multitude of other topics (i.e., flooding the discourse).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SYSML: StYlometry with Structure and Multitask Learning: Implications for Darknet Forum Migrant Analysis. (arXiv:2104.00764v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00764">
<div class="article-summary-box-inner">
<span><p>Darknet market forums are frequently used to exchange illegal goods and
services between parties who use encryption to conceal their identities. The
Tor network is used to host these markets, which guarantees additional
anonymization from IP and location tracking, making it challenging to link
across malicious users using multiple accounts (sybils). Additionally, users
migrate to new forums when one is closed, making it difficult to link users
across multiple forums. We develop a novel stylometry-based multitask learning
approach for natural language and interaction modeling using graph embeddings
to construct low-dimensional representations of short episodes of user activity
for authorship attribution. We provide a comprehensive evaluation of our
methods across four different darknet forums demonstrating its efficacy over
the state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X
on Recall@10.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IGA : An Intent-Guided Authoring Assistant. (arXiv:2104.07000v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07000">
<div class="article-summary-box-inner">
<span><p>While large-scale pretrained language models have significantly improved
writing assistance functionalities such as autocomplete, more complex and
controllable writing assistants have yet to be explored. We leverage advances
in language modeling to build an interactive writing assistant that generates
and rephrases text according to fine-grained author specifications. Users
provide input to our Intent-Guided Assistant (IGA) in the form of text
interspersed with tags that correspond to specific rhetorical directives (e.g.,
adding description or contrast, or rephrasing a particular sentence). We
fine-tune a language model on a dataset heuristically-labeled with author
intent, which allows IGA to fill in these tags with generated text that users
can subsequently edit to their liking. A series of automatic and crowdsourced
evaluations confirm the quality of IGA's generated outputs, while a small-scale
user study demonstrates author preference for IGA over baseline methods in a
creative writing task. We release our dataset, code, and demo to spur further
research into AI-assisted writing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Alignment-Agnostic Model for Chinese Text Error Correction. (arXiv:2104.07190v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07190">
<div class="article-summary-box-inner">
<span><p>This paper investigates how to correct Chinese text errors with types of
mistaken, missing and redundant characters, which is common for Chinese native
speakers. Most existing models based on detect-correct framework can correct
mistaken characters errors, but they cannot deal with missing or redundant
characters. The reason is that lengths of sentences before and after correction
are not the same, leading to the inconsistence between model inputs and
outputs. Although the Seq2Seq-based or sequence tagging methods provide
solutions to the problem and achieved relatively good results on English
context, but they do not perform well in Chinese context according to our
experimental results. In our work, we propose a novel detect-correct framework
which is alignment-agnostic, meaning that it can handle both text aligned and
non-aligned occasions, and it can also serve as a cold start model when there
are no annotated data provided. Experimental results on three datasets
demonstrate that our method is effective and achieves the best performance
among existing published models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multivalent Entailment Graphs for Question Answering. (arXiv:2104.07846v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07846">
<div class="article-summary-box-inner">
<span><p>Drawing inferences between open-domain natural language predicates is a
necessity for true language understanding. There has been much progress in
unsupervised learning of entailment graphs for this purpose. We make three
contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to
model entailment between predicates of different valencies, like DEFEAT(Biden,
Trump) entails WIN(Biden); (2) we actualize this theory by learning
unsupervised Multivalent Entailment Graphs of open-domain predicates; and (3)
we demonstrate the capabilities of these graphs on a novel question answering
task. We show that directional entailment is more helpful for inference than
bidirectional similarity on questions of fine-grained semantics. We also show
that drawing on evidence across valencies answers more questions than by using
only the same valency evidence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Probing Across Time: What Does RoBERTa Know and When?. (arXiv:2104.07885v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07885">
<div class="article-summary-box-inner">
<span><p>Models of language trained on very large corpora have been demonstrated
useful for NLP. As fixed artifacts, they have become the object of intense
study, with many researchers "probing" the extent to which linguistic
abstractions, factual and commonsense knowledge, and reasoning abilities they
acquire and readily demonstrate. Building on this line of work, we consider a
new question: for types of knowledge a language model learns, when during
(pre)training are they acquired? We plot probing performance across iterations,
using RoBERTa as a case study. Among our findings: linguistic knowledge is
acquired fast, stably, and robustly across domains. Facts and commonsense are
slower and more domain-sensitive. Reasoning abilities are, in general, not
stably acquired. As new datasets, pretraining protocols, and probes emerge, we
believe that probing-across-time analyses can help researchers understand the
complex, intermingled learning that these models undergo and guide us toward
more efficient approaches that accomplish necessary learning faster.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models are Few-Shot Butlers. (arXiv:2104.07972v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07972">
<div class="article-summary-box-inner">
<span><p>Pretrained language models demonstrate strong performance in most NLP tasks
when fine-tuned on small task-specific datasets. Hence, these autoregressive
models constitute ideal agents to operate in text-based environments where
language understanding and generative capabilities are essential. Nonetheless,
collecting expert demonstrations in such environments is a time-consuming
endeavour. We introduce a two-stage procedure to learn from a small set of
demonstrations and further improve by interacting with an environment. We show
that language models fine-tuned with only 1.2% of the expert demonstrations and
a simple reinforcement learning algorithm achieve a 51% absolute improvement in
success rate over existing methods in the ALFWorld environment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint Passage Ranking for Diverse Multi-Answer Retrieval. (arXiv:2104.08445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08445">
<div class="article-summary-box-inner">
<span><p>We study multi-answer retrieval, an under-explored problem that requires
retrieving passages to cover multiple distinct answers for a given question.
This task requires joint modeling of retrieved passages, as models should not
repeatedly retrieve passages containing the same answer at the cost of missing
a different valid answer. In this paper, we introduce JPR, the first joint
passage retrieval model for multi-answer retrieval. JPR makes use of an
autoregressive reranker that selects a sequence of passages, each conditioned
on previously selected passages. JPR is trained to select passages that cover
new answers at each timestep and uses a tree-decoding algorithm to enable
flexibility in the degree of diversity. Compared to prior approaches, JPR
achieves significantly better answer coverage on three multi-answer datasets.
When combined with downstream question answering, the improved retrieval
enables larger answer generation models since they need to consider fewer
passages, establishing a new state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Context-Aware Interaction Network for Question Matching. (arXiv:2104.08451v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08451">
<div class="article-summary-box-inner">
<span><p>Impressive milestones have been achieved in text matching by adopting a
cross-attention mechanism to capture pertinent semantic connections between two
sentence representations. However, regular cross-attention focuses on
word-level links between the two input sequences, neglecting the importance of
contextual information. We propose a context-aware interaction network (COIN)
to properly align two sequences and infer their semantic relationship.
Specifically, each interaction block includes (1) a context-aware
cross-attention mechanism to effectively integrate contextual information when
aligning two sequences, and (2) a gate fusion layer to flexibly interpolate
aligned representations. We apply multiple stacked interaction blocks to
produce alignments at different levels and gradually refine the attention
results. Experiments on two question matching datasets and detailed analyses
demonstrate the effectiveness of our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages with Adversarial Examples. (arXiv:2104.08639v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08639">
<div class="article-summary-box-inner">
<span><p>Capturing word meaning in context and distinguishing between correspondences
and variations across languages is key to building successful multilingual and
cross-lingual text representation models. However, existing multilingual
evaluation datasets that evaluate lexical semantics "in-context" have various
limitations. In particular, 1) their language coverage is restricted to
high-resource languages and skewed in favor of only a few language families and
areas, 2) a design that makes the task solvable via superficial cues, which
results in artificially inflated (and sometimes super-human) performances of
pretrained encoders, on many target languages, which limits their usefulness
for model probing and diagnostics, and 3) little support for cross-lingual
evaluation. In order to address these gaps, we present AM2iCo (Adversarial and
Multilingual Meaning in Context), a wide-coverage cross-lingual and
multilingual evaluation set; it aims to faithfully assess the ability of
state-of-the-art (SotA) representation models to understand the identity of
word meaning in cross-lingual contexts for 14 language pairs. We conduct a
series of experiments in a wide range of setups and demonstrate the challenging
nature of AM2iCo. The results reveal that current SotA pretrained encoders
substantially lag behind human performance, and the largest gaps are observed
for low-resource languages and languages dissimilar to English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Answers with Entailment Trees. (arXiv:2104.08661v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08661">
<div class="article-summary-box-inner">
<span><p>Our goal, in the context of open-domain textual question-answering (QA), is
to explain answers by showing the line of reasoning from what is known to the
answer, rather than simply showing a fragment of textual evidence (a
"rationale'"). If this could be done, new opportunities for understanding and
debugging the system's reasoning become possible. Our approach is to generate
explanations in the form of entailment trees, namely a tree of multipremise
entailment steps from facts that are known, through intermediate conclusions,
to the hypothesis of interest (namely the question + answer). To train a model
with this skill, we created ENTAILMENTBANK, the first dataset to contain
multistep entailment trees. Given a hypothesis (question + answer), we define
three increasingly difficult explanation tasks: generate a valid entailment
tree given (a) all relevant sentences (b) all relevant and some irrelevant
sentences, or (c) a corpus. We show that a strong language model can partially
solve these tasks, in particular when the relevant sentences are included in
the input (e.g., 35% of trees for (a) are perfect), and with indications of
generalization to other domains. This work is significant as it provides a new
type of dataset (multistep entailments) and baselines, offering a new avenue
for the community to generate richer, more systematic explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting. (arXiv:2104.09691v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09691">
<div class="article-summary-box-inner">
<span><p>Since the seminal work of Mikolov et al. (2013a) and Bojanowski et al.
(2017), word representations of shallow log-bilinear language models have found
their way into many NLP applications. Mikolov et al. (2018) introduced a
positional log-bilinear language model, which has characteristics of an
attention-based language model and which has reached state-of-the-art
performance on the intrinsic word analogy task. However, the positional model
has never been evaluated on qualitative criteria or extrinsic tasks and its
speed is impractical.
</p>
<p>We outline the similarities between the attention mechanism and the
positional model, and we propose a constrained positional model, which adapts
the sparse attention mechanism of Dai et al. (2018). We evaluate the positional
and constrained positional models on three novel qualitative criteria and on
the extrinsic language modeling task of Botha and Blunsom (2014).
</p>
<p>We show that the positional and constrained positional models contain
interpretable information about word order and outperform the subword model of
Bojanowski et al. (2017) on language modeling. We also show that the
constrained positional model outperforms the positional model on language
modeling and is twice as fast.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRILL: Dynamic Representations for Imbalanced Lifelong Learning. (arXiv:2105.08445v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08445">
<div class="article-summary-box-inner">
<span><p>Continual or lifelong learning has been a long-standing challenge in machine
learning to date, especially in natural language processing (NLP). Although
state-of-the-art language models such as BERT have ushered in a new era in this
field due to their outstanding performance in multitask learning scenarios,
they suffer from forgetting when being exposed to a continuous stream of data
with shifting data distributions. In this paper, we introduce DRILL, a novel
continual learning architecture for open-domain text classification. DRILL
leverages a biologically inspired self-organizing neural architecture to
selectively gate latent language representations from BERT in a
task-incremental manner. We demonstrate in our experiments that DRILL
outperforms current methods in a realistic scenario of imbalanced,
non-stationary data without prior knowledge about task boundaries. To the best
of our knowledge, DRILL is the first of its kind to use a self-organizing
neural architecture for open-domain lifelong learning in NLP.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What if This Modified That? Syntactic Interventions via Counterfactual Embeddings. (arXiv:2105.14002v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14002">
<div class="article-summary-box-inner">
<span><p>Neural language models exhibit impressive performance on a variety of tasks,
but their internal reasoning may be difficult to understand. Prior art aims to
uncover meaningful properties within model representations via probes, but it
is unclear how faithfully such probes portray information that the models
actually use. To overcome such limitations, we propose a technique, inspired by
causal analysis, for generating counterfactual embeddings within models. In
experiments testing our technique, we produce evidence that suggests some
BERT-based models use a tree-distance-like representation of syntax in
downstream prediction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"You made me feel this way": Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions using Speech Data. (arXiv:2106.01526v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01526">
<div class="article-summary-box-inner">
<span><p>How romantic partners interact with each other during a conflict influences
how they feel at the end of the interaction and is predictive of whether the
partners stay together in the long term. Hence understanding the emotions of
each partner is important. Yet current approaches that are used include
self-reports which are burdensome and hence limit the frequency of this data
collection. Automatic emotion prediction could address this challenge. Insights
from psychology research indicate that partners' behaviors influence each
other's emotions in conflict interaction and hence, the behavior of both
partners could be considered to better predict each partner's emotion. However,
it is yet to be investigated how doing so compares to only using each partner's
own behavior in terms of emotion prediction performance. In this work, we used
BERT to extract linguistic features (i.e., what partners said) and openSMILE to
extract paralinguistic features (i.e., how they said it) from a data set of 368
German-speaking Swiss couples (N = 736 individuals) who were videotaped during
an 8-minutes conflict interaction in the laboratory. Based on those features,
we trained machine learning models to predict if partners feel positive or
negative after the conflict interaction. Our results show that including the
behavior of the other partner improves the prediction performance. Furthermore,
for men, considering how their female partners spoke is most important and for
women considering what their male partner said is most important in getting
better prediction performance. This work is a step towards automatically
recognizing each partners' emotion based on the behavior of both, which would
enable a better understanding of couples in research, therapy, and the real
world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions. (arXiv:2106.01536v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01536">
<div class="article-summary-box-inner">
<span><p>Many processes in psychology are complex, such as dyadic interactions between
two interacting partners (e.g. patient-therapist, intimate relationship
partners). Nevertheless, many basic questions about interactions are difficult
to investigate because dyadic processes can be within a person and between
partners, they are based on multimodal aspects of behavior and unfold rapidly.
Current analyses are mainly based on the behavioral coding method, whereby
human coders annotate behavior based on a coding schema. But coding is
labor-intensive, expensive, slow, focuses on few modalities. Current approaches
in psychology use LIWC for analyzing couples' interactions. However, advances
in natural language processing such as BERT could enable the development of
systems to potentially automate behavioral coding, which in turn could
substantially improve psychological research. In this work, we train machine
learning models to automatically predict positive and negative communication
behavioral codes of 368 German-speaking Swiss couples during an 8-minute
conflict interaction on a fine-grained scale (10-seconds sequences) using
linguistic features and paralinguistic features derived with openSMILE. Our
results show that both simpler TF-IDF features as well as more complex BERT
features performed better than LIWC, and that adding paralinguistic features
did not improve the performance. These results suggest it might be time to
consider modern alternatives to LIWC, the de facto linguistic features in
psychology, for prediction tasks in couples research. This work is a further
step towards the automated coding of couples' behavior which could enhance
couple research and therapy, and be utilized for other dyadic interactions as
well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Programming Puzzles. (arXiv:2106.05784v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05784">
<div class="article-summary-box-inner">
<span><p>We introduce a new type of programming challenge called programming puzzles,
as an objective and comprehensive evaluation of program synthesis, and release
an open-source dataset of Python Programming Puzzles (P3). Each puzzle is
defined by a short Python program $f$, and the goal is to find an input $x$
which makes $f$ output "True". The puzzles are objective in that each one is
specified entirely by the source code of its verifier $f$, so evaluating $f(x)$
is all that is needed to test a candidate solution $x$. They do not require an
answer key or input/output examples, nor do they depend on natural language
understanding. The dataset is comprehensive in that it spans problems of a
range of difficulties and domains, ranging from trivial string manipulation
problems that are immediately obvious to human programmers (but not necessarily
to AI), to classic programming puzzles (e.g., Towers of Hanoi), to
interview/competitive-programming problems (e.g., dynamic programming), to
longstanding open problems in algorithms and mathematics (e.g., factoring). The
objective nature of P3 readily supports self-supervised bootstrapping. We
develop baseline enumerative program synthesis and GPT-3 solvers that are
capable of solving easy puzzles -- even without access to any reference
solutions -- by learning from their own past solutions. Based on a small user
study, we find puzzle difficulty to correlate between human programmers and the
baseline AI solvers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised and Unsupervised Sense Annotation via Translations. (arXiv:2106.06462v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06462">
<div class="article-summary-box-inner">
<span><p>Acquisition of multilingual training data continues to be a challenge in word
sense disambiguation (WSD). To address this problem, unsupervised approaches
have been proposed to automatically generate sense annotations for training
supervised WSD systems. We present three new methods for creating
sense-annotated corpora which leverage translations, parallel bitexts, lexical
resources, as well as contextual and synset embeddings. Our semi-supervised
method applies machine translation to transfer existing sense annotations to
other languages. Our two unsupervised methods refine sense annotations produced
by a knowledge-based WSD system via lexical translations in a parallel corpus.
We obtain state-of-the-art results on standard WSD benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SAS: Self-Augmented Strategy for Language Model Pre-training. (arXiv:2106.07176v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07176">
<div class="article-summary-box-inner">
<span><p>The core of a self-supervised learning method for pre-training language
models includes the design of appropriate data augmentation and corresponding
pre-training task(s). Most data augmentations in language model pre-training
are context-independent. The seminal contextualized augmentation recently
proposed by the ELECTRA requires a separate generator, which leads to extra
computation cost as well as the challenge in adjusting the capability of its
generator relative to that of the other model component(s). We propose a
self-augmented strategy (SAS) that uses a single forward pass through the model
to augment the input data for model training in the next epoch. Essentially our
strategy eliminates a separate generator network and uses only one network to
generate the data augmentation and undertake two pre-training tasks (the MLM
task and the RTD task) jointly, which naturally avoids the challenge in
adjusting the generator's capability as well as reduces the computation cost.
Additionally, our SAS is a general strategy such that it can seamlessly
incorporate many new techniques emerging recently or in the future, such as the
disentangled attention mechanism recently proposed by the DeBERTa model. Our
experiments show that our SAS is able to outperform the ELECTRA and other
state-of-the-art models in the GLUE tasks with the same or less computation
cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware Fusion. (arXiv:2106.07857v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07857">
<div class="article-summary-box-inner">
<span><p>Generating personalized responses is one of the major challenges in natural
human-robot interaction. Current researches in this field mainly focus on
generating responses consistent with the robot's pre-assigned persona, while
ignoring the user's persona. Such responses may be inappropriate or even
offensive, which may lead to the bad user experience. Therefore, we propose a
bilateral personalized dialogue generation (BPDG) method with dynamic
persona-aware fusion via multi-task transfer learning to generate responses
consistent with both personas. The proposed method aims to accomplish three
learning tasks: 1) an encoder is trained with dialogue utterances added with
corresponded personalized attributes and relative position (language model
task), 2) a dynamic persona-aware fusion module predicts the persona presence
to adaptively fuse the contextual and bilateral personas encodings (persona
prediction task) and 3) a decoder generates natural, fluent and personalized
responses (dialogue generation task). To make the generated responses more
personalized and bilateral persona-consistent, the Conditional Mutual
Information Maximum (CMIM) criterion is adopted to select the final response
from the generated candidates. The experimental results show that the proposed
method outperforms several state-of-the-art methods in terms of both automatic
and manual evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comparative Study of Modular and Joint Approaches for Speaker-Attributed ASR on Monaural Long-Form Audio. (arXiv:2107.02852v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02852">
<div class="article-summary-box-inner">
<span><p>Speaker-attributed automatic speech recognition (SA-ASR) is a task to
recognize "who spoke what" from multi-talker recordings. An SA-ASR system
usually consists of multiple modules such as speech separation, speaker
diarization and ASR. On the other hand, considering the joint optimization, an
end-to-end (E2E) SA-ASR model has recently been proposed with promising results
on simulation data. In this paper, we present our recent study on the
comparison of such modular and joint approaches towards SA-ASR on real monaural
recordings. We develop state-of-the-art SA-ASR systems for both modular and
joint approaches by leveraging large-scale training data, including 75 thousand
hours of ASR training data and the VoxCeleb corpus for speaker representation
learning. We also propose a new pipeline that performs the E2E SA-ASR model
after speaker clustering. Our evaluation on the AMI meeting corpus reveals that
after fine-tuning with a small real data, the joint system performs 8.9--29.9%
better in accuracy compared to the best modular system while the modular system
performs better before such fine-tuning. We also conduct various error analyses
to show the remaining issues for the monaural SA-ASR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08661">
<div class="article-summary-box-inner">
<span><p>We present Translatotron 2, a neural direct speech-to-speech translation
model that can be trained end-to-end. Translatotron 2 consists of a speech
encoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention
module that connects all the previous three components. Experimental results
suggest that Translatotron 2 outperforms the original Translatotron by a large
margin in terms of translation quality and predicted speech naturalness, and
drastically improves the robustness of the predicted speech by mitigating
over-generation, such as babbling or long pause. We also propose a new method
for retaining the source speaker's voice in the translated speech. The trained
model is restricted to retain the source speaker's voice, but unlike the
original Translatotron, it is not able to generate speech in a different
speaker's voice, making the model more robust for production deployment, by
mitigating potential misuse for creating spoofing audio artifacts. When the new
method is used together with a simple concatenation-based data augmentation,
the trained Translatotron 2 model is able to retain each speaker's voice for
input with speaker turns.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural News Recommendation with Collaborative News Encoding and Structural User Encoding. (arXiv:2109.00750v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00750">
<div class="article-summary-box-inner">
<span><p>Automatic news recommendation has gained much attention from the academic
community and industry. Recent studies reveal that the key to this task lies
within the effective representation learning of both news and users. Existing
works typically encode news title and content separately while neglecting their
semantic interaction, which is inadequate for news text comprehension. Besides,
previous models encode user browsing history without leveraging the structural
correlation of user browsed news to reflect user interests explicitly. In this
work, we propose a news recommendation framework consisting of collaborative
news encoding (CNE) and structural user encoding (SUE) to enhance news and user
representation learning. CNE equipped with bidirectional LSTMs encodes news
title and content collaboratively with cross-selection and cross-attention
modules to learn semantic-interactive news representations. SUE utilizes graph
convolutional networks to extract cluster-structural features of user history,
followed by intra-cluster and inter-cluster attention modules to learn
hierarchical user interest representations. Experiment results on the MIND
dataset validate the effectiveness of our model to improve the performance of
news recommendation. Our code is released at
https://github.com/Veason-silverbullet/NNR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers. (arXiv:2109.00799v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00799">
<div class="article-summary-box-inner">
<span><p>Developing automatic Math Word Problem (MWP) solvers has been an interest of
NLP researchers since the 1960s. Over the last few years, there are a growing
number of datasets and deep learning-based methods proposed for effectively
solving MWPs. However, most existing methods are benchmarked soly on one or two
datasets, varying in different configurations, which leads to a lack of
unified, standardized, fair, and comprehensive comparison between methods. This
paper presents MWPToolkit, the first open-source framework for solving MWPs. In
MWPToolkit, we decompose the procedure of existing MWP solvers into multiple
core components and decouple their models into highly reusable modules. We also
provide a hyper-parameter search function to boost the performance. In total,
we implement and compare 17 MWP solvers on 4 widely-used single equation
generation benchmarks and 2 multiple equations generation benchmarks. These
features enable our MWPToolkit to be suitable for researchers to reproduce
advanced baseline models and develop new MWP solvers quickly. Code and
documents are available at https://github.com/LYH-YF/MWPToolkit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02051">
<div class="article-summary-box-inner">
<span><p>Many endeavors have sought to develop countermeasure techniques as
enhancements on Automatic Speaker Verification (ASV) systems, in order to make
them more robust against spoof attacks. As evidenced by the latest ASVspoof
2019 countermeasure challenge, models currently deployed for the task of ASV
are, at their best, devoid of suitable degrees of generalization to unseen
attacks. Upon further investigation of the proposed methods, it appears that a
broader three-tiered view of the proposed systems. comprised of the classifier,
feature extraction phase, and model loss function, may to some extent lessen
the problem. Accordingly, the present study proposes the Efficient Attention
Branch Network (EABN) modular architecture with a combined loss function to
address the generalization problem...
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Language Models with Plug-and-Play Large-Scale Commonsense. (arXiv:2109.02572v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02572">
<div class="article-summary-box-inner">
<span><p>We study how to enhance language models (LMs) with textual commonsense
knowledge. Previous work (e.g., KnowBERT) has focused on the integrating entity
knowledge from knowledge graphs. In order to introduce the external entity
embeddings, they learn to jointly represent the original sentences and external
knowledge by pre-training on a large scale corpus. However, when switching to
textual commonsense, unlike the light entity embeddings, the encoding of
commonsense descriptions is heavy. Therefore, the pre-training for learning to
jointly represent the target sentence and external commonsense descriptions is
unaffordable. On the other hand, since pre-trained LMs for representing the
target sentences alone are readily available, is it feasible to introduce
commonsense knowledge in downstream tasks by fine-tuning them only? In this
paper, we propose a plug-and-play method for large-scale commonsense
integration without pre-training. Our method is inspired by the observation
that in the regular fine-tuning for downstream tasks where no external
knowledge was introduced, the variation in the parameters of the language model
was minor. Our method starts from a pre-trained LM that represents the target
sentences only (e.g., BERT). We think that the pre-training for joint
representation learning can be avoided, if the joint representation reduces the
impact of parameters on the starting LM. Previous methods such as KnowBERT
proposed complex modifications to the vanilla LM to introduce external
knowledge. Our model (Cook-Transformer, COmmOnsense Knowledge-enhanced
Transformer), on the other hand, hardly changes the vanilla LM except adding a
knowledge token in each Transformer layer. In a variety of experiments,
COOK-Transformer-based BERT/RoBERTa improve their effect without any
pre-training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Select One Among All? An Extensive Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding. (arXiv:2109.05696v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05696">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is a model compression algorithm that helps
transfer the knowledge of a large neural network into a smaller one. Even
though KD has shown promise on a wide range of Natural Language Processing
(NLP) applications, little is understood about how one KD algorithm compares to
another and whether these approaches can be complimentary to each other. In
this work, we evaluate various KD algorithms on in-domain, out-of-domain and
adversarial testing. We propose a framework to assess the adversarial
robustness of multiple KD algorithms. Moreover, we introduce a new KD
algorithm, Combined-KD, which takes advantage of two promising approaches
(better training scheme and more efficient data augmentation). Our extensive
experimental results show that Combined-KD achieves state-of-the-art results on
the GLUE benchmark, out-of-domain generalization, and adversarial robustness
compared to competitive methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation. (arXiv:2109.05778v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05778">
<div class="article-summary-box-inner">
<span><p>Open-domain dialogue generation in natural language processing (NLP) is by
default a pure-language task, which aims to satisfy human need for daily
communication on open-ended topics by producing related and informative
responses. In this paper, we point out that hidden images, named as visual
impressions (VIs), can be explored from the text-only data to enhance dialogue
understanding and help generate better responses. Besides, the semantic
dependency between an dialogue post and its response is complicated, e.g., few
word alignments and some topic transitions. Therefore, the visual impressions
of them are not shared, and it is more reasonable to integrate the response
visual impressions (RVIs) into the decoder, rather than the post visual
impressions (PVIs). However, both the response and its RVIs are not given
directly in the test process. To handle the above issues, we propose a
framework to explicitly construct VIs based on pure-language dialogue datasets
and utilize them for better dialogue understanding and generation.
Specifically, we obtain a group of images (PVIs) for each post based on a
pre-trained word-image mapping model. These PVIs are used in a co-attention
encoder to get a post representation with both visual and textual information.
Since the RVIs are not provided directly during testing, we design a cascade
decoder that consists of two sub-decoders. The first sub-decoder predicts the
content words in response, and applies the word-image mapping model to get
those RVIs. Then, the second sub-decoder generates the response based on the
post and RVIs. Experimental results on two open-domain dialogue datasets show
that our proposed approach achieves superior performance over competitive
baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. (arXiv:2109.07102v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07102">
<div class="article-summary-box-inner">
<span><p>There have been many efforts to try to understand what gram-matical knowledge
(e.g., ability to understand the part of speech of a token) is encoded in large
pre-trained language models (LM). This is done through 'Edge Probing' (EP)
tests: simple ML models that predict the grammatical properties ofa span
(whether it has a particular part of speech) using only the LM's token
representations. However, most NLP applications use fine-tuned LMs. Here, we
ask: if a LM is fine-tuned, does the encoding of linguistic information in it
change, as measured by EP tests? Conducting experiments on multiple
question-answering (QA) datasets, we answer that question negatively: the EP
test results do not change significantly when the fine-tuned QA model performs
well or in adversarial situations where the model is forced to learn wrong
correlations. However, a critical analysis of the EP task datasets reveals that
EP models may rely on spurious correlations to make predictions. This indicates
even if fine-tuning changes the encoding of such knowledge, the EP tests might
fail to measure it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07680">
<div class="article-summary-box-inner">
<span><p>Identification of user's opinions from natural language text has become an
exciting field of research due to its growing applications in the real world.
The research field is known as sentiment analysis and classification, where
aspect category detection (ACD) and aspect category polarity (ACP) are two
important sub-tasks of aspect-based sentiment analysis. The goal in ACD is to
specify which aspect of the entity comes up in opinion while ACP aims to
specify the polarity of each aspect category from the ACD task. The previous
works mostly propose separate solutions for these two sub-tasks. This paper
focuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The
proposed method carries out multi-label classification where four different
deep models were employed and comparatively evaluated to examine their
performance. A dataset of Persian reviews was collected from CinemaTicket
website including 2200 samples from 14 categories. The developed models were
evaluated using the collected dataset in terms of example-based and label-based
metrics. The results indicate the high applicability and preference of the CNN
and GRU models in comparison to LSTM and Bi-LSTM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation. (arXiv:2109.07779v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07779">
<div class="article-summary-box-inner">
<span><p>Researches on dialogue empathy aim to endow an agent with the capacity of
accurate understanding and proper responding for emotions. Existing models for
empathetic dialogue generation focus on the emotion flow in one direction, that
is, from the context to response. We argue that conducting an empathetic
conversation is a bidirectional process, where empathy occurs when the emotions
of two interlocutors could converge on the same point, i.e., reaching an
emotion consensus. Besides, we also find that the empathetic dialogue corpus is
extremely limited, which further restricts the model performance. To address
the above issues, we propose a dual-generative model, Dual-Emp, to
simultaneously construct the emotion consensus and utilize some external
unpaired data. Specifically, our model integrates a forward dialogue model, a
backward dialogue model, and a discrete latent variable representing the
emotion consensus into a unified architecture. Then, to alleviate the
constraint of paired data, we extract unpaired emotional data from open-domain
conversations and employ Dual-Emp to produce pseudo paired empathetic samples,
which is more efficient and low-cost than the human annotation. Automatic and
human evaluations demonstrate that our method outperforms competitive baselines
in producing coherent and empathetic responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v2 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08270">
<div class="article-summary-box-inner">
<span><p>Language models (LMs) are sentence-completion engines trained on massive
corpora. LMs have emerged as a significant breakthrough in natural-language
processing, providing capabilities that go far beyond sentence completion
including question answering, summarization, and natural-language inference.
While many of these capabilities have potential application to cognitive
systems, exploiting language models as a source of task knowledge, especially
for task learning, offers significant, near-term benefits. We introduce
language models and the various tasks to which they have been applied and then
review methods of knowledge extraction from language models. The resulting
analysis outlines both the challenges and opportunities for using language
models as a new knowledge source for cognitive systems. It also identifies
possible ways to improve knowledge extraction from language models using the
capabilities provided by cognitive systems. Central to success will be the
ability of a cognitive agent to itself learn an abstract model of the knowledge
implicit in the LM as well as methods to extract high-quality knowledge
effectively and efficiently. To illustrate, we introduce a hypothetical robot
agent and describe how language models could extend its task knowledge and
improve its performance and the kinds of knowledge and methods the agent can
use to exploit the knowledge within a language model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08406">
<div class="article-summary-box-inner">
<span><p>Despite the success of fine-tuning pretrained language encoders like BERT for
downstream natural language understanding (NLU) tasks, it is still poorly
understood how neural networks change after fine-tuning. In this work, we use
centered kernel alignment (CKA), a method for comparing learned
representations, to measure the similarity of representations in task-tuned
models across layers. In experiments across twelve NLU tasks, we discover a
consistent block diagonal structure in the similarity of representations within
fine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of
earlier and later layers, but not between them. The similarity of later layer
representations implies that later layers only marginally contribute to task
performance, and we verify in experiments that the top few layers of fine-tuned
Transformers can be discarded without hurting performance, even with no further
tuning.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-21 23:09:13.526947250 UTC">2021-09-21 23:09:13 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>