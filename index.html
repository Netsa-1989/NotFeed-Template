<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-10-13T01:30:00Z">10-13</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition. (arXiv:2110.05571v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05571">
<div class="article-summary-box-inner">
<span><p>The Transformer architecture has been well adopted as a dominant architecture
in most sequence transduction tasks including automatic speech recognition
(ASR), since its attention mechanism excels in capturing long-range
dependencies. While models built solely upon attention can be better
parallelized than regular RNN, a novel network architecture, SRU++, was
recently proposed. By combining the fast recurrence and attention mechanism,
SRU++ exhibits strong capability in sequence modeling and achieves
near-state-of-the-art results in various language modeling and machine
translation tasks with improved compute efficiency. In this work, we present
the advantages of applying SRU++ in ASR tasks by comparing with Conformer
across multiple ASR benchmarks and study how the benefits can be generalized to
long-form speech inputs. On the popular LibriSpeech benchmark, our SRU++ model
achieves 2.0% / 4.7% WER on test-clean / test-other, showing competitive
performances compared with the state-of-the-art Conformer encoder under the
same set-up. Specifically, SRU++ can surpass Conformer on long-form speech
input with a large margin, based on our analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Spatial Data Mining of Public Transport Incidents reported in Social Media. (arXiv:2110.05573v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05573">
<div class="article-summary-box-inner">
<span><p>Public transport agencies use social media as an essential tool for
communicating mobility incidents to passengers. However, while the short term,
day-to-day information about transport phenomena is usually posted in social
media with low latency, its availability is short term as the content is rarely
made an aggregated form. Social media communication of transport phenomena
usually lacks GIS annotations as most social media platforms do not allow
attaching non-POI GPS coordinates to posts. As a result, the analysis of
transport phenomena information is minimal. We collected three years of social
media posts of a polish public transport company with user comments. Through
exploration, we infer a six-class transport information typology. We
successfully build an information type classifier for social media posts,
detect stop names in posts, and relate them to GPS coordinates, obtaining a
spatial understanding of long-term aggregated phenomena. We show that our
approach enables citizen science and use it to analyze the impact of three
years of infrastructure incidents on passenger mobility, and the sentiment and
reaction scale towards each of the events. All these results are achieved for
Polish, an under-resourced language when it comes to spatial language
understanding, especially in social media contexts. To improve the situation,
we released two of our annotated data sets: social media posts with incident
type labels and matched stop names and social media comments with the annotated
sentiment. We also opensource the experimental codebase.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalizing to New Domains by Mapping Natural Language to Lifted LTL. (arXiv:2110.05603v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05603">
<div class="article-summary-box-inner">
<span><p>Recent work on using natural language to specify commands to robots has
grounded that language to LTL. However, mapping natural language task
specifications to LTL task specifications using language models require
probability distributions over finite vocabulary. Existing state-of-the-art
methods have extended this finite vocabulary to include unseen terms from the
input sequence to improve output generalization. However, novel
out-of-vocabulary atomic propositions cannot be generated using these methods.
To overcome this, we introduce an intermediate contextual query representation
which can be learned from single positive task specification examples,
associating a contextual query with an LTL template. We demonstrate that this
intermediate representation allows for generalization over unseen object
references, assuming accurate groundings are available. We compare our method
of mapping natural language task specifications to intermediate contextual
queries against state-of-the-art CopyNet models capable of translating natural
language to LTL, by evaluating whether correct LTL for manipulation and
navigation task specifications can be output, and show that our method
outperforms the CopyNet model on unseen object references. We demonstrate that
the grounded LTL our method outputs can be used for planning in a simulated
OO-MDP environment. Finally, we discuss some common failure modes encountered
when translating natural language task specifications to grounded LTL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TCube: Domain-Agnostic Neural Time-series Narration. (arXiv:2110.05633v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05633">
<div class="article-summary-box-inner">
<span><p>The task of generating rich and fluent narratives that aptly describe the
characteristics, trends, and anomalies of time-series data is invaluable to the
sciences (geology, meteorology, epidemiology) or finance (trades, stocks, or
sales and inventory). The efforts for time-series narration hitherto are
domain-specific and use predefined templates that offer consistency but lead to
mechanical narratives. We present TCube (Time-series-to-text), a
domain-agnostic neural framework for time-series narration, that couples the
representation of essential time-series elements in the form of a dense
knowledge graph and the translation of said knowledge graph into rich and
fluent narratives through the transfer-learning capabilities of PLMs
(Pre-trained Language Models). TCube's design primarily addresses the challenge
that lies in building a neural framework in the complete paucity of annotated
training data for time-series. The design incorporates knowledge graphs as an
intermediary for the representation of essential time-series elements which can
be linearized for textual translation. To the best of our knowledge, TCube is
the first investigation of the use of neural strategies for time-series
narration. Through extensive evaluations, we show that TCube can improve the
lexical diversity of the generated narratives by up to 65.38% while still
maintaining grammatical integrity. The practicality and deployability of TCube
is further validated through an expert review (n=21) where 76.2% of
participating experts wary of auto-generated narratives favored TCube as a
deployable system for time-series narration due to its richer narratives. Our
code-base, models, and datasets, with detailed instructions for reproducibility
is publicly hosted at https://github.com/Mandar-Sharma/TCube.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learned Construction Grammars Converge Across Registers Given Increased Exposure. (arXiv:2110.05663v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05663">
<div class="article-summary-box-inner">
<span><p>This paper measures the impact of increased exposure on whether learned
construction grammars converge onto shared representations when trained on data
from different registers. Register influences the frequency of constructions,
with some structures common in formal but not informal usage. We expect that a
grammar induction algorithm exposed to different registers will acquire
different constructions. To what degree does increased exposure lead to the
convergence of register-specific grammars? The experiments in this paper
simulate language learning in 12 languages (half Germanic and half Romance)
with corpora representing three registers (Twitter, Wikipedia, Web). These
simulations are repeated with increasing amounts of exposure, from 100k to 2
million words, to measure the impact of exposure on the convergence of
grammars. The results show that increased exposure does lead to converging
grammars across all languages. In addition, a shared core of register-universal
constructions remains constant across increasing amounts of exposure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are you doing what I say? On modalities alignment in ALFRED. (arXiv:2110.05665v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05665">
<div class="article-summary-box-inner">
<span><p>ALFRED is a recently proposed benchmark that requires a model to complete
tasks in simulated house environments specified by instructions in natural
language. We hypothesize that key to success is accurately aligning the text
modality with visual inputs. Motivated by this, we inspect how well existing
models can align these modalities using our proposed intrinsic metric, boundary
adherence score (BAS). The results show the previous models are indeed failing
to perform proper alignment. To address this issue, we introduce approaches
aimed at improving model alignment and demonstrate how improved alignment,
improves end task performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05679">
<div class="article-summary-box-inner">
<span><p>Differentially Private (DP) learning has seen limited success for building
large deep learning models of text, and attempts at straightforwardly applying
Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have
resulted in large performance drops and high computational overhead. We show
that this performance drop can be mitigated with (1) the use of large
pretrained models; (2) hyperparameters that suit DP optimization; and (3)
fine-tuning objectives aligned with the pretraining procedure. With these
factors set right, we obtain private NLP models that outperform
state-of-the-art private training approaches and strong non-private baselines
-- by directly fine-tuning pretrained models with DP optimization on
moderately-sized corpora. To address the computational challenge of running
DP-SGD with large Transformers, we propose a memory saving technique that
allows clipping in DP-SGD to run without instantiating per-example gradients
for any layer in the model. The technique enables privately training
Transformers with almost the same memory cost as non-private training at a
modest run-time overhead. Contrary to conventional wisdom that DP optimization
fails at learning high-dimensional models (due to noise that scales with
dimension) empirical results reveal that private learning with pretrained
models tends to not suffer from dimension-dependent performance degradation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation. (arXiv:2110.05691v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05691">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation (NMT) models are known to suffer from noisy
inputs. To make models robust, we generate adversarial augmentation samples
that attack the model and preserve the source-side semantic meaning at the same
time. To generate such samples, we propose a doubly-trained architecture that
pairs two NMT models of opposite translation directions with a joint loss
function, which combines the target-side attack and the source-side semantic
similarity constraint. The results from our experiments across three different
language pairs and two evaluation metrics show that these adversarial samples
improve the model robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Releasing Annotator-Level Labels and Information in Datasets. (arXiv:2110.05699v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05699">
<div class="article-summary-box-inner">
<span><p>A common practice in building NLP datasets, especially using crowd-sourced
annotations, involves obtaining multiple annotator judgements on the same data
instances, which are then flattened to produce a single "ground truth" label or
score, through majority voting, averaging, or adjudication. While these
approaches may be appropriate in certain annotation tasks, such aggregations
overlook the socially constructed nature of human perceptions that annotations
for relatively more subjective tasks are meant to capture. In particular,
systematic disagreements between annotators owing to their socio-cultural
backgrounds and/or lived experiences are often obfuscated through such
aggregations. In this paper, we empirically demonstrate that label aggregation
may introduce representational biases of individual and group perspectives.
Based on this finding, we propose a set of recommendations for increased
utility and transparency of datasets for downstream use cases.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05717">
<div class="article-summary-box-inner">
<span><p>Temporal language grounding in videos aims to localize the temporal span
relevant to the given query sentence. Previous methods treat it either as a
boundary regression task or a span extraction task. This paper will formulate
temporal language grounding into video reading comprehension and propose a
Relation-aware Network (RaNet) to address it. This framework aims to select a
video moment choice from the predefined answer set with the aid of
coarse-and-fine choice-query interaction and choice-choice relation
construction. A choice-query interactor is proposed to match the visual and
textual information simultaneously in sentence-moment and token-moment levels,
leading to a coarse-and-fine cross-modal interaction. Moreover, a novel
multi-choice relation constructor is introduced by leveraging graph convolution
to capture the dependencies among video moment choices for the best choice
selection. Extensive experiments on ActivityNet-Captions, TACoS, and
Charades-STA demonstrate the effectiveness of our solution. Codes will be
released soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations. (arXiv:2110.05719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05719">
<div class="article-summary-box-inner">
<span><p>Majority voting and averaging are common approaches employed to resolve
annotator disagreements and derive single ground truth labels from multiple
annotations. However, annotators may systematically disagree with one another,
often reflecting their individual biases and values, especially in the case of
subjective tasks such as detecting affect, aggression, and hate speech.
Annotator disagreements may capture important nuances in such tasks that are
often ignored while aggregating annotations to a single ground truth. In order
to address this, we investigate the efficacy of multi-annotator models. In
particular, our multi-task based approach treats predicting each annotators'
judgements as separate subtasks, while sharing a common learned representation
of the task. We show that this approach yields same or better performance than
aggregating labels in the data prior to training across seven different binary
classification tasks. Our approach also provides a way to estimate uncertainty
in predictions, which we demonstrate better correlate with annotation
disagreements than traditional methods. Being able to model uncertainty is
especially useful in deployment scenarios where knowing when not to make a
prediction is important.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LightSeq: Accelerated Training for Transformer-based Models on GPUs. (arXiv:2110.05722v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05722">
<div class="article-summary-box-inner">
<span><p>Transformer-based models have proven to be powerful in many natural language,
computer vision, and speech recognition applications. It is expensive to train
these types of models due to unfixed input length, complex computation, and
large numbers of parameters. Existing systems either only focus on efficient
inference or optimize only BERT-like encoder models. In this paper, we present
LightSeq, a system for efficient training of Transformer-based models on GPUs.
We propose a series of GPU optimization techniques tailored to computation flow
and memory access patterns of neural layers in Transformers. LightSeq supports
a variety of network architectures, including BERT (encoder-only), GPT
(decoder-only), and Transformer (encoder-decoder). Our experiments on GPUs with
varying models and datasets show that LightSeq is 1.4-3.5x faster than previous
systems. In particular, it gains 308% training speedup compared with existing
systems on a large public machine translation benchmark (WMT14 English-German).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prediction of Political Leanings of Chinese Speaking Twitter Users. (arXiv:2110.05723v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05723">
<div class="article-summary-box-inner">
<span><p>This work presents a supervised method for generating a classifier model of
the stances held by Chinese-speaking politicians and other Twitter users. Many
previous works of political tweets prediction exist on English tweets, but to
the best of our knowledge, this is the first work that builds prediction model
on Chinese political tweets. It firstly collects data by scraping tweets of
famous political figure and their related users. It secondly defines the
political spectrum in two groups: the group that shows approvals to the Chinese
Communist Party and the group that does not. Since there are not space between
words in Chinese to identify the independent words, it then completes
segmentation and vectorization by Jieba, a Chinese segmentation tool. Finally,
it trains the data collected from political tweets and produce a classification
model with high accuracy for understanding users' political stances from their
tweets on Twitter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Anatomy of OntoGUM--Adapting GUM to the OntoNotes Scheme to Evaluate Robustness of SOTA Coreference Algorithms. (arXiv:2110.05727v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05727">
<div class="article-summary-box-inner">
<span><p>SOTA coreference resolution produces increasingly impressive scores on the
OntoNotes benchmark. However lack of comparable data following the same scheme
for more genres makes it difficult to evaluate generalizability to open domain
data. Zhu et al. (2021) introduced the creation of the OntoGUM corpus for
evaluating geralizability of the latest neural LM-based end-to-end systems.
This paper covers details of the mapping process which is a set of
deterministic rules applied to the rich syntactic and discourse annotations
manually annotated in the GUM corpus. Out-of-domain evaluation across 12 genres
shows nearly 15-20% degradation for both deterministic and deep learning
systems, indicating a lack of generalizability or covert overfitting in
existing coreference resolution models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VarArray: Array-Geometry-Agnostic Continuous Speech Separation. (arXiv:2110.05745v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05745">
<div class="article-summary-box-inner">
<span><p>Continuous speech separation using a microphone array was shown to be
promising in dealing with the speech overlap problem in natural conversation
transcription. This paper proposes VarArray, an array-geometry-agnostic speech
separation neural network model. The proposed model is applicable to any number
of microphones without retraining while leveraging the nonlinear correlation
between the input channels. The proposed method adapts different elements that
were proposed before separately, including transform-average-concatenate,
conformer speech separation, and inter-channel phase differences, and combines
them in an efficient and cohesive way. Large-scale evaluation was performed
with two real meeting transcription tasks by using a fully developed
transcription system requiring no prior knowledge such as reference
segmentations, which allowed us to measure the impact that the continuous
speech separation system could have in realistic settings. The proposed model
outperformed a previous approach to array-geometry-agnostic modeling for all of
the geometry configurations considered, achieving asclite-based
speaker-agnostic word error rates of 17.5% and 20.4% for the AMI development
and evaluation sets, respectively, in the end-to-end setting using no
ground-truth segmentations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SEPP: Similarity Estimation of Predicted Probabilities for Defending and Detecting Adversarial Text. (arXiv:2110.05748v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05748">
<div class="article-summary-box-inner">
<span><p>There are two cases describing how a classifier processes input text, namely,
misclassification and correct classification. In terms of misclassified texts,
a classifier handles the texts with both incorrect predictions and adversarial
texts, which are generated to fool the classifier, which is called a victim.
Both types are misunderstood by the victim, but they can still be recognized by
other classifiers. This induces large gaps in predicted probabilities between
the victim and the other classifiers. In contrast, text correctly classified by
the victim is often successfully predicted by the others and induces small
gaps. In this paper, we propose an ensemble model based on similarity
estimation of predicted probabilities (SEPP) to exploit the large gaps in the
misclassified predictions in contrast to small gaps in the correct
classification. SEPP then corrects the incorrect predictions of the
misclassified texts. We demonstrate the resilience of SEPP in defending and
detecting adversarial texts through different types of victim classifiers,
classification tasks, and adversarial attacks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary. (arXiv:2110.05750v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05750">
<div class="article-summary-box-inner">
<span><p>Sports game summarization aims to generate news articles from live text
commentaries. A recent state-of-the-art work, SportsSum, not only constructs a
large benchmark dataset, but also proposes a two-step framework. Despite its
great contributions, the work has three main drawbacks: 1) the noise existed in
SportsSum dataset degrades the summarization performance; 2) the neglect of
lexical overlap between news and commentaries results in low-quality
pseudo-labeling algorithm; 3) the usage of directly concatenating rewritten
sentences to form news limits its practicability. In this paper, we publish a
new benchmark dataset SportsSum2.0, together with a modified summarization
framework. In particular, to obtain a clean dataset, we employ crowd workers to
manually clean the original dataset. Moreover, the degree of lexical overlap is
incorporated into the generation of pseudo labels. Further, we introduce a
reranker-enhanced summarizer to take into account the fluency and
expressiveness of the summarized news. Extensive experiments show that our
model outperforms the state-of-the-art baseline.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware Pre-Training. (arXiv:2110.05752v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05752">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning (SSL) is a long-standing goal for speech processing,
since it utilizes large-scale unlabeled data and avoids extensive human
labeling. Recent years witness great successes in applying self-supervised
learning in speech recognition, while limited exploration was attempted in
applying SSL for modeling speaker characteristics. In this paper, we aim to
improve the existing SSL framework for speaker representation learning. Two
methods are introduced for enhancing the unsupervised speaker information
extraction. First, we apply the multi-task learning to the current SSL
framework, where we integrate the utterance-wise contrastive loss with the SSL
objective function. Second, for better speaker discrimination, we propose an
utterance mixing strategy for data augmentation, where additional overlapped
utterances are created unsupervisely and incorporate during training. We
integrate the proposed methods into the HuBERT framework. Experiment results on
SUPERB benchmark show that the proposed system achieves state-of-the-art
performance in universal representation learning, especially for speaker
identification oriented tasks. An ablation study is performed verifying the
efficacy of each proposed method. Finally, we scale up training dataset to 94
thousand hours public audio data and achieve further performance improvement in
all SUPERB tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Cognitive Factors in Lexical Decline. (arXiv:2110.05775v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05775">
<div class="article-summary-box-inner">
<span><p>We adopt an evolutionary view on language change in which cognitive factors
(in addition to social ones) affect the fitness of words and their success in
the linguistic ecosystem. Specifically, we propose a variety of
psycholinguistic factors -- semantic, distributional, and phonological -- that
we hypothesize are predictive of lexical decline, in which words greatly
decrease in frequency over time. Using historical data across three languages
(English, French, and German), we find that most of our proposed factors show a
significant difference in the expected direction between each curated set of
declining words and their matched stable words. Moreover, logistic regression
analyses show that semantic and distributional factors are significant in
predicting declining words. Further diachronic analysis reveals that declining
words tend to decrease in the diversity of their lexical contexts over time,
gradually narrowing their 'ecological niches'.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">We've had this conversation before: A Novel Approach to Measuring Dialog Similarity. (arXiv:2110.05780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05780">
<div class="article-summary-box-inner">
<span><p>Dialog is a core building block of human natural language interactions. It
contains multi-party utterances used to convey information from one party to
another in a dynamic and evolving manner. The ability to compare dialogs is
beneficial in many real world use cases, such as conversation analytics for
contact center calls and virtual agent design.
</p>
<p>We propose a novel adaptation of the edit distance metric to the scenario of
dialog similarity. Our approach takes into account various conversation aspects
such as utterance semantics, conversation flow, and the participants. We
evaluate this new approach and compare it to existing document similarity
measures on two publicly available datasets. The results demonstrate that our
method outperforms the other approaches in capturing dialog flow, and is better
aligned with the human perception of conversation similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTraffic: A Robust BERT-Based Approach for Speaker Change Detection and Role Identification of Air-Traffic Communications. (arXiv:2110.05781v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05781">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) is gaining special interest in Air Traffic
Control (ATC). ASR allows transcribing the communications between air traffic
controllers (ATCOs) and pilots. These transcriptions are used to extract ATC
command types and named entities such as aircraft callsigns. One common problem
is when the Speech Activity Detection (SAD) or diarization system fails and
then two or more single speaker segments are in the same recording,
jeopardizing the overall system's performance. We developed a system that
combines the segmentation of a SAD module with a BERT-based model that performs
Speaker Change Detection (SCD) and Speaker Role Identification (SRI) based on
ASR transcripts (i.e., diarization + SRI). This research demonstrates on a
real-life ATC test set that performing diarization directly on textual data
surpass acoustic level diarization. The proposed model reaches up to
~0.90/~0.95 F1-score on ATCO/pilot for SRI on several test sets. The text-based
diarization system brings a 27% relative improvement on Diarization Error Rate
(DER) compared to standard acoustic-based diarization. These results were on
ASR transcripts of a challenging ATC test set with an estimated ~13% word error
rate, validating the approach's robustness even on noisy ASR transcripts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting TTS models For New Speakers using Transfer Learning. (arXiv:2110.05798v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05798">
<div class="article-summary-box-inner">
<span><p>Training neural text-to-speech (TTS) models for a new speaker typically
requires several hours of high quality speech data. Prior works on voice
cloning attempt to address this challenge by adapting pre-trained multi-speaker
TTS models for a new voice, using a few minutes of speech data of the new
speaker. However, publicly available large multi-speaker datasets are often
noisy, thereby resulting in TTS models that are not suitable for use in
products. We address this challenge by proposing transfer-learning guidelines
for adapting high quality single-speaker TTS models for a new speaker, using
only a few minutes of speech data. We conduct an extensive study using
different amounts of data for a new speaker and evaluate the synthesized speech
in terms of naturalness and voice/style similarity to the target speaker. We
find that fine-tuning a single-speaker TTS model on just 30 minutes of data,
can yield comparable performance to a model trained from scratch on more than
27 hours of data for both male and female target speakers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Balancing Average and Worst-case Accuracy in Multitask Learning. (arXiv:2110.05838v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05838">
<div class="article-summary-box-inner">
<span><p>When training and evaluating machine learning models on a large number of
tasks, it is important to not only look at average task accuracy -- which may
be biased by easy or redundant tasks -- but also worst-case accuracy (i.e. the
performance on the task with the lowest accuracy). In this work, we show how to
use techniques from the distributionally robust optimization (DRO) literature
to improve worst-case performance in multitask learning. We highlight several
failure cases of DRO when applied off-the-shelf and present an improved method,
Lookahead-DRO (L-DRO), which mitigates these issues. The core idea of L-DRO is
to anticipate the interaction between tasks during training in order to choose
a dynamic re-weighting of the various task losses, which will (i) lead to
minimal worst-case loss and (ii) train on as many tasks as possible. After
demonstrating the efficacy of L-DRO on a small controlled synthetic setting, we
evaluate it on two realistic benchmarks: a multitask version of the CIFAR-100
image classification dataset and a large-scale multilingual language modeling
experiment. Our empirical results show that L-DRO achieves a better trade-off
between average and worst-case accuracy with little computational overhead
compared to several strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluation of Abstractive Summarisation Models with Machine Translation in Deliberative Processes. (arXiv:2110.05847v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05847">
<div class="article-summary-box-inner">
<span><p>We present work on summarising deliberative processes for non-English
languages. Unlike commonly studied datasets, such as news articles, this
deliberation dataset reflects difficulties of combining multiple narratives,
mostly of poor grammatical quality, in a single text. We report an extensive
evaluation of a wide range of abstractive summarisation models in combination
with an off-the-shelf machine translation model. Texts are translated into
English, summarised, and translated back to the original language. We obtain
promising results regarding the fluency, consistency and relevance of the
summaries produced. Our approach is easy to implement for many languages for
production purposes by simply changing the translation model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">text2sdg: An open-source solution to monitoring sustainable development goals from text. (arXiv:2110.05856v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05856">
<div class="article-summary-box-inner">
<span><p>Monitoring progress on the United Nations Sustainable Development Goals
(SDGs) is important for both academic and non-academic organizations. Existing
approaches to monitoring SDGs have focused on specific data types, namely,
publications listed in proprietary research databases. We present the text2sdg
R package, a user-friendly, open-source package that detects SDGs in any kind
of text data using several different query systems from any text source. The
text2sdg package thereby facilitates the monitoring of SDGs for a wide array of
text sources and provides a much-needed basis for validating and improving
extant methods to detect SDGs from text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetricGAN-U: Unsupervised speech enhancement/ dereverberation based only on noisy/ reverberated speech. (arXiv:2110.05866v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05866">
<div class="article-summary-box-inner">
<span><p>Most of the deep learning-based speech enhancement models are learned in a
supervised manner, which implies that pairs of noisy and clean speech are
required during training. Consequently, several noisy speeches recorded in
daily life cannot be used to train the model. Although certain unsupervised
learning frameworks have also been proposed to solve the pair constraint, they
still require clean speech or noise for training. Therefore, in this paper, we
propose MetricGAN-U, which stands for MetricGAN-unsupervised, to further
release the constraint from conventional unsupervised learning. In MetricGAN-U,
only noisy speech is required to train the model by optimizing non-intrusive
speech quality metrics. The experimental results verified that MetricGAN-U
outperforms baselines in both objective and subjective metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages. (arXiv:2110.05877v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05877">
<div class="article-summary-box-inner">
<span><p>AI technologies for Natural Languages have made tremendous progress recently.
However, commensurate progress has not been made on Sign Languages, in
particular, in recognizing signs as individual words or as complete sentences.
We introduce OpenHands, a library where we take four key ideas from the NLP
community for low-resource languages and apply them to sign languages for
word-level recognition. First, we propose using pose extracted through
pretrained models as the standard modality of data to reduce training time and
enable efficient inference, and we release standardized pose datasets for 6
different sign languages - American, Argentinian, Chinese, Greek, Indian, and
Turkish. Second, we train and release checkpoints of 4 pose-based isolated sign
language recognition models across all 6 languages, providing baselines and
ready checkpoints for deployment. Third, to address the lack of labelled data,
we propose self-supervised pretraining on unlabelled data. We curate and
release the largest pose-based pretraining dataset on Indian Sign Language
(Indian-SL). Fourth, we compare different pretraining strategies and for the
first time establish that pretraining is effective for sign language
recognition by demonstrating (a) improved fine-tuning performance especially in
low-resource settings, and (b) high crosslingual transfer from Indian-SL to few
other sign languages. We open-source all models and datasets in OpenHands with
a hope that it makes research in sign languages more accessible, available here
at https://github.com/AI4Bharat/OpenHands .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigation on Data Adaptation Techniques for Neural Named Entity Recognition. (arXiv:2110.05892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05892">
<div class="article-summary-box-inner">
<span><p>Data processing is an important step in various natural language processing
tasks. As the commonly used datasets in named entity recognition contain only a
limited number of samples, it is important to obtain additional labeled data in
an efficient and reliable manner. A common practice is to utilize large
monolingual unlabeled corpora. Another popular technique is to create synthetic
data from the original labeled data (data augmentation). In this work, we
investigate the impact of these two methods on the performance of three
different named entity recognition tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05896">
<div class="article-summary-box-inner">
<span><p>Trained on the large corpus, pre-trained language models (PLMs) can capture
different levels of concepts in context and hence generate universal language
representations. They can benefit multiple downstream natural language
processing (NLP) tasks. Although PTMs have been widely used in most NLP
applications, especially for high-resource languages such as English, it is
under-represented in Lao NLP research. Previous work on Lao has been hampered
by the lack of annotated datasets and the sparsity of language resources. In
this work, we construct a text classification dataset to alleviate the
resource-scare situation of the Lao language. We additionally present the first
transformer-based PTMs for Lao with four versions: BERT-small, BERT-base,
ELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:
part-of-speech tagging and text classification. Experiments demonstrate the
effectiveness of our Lao models. We will release our models and datasets to the
community, hoping to facilitate the future development of Lao NLP applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Order Does Not Matter For Speech Recognition. (arXiv:2110.05994v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05994">
<div class="article-summary-box-inner">
<span><p>In this paper, we study training of automatic speech recognition system in a
weakly supervised setting where the order of words in transcript labels of the
audio training data is not known. We train a word-level acoustic model which
aggregates the distribution of all output frames using LogSumExp operation and
uses a cross-entropy loss to match with the ground-truth words distribution.
Using the pseudo-labels generated from this model on the training set, we then
train a letter-based acoustic model using Connectionist Temporal Classification
loss. Our system achieves 2.4%/5.3% on test-clean/test-other subsets of
LibriSpeech, which is competitive with the supervised baseline's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer. (arXiv:2110.05999v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05999">
<div class="article-summary-box-inner">
<span><p>Despite the recent advances in applying pre-trained language models to
generate high-quality texts, generating long passages that maintain long-range
coherence is yet challenging for these models. In this paper, we propose
DiscoDVT, a discourse-aware discrete variational Transformer to tackle the
incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes
the global structure of the text and then applies it to guide the generation
process at each decoding step. To further embed discourse-aware information
into the discrete latent representations, we introduce an auxiliary objective
to model the discourse relations within the text. We conduct extensive
experiments on two open story generation datasets and demonstrate that the
latent codes learn meaningful correspondence to the discourse structures that
guide the model to generate long texts with better long-range coherence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Model Supervised by Understanding Map. (arXiv:2110.06043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06043">
<div class="article-summary-box-inner">
<span><p>Inspired by the notion of Center of Mass in physics, an extension called
Semantic Center of Mass (SCOM) is proposed, and used to discover the abstract
"topic" of a document. The notion is under a framework model called
Understanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM
is to let both the document content and a semantic network -- specifically,
Understanding Map -- play a role, in interpreting the meaning of a document.
Based on different justifications, three possible methods are devised to
discover the SCOM of a document. Some experiments on artificial documents and
Understanding Maps are conducted to test their outcomes. In addition, its
ability of vectorization of documents and capturing sequential information are
tested. We also compared UM-S-TM with probabilistic topic models like Latent
Dirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects. (arXiv:2110.06078v1 [q-bio.NC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06078">
<div class="article-summary-box-inner">
<span><p>A popular approach to decompose the neural bases of language consists in
correlating, across individuals, the brain responses to different stimuli (e.g.
regular speech versus scrambled words, sentences, or paragraphs). Although
successful, this `model-free' approach necessitates the acquisition of a large
and costly set of neuroimaging data. Here, we show that a model-based approach
can reach equivalent results within subjects exposed to natural stimuli. We
capitalize on the recently-discovered similarities between deep language models
and the human brain to compute the mapping between i) the brain responses to
regular speech and ii) the activations of deep language models elicited by
modified stimuli (e.g. scrambled words, sentences, or paragraphs). Our
model-based approach successfully replicates the seminal study of Lerner et al.
(2011), which revealed the hierarchy of language areas by comparing the
functional-magnetic resonance imaging (fMRI) of seven subjects listening to
7min of both regular and scrambled narratives. We further extend and precise
these results to the brain signals of 305 individuals listening to 4.1 hours of
narrated stories. Overall, this study paves the way for efficient and flexible
analyses of the brain bases of language.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A large scale lexical and semantic analysis of Spanish language variations in Twitter. (arXiv:2110.06128v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06128">
<div class="article-summary-box-inner">
<span><p>Dialectometry is a discipline devoted to studying the variations of a
language around a geographical region. One of their goals is the creation of
linguistic atlases capturing the similarities and differences of the language
under study around the area in question. For instance, Spanish is one of the
most spoken languages across the world, but not necessarily Spanish is written
and spoken in the same way in different countries. This manuscript presents a
broad analysis describing lexical and semantic relationships among 26
Spanish-speaking countries around the globe. For this study, we analyze
four-year of the Twitter geotagged public stream to provide an extensive survey
of the Spanish language vocabularies of different countries, its distributions,
semantic usage of terms, and emojis. We also offer open regional word-embedding
resources for Spanish Twitter to help other researchers and practitioners take
advantage of regionalized models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extracting Feelings of People Regarding COVID-19 by Social Network Mining. (arXiv:2110.06151v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06151">
<div class="article-summary-box-inner">
<span><p>In 2020, COVID-19 became the chief concern of the world and is still
reflected widely in all social networks. Each day, users post millions of
tweets and comments on this subject, which contain significant implicit
information about the public opinion. In this regard, a dataset of
COVID-related tweets in English language is collected, which consists of more
than two million tweets from March 23 to June 23 of 2020 to extract the
feelings of the people in various countries in the early stages of this
outbreak. To this end, first, we use a lexicon-based approach in conjunction
with the GeoNames geographic database to label the tweets with their locations.
Next, a method based on the recently introduced and widely cited RoBERTa model
is proposed to analyze their sentimental content. After that, the trend graphs
of the frequency of tweets as well as sentiments are produced for the world and
the nations that were more engaged with COVID-19. Graph analysis shows that the
frequency graphs of the tweets for the majority of nations are significantly
correlated with the official statistics of the daily afflicted in them.
Moreover, several implicit knowledge is extracted and discussed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mention Memory: incorporating textual knowledge into Transformers through entity mention attention. (arXiv:2110.06176v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.06176">
<div class="article-summary-box-inner">
<span><p>Natural language understanding tasks such as open-domain question answering
often require retrieving and assimilating factual information from multiple
sources. We propose to address this problem by integrating a semi-parametric
representation of a large text corpus into a Transformer model as a source of
factual knowledge. Specifically, our method represents knowledge with `mention
memory', a table of dense vector representations of every entity mention in a
corpus. The proposed model - TOME - is a Transformer that accesses the
information through internal memory layers in which each entity mention in the
input passage attends to the mention memory. This approach enables synthesis of
and reasoning over many disparate sources of information within a single
Transformer model. In experiments using a memory of 150 million Wikipedia
mentions, TOME achieves strong performance on several open-domain
knowledge-intensive tasks, including the claim verification benchmarks HoVer
and FEVER and several entity-based QA benchmarks. We also show that the model
learns to attend to informative mentions without any direct supervision.
Finally we demonstrate that the model can generalize to new unseen entities by
updating the memory without retraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12804">
<div class="article-summary-box-inner">
<span><p>This work demonstrates that using the objective with independence assumption
for modelling the span probability $P(a_s,a_e) = P(a_s)P(a_e)$ of span starting
at position $a_s$ and ending at position $a_e$ has adverse effects. Therefore
we propose multiple approaches to modelling joint probability $P(a_s,a_e)$
directly. Among those, we propose a compound objective, composed from the joint
probability while still keeping the objective with independence assumption as
an auxiliary objective. We find that the compound objective is consistently
superior or equal to other assumptions in exact match. Additionally, we
identified common errors caused by the assumption of independence and manually
checked the counterpart predictions, demonstrating the impact of the compound
objective on the real examples. Our findings are supported via experiments with
three extractive QA models (BIDAF, BERT, ALBERT) over six datasets and our
code, individual results and manual analysis are available online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Minimal Supervision for Morphological Inflection. (arXiv:2104.08512v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08512">
<div class="article-summary-box-inner">
<span><p>Neural models for the various flavours of morphological inflection tasks have
proven to be extremely accurate given ample labeled data -- data that may be
slow and costly to obtain. In this work we aim to overcome this annotation
bottleneck by bootstrapping labeled data from a seed as little as {\em five}
labeled paradigms, accompanied by a large bulk of unlabeled text. Our approach
exploits different kinds of regularities in morphological systems in a
two-phased setup, where word tagging based on {\em analogies} is followed by
word pairing based on {\em distances}. We experiment with the Paradigm Cell
Filling Problem over eight typologically different languages, and find that, in
languages with relatively simple morphology, orthographic regularities on their
own allow inflection models to achieve respectable accuracy. Combined
orthographic and semantic regularities alleviate difficulties with particularly
complex morpho-phonological systems. Our results suggest that hand-crafting
many tagged examples might be an unnecessary effort. However, more work is
needed in order to address rarely used forms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding. (arXiv:2104.12763v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12763">
<div class="article-summary-box-inner">
<span><p>Multi-modal reasoning systems rely on a pre-trained object detector to
extract regions of interest from the image. However, this crucial module is
typically used as a black box, trained independently of the downstream task and
on a fixed vocabulary of objects and attributes. This makes it challenging for
such systems to capture the long tail of visual concepts expressed in free form
text. In this paper we propose MDETR, an end-to-end modulated detector that
detects objects in an image conditioned on a raw text query, like a caption or
a question. We use a transformer-based architecture to reason jointly over text
and image by fusing the two modalities at an early stage of the model. We
pre-train the network on 1.3M text-image pairs, mined from pre-existing
multi-modal datasets having explicit alignment between phrases in text and
objects in the image. We then fine-tune on several downstream tasks such as
phrase grounding, referring expression comprehension and segmentation,
achieving state-of-the-art results on popular benchmarks. We also investigate
the utility of our model as an object detector on a given label set when
fine-tuned in a few-shot setting. We show that our pre-training approach
provides a way to handle the long tail of object categories which have very few
labelled instances. Our approach can be easily extended for visual question
answering, achieving competitive performance on GQA and CLEVR. The code and
models are available at https://github.com/ashkamath/mdetr.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prosodic segmentation for parsing spoken dialogue. (arXiv:2105.12667v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12667">
<div class="article-summary-box-inner">
<span><p>Parsing spoken dialogue poses unique difficulties, including disfluencies and
unmarked boundaries between sentence-like units. Previous work has shown that
prosody can help with parsing disfluent speech (Tran et al. 2018), but has
assumed that the input to the parser is already segmented into sentence-like
units (SUs), which isn't true in existing speech applications. We investigate
how prosody affects a parser that receives an entire dialogue turn as input (a
turn-based model), instead of gold standard pre-segmented SUs (an SU-based
model). In experiments on the English Switchboard corpus, we find that when
using transcripts alone, the turn-based model has trouble segmenting SUs,
leading to worse parse performance than the SU-based model. However, prosody
can effectively replace gold standard SU boundaries: with prosody, the
turn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1
score, respectively), despite performing two tasks (SU segmentation and
parsing) rather than one (parsing alone). Analysis shows that pitch and
intensity features are the most important for this corpus, since they allow the
model to correctly distinguish an SU boundary from a speech disfluency -- a
distinction that the model otherwise struggles to make.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05707">
<div class="article-summary-box-inner">
<span><p>Fact verification has attracted a lot of attention in the machine learning
and natural language processing communities, as it is one of the key methods
for detecting misinformation. Existing large-scale benchmarks for this task
have focused mostly on textual sources, i.e. unstructured information, and thus
ignored the wealth of information available in structured formats, such as
tables. In this paper we introduce a novel dataset and benchmark, Fact
Extraction and VERification Over Unstructured and Structured information
(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated
with evidence in the form of sentences and/or cells from tables in Wikipedia,
as well as a label indicating whether this evidence supports, refutes, or does
not provide enough information to reach a verdict. Furthermore, we detail our
efforts to track and minimize the biases present in the dataset and could be
exploited by models, e.g. being able to predict the label without using
evidence. Finally, we develop a baseline for verifying claims against text and
tables which predicts both the correct evidence and verdict for 18% of the
claims.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07250">
<div class="article-summary-box-inner">
<span><p>In knowledge graph embedding, the theoretical relationship between the
softmax cross-entropy and negative sampling loss functions has not been
investigated. This makes it difficult to fairly compare the results of the two
different loss functions. We attempted to solve this problem by using the
Bregman divergence to provide a unified interpretation of the softmax
cross-entropy and negative sampling loss functions. Under this interpretation,
we can derive theoretical findings for fair comparison. Experimental results on
the FB15k-237 and WN18RR datasets show that the theoretical findings are valid
in practical settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07306">
<div class="article-summary-box-inner">
<span><p>A major challenge in structured prediction is to represent the
interdependencies within output structures. When outputs are structured as
sequences, linear-chain conditional random fields (CRFs) are a widely used
model class which can learn \textit{local} dependencies in the output. However,
the CRF's Markov assumption makes it impossible for CRFs to represent
distributions with \textit{nonlocal} dependencies, and standard CRFs are unable
to respect nonlocal constraints of the data (such as global arity constraints
on output labels). We present a generalization of CRFs that can enforce a broad
class of constraints, including nonlocal ones, by specifying the space of
possible output structures as a regular language $\mathcal{L}$. The resulting
regular-constrained CRF (RegCCRF) has the same formal properties as a standard
CRF, but assigns zero probability to all label sequences not in $\mathcal{L}$.
Notably, RegCCRFs can incorporate their constraints during training, while
related models only enforce constraints during decoding. We prove that
constrained training is never worse than constrained decoding, and show
empirically that it can be substantially better in practice. Additionally, we
demonstrate a practical benefit on downstream tasks by incorporating a RegCCRF
into a deep neural model for semantic role labeling, exceeding state-of-the-art
results on a standard dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01951">
<div class="article-summary-box-inner">
<span><p>The task of learning from only a few examples (called a few-shot setting) is
of key importance and relevance to a real-world setting. For question answering
(QA), the current state-of-the-art pre-trained models typically need
fine-tuning on tens of thousands of examples to obtain good results. Their
performance degrades significantly in a few-shot setting (&lt; 100 examples). To
address this, we propose a simple fine-tuning framework that leverages
pre-trained text-to-text models and is directly aligned with their pre-training
framework. Specifically, we construct the input as a concatenation of the
question, a mask token representing the answer span and a context. Given this
input, the model is fine-tuned using the same objective as that of its
pre-training objective. Through experimental studies on various few-shot
configurations, we show that this formulation leads to significant gains on
multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when
there are only 16 training examples). The gains extend further when used with
larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)
and translate well to a multilingual setting . On the multilingual TydiQA
benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of
upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64
training examples). We conduct detailed ablation studies to analyze factors
contributing to these gains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-task Voice Activated Framework using Self-supervised Learning. (arXiv:2110.01077v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.01077">
<div class="article-summary-box-inner">
<span><p>Self-supervised learning methods such as wav2vec 2.0 have shown promising
results in learning speech representations from unlabelled and untranscribed
speech data that are useful for speech recognition. Since these representations
are learned without any task-specific supervision, they can also be useful for
other voice-activated tasks like speaker verification, keyword spotting,
emotion classification etc. In our work, we propose a general purpose framework
for adapting a pre-trained wav2vec 2.0 model for different voice-activated
tasks. We develop downstream network architectures that operate on the
contextualized speech representations of wav2vec 2.0 to adapt the
representations for solving a given task. Finally, we extend our framework to
perform multi-task learning by jointly optimizing the network parameters on
multiple voice activated tasks using a shared transformer backbone. Both of our
single and multi-task frameworks achieve state-of-the-art results in speaker
verification and keyword spotting benchmarks. Our best performing models
achieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and
VoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0
keyword spotting dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence-to-Sequence Lexical Normalization with Multilingual Transformers. (arXiv:2110.02869v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.02869">
<div class="article-summary-box-inner">
<span><p>Current benchmark tasks for natural language processing contain text that is
qualitatively different from the text used in informal day to day digital
communication. This discrepancy has led to severe performance degradation of
state-of-the-art NLP models when fine-tuned on real-world data. One way to
resolve this issue is through lexical normalization, which is the process of
transforming non-standard text, usually from social media, into a more
standardized form. In this work, we propose a sentence-level
sequence-to-sequence model based on mBART, which frames the problem as a
machine translation problem. As the noisy text is a pervasive problem across
languages, not just English, we leverage the multi-lingual pre-training of
mBART to fine-tune it to our data. While current approaches mainly operate at
the word or subword level, we argue that this approach is straightforward from
a technical standpoint and builds upon existing pre-trained transformer
networks. Our results show that while word-level, intrinsic, performance
evaluation is behind other methods, our model improves performance on
extrinsic, downstream tasks through normalization compared to models operating
on raw, unprocessed, social media text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.03370">
<div class="article-summary-box-inner">
<span><p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus
consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly
labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in
total. We collect the data from YouTube and Podcast, which covers a variety of
speaking styles, scenarios, domains, topics, and noisy conditions. An optical
character recognition (OCR) based method is introduced to generate the
audio/text segmentation candidates for the YouTube data on its corresponding
video captions, while a high-quality ASR transcription system is used to
generate audio/text pair candidates for the Podcast data. Then we propose a
novel end-to-end label error detection approach to further validate and filter
the candidates. We also provide three manually labelled high-quality test sets
along with WenetSpeech for evaluation -- Dev for cross-validation purpose in
training, Test_Net, collected from Internet for matched test, and
Test\_Meeting, recorded from real meetings for more challenging mismatched
test. Baseline systems trained with WenetSpeech are provided for three popular
speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition
results on the three test sets are also provided as benchmarks. To the best of
our knowledge, WenetSpeech is the current largest open-sourced Mandarin speech
corpus with transcriptions, which benefits research on production-level speech
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04260">
<div class="article-summary-box-inner">
<span><p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can
easily scale to have outrageously large amounts of parameters without
significant increase in computational cost. However, SAMs are reported to be
parameter inefficient such that larger models do not always lead to better
performance. While most on-going research focuses on improving SAMs models by
exploring methods of routing inputs to experts, our analysis reveals that such
research might not lead to the solution we expect, i.e., the commonly-used
routing methods based on gating mechanisms do not work better than randomly
routing inputs to experts. In this paper, we propose a new expert-based model,
THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,
such as the Switch Transformer, experts in THOR are randomly activated for each
input during training and inference. THOR models are trained using a
consistency regularized loss, where experts learn not only from training data
but also from other experts as teachers, such that all the experts make
consistent predictions. We validate the effectiveness of THOR on machine
translation tasks. Results show that THOR models are more parameter efficient
in that they significantly outperform the Transformer and MoE models across
various settings. For example, in multilingual translation, THOR outperforms
the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as
that of a state-of-the-art MoE model that is 18 times larger. Our code is
publicly available at:
https://github.com/microsoft/Stochastic-Mixture-of-Experts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning. (arXiv:2110.04725v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04725">
<div class="article-summary-box-inner">
<span><p>Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot
and Few-Shot learning on many natural language processing (NLP) tasks by
scaling up model size, dataset size and the amount of computation. However,
training a model like GPT-3 requires huge amount of computational resources
which makes it challengeable to researchers. In this work, we propose a method
that incorporates large-scale distributed training performance into model
architecture design. With this method, Yuan 1.0, the current largest singleton
language model with 245B parameters, achieves excellent performance on
thousands GPUs during training, and the state-of-the-art results on NLP tasks.
A data processing method is designed to efficiently filter massive amount of
raw data. The current largest high-quality Chinese corpus with 5TB high quality
texts is built based on this method. In addition, a calibration and label
expansion method is proposed to improve the Zero-Shot and Few-Shot performance,
and steady improvement is observed on the accuracy of various tasks. Yuan 1.0
presents strong capacity of natural language generation, and the generated
articles are difficult to distinguish from the human-written ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Advances in Multi-turn Dialogue Comprehension: A Survey. (arXiv:2110.04984v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.04984">
<div class="article-summary-box-inner">
<span><p>Training machines to understand natural language and interact with humans is
an elusive and essential task of artificial intelligence. A diversity of
dialogue systems has been designed with the rapid development of deep learning
techniques, especially the recent pre-trained language models (PrLMs). Among
these studies, the fundamental yet challenging type of task is dialogue
comprehension whose role is to teach the machines to read and comprehend the
dialogue context before responding. In this paper, we review the previous
methods from the technical perspective of dialogue modeling for the dialogue
comprehension task. We summarize the characteristics and challenges of dialogue
comprehension in contrast to plain-text reading comprehension. Then, we discuss
three typical patterns of dialogue modeling. In addition, we categorize
dialogue-related pre-training techniques which are employed to enhance PrLMs in
dialogue scenarios. Finally, we highlight the technical advances in recent
years and point out the lessons from the empirical analysis and the prospects
towards a new frontier of researches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05006">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PLMs) have been the de facto paradigm for most
natural language processing (NLP) tasks. This also benefits biomedical domain:
researchers from informatics, medicine, and computer science (CS) communities
propose various PLMs trained on biomedical datasets, e.g., biomedical text,
electronic health records, protein, and DNA sequences for various biomedical
tasks. However, the cross-discipline characteristics of biomedical PLMs hinder
their spreading among communities; some existing works are isolated from each
other without comprehensive comparison and discussions. It expects a survey
that not only systematically reviews recent advances of biomedical PLMs and
their applications but also standardizes terminology and benchmarks. In this
paper, we summarize the recent progress of pre-trained language models in the
biomedical domain and their applications in biomedical downstream tasks.
Particularly, we discuss the motivations and propose a taxonomy of existing
biomedical PLMs. Their applications in biomedical downstream tasks are
exhaustively discussed. At last, we illustrate various limitations and future
trends, which we hope can provide inspiration for the future research of the
research community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ViSeRet: A simple yet effective approach to moment retrieval via fine-grained video segmentation. (arXiv:2110.05146v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2110.05146">
<div class="article-summary-box-inner">
<span><p>Video-text retrieval has many real-world applications such as media
analytics, surveillance, and robotics. This paper presents the 1st place
solution to the video retrieval track of the ICCV VALUE Challenge 2021. We
present a simple yet effective approach to jointly tackle two video-text
retrieval tasks (video retrieval and video corpus moment retrieval) by
leveraging the model trained only on the video retrieval task. In addition, we
create an ensemble model that achieves the new state-of-the-art performance on
all four datasets (TVr, How2r, YouCook2r, and VATEXr) presented in the VALUE
Challenge.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-10-13 23:09:27.581276505 UTC">2021-10-13 23:09:27 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>