{"site_title":"","build_time":"2021-08-09T11:51:42.541655986Z","days":[{"date":"2021-08-09","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02830","description":"<p>Hate speech is a specific type of controversial content that is widely\nlegislated as a crime that must be identified and blocked. However, due to the\nsheer volume and velocity of the Twitter data stream, hate speech detection\ncannot be performed manually. To address this issue, several studies have been\nconducted for hate speech detection in European languages, whereas little\nattention has been paid to low-resource South Asian languages, making the\nsocial media vulnerable for millions of users. In particular, to the best of\nour knowledge, no study has been conducted for hate speech detection in Roman\nUrdu text, which is widely used in the sub-continent. In this study, we have\nscrapped more than 90,000 tweets and manually parsed them to identify 5,000\nRoman Urdu tweets. Subsequently, we have employed an iterative approach to\ndevelop guidelines and used them for generating the Hate Speech Roman Urdu 2020\ncorpus. The tweets in the this corpus are classified at three levels:\nNeutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another\ncontribution, we have used five supervised learning techniques, including a\ndeep learning technique, to evaluate and compare their effectiveness for hate\nspeech detection. The results show that Logistic Regression outperformed all\nother techniques, including deep learning techniques for the two levels of\nclassification, by achieved an F1 score of 0.906 for distinguishing between\nNeutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate\nspeech tweets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Moin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1\">Khurram Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kamran Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02854","description":"<p>Languages differ in terms of the absence or presence of gender features, the\nnumber of gender classes and whether and where gender features are explicitly\nmarked. These cross-linguistic differences can lead to ambiguities that are\ndifficult to resolve, especially for sentence-level MT systems. The\nidentification of ambiguity and its subsequent resolution is a challenging task\nfor which currently there aren't any specific resources or challenge sets\navailable. In this paper, we introduce gENder-IT, an English--Italian challenge\nset focusing on the resolution of natural gender phenomena by providing\nword-level gender tags on the English source side and multiple gender\nalternative translations, where needed, on the Italian target side.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monti_J/0/1/0/all/0/1\">Johanna Monti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Abstractiveness-Factuality Tradeoff With Nonlinear Abstractiveness Constraints. (arXiv:2108.02859v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02859","description":"<p>We analyze the tradeoff between factuality and abstractiveness of summaries.\nWe introduce abstractiveness constraints to control the degree of\nabstractiveness at decoding time, and we apply this technique to characterize\nthe abstractiveness-factuality tradeoff across multiple widely-studied\ndatasets, using extensive human evaluations. We train a neural summarization\nmodel on each dataset and visualize the rates of change in factuality as we\ngradually increase abstractiveness using our abstractiveness constraints. We\nobserve that, while factuality generally drops with increased abstractiveness,\ndifferent datasets lead to different rates of factuality decay. We propose new\nmeasures to quantify the tradeoff between factuality and abstractiveness, incl.\nmuQAGS, which balances factuality with abstractiveness. We also quantify this\ntradeoff in previous works, aiming to establish baselines for the\nabstractiveness-factuality tradeoff that future publications can compare\nagainst.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Markus Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_F/0/1/0/all/0/1\">Feng Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sujith Ravi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering. (arXiv:2108.02866v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02866","description":"<p>The current state-of-the-art generative models for open-domain question\nanswering (ODQA) have focused on generating direct answers from unstructured\ntextual information. However, a large amount of world's knowledge is stored in\nstructured databases, and need to be accessed using query languages such as\nSQL. Furthermore, query languages can answer questions that require complex\nreasoning, as well as offering full explainability. In this paper, we propose a\nhybrid framework that takes both textual and tabular evidence as input and\ngenerates either direct answers or SQL queries depending on which form could\nbetter answer the question. The generated SQL queries can then be executed on\nthe associated databases to obtain the final answers. To the best of our\nknowledge, this is the first paper that applies Text2SQL to ODQA tasks.\nEmpirically, we demonstrate that on several ODQA datasets, the hybrid methods\nconsistently outperforms the baseline models that only take homogeneous input\nby a large margin. Specifically we achieve state-of-the-art performance on\nOpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate\nthat the being able to generate structural SQL queries can always bring gains,\nespecially for those questions that requires complex reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Hanbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1\">Patrick Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolution of emotion semantics. (arXiv:2108.02887v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02887","description":"<p>Humans possess the unique ability to communicate emotions through language.\nAlthough concepts like anger or awe are abstract, there is a shared consensus\nabout what these English emotion words mean. This consensus may give the\nimpression that their meaning is static, but we propose this is not the case.\nWe cannot travel back to earlier periods to study emotion concepts directly,\nbut we can examine text corpora, which have partially preserved the meaning of\nemotion words. Using natural language processing of historical text, we found\nevidence for semantic change in emotion words over the past century and that\nvarying rates of change were predicted in part by an emotion concept's\nprototypicality - how representative it is of the broader category of\n\"emotion\". Prototypicality negatively correlated with historical rates of\nemotion semantic change obtained from text-based word embeddings, beyond more\nestablished variables including usage frequency in English and a second\ncomparison language, French. This effect for prototypicality did not\nconsistently extend to the semantic category of birds, suggesting its relevance\nfor predicting semantic change may be category-dependent. Our results suggest\nemotion semantics are evolving over time, with prototypical emotion words\nremaining semantically stable, while other emotion words evolve more freely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">Aotao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stellar_J/0/1/0/all/0/1\">Jennifer E. Stellar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02899","description":"<p>Document digitization is essential for the digital transformation of our\nsocieties, yet a crucial step in the process, Optical Character Recognition\n(OCR), is still not perfect. Even commercial OCR systems can produce\nquestionable output depending on the fidelity of the scanned documents. In this\npaper, we demonstrate an effective framework for mitigating OCR errors for any\ndownstream NLP task, using Named Entity Recognition (NER) as an example. We\nfirst address the data scarcity problem for model training by constructing a\ndocument synthesis pipeline, generating realistic but degraded data with NER\nlabels. We measure the NER accuracy drop at various degradation levels and show\nthat a text restoration model, trained on the degraded data, significantly\ncloses the NER accuracy gaps caused by OCR errors, including on an\nout-of-domain dataset. For the benefit of the community, we have made the\ndocument synthesis pipeline available as an open-source project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1\">Amit Gupte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1\">Alexey Romanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1\">Sahitya Mantravadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1\">Dalitso Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Raza Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1\">Lakshmanan Ramu Meenal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundar Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LadRa-Net: Locally-Aware Dynamic Re-read Attention Net for Sentence Semantic Matching. (arXiv:2108.02915v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02915","description":"<p>Sentence semantic matching requires an agent to determine the semantic\nrelation between two sentences, which is widely used in various natural\nlanguage tasks, such as Natural Language Inference (NLI), Paraphrase\nIdentification (PI), and so on. Much recent progress has been made in this\narea, especially attention-based methods and pre-trained language model based\nmethods. However, most of these methods focus on all the important parts in\nsentences in a static way and only emphasize how important the words are to the\nquery, inhibiting the ability of attention mechanism. In order to overcome this\nproblem and boost the performance of attention mechanism, we propose a novel\ndynamic re-read attention, which can pay close attention to one small region of\nsentences at each step and re-read the important parts for better sentence\nrepresentations. Based on this attention variation, we develop a novel Dynamic\nRe-read Network (DRr-Net) for sentence semantic matching. Moreover, selecting\none small region in dynamic re-read attention seems insufficient for sentence\nsemantics, and employing pre-trained language models as input encoders will\nintroduce incomplete and fragile representation problems. To this end, we\nextend DRrNet to Locally-Aware Dynamic Re-read Attention Net (LadRa-Net), in\nwhich local structure of sentences is employed to alleviate the shortcoming of\nByte-Pair Encoding (BPE) in pre-trained language models and boost the\nperformance of dynamic reread attention. Extensive experiments on two popular\nsentence semantic matching tasks demonstrate that DRr-Net can significantly\nimprove the performance of sentence semantic matching. Meanwhile, LadRa-Net is\nable to achieve better performance by considering the local structures of\nsentences. In addition, it is exceedingly interesting that some discoveries in\nour experiments are consistent with some findings of psychological research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1\">Guangyi Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Le Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02923","description":"<p>Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02941","description":"<p>Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1\">Harm de Wet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Semantic Regression for Text Generation. (arXiv:2108.02984v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02984","description":"<p>Recall the classical text generation works, the generation framework can be\nbriefly divided into two phases: \\textbf{idea reasoning} and \\textbf{surface\nrealization}. The target of idea reasoning is to figure out the main idea which\nwill be presented in the following talking/writing periods. Surface realization\naims to arrange the most appropriate sentence to depict and convey the\ninformation distilled from the main idea. However, the current popular\ntoken-by-token text generation methods ignore this crucial process and suffer\nfrom many serious issues, such as idea/topic drift. To tackle the problems and\nrealize this two-phase paradigm, we propose a new framework named Sentence\nSemantic Regression (\\textbf{SSR}) based on sentence-level language modeling.\nFor idea reasoning, two architectures \\textbf{SSR-AR} and \\textbf{SSR-NonAR}\nare designed to conduct sentence semantic regression autoregressively (like\nGPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a\nmixed-granularity sentence decoder is designed to generate text with better\nconsistency by jointly incorporating the predicted sentence-level main idea as\nwell as the preceding contextual token-level information. We conduct\nexperiments on four tasks of story ending prediction, story ending generation,\ndialogue generation, and sentence infilling. The results show that SSR can\nobtain better performance in terms of automatic metrics and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell me a story about yourself: The words of shopping experience and self-satisfaction. (arXiv:2108.03016v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03016","description":"<p>In this paper we investigate the verbal expression of shopping experience\nobtained by a sample of customers asked to freely verbalize how they felt when\nentering a store. Using novel tools of Text Mining and Social Network Analysis,\nwe analyzed the interviews to understand the connection between the emotions\naroused during the shopping experience, satisfaction and the way participants\nlink these concepts to self-satisfaction and self-identity. The results show a\nprominent role of emotions in the discourse about the shopping experience\nbefore purchasing and an inward-looking connection to the self. Our results\nalso suggest that modern retail environment should enhance the hedonic shopping\nexperience in terms of fun, fantasy, moods, and emotions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Petruzzellis_L/0/1/0/all/0/1\">L Petruzzellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visentin_M/0/1/0/all/0/1\">M Visentin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chebat_J/0/1/0/all/0/1\">J.-C. Chebat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03067","description":"<p>This paper demonstrates a two-stage method for deriving insights from social\nmedia data relating to disinformation by applying a combination of geospatial\nclassification and embedding-based language modelling across multiple\nlanguages. In particular, the analysis in centered on Twitter and\ndisinformation for three European languages: English, French and Spanish.\nFirstly, Twitter data is classified into European and non-European sets using\nBERT. Secondly, Word2vec is applied to the classified texts resulting in\nEurocentric, non-Eurocentric and global representations of the data for the\nthree target languages. This comparative analysis demonstrates not only the\nefficacy of the classification method but also highlights geographic, temporal\nand linguistic differences in the disinformation-related media. Thus, the\ncontributions of the work are threefold: (i) a novel language-independent\ntransformer-based geolocation method; (ii) an analytical approach that exploits\nlexical specificity and word embeddings to interrogate user-generated content;\nand (iii) a dataset of 36 million disinformation related tweets in English,\nFrench and Spanish.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1\">David Tuxworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1\">Alun Preece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1\">David Rogers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWSR: A Chinese Dataset and Lexicon for Online Sexism Detection. (arXiv:2108.03070v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03070","description":"<p>Online sexism has become an increasing concern in social media platforms as\nit has affected the healthy development of the Internet and can have negative\neffects in society. While research in the sexism detection domain is growing,\nmost of this research focuses on English as the language and on Twitter as the\nplatform. Our objective here is to broaden the scope of this research by\nconsidering the Chinese language on Sina Weibo. We propose the first Chinese\nsexism dataset -- Sina Weibo Sexism Review (SWSR) dataset --, as well as a\nlarge Chinese lexicon SexHateLex made of abusive and gender-related terms. We\nintroduce our data collection and annotation process, and provide an\nexploratory analysis of the dataset characteristics to validate its quality and\nto show how sexism is manifested in Chinese. The SWSR dataset provides labels\nat different levels of granularity including (i) sexism or non-sexism, (ii)\nsexism category and (iii) target type, which can be exploited, among others,\nfor building computational methods to identify and investigate finer-grained\ngender-related abusive language. We conduct experiments for the three sexism\nclassification tasks making use of state-of-the-art machine learning models.\nOur results show competitive performance, providing a benchmark for sexism\ndetection in the Chinese language, as well as an error analysis highlighting\nopen challenges needing more research in Chinese NLP. The SWSR dataset and\nSexHateLex lexicon are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaohan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Capsule Network for Hate Speech Detection in Social Media. (arXiv:2108.03089v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03089","description":"<p>Most hate speech detection research focuses on a single language, generally\nEnglish, which limits their generalisability to other languages. In this paper\nwe investigate the cross-lingual hate speech detection task, tackling the\nproblem by adapting the hate speech resources from one language to another. We\npropose a cross-lingual capsule network learning model coupled with extra\ndomain-specific lexical semantics for hate speech (CCNL-Ex). Our model achieves\nstate-of-the-art performance on benchmark datasets from AMI@Evalita2018 and\nAMI@Ibereval2018 involving three languages: English, Spanish and Italian,\noutperforming state-of-the-art baselines on all six language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Aiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1912.07942","description":"<p>To continuously improve quality and reflect changes in data, machine learning\napplications have to regularly retrain and update their core models. We show\nthat a differential analysis of language model snapshots before and after an\nupdate can reveal a surprising amount of detailed information about changes in\nthe training data. We propose two new metrics---\\emph{differential score} and\n\\emph{differential rank}---for analyzing the leakage due to updates of natural\nlanguage models. We perform leakage analysis using these metrics across models\ntrained on several different datasets using different methods and\nconfigurations. We discuss the privacy implications of our findings, propose\nmitigation strategies and evaluate their effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1\">Santiago Zanella-B&#xe9;guelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1\">Victor R&#xfc;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1\">Andrew Paverd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1\">Boris K&#xf6;pf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models. (arXiv:2102.04130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.04130","description":"<p>The capabilities of natural language models trained on large-scale data have\nincreased immensely over the past few years. Open source libraries such as\nHuggingFace have made these models easily available and accessible. While prior\nresearch has identified biases in large language models, this paper considers\nbiases contained in the most popular versions of these models when applied\n`out-of-the-box' for downstream tasks. We focus on generative language models\nas they are well-suited for extracting biases inherited from training data.\nSpecifically, we conduct an in-depth analysis of GPT-2, which is the most\ndownloaded text generation model on HuggingFace, with over half a million\ndownloads in the past month alone. We assess biases related to occupational\nassociations for different protected categories by intersecting gender with\nreligion, sexuality, ethnicity, political affiliation, and continental name\norigin. Using a template-based data collection pipeline, we collect 396K\nsentence completions made by GPT-2 and find: (i) The machine-predicted jobs are\nless diverse and more stereotypical for women than for men, especially for\nintersections; (ii) Intersectional interactions are highly relevant for\noccupational associations, which we quantify by fitting 262 logistic models;\n(iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity\ndistribution found in US Labour Bureau data, and even pulls the\nsocietally-skewed distribution towards gender parity in cases where its\npredictions deviate from real labor market observations. This raises the\nnormative question of what language models _should_ learn - whether they should\nreflect or correct for existing inequalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1\">Yennie Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1\">Haider Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benussi_E/0/1/0/all/0/1\">Elias Benussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpin_F/0/1/0/all/0/1\">Filippo Volpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_F/0/1/0/all/0/1\">Frederic A. Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1\">Aleksandar Shtedritski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.12700","description":"<p>With the increasing scale of search engine marketing, designing an efficient\nbidding system is becoming paramount for the success of e-commerce companies.\nThe critical challenges faced by a modern industrial-level bidding system\ninclude: 1. the catalog is enormous, and the relevant bidding features are of\nhigh sparsity; 2. the large volume of bidding requests induces significant\ncomputation burden to both the offline and online serving. Leveraging\nextraneous user-item information proves essential to mitigate the sparsity\nissue, for which we exploit the natural language signals from the users' query\nand the contextual knowledge from the products. In particular, we extract the\nvector representations of ads via the Transformer model and leverage their\ngeometric relation to building collaborative bidding predictions via\nclustering. The two-step procedure also significantly reduces the computation\nstress of bid evaluation and optimization. In this paper, we introduce the\nend-to-end structure of the bidding system for search engine marketing for\nWalmart e-commerce, which successfully handles tens of millions of bids each\nday. We analyze the online and offline performances of our approach and discuss\nhow we find it as a production-efficient solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1\">Cheng Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation. (arXiv:2108.01682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01682","description":"<p>Multimodal target/aspect sentiment classification combines multimodal\nsentiment analysis and aspect/target sentiment classification. The goal of the\ntask is to combine vision and language to understand the sentiment towards a\ntarget entity in a sentence. Twitter is an ideal setting for the task because\nit is inherently multimodal, highly emotional, and affects real world events.\nHowever, multimodal tweets are short and accompanied by complex, possibly\nirrelevant images. We introduce a two-stream model that translates images in\ninput space using an object-aware transformer followed by a single-pass\nnon-autoregressive text generation approach. We then leverage the translation\nto construct an auxiliary sentence that provides multimodal information to a\nlanguage model. Our approach increases the amount of text available to the\nlanguage model and distills the object-level information in complex images. We\nachieve state-of-the-art performance on two multimodal Twitter datasets without\nmodifying the internals of the language model to accept multimodal data,\ndemonstrating the effectiveness of our translation. In addition, we explain a\nfailure mode of a popular approach for aspect sentiment analysis when applied\nto tweets. Our code is available at\n\\textcolor{blue}{\\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.IR updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.IR","description":"Computer Science -- Information Retrieval (cs.IR) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Real-Time Visual Analysis of High-Volume Social Media Posts. (arXiv:2108.03052v1 [cs.HC])","link":"http://arxiv.org/abs/2108.03052","description":"<p>Breaking news and first-hand reports often trend on social media platforms\nbefore traditional news outlets cover them. The real-time analysis of posts on\nsuch platforms can reveal valuable and timely insights for journalists,\npoliticians, business analysts, and first responders, but the high number and\ndiversity of new posts pose a challenge. In this work, we present an\ninteractive system that enables the visual analysis of streaming social media\ndata on a large scale in real-time. We propose an efficient and explainable\ndynamic clustering algorithm that powers a continuously updated visualization\nof the current thematic landscape as well as detailed visual summaries of\nspecific topics of interest. Our parallel clustering strategy provides an\nadaptive stream with a digestible but diverse selection of recent posts related\nto relevant topics. We also integrate familiar visual metaphors that are highly\ninterlinked for enabling both explorative and more focused monitoring tasks.\nAnalysts can gradually increase the resolution to dive deeper into particular\ntopics. In contrast to previous work, our system also works with non-geolocated\nposts and avoids extensive preprocessing such as detecting events. We evaluated\nour dynamic clustering algorithm and discuss several use cases that show the\nutility of our system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knittel_J/0/1/0/all/0/1\">Johannes Knittel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1\">Steffen Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1\">Tan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingcai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shixia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ertl_T/0/1/0/all/0/1\">Thomas Ertl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation. (arXiv:2103.06523v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2103.06523","description":"<p>BERT-based Neural Ranking Models (NRMs) can be classified according to how\nthe query and document are encoded through BERT's self-attention layers -\nbi-encoder versus cross-encoder. Bi-encoder models are highly efficient because\nall the documents can be pre-processed before the query time, but their\nperformance is inferior compared to cross-encoder models. Both models utilize a\nranker that receives BERT representations as the input and generates a\nrelevance score as the output. In this work, we propose a method where\nmulti-teacher distillation is applied to a cross-encoder NRM and a bi-encoder\nNRM to produce a bi-encoder NRM with two rankers. The resulting student\nbi-encoder achieves an improved performance by simultaneously learning from a\ncross-encoder teacher and a bi-encoder teacher and also by combining relevance\nscores from the two rankers. We call this method TRMD (Two Rankers and\nMulti-teacher Distillation). In the experiments, TwinBERT and ColBERT are\nconsidered as baseline bi-encoders. When monoBERT is used as the cross-encoder\nteacher, together with either TwinBERT or ColBERT as the bi-encoder teacher,\nTRMD produces a student bi-encoder that performs better than the corresponding\nbaseline bi-encoder. For P@20, the maximum improvement was 11.4%, and the\naverage improvement was 6.8%. As an additional experiment, we considered\nproducing cross-encoder students with TRMD, and found that it could also\nimprove the cross-encoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaekeol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1\">Euna Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1\">Jangwon Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1\">Wonjong Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Bandwidth Auto-encoder for Hybrid Recommender Systems. (arXiv:2105.07597v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2105.07597","description":"<p>Hybrid recommendations have recently attracted a lot of attention where user\nfeatures are utilized as auxiliary information to address the sparsity problem\ncaused by insufficient user-item interactions. However, extracted user features\ngenerally contain rich multimodal information, and most of them are irrelevant\nto the recommendation purpose. Therefore, excessive reliance on these features\nwill make the model overfit on noise and difficult to generalize. In this\narticle, we propose a variational bandwidth auto-encoder (VBAE) for\nrecommendations, aiming to address the sparsity and noise problems\nsimultaneously. VBAE first encodes user collaborative and feature information\ninto Gaussian latent variables via deep neural networks to capture non-linear\nuser similarities. Moreover, by considering the fusion of collaborative and\nfeature variables as a virtual communication channel from an\ninformation-theoretic perspective, we introduce a user-dependent channel to\ndynamically control the information allowed to be accessed from the feature\nembeddings. A quantum-inspired uncertainty measurement of the hidden rating\nembeddings is proposed accordingly to infer the channel bandwidth by\ndisentangling the uncertainty information in the ratings from the semantic\ninformation. Through this mechanism, VBAE incorporates adequate auxiliary\ninformation from user features if collaborative information is insufficient,\nwhile avoiding excessive reliance on noisy user features to improve its\ngeneralization ability to new users. Extensive experiments conducted on three\nreal-world datasets demonstrate the effectiveness of the proposed method. Codes\nand datasets are released at https://github.com/yaochenzhu/vbae.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaochen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zipf Matrix Factorization : Matrix Factorization with Matthew Effect Reduction. (arXiv:2106.07347v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2106.07347","description":"<p>Recommender system recommends interesting items to users based on users' past\ninformation history. Researchers have been paying attention to improvement of\nalgorithmic performance such as MAE and precision@K. Major techniques such as\nmatrix factorization and learning to rank are optimized based on such\nevaluation metrics. However, the intrinsic Matthew Effect problem poses great\nthreat to the fairness of the recommender system, and the unfairness problem\ncannot be resolved by optimization of traditional metrics. In this paper, we\npropose a novel algorithm that incorporates Matthew Effect reduction with the\nmatrix factorization framework. We demonstrate that our approach can boost the\nfairness of the algorithm and enhances performance evaluated by traditional\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Information Retrieval"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.MM updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.MM","description":"Computer Science -- Multimedia (cs.MM) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02953","description":"<p>This paper investigates a valuable setting called few-shot unsupervised\ndomain adaptation (FS-UDA), which has not been sufficiently studied in the\nliterature. In this setting, the source domain data are labelled, but with\nfew-shot per category, while the target domain data are unlabelled. To address\nthe FS-UDA setting, we develop a general UDA model to solve the following two\nkey issues: the few-shot labeled data per category and the domain adaptation\nbetween support and query sets. Our model is general in that once trained it\nwill be able to be applied to various FS-UDA tasks from the same source and\ntarget domains. Inspired by the recent local descriptor based few-shot learning\n(FSL), our general UDA model is fully built upon local descriptors (LDs) for\nimage classification and domain adaptation. By proposing a novel concept called\nsimilarity patterns (SPs), our model not only effectively considers the spatial\nrelationship of LDs that was ignored in previous FSL methods, but also makes\nthe learned image similarity better serve the required domain alignment.\nSpecifically, we propose a novel IMage-to-class sparse Similarity Encoding\n(IMSE) method. It learns SPs to extract the local discriminative information\nfor classification and meanwhile aligns the covariance matrix of the SPs for\ndomain adaptation. Also, domain adversarial training and multi-scale local\nfeature matching are performed upon LDs. Extensive experiments conducted on a\nmulti-domain benchmark dataset DomainNet demonstrates the state-of-the-art\nperformance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,\nour IMSE can also show better performance than most of recent FSL methods on\nminiImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wanqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Multimedia"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02798","description":"<p>Fundus photography is the primary method for retinal imaging and essential\nfor diabetic retinopathy prevention. Automated segmentation of fundus\nphotographs would improve the quality, capacity, and cost-effectiveness of eye\ncare screening programs. However, current segmentation methods are not robust\ntowards the diversity in imaging conditions and pathologies typical for\nreal-world clinical applications. To overcome these limitations, we utilized\ncontrastive self-supervised learning to exploit the large variety of unlabeled\nfundus images in the publicly available EyePACS dataset. We pre-trained an\nencoder of a U-Net, which we later fine-tuned on several retinal vessel and\nlesion segmentation datasets. We demonstrate for the first time that by using\ncontrastive self-supervised learning, the pre-trained network can recognize\nblood vessels, optic disc, fovea, and various lesions without being provided\nany labels. Furthermore, when fine-tuned on a downstream blood vessel\nsegmentation task, such pre-trained networks achieve state-of-the-art\nperformance on images from different datasets. Additionally, the pre-training\nalso leads to shorter training times and an improved few-shot performance on\nboth blood vessel and lesion segmentation tasks. Altogether, our results\nshowcase the benefits of contrastive self-supervised pre-training which can\nplay a crucial role in real-world clinical applications requiring robust models\nable to adapt to new devices with only a few annotated samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1\">Anja Zenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1\">Marcel Kollovieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1\">Dominik J&#xfc;stel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1\">Vasilis Ntziachristos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A volumetric change detection framework using UAV oblique photogrammetry - A case study of ultra-high-resolution monitoring of progressive building collapse. (arXiv:2108.02800v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02800","description":"<p>In this paper, we present a case study that performs an unmanned aerial\nvehicle (UAV) based fine-scale 3D change detection and monitoring of\nprogressive collapse performance of a building during a demolition event.\nMulti-temporal oblique photogrammetry images are collected with 3D point clouds\ngenerated at different stages of the demolition. The geometric accuracy of the\ngenerated point clouds has been evaluated against both airborne and terrestrial\nLiDAR point clouds, achieving an average distance of 12 cm and 16 cm for roof\nand facade respectively. We propose a hierarchical volumetric change detection\nframework that unifies multi-temporal UAV images for pose estimation (free of\nground control points), reconstruction, and a coarse-to-fine 3D density change\nanalysis. This work has provided a solution capable of addressing change\ndetection on full 3D time-series datasets where dramatic scene content changes\nare presented progressively. Our change detection results on the building\ndemolition event have been evaluated against the manually marked ground-truth\nchanges and have achieved an F-1 score varying from 0.78 to 0.92, with\nconsistently high precision (0.92 - 0.99). Volumetric changes through the\ndemolition progress are derived from change detection and have shown to\nfavorably reflect the qualitative and quantitative building demolition\nprogression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ningli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Debao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strasbaugh_C/0/1/0/all/0/1\">Chris Strasbaugh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezen_H/0/1/0/all/0/1\">Halil Sezen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Twins Talk & Alternative Calculations. (arXiv:2108.02807v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02807","description":"<p>Inspired by how the human brain employs a higher number of neural pathways\nwhen describing a highly focused subject, we show that deep attentive models\nused for the main vision-language task of image captioning, could be extended\nto achieve better performance. Image captioning bridges a gap between computer\nvision and natural language processing. Automated image captioning is used as a\ntool to eliminate the need for human agent for creating descriptive captions\nfor unseen images.Automated image captioning is challenging and yet\ninteresting. One reason is that AI based systems capable of generating\nsentences that describe an input image could be used in a wide variety of tasks\nbeyond generating captions for unseen images found on web or uploaded to social\nmedia. For example, in biology and medical sciences, these systems could\nprovide researchers and physicians with a brief linguistic description of\nrelevant images, potentially expediting their work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zohourianshahzadi_Z/0/1/0/all/0/1\">Zanyar Zohourianshahzadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1\">Jugal K. Kalita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications. (arXiv:2108.02818v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02818","description":"<p>Recently, there have been breakthroughs in computer vision (\"CV\") models that\nare more generalizable with the advent of models such as CLIP and ALIGN. In\nthis paper, we analyze CLIP and highlight some of the challenges such models\npose. CLIP reduces the need for task specific training data, potentially\nopening up many niche tasks to automation. CLIP also allows its users to\nflexibly specify image classification classes in natural language, which we\nfind can shift how biases manifest. Additionally, through some preliminary\nprobes we find that CLIP can inherit biases found in prior computer vision\nsystems. Given the wide and unpredictable domain of uses for such models, this\nraises questions regarding what sufficiently safe behaviour for such systems\nmay look like. These results add evidence to the growing body of work calling\nfor a change in the notion of a 'better' model--to move beyond simply looking\nat higher accuracy at task-oriented capability evaluations, and towards a\nbroader 'better' that takes into account deployment-critical features such as\ndifferent use contexts, and people who interact with the model when thinking\nabout model deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sandhini Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jack Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1\">Alec Radford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1\">Miles Brundage</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning. (arXiv:2108.02832v1 [eess.IV])","link":"http://arxiv.org/abs/2108.02832","description":"<p>Most of the existing works in supervised spatio-temporal video\nsuper-resolution (STVSR) heavily rely on a large-scale external dataset\nconsisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution\nhigh-frame-rate (HR-HFR) videos. Despite their remarkable performance, these\nmethods make a prior assumption that the low-resolution video is obtained by\ndown-scaling the high-resolution video using a known degradation kernel, which\ndoes not hold in practical settings. Another problem with these methods is that\nthey cannot exploit instance-specific internal information of video at testing\ntime. Recently, deep internal learning approaches have gained attention due to\ntheir ability to utilize the instance-specific statistics of a video. However,\nthese methods have a large inference time as they require thousands of gradient\nupdates to learn the intrinsic structure of the data. In this work, we\npresentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as\nwell as internal, information through meta-transfer learning and internal\nlearning, respectively. Specifically, meta-learning is employed to obtain\nadaptive parameters, using a large-scale external dataset, that can adapt\nquickly to the novel condition (degradation model) of the given test video\nduring the internal learning task, thereby exploiting external and internal\ninformation of a video for super-resolution. The model trained using our\napproach can quickly adapt to a specific video condition with only a few\ngradient updates, which reduces the inference time significantly. Extensive\nexperiments on standard datasets demonstrate that our method performs favorably\nagainst various state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Akash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jonnalagedda_P/0/1/0/all/0/1\">Padmaja Jonnalagedda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elaborative Rehearsal for Zero-shot Action Recognition. (arXiv:2108.02833v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02833","description":"<p>The growing number of action classes has posed a new challenge for video\nunderstanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.\nThe ZSAR task aims to recognize target (unseen) actions without training\nexamples by leveraging semantic representations to bridge seen and unseen\nactions. However, due to the complexity and diversity of actions, it remains\nchallenging to semantically represent action classes and transfer knowledge\nfrom seen data. In this work, we propose an ER-enhanced ZSAR model inspired by\nan effective human memory technique Elaborative Rehearsal (ER), which involves\nelaborating a new concept and relating it to known concepts. Specifically, we\nexpand each action class as an Elaborative Description (ED) sentence, which is\nmore discriminative than a class name and less costly than manual-defined\nattributes. Besides directly aligning class semantics with videos, we\nincorporate objects from the video as Elaborative Concepts (EC) to improve\nvideo semantics and generalization from seen actions to unseen actions. Our\nER-enhanced ZSAR model achieves state-of-the-art results on three existing\nbenchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics\ndataset to overcome limitations of current benchmarks and demonstrate the first\ncase where ZSAR performance is comparable to few-shot learning baselines on\nthis more realistic setting. We will release our codes and collected EDs at\nhttps://github.com/DeLightCMU/ElaborativeRehearsal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based fusion of semantic boundary and non-boundary information to improve semantic segmentation. (arXiv:2108.02840v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02840","description":"<p>This paper introduces a method for image semantic segmentation grounded on a\nnovel fusion scheme, which takes place inside a deep convolutional neural\nnetwork. The main goal of our proposal is to explore object boundary\ninformation to improve the overall segmentation performance. Unlike previous\nworks that combine boundary and segmentation features, or those that use\nboundary information to regularize semantic segmentation, we instead propose a\nnovel approach that embodies boundary information onto segmentation. For that,\nour semantic segmentation method uses two streams, which are combined through\nan attention gate, forming an end-to-end Y-model. To the best of our knowledge,\nours is the first work to show that boundary detection can improve semantic\nsegmentation when fused through a semantic fusion gate (attention model). We\nperformed an extensive evaluation of our method over public data sets. We found\ncompetitive results on all data sets after comparing our proposed model with\nother twelve state-of-the-art segmenters, considering the same training\nconditions. Our proposed model achieved the best mIoU on the CityScapes,\nCamVid, and Pascal Context data sets, and the second best on Mapillary Vistas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fontinele_J/0/1/0/all/0/1\">Jefferson Fontinele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefundes_G/0/1/0/all/0/1\">Gabriel Lefundes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1\">Luciano Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])","link":"http://arxiv.org/abs/2108.02846","description":"<p>Human-robot collaboration is an essential research topic in artificial\nintelligence (AI), enabling researchers to devise cognitive AI systems and\naffords an intuitive means for users to interact with the robot. Of note,\ncommunication plays a central role. To date, prior studies in embodied agent\nnavigation have only demonstrated that human languages facilitate communication\nby instructions in natural languages. Nevertheless, a plethora of other forms\nof communication is left unexplored. In fact, human communication originated in\ngestures and oftentimes is delivered through multimodal cues, e.g. \"go there\"\nwith a pointing gesture. To bridge the gap and fill in the missing dimension of\ncommunication in embodied agent navigation, we propose investigating the\neffects of using gestures as the communicative interface instead of verbal\ncues. Specifically, we develop a VR-based 3D simulation environment, named\nGes-THOR, based on AI2-THOR platform. In this virtual environment, a human\nplayer is placed in the same virtual scene and shepherds the artificial agent\nusing only gestures. The agent is tasked to solve the navigation problem guided\nby natural gestures with unknown semantics; we do not use any predefined\ngestures due to the diversity and versatile nature of human gestures. We argue\nthat learning the semantics of natural gestures is mutually beneficial to\nlearning the navigation task--learn to communicate and communicate to learn. In\na series of experiments, we demonstrate that human gesture cues, even without\npredefined semantics, improve the object-goal navigation for an embodied agent,\noutperforming various state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-Ju Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DRIMR: 3D Reconstruction and Imaging via mmWave Radar based on Deep Learning. (arXiv:2108.02858v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02858","description":"<p>mmWave radar has been shown as an effective sensing technique in low\nvisibility, smoke, dusty, and dense fog environment. However tapping the\npotential of radar sensing to reconstruct 3D object shapes remains a great\nchallenge, due to the characteristics of radar data such as sparsity, low\nresolution, specularity, high noise, and multi-path induced shadow reflections\nand artifacts. In this paper we propose 3D Reconstruction and Imaging via\nmmWave Radar (3DRIMR), a deep learning based architecture that reconstructs 3D\nshape of an object in dense detailed point cloud format, based on sparse raw\nmmWave radar intensity data. The architecture consists of two back-to-back\nconditional GAN deep neural networks: the first generator network generates 2D\ndepth images based on raw radar intensity data, and the second generator\nnetwork outputs 3D point clouds based on the results of the first generator.\nThe architecture exploits both convolutional neural network's convolutional\noperation (that extracts local structure neighborhood information) and the\nefficiency and detailed geometry capture capability of point clouds (other than\ncostly voxelization of 3D space or distance fields). Our experiments have\ndemonstrated 3DRIMR's effectiveness in reconstructing 3D objects, and its\nperformance improvement over standard techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yue Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhuoming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Deqiang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02870","description":"<p>Covid-19 detection at an early stage can aid in an effective treatment and\nisolation plan to prevent its spread. Recently, transfer learning has been used\nfor Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major\nlimitations inherent to these proposed methods is limited labeled dataset size\nthat affects the reliability of Covid-19 diagnosis and disease progression. In\nthis work, we demonstrate that how we can augment limited X-ray images data by\nusing Contrast limited adaptive histogram equalization (CLAHE) to train the\nlast layer of the pre-trained deep learning models to mitigate the bias of\ntransfer learning for Covid-19 detection. We transfer learned various\npre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,\nand GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.\nThe experiment results reveal that the CLAHE-based augmentation to various\npre-trained deep learning models significantly improves the model efficiency.\nThe pre-trained VCG-16 model with CLAHEbased augmented images achieves a\nsensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when\ntrained on non-augmented data. Other models demonstrate a value of less than\n60% when trained on non-augmented data. Our results reveal that the sample bias\ncan negatively impact the performance of transfer learning which is\nsignificantly improved by using CLAHE-based augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1\">Aparna Reji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Lifespan Face Synthesis. (arXiv:2108.02874v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02874","description":"<p>A lifespan face synthesis (LFS) model aims to generate a set of\nphoto-realistic face images of a person's whole life, given only one snapshot\nas reference. The generated face image given a target age code is expected to\nbe age-sensitive reflected by bio-plausible transformations of shape and\ntexture, while being identity preserving. This is extremely challenging because\nthe shape and texture characteristics of a face undergo separate and highly\nnonlinear transformations w.r.t. age. Most recent LFS models are based on\ngenerative adversarial networks (GANs) whereby age code conditional\ntransformations are applied to a latent face representation. They benefit\ngreatly from the recent advancements of GANs. However, without explicitly\ndisentangling their latent representations into the texture, shape and identity\nfactors, they are fundamentally limited in modeling the nonlinear age-related\ntransformation on texture and shape whilst preserving identity. In this work, a\nnovel LFS model is proposed to disentangle the key face characteristics\nincluding shape, texture and identity so that the unique shape and texture age\ntransformations can be modeled effectively. This is achieved by extracting\nshape, texture and identity features separately from an encoder. Critically,\ntwo transformation modules, one conditional convolution based and the other\nchannel attention based, are designed for modeling the nonlinear shape and\ntexture feature transformations respectively. This is to accommodate their\nrather distinct aging processes and ensure that our synthesized images are both\nage-sensitive and identity preserving. Extensive experiments show that our LFS\nmodel is clearly superior to the state-of-the-art alternatives. Codes and demo\nare available on our project website:\n\\url{https://senhe.github.io/projects/iccv_2021_lifespan_face}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-domain Generalization from a Single Source: A Uncertainty Quantification Approach. (arXiv:2108.02888v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02888","description":"<p>We study a worst-case scenario in generalization: Out-of-domain\ngeneralization from a single source. The goal is to learn a robust model from a\nsingle source and expect it to generalize over many unknown distributions. This\nchallenging problem has been seldom investigated while existing solutions\nsuffer from various limitations such as the ignorance of uncertainty assessment\nand label augmentation. In this paper, we propose uncertainty-guided domain\ngeneralization to tackle the aforementioned limitations. The key idea is to\naugment the source capacity in both feature and label spaces, while the\naugmentation is guided by uncertainty assessment. To the best of our knowledge,\nthis is the first work to (1) quantify the generalization uncertainty from a\nsingle source and (2) leverage it to guide both feature and label augmentation\nfor robust generalization. The model training and deployment are effectively\norganized in a Bayesian meta-learning framework. We conduct extensive\ncomparisons and ablation study to validate our approach. The results prove our\nsuperior performance in a wide scope of tasks including image classification,\nsemantic segmentation, text classification, and speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1\">Fengchun Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Basis Scaling and Double Pruning for Efficient Transfer Learning. (arXiv:2108.02893v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02893","description":"<p>Transfer learning allows the reuse of deep learning features on new datasets\nwith limited data. However, the resulting models could be unnecessarily large\nand thus inefficient. Although network pruning can be applied to improve\ninference efficiency, existing algorithms usually require fine-tuning and may\nnot be suitable for small datasets. In this paper, we propose an algorithm that\ntransforms the convolutional weights into the subspaces of orthonormal bases\nwhere a model is pruned. Using singular value decomposition, we decompose a\nconvolutional layer into two layers: a convolutional layer with the orthonormal\nbasis vectors as the filters, and a layer that we name \"BasisScalingConv\",\nwhich is responsible for rescaling the features and transforming them back to\nthe original space. As the filters in each transformed layer are linearly\nindependent with known relative importance, pruning can be more effective and\nstable, and fine tuning individual weights is unnecessary. Furthermore, as the\nnumbers of input and output channels of the original convolutional layer remain\nunchanged, basis pruning is applicable to virtually all network architectures.\nBasis pruning can also be combined with existing pruning algorithms for double\npruning to further increase the pruning capability. With less than 1% reduction\nin the classification accuracy, we can achieve pruning ratios up to 98.9% in\nparameters and 98.6% in FLOPs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Ken C. L. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1\">Satyananda Kashyap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Mehdi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02923","description":"<p>Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02924","description":"<p>While image understanding on recognition-level has achieved remarkable\nadvancements, reliable visual scene understanding requires comprehensive image\nunderstanding on recognition-level but also cognition-level, which calls for\nexploiting the multi-source information as well as learning different levels of\nunderstanding and extensive commonsense knowledge. In this paper, we propose a\nnovel Cognitive Attention Network (CAN) for visual commonsense reasoning to\nachieve interpretable visual understanding. Specifically, we first introduce an\nimage-text fusion module to fuse information from images and text collectively.\nSecond, a novel inference module is designed to encode commonsense among image,\nquery and response. Extensive experiments on large-scale Visual Commonsense\nReasoning (VCR) benchmark dataset demonstrate the effectiveness of our\napproach. The implementation is publicly available at\nhttps://github.com/tanjatang/CAN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1\">Kea Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1\">Tyler Derr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features. (arXiv:2108.02927v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02927","description":"<p>Image Retrieval is a fundamental task of obtaining images similar to the\nquery one from a database. A common image retrieval practice is to firstly\nretrieve candidate images via similarity search using global image features and\nthen re-rank the candidates by leveraging their local features. Previous\nlearning-based studies mainly focus on either global or local image\nrepresentation learning to tackle the retrieval task. In this paper, we abandon\nthe two-stage paradigm and seek to design an effective single-stage solution by\nintegrating local and global information inside images into compact image\nrepresentations. Specifically, we propose a Deep Orthogonal Local and Global\n(DOLG) information fusion framework for end-to-end image retrieval. It\nattentively extracts representative local information with multi-atrous\nconvolutions and self-attention at first. Components orthogonal to the global\nimage representation are then extracted from the local information. At last,\nthe orthogonal components are concatenated with the global representation as a\ncomplementary, and then aggregation is performed to generate the final\nrepresentation. The whole framework is end-to-end differentiable and can be\ntrained with image-level labels. Extensive experimental results validate the\neffectiveness of our solution and show that our model achieves state-of-the-art\nimage retrieval performances on Revisited Oxford and Paris datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Miao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Baorong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xuetong Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jizhou Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VinaFood21: A Novel Dataset for Evaluating Vietnamese Food Recognition. (arXiv:2108.02929v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02929","description":"<p>Vietnam is such an attractive tourist destination with its stunning and\npristine landscapes and its top-rated unique food and drink. Among thousands of\nVietnamese dishes, foreigners and native people are interested in easy-to-eat\ntastes and easy-to-do recipes, along with reasonable prices, mouthwatering\nflavors, and popularity. Due to the diversity and almost all the dishes have\nsignificant similarities and the lack of quality Vietnamese food datasets, it\nis hard to implement an auto system to classify Vietnamese food, therefore,\nmake people easier to discover Vietnamese food. This paper introduces a new\nVietnamese food dataset named VinaFood21, which consists of 13,950 images\ncorresponding to 21 dishes. We use 10,044 images for model training and 6,682\ntest images to classify each food in the VinaFood21 dataset and achieved an\naverage accuracy of 74.81% when fine-tuning CNN EfficientNet-B0.\n(https://github.com/nguyenvd-uit/uit-together-dataset)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuan Trong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuan Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1\">Dung Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Ngoc Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_N/0/1/0/all/0/1\">Nguyen D. Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Khang Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detailed Avatar Recovery from Single Image. (arXiv:2108.02931v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02931","description":"<p>This paper presents a novel framework to recover \\emph{detailed} avatar from\na single image. It is a challenging task due to factors such as variations in\nhuman shapes, body poses, texture, and viewpoints. Prior methods typically\nattempt to recover the human body shape using a parametric-based template that\nlacks the surface details. As such resulting body shape appears to be without\nclothing. In this paper, we propose a novel learning-based framework that\ncombines the robustness of the parametric model with the flexibility of\nfree-form 3D deformation. We use the deep neural networks to refine the 3D\nshape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the\nconstraints from body joints, silhouettes, and per-pixel shading information.\nOur method can restore detailed human body shapes with complete textures beyond\nskinned models. Experiments demonstrate that our method has outperformed\nprevious state-of-the-art approaches, achieving better accuracy in terms of\nboth 2D IoU number and 3D metric distance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haotian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Synthetic to Real: Image Dehazing Collaborating with Unlabeled Real Data. (arXiv:2108.02934v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02934","description":"<p>Single image dehazing is a challenging task, for which the domain shift\nbetween synthetic training data and real-world testing images usually leads to\ndegradation of existing methods. To address this issue, we propose a novel\nimage dehazing framework collaborating with unlabeled real data. First, we\ndevelop a disentangled image dehazing network (DID-Net), which disentangles the\nfeature representations into three component maps, i.e. the latent haze-free\nimage, the transmission map, and the global atmospheric light estimate,\nrespecting the physical model of a haze process. Our DID-Net predicts the three\ncomponent maps by progressively integrating features across scales, and refines\neach map by passing an independent refinement network. Then a\ndisentangled-consistency mean-teacher network (DMT-Net) is employed to\ncollaborate unlabeled real data for boosting single image dehazing.\nSpecifically, we encourage the coarse predictions and refinements of each\ndisentangled component to be consistent between the student and teacher\nnetworks by using a consistency loss on unlabeled real data. We make comparison\nwith 13 state-of-the-art dehazing methods on a new collected dataset (Haze4K)\nand two widely-used dehazing datasets (i.e., SOTS and HazeRD), as well as on\nreal-world hazy images. Experimental results demonstrate that our method has\nobvious quantitative and qualitative improvements over the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Shunda Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Liang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-frequency shape recovery from shading by CNN and domain adaptation. (arXiv:2108.02937v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02937","description":"<p>Importance of structured-light based one-shot scanning technique is\nincreasing because of its simple system configuration and ability of capturing\nmoving objects. One severe limitation of the technique is that it can capture\nonly sparse shape, but not high frequency shapes, because certain area of\nprojection pattern is required to encode spatial information. In this paper, we\npropose a technique to recover high-frequency shapes by using shading\ninformation, which is captured by one-shot RGB-D sensor based on structured\nlight with single camera. Since color image comprises shading information of\nobject surface, high-frequency shapes can be recovered by shape from shading\ntechniques. Although multiple images with different lighting positions are\nrequired for shape from shading techniques, we propose a learning based\napproach to recover shape from a single image. In addition, to overcome the\nproblem of preparing sufficient amount of data for training, we propose a new\ndata augmentation method for high-frequency shapes using synthetic data and\ndomain adaptation. Experimental results are shown to confirm the effectiveness\nof the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tokieda_K/0/1/0/all/0/1\">Kodai Tokieda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwaguchi_T/0/1/0/all/0/1\">Takafumi Iwaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1\">Hiroshi Kawasaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. (arXiv:2108.02938v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02938","description":"<p>Denoising diffusion probabilistic models (DDPM) have shown remarkable\nperformance in unconditional image generation. However, due to the\nstochasticity of the generative process in DDPM, it is challenging to generate\nimages with the desired semantics. In this work, we propose Iterative Latent\nVariable Refinement (ILVR), a method to guide the generative process in DDPM to\ngenerate high-quality images based on a given reference image. Here, the\nrefinement of the generative process in DDPM enables a single DDPM to sample\nimages from various sets directed by the reference image. The proposed ILVR\nmethod generates high-quality images while controlling the generation. The\ncontrollability of our method allows adaptation of a single DDPM without any\nadditional learning in various image generation tasks, such as generation from\nvarious downsampling factors, multi-domain image translation, paint-to-image,\nand editing with scribbles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jooyoung Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1\">Yonghyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1\">Youngjune Gwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02940","description":"<p>In recent years, many deep learning models have been adopted in autonomous\ndriving. At the same time, these models introduce new vulnerabilities that may\ncompromise the safety of autonomous vehicles. Specifically, recent studies have\ndemonstrated that adversarial attacks can cause a significant decline in\ndetection precision of deep learning-based 3D object detection models. Although\ndriving safety is the ultimate concern for autonomous driving, there is no\ncomprehensive study on the linkage between the performance of deep learning\nmodels and the driving safety of autonomous vehicles under adversarial attacks.\nIn this paper, we investigate the impact of two primary types of adversarial\nattacks, perturbation attacks and patch attacks, on the driving safety of\nvision-based autonomous vehicles rather than the detection precision of deep\nlearning models. In particular, we consider two state-of-the-art models in\nvision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving\nsafety, we propose an end-to-end evaluation framework with a set of driving\nsafety performance metrics. By analyzing the results of our extensive\nevaluation experiments, we find that (1) the attack's impact on the driving\nsafety of autonomous vehicles and the attack's impact on the precision of 3D\nobject detectors are decoupled, and (2) the DSGN model demonstrates stronger\nrobustness to adversarial attacks than the Stereo R-CNN model. In addition, we\nfurther investigate the causes behind the two findings with an ablation study.\nThe findings of this paper provide a new perspective to evaluate adversarial\nattacks and guide the selection of deep learning models in autonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kejie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohua Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Based Food Analysis for Automatic Dietary Assessment. (arXiv:2108.02947v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02947","description":"<p>Background: Maintaining a healthy diet is vital to avoid health-related\nissues, e.g., undernutrition, obesity and many non-communicable diseases. An\nindispensable part of the health diet is dietary assessment. Traditional manual\nrecording methods are burdensome and contain substantial biases and errors.\nRecent advances in Artificial Intelligence, especially computer vision\ntechnologies, have made it possible to develop automatic dietary assessment\nsolutions, which are more convenient, less time-consuming and even more\naccurate to monitor daily food intake.\n</p>\n<p>Scope and approach: This review presents one unified Vision-Based Dietary\nAssessment (VBDA) framework, which generally consists of three stages: food\nimage analysis, volume estimation and nutrient derivation. Vision-based food\nanalysis methods, including food recognition, detection and segmentation, are\nsystematically summarized, and methods of volume estimation and nutrient\nderivation are also given. The prosperity of deep learning makes VBDA gradually\nmove to an end-to-end implementation, which applies food images to a single\nnetwork to directly estimate the nutrition. The recently proposed end-to-end\nmethods are also discussed. We further analyze existing dietary assessment\ndatasets, indicating that one large-scale benchmark is urgently needed, and\nfinally highlight key challenges and future trends for VBDA.\n</p>\n<p>Key findings and conclusions: After thorough exploration, we find that\nmulti-task end-to-end deep learning approaches are one important trend of VBDA.\nDespite considerable research progress, many challenges remain for VBDA due to\nthe meal complexity. We also provide the latest ideas for future development of\nVBDA, e.g., fine-grained food analysis and accurate volume estimation. This\nsurvey aims to encourage researchers to propose more practical solutions for\nVBDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Biological Anatomical Landmark Detection in Colonoscopy Videos. (arXiv:2108.02948v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02948","description":"<p>Colonoscopy is a standard imaging tool for visualizing the entire\ngastrointestinal (GI) tract of patients to capture lesion areas. However, it\ntakes the clinicians excessive time to review a large number of images\nextracted from colonoscopy videos. Thus, automatic detection of biological\nanatomical landmarks within the colon is highly demanded, which can help reduce\nthe burden of clinicians by providing guidance information for the locations of\nlesion areas. In this article, we propose a novel deep learning-based approach\nto detect biological anatomical landmarks in colonoscopy videos. First, raw\ncolonoscopy video sequences are pre-processed to reject interference frames.\nSecond, a ResNet-101 based network is used to detect three biological\nanatomical landmarks separately to obtain the intermediate detection results.\nThird, to achieve more reliable localization of the landmark periods within the\nwhole video period, we propose to post-process the intermediate detection\nresults by identifying the incorrectly predicted frames based on their temporal\ndistribution and reassigning them back to the correct class. Finally, the\naverage detection accuracy reaches 99.75\\%. Meanwhile, the average IoU of 0.91\nshows a high degree of similarity between our predicted landmark periods and\nground truth. The experimental results demonstrate that our proposed model is\ncapable of accurately detecting and localizing biological anatomical landmarks\nfrom colonoscopy videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1\">Kaiwei Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chengwei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yibing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Nachuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiankun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Max Q.-H. Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02953","description":"<p>This paper investigates a valuable setting called few-shot unsupervised\ndomain adaptation (FS-UDA), which has not been sufficiently studied in the\nliterature. In this setting, the source domain data are labelled, but with\nfew-shot per category, while the target domain data are unlabelled. To address\nthe FS-UDA setting, we develop a general UDA model to solve the following two\nkey issues: the few-shot labeled data per category and the domain adaptation\nbetween support and query sets. Our model is general in that once trained it\nwill be able to be applied to various FS-UDA tasks from the same source and\ntarget domains. Inspired by the recent local descriptor based few-shot learning\n(FSL), our general UDA model is fully built upon local descriptors (LDs) for\nimage classification and domain adaptation. By proposing a novel concept called\nsimilarity patterns (SPs), our model not only effectively considers the spatial\nrelationship of LDs that was ignored in previous FSL methods, but also makes\nthe learned image similarity better serve the required domain alignment.\nSpecifically, we propose a novel IMage-to-class sparse Similarity Encoding\n(IMSE) method. It learns SPs to extract the local discriminative information\nfor classification and meanwhile aligns the covariance matrix of the SPs for\ndomain adaptation. Also, domain adversarial training and multi-scale local\nfeature matching are performed upon LDs. Extensive experiments conducted on a\nmulti-domain benchmark dataset DomainNet demonstrates the state-of-the-art\nperformance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,\nour IMSE can also show better performance than most of recent FSL methods on\nminiImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wanqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smooth Mesh Estimation from Depth Data using Non-Smooth Convex Optimization. (arXiv:2108.02957v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02957","description":"<p>Meshes are commonly used as 3D maps since they encode the topology of the\nscene while being lightweight.\n</p>\n<p>Unfortunately, 3D meshes are mathematically difficult to handle directly\nbecause of their combinatorial and discrete nature.\n</p>\n<p>Therefore, most approaches generate 3D meshes of a scene after fusing depth\ndata using volumetric or other representations.\n</p>\n<p>Nevertheless, volumetric fusion remains computationally expensive both in\nterms of speed and memory.\n</p>\n<p>In this paper, we leapfrog these intermediate representations and build a 3D\nmesh directly from a depth map and the sparse landmarks triangulated with\nvisual odometry.\n</p>\n<p>To this end, we formulate a non-smooth convex optimization problem that we\nsolve using a primal-dual method.\n</p>\n<p>Our approach generates a smooth and accurate 3D mesh that substantially\nimproves the state-of-the-art on direct mesh reconstruction while running in\nreal-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosinol_A/0/1/0/all/0/1\">Antoni Rosinol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Meta-class Memory for Few-Shot Semantic Segmentation. (arXiv:2108.02958v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02958","description":"<p>Currently, the state-of-the-art methods treat few-shot semantic segmentation\ntask as a conditional foreground-background segmentation problem, assuming each\nclass is independent. In this paper, we introduce the concept of meta-class,\nwhich is the meta information (e.g. certain middle-level features) shareable\namong all classes. To explicitly learn meta-class representations in few-shot\nsegmentation task, we propose a novel Meta-class Memory based few-shot\nsegmentation method (MM-Net), where we introduce a set of learnable memory\nembeddings to memorize the meta-class information during the base class\ntraining and transfer to novel classes during the inference stage. Moreover,\nfor the $k$-shot scenario, we propose a novel image quality measurement module\nto select images from the set of support images. A high-quality class prototype\ncould be obtained with the weighted sum of support image features based on the\nquality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that\nour proposed method is able to achieve state-of-the-art results in both 1-shot\nand 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\\% mIoU on\nthe COCO dataset in 1-shot setting, which is 5.1\\% higher than the previous\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1\">Guosheng lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Tuning: Joint Prototype Transfer and Structure Regularization for Compatible Feature Learning. (arXiv:2108.02959v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02959","description":"<p>Visual retrieval system faces frequent model update and deployment. It is a\nheavy workload to re-extract features of the whole database every time.Feature\ncompatibility enables the learned new visual features to be directly compared\nwith the old features stored in the database. In this way, when updating the\ndeployed model, we can bypass the inflexible and time-consuming feature\nre-extraction process. However, the old feature space that needs to be\ncompatible is not ideal and faces the distribution discrepancy problem with the\nnew space caused by different supervision losses. In this work, we propose a\nglobal optimization Dual-Tuning method to obtain feature compatibility against\ndifferent networks and losses. A feature-level prototype loss is proposed to\nexplicitly align two types of embedding features, by transferring global\nprototype information. Furthermore, we design a component-level mutual\nstructural regularization to implicitly optimize the feature intrinsic\nstructure. Experimental results on million-scale datasets demonstrate that our\nDual-Tuning is able to obtain feature compatibility without sacrificing\nperformance. (Our code will be avaliable at\nhttps://github.com/yanbai1993/Dual-Tuning)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jile Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xuetao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Spatial Labeling Redundancy for Semi-supervised Crowd Counting. (arXiv:2108.02970v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02970","description":"<p>Labeling is onerous for crowd counting as it should annotate each individual\nin crowd images. Recently, several methods have been proposed for\nsemi-supervised crowd counting to reduce the labeling efforts. Given a limited\nlabeling budget, they typically select a few crowd images and densely label all\nindividuals in each of them. Despite the promising results, we argue the\nNone-or-All labeling strategy is suboptimal as the densely labeled individuals\nin each crowd image usually appear similar while the massive unlabeled crowd\nimages may contain entirely diverse individuals. To this end, we propose to\nbreak the labeling chain of previous methods and make the first attempt to\nreduce spatial labeling redundancy for semi-supervised crowd counting. First,\ninstead of annotating all the regions in each crowd image, we propose to\nannotate the representative ones only. We analyze the region representativeness\nfrom both vertical and horizontal directions, and formulate them as cluster\ncenters of Gaussian Mixture Models. Additionally, to leverage the rich\nunlabeled regions, we exploit the similarities among individuals in each crowd\nimage to directly supervise the unlabeled regions via feature propagation\ninstead of the error-prone label propagation employed in the previous methods.\nIn this way, we can transfer the original spatial labeling redundancy caused by\nindividual similarities to effective supervision signals on the unlabeled\nregions. Extensive experiments on the widely-used benchmarks demonstrate that\nour method can outperform previous best approaches by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Liangyu Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanjie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Domain Adaptive Crowd Counting via Point-derived Segmentation. (arXiv:2108.02980v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02980","description":"<p>Existing domain adaptation methods for crowd counting view each crowd image\nas a whole and reduce domain discrepancies on crowds and backgrounds\nsimultaneously. However, we argue that these methods are suboptimal, as crowds\nand backgrounds have quite different characteristics and backgrounds may vary\ndramatically in different crowd scenes (see Fig.~\\ref{teaser}). This makes\ncrowds not well aligned across domains together with backgrounds in a holistic\nmanner. To this end, we propose to untangle crowds and backgrounds from crowd\nimages and design fine-grained domain adaption methods for crowd counting.\nDifferent from other tasks which possess region-based fine-grained annotations\n(e.g., segments or bounding boxes), crowd counting only annotates one point on\neach human head, which impedes the implementation of fine-grained adaptation\nmethods. To tackle this issue, we propose a novel and effective schema to learn\ncrowd segmentation from point-level crowd counting annotations in the context\nof Multiple Instance Learning. We further leverage the derived segments to\npropose a crowd-aware fine-grained domain adaptation framework for crowd\ncounting, which consists of two novel adaptation modules, i.e., Crowd Region\nTransfer (CRT) and Crowd Density Alignment (CDA). Specifically, the CRT module\nis designed to guide crowd features transfer across domains beyond background\ndistractions, and the CDA module dedicates to constraining the target-domain\ncrowd density distributions. Extensive experiments on multiple cross-domain\nsettings (i.e., Synthetic $\\rightarrow$ Real, Fixed $\\rightarrow$ Fickle,\nNormal $\\rightarrow$ BadWeather) demonstrate the superiority of the proposed\nmethod compared with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Sucheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanjie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongmin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shengfeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Contrastive Learning by Visualizing Feature Transformation. (arXiv:2108.02982v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02982","description":"<p>Contrastive learning, which aims at minimizing the distance between positive\npairs while maximizing that of negative ones, has been widely and successfully\napplied in unsupervised feature learning, where the design of positive and\nnegative (pos/neg) pairs is one of its keys. In this paper, we attempt to\ndevise a feature-level data manipulation, differing from data augmentation, to\nenhance the generic contrastive self-supervised learning. To this end, we first\ndesign a visualization scheme for pos/neg score (Pos/neg score indicates cosine\nsimilarity of pos/neg pair.) distribution, which enables us to analyze,\ninterpret and understand the learning process. To our knowledge, this is the\nfirst attempt of its kind. More importantly, leveraging this tool, we gain some\nsignificant observations, which inspire our novel Feature Transformation\nproposals including the extrapolation of positives. This operation creates\nharder positives to boost the learning because hard positives enable the model\nto be more view-invariant. Besides, we propose the interpolation among\nnegatives, which provides diversified negatives and makes the model more\ndiscriminative. It is the first attempt to deal with both challenges\nsimultaneously. Experiment results show that our proposed Feature\nTransformation can improve at least 6.0% accuracy on ImageNet-100 over MoCo\nbaseline, and about 2.0% accuracy on ImageNet-1K over the MoCoV2 baseline.\nTransferring to the downstream tasks successfully demonstrate our model is less\ntask-bias. Visualization tools and codes\nhttps://github.com/DTennant/CL-Visualizing-Feature-Transformation .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenglong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images. (arXiv:2108.02996v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02996","description":"<p>Semantic segmentation of medical images is an essential first step in\ncomputer-aided diagnosis systems for many applications. However, given many\ndisparate imaging modalities and inherent variations in the patient data, it is\ndifficult to consistently achieve high accuracy using modern deep neural\nnetworks (DNNs). This has led researchers to propose interactive image\nsegmentation techniques where a medical expert can interactively correct the\noutput of a DNN to the desired accuracy. However, these techniques often need\nseparate training data with the associated human interactions, and do not\ngeneralize to various diseases, and types of medical images. In this paper, we\nsuggest a novel conditional inference technique for DNNs which takes the\nintervention by a medical expert as test time constraints and performs\ninference conditioned upon these constraints. Our technique is generic can be\nused for medical images from any modality. Unlike other methods, our approach\ncan correct multiple structures simultaneously and add structures missed at\ninitial segmentation. We report an improvement of 13.3, 12.5, 17.8, 10.2, and\n12.4 times in user annotation time than full human annotation for the nucleus,\nmultiple cells, liver and tumor, organ, and brain segmentation respectively. We\nreport a time saving of 2.8, 3.0, 1.9, 4.4, and 8.6 fold compared to other\ninteractive segmentation techniques. Our method can be useful to clinicians for\ndiagnosis and post-surgical follow-up with minimal intervention from the\nmedical expert. The source-code and the detailed results are available here\n[1].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sambaturu_B/0/1/0/all/0/1\">Bhavani Sambaturu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ashutosh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02998","description":"<p>The aortic vessel tree is composed of the aorta and its branching arteries,\nand plays a key role in supplying the whole body with blood. Aortic diseases,\nlike aneurysms or dissections, can lead to an aortic rupture, whose treatment\nwith open surgery is highly risky. Therefore, patients commonly undergo drug\ntreatment under constant monitoring, which requires regular inspections of the\nvessels through imaging. The standard imaging modality for diagnosis and\nmonitoring is computed tomography (CT), which can provide a detailed picture of\nthe aorta and its branching vessels if combined with a contrast agent,\nresulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree\ngeometry from consecutive CTAs, are overlaid and compared. This allows to not\nonly detect changes in the aorta, but also more peripheral vessel tree changes,\ncaused by the primary pathology or newly developed. When performed manually,\nthis reconstruction requires slice by slice contouring, which could easily take\na whole day for a single aortic vessel tree and, hence, is not feasible in\nclinical practice. Automatic or semi-automatic vessel tree segmentation\nalgorithms, on the other hand, can complete this task in a fraction of the\nmanual execution time and run in parallel to the clinical routine of the\nclinicians. In this paper, we systematically review computing techniques for\nthe automatic and semi-automatic segmentation of the aortic vessel tree. The\nreview concludes with an in-depth discussion on how close these\nstate-of-the-art approaches are to an application in clinical practice and how\nactive this research field is, taking into account the number of publications,\ndatasets and challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fen-hua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AceNAS: Learning to Rank Ace Neural Architectures with Weak Supervision of Weight Sharing. (arXiv:2108.03001v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03001","description":"<p>Architecture performance predictors have been widely used in neural\narchitecture search (NAS). Although they are shown to be simple and effective,\nthe optimization objectives in previous arts (e.g., precise accuracy estimation\nor perfect ranking of all architectures in the space) did not capture the\nranking nature of NAS. In addition, a large number of ground-truth\narchitecture-accuracy pairs are usually required to build a reliable predictor,\nmaking the process too computationally expensive. To overcome these, in this\npaper, we look at NAS from a novel point of view and introduce Learning to Rank\n(LTR) methods to select the best (ace) architectures from a space.\nSpecifically, we propose to use Normalized Discounted Cumulative Gain (NDCG) as\nthe target metric and LambdaRank as the training algorithm. We also propose to\nleverage weak supervision from weight sharing by pretraining architecture\nrepresentation on weak labels obtained from the super-net and then finetuning\nthe ranking model using a small number of architectures trained from scratch.\nExtensive experiments on NAS benchmarks and large-scale search spaces\ndemonstrate that our approach outperforms SOTA with a significantly reduced\nsearch cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chenqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanlu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Lyna Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiaotian Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuqing Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])","link":"http://arxiv.org/abs/2108.03002","description":"<p>More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">HongBing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">XinYi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">HongTao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">YaJing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MmWave Radar and Vision Fusion based Object Detection for Autonomous Driving: A Survey. (arXiv:2108.03004v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03004","description":"<p>With autonomous driving developing in a booming stage, accurate object\ndetection in complex scenarios attract wide attention to ensure the safety of\nautonomous driving. Millimeter wave (mmWave) radar and vision fusion is a\nmainstream solution for accurate obstacle detection. This article presents a\ndetailed survey on mmWave radar and vision fusion based obstacle detection\nmethods. Firstly, we introduce the tasks, evaluation criteria and datasets of\nobject detection for autonomous driving. Then, the process of mmWave radar and\nvision fusion is divided into three parts: sensor deployment, sensor\ncalibration and sensor fusion, which are reviewed comprehensively. Especially,\nwe classify the fusion methods into data level, decision level and feature\nlevel fusion methods. Besides, we introduce the fusion of lidar and vision in\nautonomous driving in the aspects of obstacle detection, object classification\nand road segmentation, which is promising in the future. Finally, we summarize\nthis article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhiqing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengkai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huici Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyong Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Detection for Hand Hygiene Stages. (arXiv:2108.03015v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03015","description":"<p>The process of hand washing involves complex hand movements. There are six\nprincipal sequential steps for washing hands as per the World Health\nOrganisation (WHO) guidelines. In this work, a detailed description of an\naluminium rig construction for creating a robust hand-washing dataset is\ndiscussed. The preliminary results with the help of image processing and\ncomputer vision algorithms for hand pose extraction and feature detection such\nas Harris detector, Shi-Tomasi and SIFT are demonstrated. The hand hygiene\npose- Rub hands palm to palm was captured as an input image for running all the\nexperiments. The future work will focus upon processing the video recordings of\nhand movements captured and applying deep-learning solutions for the\nclassification of hand-hygiene stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courtney_J/0/1/0/all/0/1\">Jane Courtney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_D/0/1/0/all/0/1\">Damon Berry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavin_G/0/1/0/all/0/1\">Graham Gavin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Segmentation Networks to New Domains by Disentangling Latent Representations. (arXiv:2108.03021v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03021","description":"<p>Deep learning models achieve outstanding accuracy in semantic segmentation,\nhowever they require a huge amount of labeled data for their optimization.\nHence, domain adaptation approaches have come into play to transfer knowledge\nacquired on a label-abundant source domain to a related label-scarce target\ndomain. However, such models do not generalize well to data with statistical\nproperties not perfectly matching the ones of the training samples. In this\nwork, we design and carefully analyze multiple latent space-shaping\nregularization strategies that work in conjunction to reduce the domain\ndiscrepancy in semantic segmentation. In particular, we devise a feature\nclustering strategy to increase domain alignment, a feature perpendicularity\nconstraint to space apart feature belonging to different semantic classes,\nincluding those not present in the current batch, and a feature norm alignment\nstrategy to separate active and inactive channels. Additionally, we propose a\nnovel performance metric to capture the relative efficacy of an adaptation\nstrategy compared to supervised training. We verify the effectiveness of our\nframework in synthetic-to-real and real-to-real adaptation scenarios,\noutperforming previous state-of-the-art methods on multiple road scenes\nbenchmarks and using different backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1\">Francesco Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer. (arXiv:2108.03032v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03032","description":"<p>A few-shot semantic segmentation model is typically composed of a CNN\nencoder, a CNN decoder and a simple classifier (separating foreground and\nbackground pixels). Most existing methods meta-learn all three model components\nfor fast adaptation to a new class. However, given that as few as a single\nsupport set image is available, effective model adaption of all three\ncomponents to the new class is extremely challenging. In this work we propose\nto simplify the meta-learning task by focusing solely on the simplest\ncomponent, the classifier, whilst leaving the encoder and decoder to\npre-training. We hypothesize that if we pre-train an off-the-shelf segmentation\nmodel over a set of diverse training classes with sufficient annotations, the\nencoder and decoder can capture rich discriminative features applicable for any\nunseen classes, rendering the subsequent meta-learning stage unnecessary. For\nthe classifier meta-learning, we introduce a Classifier Weight Transformer\n(CWT) designed to dynamically adapt the supportset trained classifier's weights\nto each query image in an inductive way. Extensive experiments on two standard\nbenchmarks show that despite its simplicity, our method outperforms the\nstate-of-the-art alternatives, often by a large margin.Code is available on\nhttps://github.com/zhiheLu/CWTfor-FSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+lu_Z/0/1/0/all/0/1\">Zhihe lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03064","description":"<p>We propose a self-supervised contrastive learning approach for facial\nexpression recognition (FER) in videos. We propose a novel temporal\nsampling-based augmentation scheme to be utilized in addition to standard\nspatial augmentations used for contrastive learning. Our proposed temporal\naugmentation scheme randomly picks from one of three temporal sampling\ntechniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential\nsampling. This is followed by a combination of up to three standard spatial\naugmentations. We then use a deep R(2+1)D network for FER, which we train in a\nself-supervised fashion based on the augmentations and subsequently fine-tune.\nExperiments are performed on the Oulu-CASIA dataset and the performance is\ncompared to other works in FER. The results indicate that our method achieves\nan accuracy of 89.4%, setting a new state-of-the-art by outperforming other\nworks. Additional experiments and analysis confirm the considerable\ncontribution of the proposed temporal augmentation versus the existing spatial\nones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STR-GQN: Scene Representation and Rendering for Unknown Cameras Based on Spatial Transformation Routing. (arXiv:2108.03072v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03072","description":"<p>Geometry-aware modules are widely applied in recent deep learning\narchitectures for scene representation and rendering. However, these modules\nrequire intrinsic camera information that might not be obtained accurately. In\nthis paper, we propose a Spatial Transformation Routing (STR) mechanism to\nmodel the spatial properties without applying any geometric prior. The STR\nmechanism treats the spatial transformation as the message passing process, and\nthe relation between the view poses and the routing weights is modeled by an\nend-to-end trainable neural network. Besides, an Occupancy Concept Mapping\n(OCM) framework is proposed to provide explainable rationals for scene-fusion\nprocesses. We conducted experiments on several datasets and show that the\nproposed STR mechanism improves the performance of the Generative Query Network\n(GQN). The visualization results reveal that the routing process can pass the\nobserved information from one location of some view to the associated location\nin the other view, which demonstrates the advantage of the proposed model in\nterms of spatial cognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen-Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Min-Chun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Hausdorff Metric in Digital Topology. (arXiv:2108.03114v1 [cs.CG])","link":"http://arxiv.org/abs/2108.03114","description":"<p>Two objects may be close in the Hausdor? metric, yet have very different\ngeometric and topological properties. We examine other methods of comparing\ndigital images such that objects close in each of these measures have some\nsimilar geometric or topological property. Such measures may be combined with\nthe Hausdorff metric to yield a metric in which close images are similar with\nrespect to multiple properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boxer_L/0/1/0/all/0/1\">Laurence Boxer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TS4Net: Two-Stage Sample Selective Strategy for Rotating Object Detection. (arXiv:2108.03116v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03116","description":"<p>Rotating object detection has wide applications in aerial photographs, remote\nsensing images, UAVs, etc. At present, most of the rotating object detection\ndatasets focus on the field of remote sensing, and these images are usually\nshot in high-altitude scenes. However, image datasets captured at low-altitude\nareas also should be concerned, such as drone-based datasets. So we present a\nlow-altitude dronebased dataset, named UAV-ROD, aiming to promote the research\nand development in rotating object detection and UAV applications. The UAV-ROD\nconsists of 1577 images and 30,090 instances of car category annotated by\noriented bounding boxes. In particular, The UAV-ROD can be utilized for the\nrotating object detection, vehicle orientation recognition and object counting\ntasks. Compared with horizontal object detection, the regression stage of the\nrotation detection is a tricky problem. In this paper, we propose a rotating\nobject detector TS4Net, which contains anchor refinement module (ARM) and\ntwo-stage sample selective strategy (TS4). The ARM can convert preseted\nhorizontal anchors into high-quality rotated anchors through twostage anchor\nrefinement. The TS4 module utilizes different constrained sample selective\nstrategies to allocate positive and negative samples, which is adaptive to the\nregression task in different stages. Benefiting from the ARM and TS4, the\nTS4Net can achieve superior performance for rotating object detection solely\nwith one preseted horizontal anchor. Extensive experimental results on UAV-ROD\ndataset and three remote sensing datasets DOTA, HRSC2016 and UCAS-AOD\ndemonstrate that our method achieves competitive performance against most\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kai Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weixing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Dongdong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03117","description":"<p>In recent years, deep learning based methods have shown success in essential\nmedical image analysis tasks such as segmentation. Post-processing and refining\nthe results of segmentation is a common practice to decrease the\nmisclassifications originating from the segmentation network. In addition to\nwidely used methods like Conditional Random Fields (CRFs) which focus on the\nstructure of the segmented volume/area, a graph-based recent approach makes use\nof certain and uncertain points in a graph and refines the segmentation\naccording to a small graph convolutional network (GCN). However, there are two\ndrawbacks of the approach: most of the edges in the graph are assigned randomly\nand the GCN is trained independently from the segmentation network. To address\nthese issues, we define a new neighbor-selection mechanism according to feature\ndistances and combine the two networks in the training procedure. According to\nthe experimental results on pancreas segmentation from Computed Tomography (CT)\nimages, we demonstrate improvement in the quantitative measures. Also,\nexamining the dynamic neighbors created by our method, edges between\nsemantically similar image parts are observed. The proposed method also shows\nqualitative enhancements in the segmentation maps, as demonstrated in the\nvisual results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">Ufuk Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1\">Atahan Ozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1\">Yusuf H. Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for View Classification of Echocardiograms. (arXiv:2108.03124v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03124","description":"<p>Analysis of cardiac ultrasound images is commonly performed in routine\nclinical practice for quantification of cardiac function. Its increasing\nautomation frequently employs deep learning networks that are trained to\npredict disease or detect image features. However, such models are extremely\ndata-hungry and training requires labelling of many thousands of images by\nexperienced clinicians. Here we propose the use of contrastive learning to\nmitigate the labelling bottleneck. We train view classification models for\nimbalanced cardiac ultrasound datasets and show improved performance for\nviews/classes for which minimal labelled data is available. Compared to a naive\nbaseline model, we achieve an improvement in F1 score of up to 26% in those\nviews while maintaining state-of-the-art performance for the views with\nsufficiently many labelled training observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chartsias_A/0/1/0/all/0/1\">Agisilaos Chartsias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mumith_A/0/1/0/all/0/1\">Angela Mumith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1\">Jorge Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1\">Kanwal Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])","link":"http://arxiv.org/abs/2108.03131","description":"<p>The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of\nlife globally, and a critical factor in mitigating its effects is screening\nindividuals for infections, thereby allowing for both proper treatment for\nthose individuals as well as action to be taken to prevent further spread of\nthe virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a\nscreening tool as it is a much cheaper and easier to apply imaging modality\nthan others that are traditionally used for pulmonary examinations, namely\nchest x-ray and computed tomography. Given the scarcity of expert radiologists\nfor interpreting POCUS examinations in many highly affected regions around the\nworld, low-cost deep learning-driven clinical decision support solutions can\nhave a large impact during the on-going pandemic. Motivated by this, we\nintroduce COVID-Net US, a highly efficient, self-attention deep convolutional\nneural network design tailored for COVID-19 screening from lung POCUS images.\nExperimental results show that the proposed COVID-Net US can achieve an AUC of\nover 0.98 while achieving 353X lower architectural complexity, 62X lower\ncomputational complexity, and 14.3X faster inference times on a Raspberry Pi.\nClinical validation was also conducted, where select cases were reviewed and\nreported on by a practicing clinician (20 years of clinical practice)\nspecializing in intensive care (ICU) and 15 years of expertise in POCUS\ninterpretation. To advocate affordable healthcare and artificial intelligence\nfor resource-constrained environments, we have made COVID-Net US open source\nand publicly available as part of the COVID-Net open source initiative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1\">Alexander MacLean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1\">Andy Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1\">Sonny Kohli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lung Ultrasound Segmentation and Adaptation between COVID-19 and Community-Acquired Pneumonia. (arXiv:2108.03138v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03138","description":"<p>Lung ultrasound imaging has been shown effective in detecting typical\npatterns for interstitial pneumonia, as a point-of-care tool for both patients\nwith COVID-19 and other community-acquired pneumonia (CAP). In this work, we\nfocus on the hyperechoic B-line segmentation task. Using deep neural networks,\nwe automatically outline the regions that are indicative of pathology-sensitive\nartifacts and their associated sonographic patterns. With a real-world\ndata-scarce scenario, we investigate approaches to utilize both COVID-19 and\nCAP lung ultrasound data to train the networks; comparing fine-tuning and\nunsupervised domain adaptation. Segmenting either type of lung condition at\ninference may support a range of clinical applications during evolving epidemic\nstages, but also demonstrates value in resource-constrained clinical scenarios.\nAdapting real clinical data acquired from COVID-19 patients to those from CAP\npatients significantly improved Dice scores from 0.60 to 0.87 (p &lt; 0.001) and\nfrom 0.43 to 0.71 (p &lt; 0.001), on independent COVID-19 and CAP test cases,\nrespectively. It is of practical value that the improvement was demonstrated\nwith only a small amount of data in both training and adaptation data sets, a\ncommon constraint for deploying machine learning models in clinical practice.\nInterestingly, we also report that the inverse adaptation, from labelled CAP\ndata to unlabeled COVID-19 data, did not demonstrate an improvement when tested\non either condition. Furthermore, we offer a possible explanation that\ncorrelates the segmentation performance to label consistency and data domain\ndiversity in this point-of-care lung ultrasound application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mason_H/0/1/0/all/0/1\">Harry Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristoni_L/0/1/0/all/0/1\">Lorenzo Cristoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walden_A/0/1/0/all/0/1\">Andrew Walden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazzari_R/0/1/0/all/0/1\">Roberto Lazzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulimood_T/0/1/0/all/0/1\">Thomas Pulimood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grandjean_L/0/1/0/all/0/1\">Louis Grandjean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wheeler_Kingshott_C/0/1/0/all/0/1\">Claudia AM Gandini Wheeler-Kingshott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary MC Baum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03140","description":"<p>Extreme Learning Machine is a powerful classification method very competitive\nexisting classification methods. It is extremely fast at training.\nNevertheless, it cannot perform face verification tasks properly because face\nverification tasks require comparison of facial images of two individuals at\nthe same time and decide whether the two faces identify the same person. The\nstructure of Extreme Leaning Machine was not designed to feed two input data\nstreams simultaneously, thus, in 2-input scenarios Extreme Learning Machine\nmethods are normally applied using concatenated inputs. However, this setup\nconsumes two times more computational resources and it is not optimized for\nrecognition tasks where learning a separable distance metric is critical. For\nthese reasons, we propose and develop a Siamese Extreme Learning Machine\n(SELM). SELM was designed to be fed with two data streams in parallel\nsimultaneously. It utilizes a dual-stream Siamese condition in the extra\nSiamese layer to transform the data before passing it along to the hidden\nlayer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature\nexclusively trained on a variety of specific demographic groups. This feature\nenables learning and extracting of useful facial features of each group.\nExperiments were conducted to evaluate and compare the performances of SELM,\nExtreme Learning Machine, and DCNN. The experimental results showed that the\nproposed feature was able to perform correct classification at 97.87% accuracy\nand 99.45% AUC. They also showed that using SELM in conjunction with the\nproposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the\nwell-known DCNN and Extreme Leaning Machine methods by a wide margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1\">Wasu Kudisthalert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1\">Kitsuchart Pasupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELSED: Enhanced Line SEgment Drawing. (arXiv:2108.03144v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03144","description":"<p>Detecting local features, such as corners, segments or blobs, is the first\nstep in the pipeline of many Computer Vision applications. Its speed is crucial\nfor real time applications. In this paper we present ELSED, the fastest line\nsegment detector in the literature. The key for its efficiency is a local\nsegment growing algorithm that connects gradient aligned pixels in presence of\nsmall discontinuities. The proposed algorithm not only runs in devices with\nvery low end hardware, but may also be parametrized to foster the detection of\nshort or longer segments, depending on the task at hand. We also introduce new\nmetrics to evaluate the accuracy and repeatability of segment detectors. In our\nexperiments with different public benchmarks we prove that our method is the\nmost efficient in the literature and quantify the accuracy traded for such\ngain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1\">Iago Su&#xe1;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1\">Jos&#xe9; M. Buenaposada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1\">Luis Baumela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03151","description":"<p>Appearance and motion are two important sources of information in video\nobject segmentation (VOS). Previous methods mainly focus on using simplex\nsolutions, lowering the upper bound of feature collaboration among and across\nthese two cues. In this paper, we study a novel framework, termed the FSNet\n(Full-duplex Strategy Network), which designs a relational cross-attention\nmodule (RCAM) to achieve the bidirectional message propagation across embedding\nsubspaces. Furthermore, the bidirectional purification module (BPM) is\nintroduced to update the inconsistent features between the spatial-temporal\nembeddings, effectively improving the model robustness. By considering the\nmutual restraint within the full-duplex strategy, our FSNet performs the\ncross-modal feature-passing (i.e., transmission and receiving) simultaneously\nbefore the fusion and decoding stage, making it robust to various challenging\nscenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five\npopular benchmarks (i.e., DAVIS$_{16}$, FBMS, MCL, SegTrack-V2, and\nDAVSOD$_{19}$) show that our FSNet outperforms other state-of-the-arts for both\nthe VOS and video salient object detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Keren Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianbing Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Source-Free Domain Adaptation for Image Segmentation. (arXiv:2108.03152v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03152","description":"<p>Domain adaptation (DA) has drawn high interest for its capacity to adapt a\nmodel trained on labeled source data to perform well on unlabeled or weakly\nlabeled target data from a different domain. Most common DA techniques require\nconcurrent access to the input images of both the source and target domains.\nHowever, in practice, privacy concerns often impede the availability of source\nimages in the adaptation phase. This is a very frequent DA scenario in medical\nimaging, where, for instance, the source and target images could come from\ndifferent clinical sites. We introduce a source-free domain adaptation for\nimage segmentation. Our formulation is based on minimizing a label-free entropy\nloss defined over target-domain data, which we further guide with a\ndomain-invariant prior on the segmentation regions. Many priors can be derived\nfrom anatomical information. Here, a class ratio prior is estimated from\nanatomical knowledge and integrated in the form of a Kullback Leibler (KL)\ndivergence in our overall loss function. Furthermore, we motivate our overall\nloss with an interesting link to maximizing the mutual information between the\ntarget images and their label predictions. We show the effectiveness of our\nprior aware entropy minimization in a variety of domain-adaptation scenarios,\nwith different modalities and applications, including spine, prostate, and\ncardiac segmentation. Our method yields comparable results to several state of\nthe art adaptation techniques, despite having access to much less information,\nas the source images are entirely absent in our adaptation phase. Our\nstraightforward adaptation strategy uses only one network, contrary to popular\nadversarial techniques, which are not applicable to a source-free DA setting.\nOur framework can be readily used in a breadth of segmentation problems, and\nour code is publicly available: https://github.com/mathilde-b/SFDA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bateson_M/0/1/0/all/0/1\">Mathilde Bateson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1\">Hoel Kervadec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1\">Herv&#xe9; Lombaert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pattern Recognition in Vital Signs Using Spectrograms. (arXiv:2108.03168v1 [eess.SP])","link":"http://arxiv.org/abs/2108.03168","description":"<p>Spectrograms visualize the frequency components of a given signal which may\nbe an audio signal or even a time-series signal. Audio signals have higher\nsampling rate and high variability of frequency with time. Spectrograms can\ncapture such variations well. But, vital signs which are time-series signals\nhave less sampling frequency and low-frequency variability due to which,\nspectrograms fail to express variations and patterns. In this paper, we propose\na novel solution to introduce frequency variability using frequency modulation\non vital signs. Then we apply spectrograms on frequency modulated signals to\ncapture the patterns. The proposed approach has been evaluated on 4 different\nmedical datasets across both prediction and classification tasks. Significant\nresults are found showing the efficacy of the approach for vital sign signals.\nThe results from the proposed approach are promising with an accuracy of 91.55%\nand 91.67% in prediction and classification tasks respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sribhashyam_S/0/1/0/all/0/1\">Sidharth Srivatsav Sribhashyam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salekin_M/0/1/0/all/0/1\">Md Sirajus Salekin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldgof_D/0/1/0/all/0/1\">Dmitry Goldgof</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Semantic Occupancy Mapping using 3D Scene Flow and Closed-Form Bayesian Inference. (arXiv:2108.03180v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03180","description":"<p>This paper reports on a dynamic semantic mapping framework that incorporates\n3D scene flow measurements into a closed-form Bayesian inference model.\nExistence of dynamic objects in the environment cause artifacts and traces in\ncurrent mapping algorithms, leading to an inconsistent map posterior. We\nleverage state-of-the-art semantic segmentation and 3D flow estimation using\ndeep learning to provide measurements for map inference. We develop a\ncontinuous (i.e., can be queried at arbitrary resolution) Bayesian model that\npropagates the scene with flow and infers a 3D semantic occupancy map with\nbetter performance than its static counterpart. Experimental results using\npublicly available data sets show that the proposed framework generalizes its\npredecessors and improves over direct measurements from deep neural networks\nconsistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_A/0/1/0/all/0/1\">Aishwarya Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1\">Joseph Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Lu Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capodieci_A/0/1/0/all/0/1\">Andrew Capodieci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_P/0/1/0/all/0/1\">Paramsothy Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barton_K/0/1/0/all/0/1\">Kira Barton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLASS: Geometric Latent Augmentation for Shape Spaces. (arXiv:2108.03225v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03225","description":"<p>We investigate the problem of training generative models on a very sparse\ncollection of 3D models. We use geometrically motivated energies to augment and\nthus boost a sparse collection of example (training) models. We analyze the\nHessian of the as-rigid-as-possible (ARAP) energy to sample from and project to\nthe underlying (local) shape space, and use the augmented dataset to train a\nvariational autoencoder (VAE). We iterate the process of building latent spaces\nof VAE and augmenting the associated dataset, to progressively reveal a richer\nand more expressive generative space for creating geometrically and\nsemantically valid samples. Our framework allows us to train generative 3D\nmodels even with a small set of good quality 3D models, which are typically\nhard to curate. We extensively evaluate our method against a set of strong\nbaselines, provide ablation studies and demonstrate application towards\nestablishing shape correspondences. We present multiple examples of interesting\nand meaningful shape variations even when starting from as few as 3-10 training\nshapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muralikrishnan_S/0/1/0/all/0/1\">Sanjeev Muralikrishnan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir Kim</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy Mitra</a> (1 and 2) ((1) University College London, (2) Adobe Research, (3) IIT Bombay)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images. (arXiv:2108.03227v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03227","description":"<p>Bird's-Eye-View (BEV) maps have emerged as one of the most powerful\nrepresentations for scene understanding due to their ability to provide rich\nspatial context while being easy to interpret and process. However, generating\nBEV maps requires complex multi-stage paradigms that encapsulate a series of\ndistinct tasks such as depth estimation, ground plane estimation, and semantic\nsegmentation. These sub-tasks are often learned in a disjoint manner which\nprevents the model from holistic reasoning and results in erroneous BEV maps.\nMoreover, existing algorithms only predict the semantics in the BEV space,\nwhich limits their use in applications where the notion of object instances is\ncritical. In this work, we present the first end-to-end learning approach for\ndirectly predicting dense panoptic segmentation maps in the BEV, given a single\nmonocular image in the frontal view (FV). Our architecture follows the top-down\nparadigm and incorporates a novel dense transformer module consisting of two\ndistinct transformers that learn to independently map vertical and flat regions\nin the input image from the FV to the BEV. Additionally, we derive a\nmathematical formulation for the sensitivity of the FV-BEV transformation which\nallows us to intelligently weight pixels in the BEV space to account for the\nvarying descriptiveness across the FV image. Extensive evaluations on the\nKITTI-360 and nuScenes datasets demonstrate that our approach exceeds the\nstate-of-the-art in the PQ metric by 3.61 pp and 4.93 pp respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1\">Nikhil Gosala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruction of 3D Porous Media From 2D Slices. (arXiv:1901.10233v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1901.10233","description":"<p>In many branches of earth sciences, the problem of rock study on the\nmicro-level arises. However, a significant number of representative samples is\nnot always feasible. Thus the problem of the generation of samples with similar\nproperties becomes actual. In this paper, we propose a novel deep learning\narchitecture for three-dimensional porous media reconstruction from\ntwo-dimensional slices. We fit a distribution on all possible three-dimensional\nstructures of a specific type based on the given dataset of samples. Then,\ngiven partial information (central slices), we recover the three-dimensional\nstructure around such slices as the most probable one according to that\nconstructed distribution. Technically, we implement this in the form of a deep\nneural network with encoder, generator and discriminator modules. Numerical\nexperiments show that this method provides a good reconstruction in terms of\nMinkowski functionals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Volkhonskiy_D/0/1/0/all/0/1\">Denis Volkhonskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muravleva_E/0/1/0/all/0/1\">Ekaterina Muravleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudakov_O/0/1/0/all/0/1\">Oleg Sudakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1\">Denis Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belozerov_B/0/1/0/all/0/1\">Boris Belozerov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1\">Dmitry Koroteev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TightCap: 3D Human Shape Capture with Clothing Tightness Field. (arXiv:1904.02601v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.02601","description":"<p>In this paper, we present TightCap, a data-driven scheme to capture both the\nhuman shape and dressed garments accurately with only a single 3D human scan,\nwhich enables numerous applications such as virtual try-on, biometrics and body\nevaluation. To break the severe variations of the human poses and garments, we\npropose to model the clothing tightness - the displacements from the garments\nto the human shape implicitly in the global UV texturing domain. To this end,\nwe utilize an enhanced statistical human template and an effective multi-stage\nalignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on\nthis 2D representation, we propose a novel framework to predicted clothing\ntightness via a novel tightness formulation, as well as an effective\noptimization scheme to further reconstruct multi-layer human shape and garments\nunder various clothing categories and human postures. We further propose a new\nclothing tightness dataset (CTD) of human scans with a large variety of\nclothing styles, poses and corresponding ground-truth human shapes to stimulate\nfurther research. Extensive experiments demonstrate the effectiveness of our\nTightCap to achieve high-quality human shape and dressed garments\nreconstruction, as well as the further applications for clothing segmentation,\nretargeting and animation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1\">Anqi Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xui_L/0/1/0/all/0/1\">Lan Xui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning and Segmenting Dense Voxel Embeddings for 3D Neuron Reconstruction. (arXiv:1909.09872v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.09872","description":"<p>We show dense voxel embeddings learned via deep metric learning can be\nemployed to produce a highly accurate segmentation of neurons from 3D electron\nmicroscopy images. A \"metric graph\" on a set of edges between voxels is\nconstructed from the dense voxel embeddings generated by a convolutional\nnetwork. Partitioning the metric graph with long-range edges as repulsive\nconstraints yields an initial segmentation with high precision, with\nsubstantial accuracy gain for very thin objects. The convolutional embedding\nnet is reused without any modification to agglomerate the systematic splits\ncaused by complex \"self-contact\" motifs. Our proposed method achieves\nstate-of-the-art accuracy on the challenging problem of 3D neuron\nreconstruction from the brain images acquired by serial section electron\nmicroscopy. Our alternative, object-centered representation could be more\ngenerally useful for other computational tasks in automated neural circuit\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kisuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luther_K/0/1/0/all/0/1\">Kyle Luther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1\">H. Sebastian Seung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FrequentNet : A New Interpretable Deep Learning Baseline for Image Classification. (arXiv:2001.01034v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.01034","description":"<p>This paper has proposed a new baseline deep learning model of more benefits\nfor image classification. Different from the convolutional neural network(CNN)\npractice where filters are trained by back propagation to represent different\npatterns of an image, we are inspired by a method called \"PCANet\" in \"PCANet: A\nSimple Deep Learning Baseline for Image Classification?\" to choose filter\nvectors from basis vectors in frequency domain like Fourier coefficients or\nwavelets without back propagation. Researchers have demonstrated that those\nbasis in frequency domain can usually provide physical insights, which adds to\nthe interpretability of the model by analyzing the frequencies selected.\nBesides, the training process will also be more time efficient, mathematically\nclear and interpretable compared with the \"black-box\" training process of CNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kuangyan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yiming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Liao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SofGAN: A Portrait Image Generator with Dynamic Styling. (arXiv:2007.03780v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.03780","description":"<p>Recently, Generative Adversarial Networks (GANs)} have been widely used for\nportrait image generation. However, in the latent space learned by GANs,\ndifferent attributes, such as pose, shape, and texture style, are generally\nentangled, making the explicit control of specific attributes difficult. To\naddress this issue, we propose a SofGAN image generator to decouple the latent\nspace of portraits into two subspaces: a geometry space and a texture space.\nThe latent codes sampled from the two subspaces are fed to two network branches\nseparately, one to generate the 3D geometry of portraits with canonical pose,\nand the other to generate textures. The aligned 3D geometries also come with\nsemantic part segmentation, encoded as a semantic occupancy field (SOF). The\nSOF allows the rendering of consistent 2D semantic segmentation maps at\narbitrary views, which are then fused with the generated texture maps and\nstylized to a portrait photo using our semantic instance-wise (SIW) module.\nThrough extensive experiments, we show that our system can generate high\nquality portrait images with independently controllable geometry and texture\nattributes. The method also generalizes well in various applications such as\nappearance-consistent facial animation and dynamic styling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anpei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Ling Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.08428","description":"<p>Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is un-explored, i.e., the complex topology of\nthe neural network architecture. In this work, we answer the following\nquestion: \"Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?\" empirically by\nexperimenting with different hand-crafted and NAS based architectures. Our\nfindings show that, for small-scale attacks, NAS-based architectures are more\nrobust for small-scale datasets and simple tasks than hand-crafted\narchitectures. However, as the dataset's size or the task's complexity\nincrease, hand-crafted architectures are more robust than NAS-based\narchitectures. We perform the first large scale study to understand adversarial\nrobustness purely from an architectural perspective. Our results show that\nrandom sampling in the search space of DARTS (a popular NAS method) with simple\nensembling can improve the robustness to PGD attack by nearly ~12\\%. We show\nthat NAS, which is popular for SoTA accuracy, can provide adversarial accuracy\nas a free add-on without any form of adversarial training. Our results show\nthat leveraging the power of neural network topology with methods like\nensembles can be an excellent way to achieve adversarial robustness without any\nform of adversarial training. We also introduce a metric that can be used to\ncalculate the trade-off between clean accuracy and adversarial robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Devansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.08637","description":"<p>Coronaviruses constitute a family of viruses that gives rise to respiratory\ndiseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is\ncrucial for an effective treatment strategy. However, the RT-PCR test which is\nconsidered to be a gold standard in the diagnosis of COVID-19 suffers from a\nhigh false-negative rate. Chest X-ray (CXR) image analysis has emerged as a\nfeasible and effective diagnostic technique towards this objective. In this\nwork, we propose the COVID-19 classification problem as a three-class\nclassification problem to distinguish between COVID-19, normal, and pneumonia\nclasses. We propose a three-stage framework, named COV-ELM. Stage one deals\nwith preprocessing and transformation while stage two deals with feature\nextraction. These extracted features are passed as an input to the ELM at the\nthird stage, resulting in the identification of COVID-19. The choice of ELM in\nthis work has been motivated by its faster convergence, better generalization\ncapability, and shorter training time in comparison to the conventional\ngradient-based learning algorithms. As bigger and diverse datasets become\navailable, ELM can be quickly retrained as compared to its gradient-based\ncompetitor models. The proposed model achieved a macro average F1-score of 0.95\nand the overall sensitivity of ${0.94 \\pm 0.02} at a 95% confidence interval.\nWhen compared to state-of-the-art machine learning algorithms, the COV-ELM is\nfound to outperform its competitors in this three-class classification\nscenario. Further, LIME has been integrated with the proposed COV-ELM model to\ngenerate annotated CXR images. The annotations are based on the superpixels\nthat have contributed to distinguish between the different classes. It was\nobserved that the superpixels correspond to the regions of the human lungs that\nare clinically observed in COVID-19 and Pneumonia cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1\">Sheetal Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1\">Manoj Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1\">Ankit Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1\">Navin Lakhyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1\">Arpita Saggar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1\">Naveen Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation. (arXiv:2011.12498v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12498","description":"<p>Semi-supervised learning aims to boost the accuracy of a model by exploring\nunlabeled images. The state-of-the-art methods are consistency-based which\nlearn about unlabeled images by encouraging the model to give consistent\npredictions for images under different augmentations. However, when applied to\npose estimation, the methods degenerate and predict every pixel in unlabeled\nimages as background. This is because contradictory predictions are gradually\npushed to the background class due to highly imbalanced class distribution. But\nthis is not an issue in supervised learning because it has accurate labels.\nThis inspires us to stabilize the training by obtaining reliable pseudo labels.\nSpecifically, we learn two networks to mutually teach each other. In\nparticular, for each image, we compose an easy-hard pair by applying different\naugmentations and feed them to both networks. The more reliable predictions on\neasy images in each network are used to teach the other network to learn about\nthe corresponding hard images. The approach successfully avoids degeneration\nand achieves promising results on public datasets. The source code will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Rongchang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chunyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PREDATOR: Registration of 3D Point Clouds with Low Overlap. (arXiv:2011.13005v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13005","description":"<p>We introduce PREDATOR, a model for pairwise point-cloud registration with\ndeep attention to the overlap region. Different from previous work, our model\nis specifically designed to handle (also) point-cloud pairs with low overlap.\nIts key novelty is an overlap-attention block for early information exchange\nbetween the latent encodings of the two point clouds. In this way the\nsubsequent decoding of the latent representations into per-point features is\nconditioned on the respective other point cloud, and thus can predict which\npoints are not only salient, but also lie in the overlap region between the two\npoint clouds. The ability to focus on points that are relevant for matching\ngreatly improves performance: PREDATOR raises the rate of successful\nregistrations by more than 20% in the low-overlap scenario, and also sets a new\nstate of the art for the 3DMatch benchmark with 89% registration recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gojcic_Z/0/1/0/all/0/1\">Zan Gojcic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usvyatsov_M/0/1/0/all/0/1\">Mikhail Usvyatsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieser_A/0/1/0/all/0/1\">Andreas Wieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Photoacoustic Reconstruction Using Sparsity in Curvelet Frame: Image versus Data Domain. (arXiv:2011.13080v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13080","description":"<p>Curvelet frame is of special significance for photoacoustic tomography (PAT)\ndue to its sparsifying and microlocalisation properties. We derive a one-to-one\nmap between wavefront directions in image and data spaces in PAT which suggests\nnear equivalence between the recovery of the initial pressure and PAT data from\ncompressed/subsampled measurements when assuming sparsity in Curvelet frame. As\nthe latter is computationally more tractable, investigation to which extent\nthis equivalence holds conducted in this paper is of immediate practical\nsignificance. To this end we formulate and compare DR, a two step approach\nbased on the recovery of the complete volume of the photoacoustic data from the\nsubsampled data followed by the acoustic inversion, and p0R, a one step\napproach where the photoacoustic image (the initial pressure, p0) is directly\nrecovered from the subsampled data. Effective representation of the\nphotoacoustic data requires basis defined on the range of the photoacoustic\nforward operator. To this end we propose a novel wedge-restriction of Curvelet\ntransform which enables us to construct such basis. Both recovery problems are\nformulated in a variational framework. As the Curvelet frame is heavily\noverdetermined, we use reweighted l1 norm penalties to enhance the sparsity of\nthe solution. The data reconstruction problem DR is a standard compressed\nsensing recovery problem, which we solve using an ADMMtype algorithm, SALSA.\nSubsequently, the initial pressure is recovered using time reversal as\nimplemented in the k-Wave Toolbox. The p0 reconstruction problem, p0R, aims to\nrecover the photoacoustic image directly via FISTA, or ADMM when in addition\nincluding a non-negativity constraint. We compare and discuss the relative\nmerits of the two approaches and illustrate them on 2D simulated and 3D real\ndata in a fair and rigorous manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1\">Bolin Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1\">Simon R. Arridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1\">Felix Lucka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_B/0/1/0/all/0/1\">Ben T. Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1\">Nam Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beard_P/0/1/0/all/0/1\">Paul C. Beard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Edward Z. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betcke_M/0/1/0/all/0/1\">Marta M. Betcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FPCC: Fast Point Cloud Clustering for Instance Segmentation. (arXiv:2012.14618v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14618","description":"<p>Instance segmentation is an important pre-processing task in numerous\nreal-world applications, such as robotics, autonomous vehicles, and\nhuman-computer interaction. Compared with the rapid development of deep\nlearning for two-dimensional (2D) image tasks, deep learning-based instance\nsegmentation of 3D point cloud still has a lot of room for development. In\nparticular, distinguishing a large number of occluded objects of the same class\nis a highly challenging problem, which is seen in a robotic bin-picking. In a\nusual bin-picking scene, many indentical objects are stacked together and the\nmodel of the objects is known. Thus, the semantic information can be ignored;\ninstead, the focus in the bin-picking is put on the segmentation of instances.\nBased on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)\nfor instance segmentation of bin-picking scene. FPCC includes a network named\nFPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for\ninferring the geometric centers for clustering and the other for describing\nfeatures of each point. FPCC-Net extracts features of each point and infers\ngeometric center points of each instance simultaneously. After that, the\nproposed clustering algorithm clusters the remaining points to the closest\ngeometric center in feature embedding space. Experiments show that FPCC also\nsurpasses the existing works in bin-picking scenes and is more computationally\nefficient. Our code and data are available at https://github.com/xyjbaal/FPCC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yajun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arai_S/0/1/0/all/0/1\">Shogo Arai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Diyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangzhou Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosuge_K/0/1/0/all/0/1\">Kazuhiro Kosuge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05260","description":"<p>In cases of serious crime, including sexual abuse, often the only available\ninformation with demonstrated potential for identification is images of the\nhands. Since this evidence is captured in uncontrolled situations, it is\ndifficult to analyse. As global approaches to feature comparison are limited in\nthis case, it is important to extend to consider local information. In this\nwork, we propose hand-based person identification by learning both global and\nlocal deep feature representation. Our proposed method, Global and Part-Aware\nNetwork (GPA-Net), creates global and local branches on the conv-layer for\nlearning robust discriminative global and part-level features. For learning the\nlocal (part-level) features, we perform uniform partitioning on the conv-layer\nin both horizontal and vertical directions. We retrieve the parts by conducting\na soft partition without explicitly partitioning the images or requiring\nexternal cues such as pose estimation. We make extensive evaluations on two\nlarge multi-ethnic and publicly available hand datasets, demonstrating that our\nproposed method significantly outperforms competing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_R/0/1/0/all/0/1\">Ritesh Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning. (arXiv:2101.10030v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.10030","description":"<p>Anomaly detection with weakly supervised video-level labels is typically\nformulated as a multiple instance learning (MIL) problem, in which we aim to\nidentify snippets containing abnormal events, with each video represented as a\nbag of video snippets. Although current methods show effective detection\nperformance, their recognition of the positive instances, i.e., rare abnormal\nsnippets in the abnormal videos, is largely biased by the dominant negative\ninstances, especially when the abnormal events are subtle anomalies that\nexhibit only small differences compared with normal events. This issue is\nexacerbated in many methods that ignore important video temporal dependencies.\nTo address this issue, we introduce a novel and theoretically sound method,\nnamed Robust Temporal Feature Magnitude learning (RTFM), which trains a feature\nmagnitude learning function to effectively recognise the positive instances,\nsubstantially improving the robustness of the MIL approach to the negative\ninstances from abnormal videos. RTFM also adapts dilated convolutions and\nself-attention mechanisms to capture long- and short-range temporal\ndependencies to learn the feature magnitude more faithfully. Extensive\nexperiments show that the RTFM-enabled MIL model (i) outperforms several\nstate-of-the-art methods by a large margin on four benchmark data sets\n(ShanghaiTech, UCF-Crime, XD-Violence and UCSD-Peds) and (ii) achieves\nsignificantly improved subtle anomaly discriminability and sample efficiency.\nCode is available at https://github.com/tianyu0207/RTFM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W. Verjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiconNet: An Edge-preserved Connectivity-based Approach for Salient Object Detection. (arXiv:2103.00334v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00334","description":"<p>Salient object detection (SOD) is viewed as a pixel-wise saliency modeling\ntask by traditional deep learning-based methods. A limitation of current SOD\nmodels is insufficient utilization of inter-pixel information, which usually\nresults in imperfect segmentation near edge regions and low spatial coherence.\nAs we demonstrate, using a saliency mask as the only label is suboptimal. To\naddress this limitation, we propose a connectivity-based approach called\nbilateral connectivity network (BiconNet), which uses connectivity masks\ntogether with saliency masks as labels for effective modeling of inter-pixel\nrelationships and object saliency. Moreover, we propose a bilateral voting\nmodule to enhance the output connectivity map, and a novel edge feature\nenhancement method that efficiently utilizes edge-specific features. Through\ncomprehensive experiments on five benchmark datasets, we demonstrate that our\nproposed method can be plugged into any existing state-of-the-art\nsaliency-based SOD framework to improve its performance with negligible\nparameter increase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanian_Zadeh_S/0/1/0/all/0/1\">Somayyeh Soltanian-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farsiu_S/0/1/0/all/0/1\">Sina Farsiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colonoscopy Polyp Detection and Classification: Dataset Creation and Comparative Evaluations. (arXiv:2104.10824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10824","description":"<p>Colorectal cancer (CRC) is one of the most common types of cancer with a high\nmortality rate. Colonoscopy is the preferred procedure for CRC screening and\nhas proven to be effective in reducing CRC mortality. Thus, a reliable\ncomputer-aided polyp detection and classification system can significantly\nincrease the effectiveness of colonoscopy. In this paper, we create an\nendoscopic dataset collected from various sources and annotate the ground truth\nof polyp location and classification results with the help of experienced\ngastroenterologists. The dataset can serve as a benchmark platform to train and\nevaluate the machine learning models for polyp classification. We have also\ncompared the performance of eight state-of-the-art deep learning-based object\ndetection models. The results demonstrate that deep CNN models are promising in\nCRC screening. This work can serve as a baseline for future research in polyp\ndetection and classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathan_M/0/1/0/all/0/1\">Mohammad I. Fathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Krushi Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cuncong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1\">Ajay Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Amit Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jean S. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank. (arXiv:2104.13415v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13415","description":"<p>This work presents a novel approach for semi-supervised semantic\nsegmentation. The key element of this approach is our contrastive learning\nmodule that enforces the segmentation network to yield similar pixel-level\nfeature representations for same-class samples across the whole dataset. To\nachieve this, we maintain a memory bank continuously updated with relevant and\nhigh-quality feature vectors from labeled data. In an end-to-end training, the\nfeatures from both labeled and unlabeled data are optimized to be similar to\nsame-class samples from the memory bank. Our approach outperforms the current\nstate-of-the-art for semi-supervised semantic segmentation and semi-supervised\ndomain adaptation on well-known public benchmarks, with larger improvements on\nthe most challenging scenarios, i.e., less available labeled data.\nhttps://github.com/Shathe/SemiSeg-Contrastive\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1\">Inigo Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1\">Alberto Sabater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferstl_D/0/1/0/all/0/1\">David Ferstl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1\">Luis Montesano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1\">Ana C. Murillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHD360: A Benchmark Dataset for Salient Human Detection in 360{\\deg} Videos. (arXiv:2105.11578v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11578","description":"<p>Salient human detection (SHD) in dynamic 360{\\deg} immersive videos is of\ngreat importance for various applications such as robotics, inter-human and\nhuman-object interaction in augmented reality. However, 360{\\deg} video SHD has\nbeen seldom discussed in the computer vision community due to a lack of\ndatasets with large-scale omnidirectional videos and rich annotations. To this\nend, we propose SHD360, the first 360{\\deg} video SHD dataset which contains\nvarious real-life daily scenes. Our SHD360 provides six-level hierarchical\nannotations for 6,268 key frames uniformly sampled from 37,403 omnidirectional\nvideo frames at 4K resolution. Specifically, each collected frame is labeled\nwith a super-class, a sub-class, associated attributes (e.g., geometrical\ndistortion), bounding boxes and per-pixel object-/instance-level masks. As a\nresult, our SHD360 contains totally 16,238 salient human instances with\nmanually annotated pixel-wise ground truth. Since so far there is no method\nproposed for 360{\\deg} image/video SHD, we systematically benchmark 11\nrepresentative state-of-the-art salient object detection (SOD) approaches on\nour SHD360, and explore key issues derived from extensive experimenting\nresults. We hope our proposed dataset and benchmark could serve as a good\nstarting point for advancing human-centric researches towards 360{\\deg}\npanoramic data. Our dataset and benchmark is publicly available at\nhttps://github.com/PanoAsh/SHD360.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02701","description":"<p>Recent advances in brain clearing and imaging have made it possible to image\nentire mammalian brains at sub-micron resolution. These images offer the\npotential to assemble brain-wide atlases of projection neuron morphology, but\nmanual neuron reconstruction remains a bottleneck. In this paper we present a\nprobabilistic method which combines a hidden Markov state process that encodes\nneuron geometric properties with a random field appearance model of the\nflourescence process. Our method utilizes dynamic programming to efficiently\ncompute the global maximizers of what we call the \"most probable\" neuron path.\nWe applied our algorithm to the output of image segmentation models where false\nnegatives severed neuronal processes, and showed that it can follow axons in\nthe presence of noise or nearby neurons. Our method has the potential to be\nintegrated into a semi or fully automated reconstruction pipeline.\nAdditionally, it creates a framework for conditioning the probability to fixed\nstart and endpoints through which users can intervene with hard constraints to,\nfor example, rule out certain reconstructions, or assign axons to particular\ncell bodies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1\">Thomas L. Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1\">Daniel J. Tward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1\">Ulrich Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1\">Michael I. Miller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06801","description":"<p>Contrastive Learning (CL) is a recent representation learning approach, which\nencourages inter-class separability and intra-class compactness in learned\nimage representations. Since medical images often contain multiple semantic\nclasses in an image, using CL to learn representations of local features (as\nopposed to global) is important. In this work, we present a novel\nsemi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization (CR) scheme, which\nworks in synergy with CL. It addresses the problem of confirmation bias, and\nencourages better clustering in the feature space. We evaluate our method on\nfour public medical segmentation datasets and a novel histopathology dataset\nthat we introduce. Our method obtains consistent improvements over\nstate-of-the-art semi-supervised segmentation approaches for all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1\">Prashant Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1\">Ajey Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1\">Nisarg Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Prasenjit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1\">Govind Makharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1\">Prathosh AP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer-aided Interpretable Features for Leaf Image Classification. (arXiv:2106.08077v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08077","description":"<p>Plant species identification is time consuming, costly, and requires lots of\nefforts, and expertise knowledge. In recent, many researchers use deep learning\nmethods to classify plants directly using plant images. While deep learning\nmodels have achieved a great success, the lack of interpretability limit their\nwidespread application. To overcome this, we explore the use of interpretable,\nmeasurable and computer-aided features extracted from plant leaf images. Image\nprocessing is one of the most challenging, and crucial steps in\nfeature-extraction. The purpose of image processing is to improve the leaf\nimage by removing undesired distortion. The main image processing steps of our\nalgorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,\nii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove\nstalk, vi) Closing holes, and vii) Resize image. The next step after image\nprocessing is to extract features from plant leaf images. We introduced 52\ncomputationally efficient features to classify plant species. These features\nare mainly classified into four groups as: i) shape-based features, ii)\ncolor-based features, iii) texture-based features, and iv) scagnostic features.\nLength, width, area, texture correlation, monotonicity and scagnostics are to\nname few of them. We explore the ability of features to discriminate the\nclasses of interest under supervised learning and unsupervised learning\nsettings. For that, supervised dimensionality reduction technique, Linear\nDiscriminant Analysis (LDA), and unsupervised dimensionality reduction\ntechnique, Principal Component Analysis (PCA) are used to convert and visualize\nthe images from digital-image space to feature space. The results show that the\nfeatures are sufficient to discriminate the classes of interest under both\nsupervised and unsupervised learning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakshika_J/0/1/0/all/0/1\">Jayani P. G. Lakshika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talagala_T/0/1/0/all/0/1\">Thiyanga S. Talagala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Semi-Supervised Object Detection with Soft Teacher. (arXiv:2106.09018v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09018","description":"<p>This paper presents an end-to-end semi-supervised object detection approach,\nin contrast to previous more complex multi-stage methods. The end-to-end\ntraining gradually improves pseudo label qualities during the curriculum, and\nthe more and more accurate pseudo labels in turn benefit object detection\ntraining. We also propose two simple yet effective techniques within this\nframework: a soft teacher mechanism where the classification loss of each\nunlabeled bounding box is weighed by the classification score produced by the\nteacher network; a box jittering approach to select reliable pseudo boxes for\nthe learning of box regression. On the COCO benchmark, the proposed approach\noutperforms previous methods by a large margin under various labeling ratios,\ni.e. 1\\%, 5\\% and 10\\%. Moreover, our approach proves to perform also well when\nthe amount of labeled data is relatively large. For example, it can improve a\n40.9 mAP baseline detector trained using the full COCO training set by +3.6\nmAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the\nstate-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev),\nit can still significantly improve the detection accuracy by +1.5 mAP, reaching\n60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching\n52.4 mAP. Further incorporating with the Object365 pre-trained model, the\ndetection accuracy reaches 61.3 mAP and the instance segmentation accuracy\nreaches 53.0 mAP, pushing the new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengde Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05948","description":"<p>The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets. Code available at\nhttps://github.com/qlilx/odgrlm.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06652","description":"<p>This paper explores the use of self-supervised deep learning in medical\nimaging in cases where two scan modalities are available for the same subject.\nSpecifically, we use a large publicly-available dataset of over 20,000 subjects\nfrom the UK Biobank with both whole body Dixon technique magnetic resonance\n(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three\ncontributions: (i) We introduce a multi-modal image-matching contrastive\nframework, that is able to learn to match different-modality scans of the same\nsubject with high accuracy. (ii) Without any adaption, we show that the\ncorrespondences learnt during this contrastive training step can be used to\nperform automatic cross-modal scan registration in a completely unsupervised\nmanner. (iii) Finally, we use these registrations to transfer segmentation maps\nfrom the DXA scans to the MR scans where they are used to train a network to\nsegment anatomical regions without requiring ground-truth MR examples. To aid\nfurther research, our code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1\">Rhydian Windsor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1\">Amir Jamaludin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1\">Timor Kadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08430","description":"<p>In this report, we present some experienced improvements to YOLO series,\nforming a new high-performance detector -- YOLOX. We switch the YOLO detector\nto an anchor-free manner and conduct other advanced detection techniques, i.e.,\na decoupled head and the leading label assignment strategy SimOTA to achieve\nstate-of-the-art results across a large scale range of models: For YOLO-Nano\nwith only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing\nNanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in\nindustry, we boost it to 47.3% AP on COCO, outperforming the current best\npractice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as\nYOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on\nTesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on\nStreaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)\nusing a single YOLOX-L model. We hope this report can provide useful experience\nfor developers and researchers in practical scenes, and we also provide deploy\nversions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at\nhttps://github.com/Megvii-BaseDetection/YOLOX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Fusion Transformer. (arXiv:2107.09011v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09011","description":"<p>In image fusion, images obtained from different sensors are fused to generate\na single image with enhanced information. In recent years, state-of-the-art\nmethods have adopted Convolution Neural Networks (CNNs) to encode meaningful\nfeatures for image fusion. Specifically, CNN-based methods perform image fusion\nby fusing local features. However, they do not consider long-range dependencies\nthat are present in the image. Transformer-based models are designed to\novercome this by modeling the long-range dependencies with the help of\nself-attention mechanism. This motivates us to propose a novel Image Fusion\nTransformer (IFT) where we develop a transformer-based multi-scale fusion\nstrategy that attends to both local and long-range information (or global\ncontext). The proposed method follows a two-stage training approach. In the\nfirst stage, we train an auto-encoder to extract deep features at multiple\nscales. In the second stage, multi-scale features are fused using a\nSpatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of\na CNN and a transformer branch which capture local and long-range features,\nrespectively. Extensive experiments on multiple benchmark datasets show that\nthe proposed method performs better than many competitive fusion algorithms.\nFurthermore, we show the effectiveness of the proposed ST fusion strategy with\nan ablation analysis. The source code is available at:\nhttps://github.com/Vibashan/Image-Fusion-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1\">Vibashan VS</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1\">Poojan Oza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Pose Transfer with Disentangled Feature Consistency. (arXiv:2107.10984v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10984","description":"<p>Deep generative models have made great progress in synthesizing images with\narbitrary human poses and transferring poses of one person to others. However,\nmost existing approaches explicitly leverage the pose information extracted\nfrom the source images as a conditional input for the generative networks.\nMeanwhile, they usually focus on the visual fidelity of the synthesized images\nbut neglect the inherent consistency, which further confines their performance\nof pose transfer. To alleviate the current limitations and improve the quality\nof the synthesized images, we propose a pose transfer network with Disentangled\nFeature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair\nof images containing the source and target person, DFC-Net extracts pose and\nstatic information from the source and target respectively, then synthesizes an\nimage of the target person with the desired pose from the source. Moreover,\nDFC-Net leverages disentangled feature consistency losses in the adversarial\ntraining to strengthen the transfer coherence and integrates the keypoint\namplifier to enhance the pose feature extraction. Additionally, an unpaired\nsupport dataset Mixamo-Sup providing more extra pose information has been\nfurther utilized during the training to improve the generality and robustness\nof DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have\ndemonstrated DFC-Net achieves state-of-the-art performance on pose transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chengxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zheng Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Gangyi Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation. (arXiv:2108.01682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01682","description":"<p>Multimodal target/aspect sentiment classification combines multimodal\nsentiment analysis and aspect/target sentiment classification. The goal of the\ntask is to combine vision and language to understand the sentiment towards a\ntarget entity in a sentence. Twitter is an ideal setting for the task because\nit is inherently multimodal, highly emotional, and affects real world events.\nHowever, multimodal tweets are short and accompanied by complex, possibly\nirrelevant images. We introduce a two-stream model that translates images in\ninput space using an object-aware transformer followed by a single-pass\nnon-autoregressive text generation approach. We then leverage the translation\nto construct an auxiliary sentence that provides multimodal information to a\nlanguage model. Our approach increases the amount of text available to the\nlanguage model and distills the object-level information in complex images. We\nachieve state-of-the-art performance on two multimodal Twitter datasets without\nmodifying the internals of the language model to accept multimodal data,\ndemonstrating the effectiveness of our translation. In addition, we explain a\nfailure mode of a popular approach for aspect sentiment analysis when applied\nto tweets. Our code is available at\n\\textcolor{blue}{\\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1\">Zaid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2IOSR: Maximal Mutual Information Open Set Recognition. (arXiv:2108.02373v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02373","description":"<p>In this work, we aim to address the challenging task of open set recognition\n(OSR). Many recent OSR methods rely on auto-encoders to extract class-specific\nfeatures by a reconstruction strategy, requiring the network to restore the\ninput image on pixel-level. This strategy is commonly over-demanding for OSR\nsince class-specific features are generally contained in target objects, not in\nall pixels. To address this shortcoming, here we discard the pixel-level\nreconstruction strategy and pay more attention to improving the effectiveness\nof class-specific feature extraction. We propose a mutual information-based\nmethod with a streamlined architecture, Maximal Mutual Information Open Set\nRecognition (M2IOSR). The proposed M2IOSR only uses an encoder to extract\nclass-specific features by maximizing the mutual information between the given\ninput and its latent features across multiple scales. Meanwhile, to further\nreduce the open space risk, latent features are constrained to class\nconditional Gaussian distributions by a KL-divergence loss function. In this\nway, a strong function is learned to prevent the network from mapping different\nobservations to similar latent features and help the network extract\nclass-specific features with desired statistical characteristics. The proposed\nmethod significantly improves the performance of baselines and achieves new\nstate-of-the-art results on several benchmarks consistently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_K/0/1/0/all/0/1\">Keck-Voon Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global and Local Texture Randomization for Synthetic-to-Real Semantic Segmentation. (arXiv:2108.02376v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02376","description":"<p>Semantic segmentation is a crucial image understanding task, where each pixel\nof image is categorized into a corresponding label. Since the pixel-wise\nlabeling for ground-truth is tedious and labor intensive, in practical\napplications, many works exploit the synthetic images to train the model for\nreal-word image semantic segmentation, i.e., Synthetic-to-Real Semantic\nSegmentation (SRSS). However, Deep Convolutional Neural Networks (CNNs) trained\non the source synthetic data may not generalize well to the target real-world\ndata. In this work, we propose two simple yet effective texture randomization\nmechanisms, Global Texture Randomization (GTR) and Local Texture Randomization\n(LTR), for Domain Generalization based SRSS. GTR is proposed to randomize the\ntexture of source images into diverse unreal texture styles. It aims to\nalleviate the reliance of the network on texture while promoting the learning\nof the domain-invariant cues. In addition, we find the texture difference is\nnot always occurred in entire image and may only appear in some local areas.\nTherefore, we further propose a LTR mechanism to generate diverse local regions\nfor partially stylizing the source images. Finally, we implement a\nregularization of Consistency between GTR and LTR (CGL) aiming to harmonize the\ntwo proposed mechanisms during training. Extensive experiments on five publicly\navailable datasets (i.e., GTA5, SYNTHIA, Cityscapes, BDDS and Mapillary) with\nvarious SRSS settings (i.e., GTA5/SYNTHIA to Cityscapes/BDDS/Mapillary)\ndemonstrate that the proposed method is superior to the state-of-the-art\nmethods for domain generalization based SRSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Duo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFuseNet: Robust Depth Estimation with Learned Multiscopic Fusion. (arXiv:2108.02448v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02448","description":"<p>We design a multiscopic vision system that utilizes a low-cost monocular RGB\ncamera to acquire accurate depth estimation. Unlike multi-view stereo with\nimages captured at unconstrained camera poses, the proposed system controls the\nmotion of a camera to capture a sequence of images in horizontally or\nvertically aligned positions with the same parallax. In this system, we propose\na new heuristic method and a robust learning-based method to fuse multiple cost\nvolumes between the reference image and its surrounding images. To obtain\ntraining data, we build a synthetic dataset with multiscopic images. The\nexperiments on the real-world Middlebury dataset and real robot demonstration\nshow that our multiscopic vision system outperforms traditional two-frame\nstereo matching methods in depth estimation. Our code and dataset are available\nat https://sites.google.com/view/multiscopic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction. (arXiv:2108.02110v1 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.02110","description":"<p>A number of deep learning based algorithms have been proposed to recover\nhigh-quality videos from low-quality compressed ones. Among them, some restore\nthe missing details of each frame via exploring the spatiotemporal information\nof neighboring frames. However, these methods usually suffer from a narrow\ntemporal scope, thus may miss some useful details from some frames outside the\nneighboring ones. In this paper, to boost artifact removal, on the one hand, we\npropose a Recursive Fusion (RF) module to model the temporal dependency within\na long temporal range. Specifically, RF utilizes both the current reference\nframes and the preceding hidden state to conduct better spatiotemporal\ncompensation. On the other hand, we design an efficient and effective\nDeformable Spatiotemporal Attention (DSTA) module such that the model can pay\nmore effort on restoring the artifact-rich areas like the boundary area of a\nmoving object. Extensive experiments show that our method outperforms the\nexisting ones on the MFQE 2.0 dataset in terms of both fidelity and perceptual\neffect. Code is available at https://github.com/zhaominyiz/RFDA-PyTorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1\">Minyi Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/"}},{"title":"cs.LG updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.LG","description":"Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Quantum Continual Learning Overcoming Catastrophic Forgetting. (arXiv:2108.02786v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02786","description":"<p>Catastrophic forgetting describes the fact that machine learning models will\nlikely forget the knowledge of previously learned tasks after the learning\nprocess of a new one. It is a vital problem in the continual learning scenario\nand recently has attracted tremendous concern across different communities. In\nthis paper, we explore the catastrophic forgetting phenomena in the context of\nquantum machine learning. We find that, similar to those classical learning\nmodels based on neural networks, quantum learning systems likewise suffer from\nsuch forgetting problem in classification tasks emerging from various\napplication scenes. We show that based on the local geometrical information in\nthe loss function landscape of the trained model, a uniform strategy can be\nadapted to overcome the forgetting problem in the incremental learning setting.\nOur results uncover the catastrophic forgetting phenomena in quantum machine\nlearning and offer a practical method to overcome this problem, which opens a\nnew avenue for exploring potential quantum advantages towards continual\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenjie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhide Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Dong-Ling Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02798","description":"<p>Fundus photography is the primary method for retinal imaging and essential\nfor diabetic retinopathy prevention. Automated segmentation of fundus\nphotographs would improve the quality, capacity, and cost-effectiveness of eye\ncare screening programs. However, current segmentation methods are not robust\ntowards the diversity in imaging conditions and pathologies typical for\nreal-world clinical applications. To overcome these limitations, we utilized\ncontrastive self-supervised learning to exploit the large variety of unlabeled\nfundus images in the publicly available EyePACS dataset. We pre-trained an\nencoder of a U-Net, which we later fine-tuned on several retinal vessel and\nlesion segmentation datasets. We demonstrate for the first time that by using\ncontrastive self-supervised learning, the pre-trained network can recognize\nblood vessels, optic disc, fovea, and various lesions without being provided\nany labels. Furthermore, when fine-tuned on a downstream blood vessel\nsegmentation task, such pre-trained networks achieve state-of-the-art\nperformance on images from different datasets. Additionally, the pre-training\nalso leads to shorter training times and an improved few-shot performance on\nboth blood vessel and lesion segmentation tasks. Altogether, our results\nshowcase the benefits of contrastive self-supervised pre-training which can\nplay a crucial role in real-world clinical applications requiring robust models\nable to adapt to new devices with only a few annotated samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1\">Jan Kuka&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1\">Anja Zenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1\">Marcel Kollovieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1\">Dominik J&#xfc;stel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1\">Vasilis Ntziachristos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Machine Learning to Predict Game Outcomes Based on Player-Champion Experience in League of Legends. (arXiv:2108.02799v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02799","description":"<p>League of Legends (LoL) is the most widely played multiplayer online battle\narena (MOBA) game in the world. An important aspect of LoL is competitive\nranked play, which utilizes a skill-based matchmaking system to form fair\nteams. However, players' skill levels vary widely depending on which champion,\nor hero, that they choose to play as. In this paper, we propose a method for\npredicting game outcomes in ranked LoL games based on players' experience with\ntheir selected champion. Using a deep neural network, we found that game\noutcomes can be predicted with 75.1% accuracy after all players have selected\nchampions, which occurs before gameplay begins. Our results have important\nimplications for playing LoL and matchmaking. Firstly, individual champion\nskill plays a significant role in the outcome of a match, regardless of team\ncomposition. Secondly, even after the skill-based matchmaking, there is still a\nwide variance in team skill before gameplay begins. Finally, players should\nonly play champions that they have mastered, if they want to win games.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tiffany D. Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Seong Ioi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dylan S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillian_M/0/1/0/all/0/1\">Matthew G. McMillian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMahan_R/0/1/0/all/0/1\">Ryan P. McMahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Topological Data Analysis with Linear Depth and Exponential Speedup. (arXiv:2108.02811v1 [quant-ph])","link":"http://arxiv.org/abs/2108.02811","description":"<p>Quantum computing offers the potential of exponential speedups for certain\nclassical computations. Over the last decade, many quantum machine learning\n(QML) algorithms have been proposed as candidates for such exponential\nimprovements. However, two issues unravel the hope of exponential speedup for\nsome of these QML algorithms: the data-loading problem and, more recently, the\nstunning dequantization results of Tang et al. A third issue, namely the\nfault-tolerance requirements of most QML algorithms, has further hindered their\npractical realization. The quantum topological data analysis (QTDA) algorithm\nof Lloyd, Garnerone and Zanardi was one of the first QML algorithms that\nconvincingly offered an expected exponential speedup. From the outset, it did\nnot suffer from the data-loading problem. A recent result has also shown that\nthe generalized problem solved by this algorithm is likely classically\nintractable, and would therefore be immune to any dequantization efforts.\nHowever, the QTDA algorithm of Lloyd et~al. has a time complexity of\n$O(n^4/(\\epsilon^2 \\delta))$ (where $n$ is the number of data points,\n$\\epsilon$ is the error tolerance, and $\\delta$ is the smallest nonzero\neigenvalue of the restricted Laplacian) and requires fault-tolerant quantum\ncomputing, which has not yet been achieved. In this paper, we completely\noverhaul the QTDA algorithm to achieve an improved exponential speedup and\ndepth complexity of $O(n\\log(1/(\\delta\\epsilon)))$. Our approach includes three\nkey innovations: (a) an efficient realization of the combinatorial Laplacian as\na sum of Pauli operators; (b) a quantum rejection sampling approach to restrict\nthe superposition to the simplices in the complex; and (c) a stochastic rank\nestimation method to estimate the Betti numbers. We present a theoretical error\nanalysis, and the circuit and computational time and depth complexities for\nBetti number estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ubaru_S/0/1/0/all/0/1\">Shashanka Ubaru</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Akhalwaya_I/0/1/0/all/0/1\">Ismail Yunus Akhalwaya</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Squillante_M/0/1/0/all/0/1\">Mark S. Squillante</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Clarkson_K/0/1/0/all/0/1\">Kenneth L. Clarkson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Horesh_L/0/1/0/all/0/1\">Lior Horesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Potential Applications of Artificial Intelligence and Machine Learning in Radiochemistry and Radiochemical Engineering. (arXiv:2108.02814v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02814","description":"<p>Artificial intelligence and machine learning are poised to disrupt PET\nimaging from bench to clinic. In this perspective we offer insights into how\nthe technology could be applied to improve the design and synthesis of new\nradiopharmaceuticals for PET imaging, including identification of an optimal\nlabeling approach as well as strategies for radiolabeling reaction\noptimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Webb_E/0/1/0/all/0/1\">E. William Webb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scott_P/0/1/0/all/0/1\">Peter J.H. Scott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy. (arXiv:2108.02817v1 [cs.HC])","link":"http://arxiv.org/abs/2108.02817","description":"<p>Although cancer patients survive years after oncologic therapy, they are\nplagued with long-lasting or permanent residual symptoms, whose severity, rate\nof development, and resolution after treatment vary largely between survivors.\nThe analysis and interpretation of symptoms is complicated by their partial\nco-occurrence, variability across populations and across time, and, in the case\nof cancers that use radiotherapy, by further symptom dependency on the tumor\nlocation and prescribed treatment. We describe THALIS, an environment for\nvisual analysis and knowledge discovery from cancer therapy symptom data,\ndeveloped in close collaboration with oncology experts. Our approach leverages\nunsupervised machine learning methodology over cohorts of patients, and, in\nconjunction with custom visual encodings and interactions, provides context for\nnew patients based on patients with similar diagnostic features and symptom\nevolution. We evaluate this approach on data collected from a cohort of head\nand neck cancer patients. Feedback from our clinician collaborators indicates\nthat THALIS supports knowledge discovery beyond the limits of machines or\nhumans alone, and that it serves as a valuable tool in both the clinic and\nsymptom research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Floricel_C/0/1/0/all/0/1\">Carla Floricel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nipu_N/0/1/0/all/0/1\">Nafiul Nipu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggs_M/0/1/0/all/0/1\">Mikayla Biggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wentzel_A/0/1/0/all/0/1\">Andrew Wentzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canahuate_G/0/1/0/all/0/1\">Guadalupe Canahuate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijk_L/0/1/0/all/0/1\">Lisanne Van Dijk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdallah Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuller_C/0/1/0/all/0/1\">C. David Fuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marai_G/0/1/0/all/0/1\">G. Elisabeta Marai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Elementary Proof that Q-learning Converges Almost Surely. (arXiv:2108.02827v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02827","description":"<p>Watkins' and Dayan's Q-learning is a model-free reinforcement learning\nalgorithm that iteratively refines an estimate for the optimal action-value\nfunction of an MDP by stochastically \"visiting\" many state-ation pairs [Watkins\nand Dayan, 1992]. Variants of the algorithm lie at the heart of numerous recent\nstate-of-the-art achievements in reinforcement learning, including the\nsuperhuman Atari-playing deep Q-network [Mnih et al., 2015]. The goal of this\npaper is to reproduce a precise and (nearly) self-contained proof that\nQ-learning converges. Much of the available literature leverages powerful\ntheory to obtain highly generalizable results in this vein. However, this\napproach requires the reader to be familiar with and make many deep connections\nto different research areas. A student seeking to deepen their understand of\nQ-learning risks becoming caught in a vicious cycle of \"RL-learning Hell\". For\nthis reason, we give a complete proof from start to finish using only one\nexternal result from the field of stochastic approximation, despite the fact\nthat this minimal dependence on other results comes at the expense of some\n\"shininess\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Regehr_M/0/1/0/all/0/1\">Matthew T. Regehr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1\">Alex Ayoub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02830","description":"<p>Hate speech is a specific type of controversial content that is widely\nlegislated as a crime that must be identified and blocked. However, due to the\nsheer volume and velocity of the Twitter data stream, hate speech detection\ncannot be performed manually. To address this issue, several studies have been\nconducted for hate speech detection in European languages, whereas little\nattention has been paid to low-resource South Asian languages, making the\nsocial media vulnerable for millions of users. In particular, to the best of\nour knowledge, no study has been conducted for hate speech detection in Roman\nUrdu text, which is widely used in the sub-continent. In this study, we have\nscrapped more than 90,000 tweets and manually parsed them to identify 5,000\nRoman Urdu tweets. Subsequently, we have employed an iterative approach to\ndevelop guidelines and used them for generating the Hate Speech Roman Urdu 2020\ncorpus. The tweets in the this corpus are classified at three levels:\nNeutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another\ncontribution, we have used five supervised learning techniques, including a\ndeep learning technique, to evaluate and compare their effectiveness for hate\nspeech detection. The results show that Logistic Regression outperformed all\nother techniques, including deep learning techniques for the two levels of\nclassification, by achieved an F1 score of 0.906 for distinguishing between\nNeutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate\nspeech tweets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Moin Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1\">Khurram Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1\">Kamran Malik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private n-gram Extraction. (arXiv:2108.02831v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02831","description":"<p>We revisit the problem of $n$-gram extraction in the differential privacy\nsetting. In this problem, given a corpus of private text data, the goal is to\nrelease as many $n$-grams as possible while preserving user level privacy.\nExtracting $n$-grams is a fundamental subroutine in many NLP applications such\nas sentence completion, response generation for emails etc. The problem also\narises in other applications such as sequence mining, and is a generalization\nof recently studied differentially private set union (DPSU). In this paper, we\ndevelop a new differentially private algorithm for this problem which, in our\nexperiments, significantly outperforms the state-of-the-art. Our improvements\nstem from combining recent advances in DPSU, privacy accounting, and new\nheuristics for pruning in the tree-based approach initiated by Chen et al.\n(2012).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1\">Janardhan Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1\">Sergey Yekhanin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient recurrent neural network methods for anomalously diffusing single particle short and noisy trajectories. (arXiv:2108.02834v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02834","description":"<p>Anomalous diffusion occurs at very different scales in nature, from atomic\nsystems to motions in cell organelles, biological tissues or ecology, and also\nin artificial materials, such as cement. Being able to accurately measure the\nanomalous exponent associated with a given particle trajectory, thus\ndetermining whether the particle subdiffuses, superdiffuses or performs normal\ndiffusion is of key importance to understand the diffusion process. Also, it is\noften important to trustingly identify the model behind the trajectory, as this\ngives a large amount of information on the system dynamics. Both aspects are\nparticularly difficult when the input data are short and noisy trajectories. It\nis even more difficult if one cannot guarantee that the trajectories output in\nexperiments is homogeneous, hindering the statistical methods based on\nensembles of trajectories. We present a data-driven method able to infer the\nanomalous exponent and to identify the type of anomalous diffusion process\nbehind single, noisy and short trajectories, with good accuracy. This model was\nused in our participation in the Anomalous Diffusion (AnDi) Challenge. A\ncombination of convolutional and recurrent neural networks were used to achieve\nstate-of-the-art results when compared to methods participating in the AnDi\nChallenge, ranking top 4 in both classification and diffusion exponent\nregression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orts_O/0/1/0/all/0/1\">&#xd2;scar Garibo i Orts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_March_M/0/1/0/all/0/1\">Miguel A. Garcia-March</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conejero_J/0/1/0/all/0/1\">J. Alberto Conejero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lossless Multi-Scale Constitutive Elastic Relations with Artificial Intelligence. (arXiv:2108.02837v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2108.02837","description":"<p>The elastic properties of materials derive from their electronic and atomic\nnature. However, simulating bulk materials fully at these scales is not\nfeasible, so that typically homogenized continuum descriptions are used\ninstead. A seamless and lossless transition of the constitutive description of\nthe elastic response of materials between these two scales has been so far\nelusive. Here we show how this problem can be overcome by using Artificial\nIntelligence (AI). A Convolutional Neural Network (CNN) model is trained, by\ntaking the structure image of a nanoporous material as input and the\ncorresponding elasticity tensor, calculated from Molecular Statics (MS), as\noutput. Trained with the atomistic data, the CNN model captures the size- and\npore-dependency of the material's elastic properties which, on the physics\nside, can stem from surfaces and non-local effects. Such effects are often\nignored in upscaling from atomistic to classical continuum theory. To\ndemonstrate the accuracy and the efficiency of the trained CNN model, a Finite\nElement Method (FEM) based result of an elastically deformed nanoporous beam\nequipped with the CNN as constitutive law is compared with that by a full\natomistic simulation. The good agreement between the atomistic simulations and\nthe FEM-AI combination for a system with size and surface effects establishes a\nnew lossless scale bridging approach to such problems. The trained CNN model\ndeviates from the atomistic result by 9.6\\% for porosity scenarios of up to\n90\\% but it is about 230 times faster than the MS calculation and does not\nrequire to change simulation methods between different scales. The efficiency\nof the CNN evaluation together with the preservation of important atomistic\neffects makes the trained model an effective atomistically-informed\nconstitutive model for macroscopic simulations of nanoporous materials and\nsolving of inverse problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Mianroodi_J/0/1/0/all/0/1\">Jaber Rezaei Mianroodi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rezaei_S/0/1/0/all/0/1\">Shahed Rezaei</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Siboni_N/0/1/0/all/0/1\">Nima H. Siboni</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Xu_B/0/1/0/all/0/1\">Bai-Xiang Xu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Raabe_D/0/1/0/all/0/1\">Dierk Raabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Sector Rotation Methodology Using Machine Learning and Deep Learning Techniques. (arXiv:2108.02838v1 [q-fin.GN])","link":"http://arxiv.org/abs/2108.02838","description":"<p>Market indicators such as CPI and GDP have been widely used over decades to\nidentify the stage of business cycles and also investment attractiveness of\nsectors given market conditions. In this paper, we propose a two-stage\nmethodology that consists of predicting ETF prices for each sector using market\nindicators and ranking sectors based on their predicted rate of returns. We\ninitially start with choosing sector specific macroeconomic indicators and\nimplement Recursive Feature Elimination algorithm to select the most important\nfeatures for each sector. Using our prediction tool, we implement different\nRecurrent Neural Networks models to predict the future ETF prices for each\nsector. We then rank the sectors based on their predicted rate of returns. We\nselect the best performing model by evaluating the annualized return,\nannualized Sharpe ratio, and Calmar ratio of the portfolios that includes the\ntop four ranked sectors chosen by the model. We also test the robustness of the\nmodel performance with respect to lookback windows and look ahead windows. Our\nempirical results show that our methodology beats the equally weighted\nportfolio performance even in the long run. We also find that Echo State\nNetworks exhibits an outstanding performance compared to other models yet it is\nfaster to implement compared to other RNN models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1\">Tugce Karatas</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1\">Ali Hirsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Meta-Learning for Time Series Regression. (arXiv:2108.02842v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02842","description":"<p>Recent work has shown the efficiency of deep learning models such as Fully\nConvolutional Networks (FCN) or Recurrent Neural Networks (RNN) to deal with\nTime Series Regression (TSR) problems. These models sometimes need a lot of\ndata to be able to generalize, yet the time series are sometimes not long\nenough to be able to learn patterns. Therefore, it is important to make use of\ninformation across time series to improve learning. In this paper, we will\nexplore the idea of using meta-learning for quickly adapting model parameters\nto new short-history time series by modifying the original idea of Model\nAgnostic Meta-Learning (MAML) \\cite{finn2017model}. Moreover, based on prior\nwork on multimodal MAML \\cite{vuorio2019multimodal}, we propose a method for\nconditioning parameters of the model through an auxiliary network that encodes\nglobal information of the time series to extract meta-features. Finally, we\napply the data to time series of different domains, such as pollution\nmeasurements, heart-rate sensors, and electrical battery data. We show\nempirically that our proposed meta-learning method learns TSR with few data\nfast and outperforms the baselines in 9 of 12 experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arango_S/0/1/0/all/0/1\">Sebastian Pineda Arango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_F/0/1/0/all/0/1\">Felix Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhusudhanan_K/0/1/0/all/0/1\">Kiran Madhusudhanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1\">Lars Schmidt-Thieme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])","link":"http://arxiv.org/abs/2108.02846","description":"<p>Human-robot collaboration is an essential research topic in artificial\nintelligence (AI), enabling researchers to devise cognitive AI systems and\naffords an intuitive means for users to interact with the robot. Of note,\ncommunication plays a central role. To date, prior studies in embodied agent\nnavigation have only demonstrated that human languages facilitate communication\nby instructions in natural languages. Nevertheless, a plethora of other forms\nof communication is left unexplored. In fact, human communication originated in\ngestures and oftentimes is delivered through multimodal cues, e.g. \"go there\"\nwith a pointing gesture. To bridge the gap and fill in the missing dimension of\ncommunication in embodied agent navigation, we propose investigating the\neffects of using gestures as the communicative interface instead of verbal\ncues. Specifically, we develop a VR-based 3D simulation environment, named\nGes-THOR, based on AI2-THOR platform. In this virtual environment, a human\nplayer is placed in the same virtual scene and shepherds the artificial agent\nusing only gestures. The agent is tasked to solve the navigation problem guided\nby natural gestures with unknown semantics; we do not use any predefined\ngestures due to the diversity and versatile nature of human gestures. We argue\nthat learning the semantics of natural gestures is mutually beneficial to\nlearning the navigation task--learn to communicate and communicate to learn. In\na series of experiments, we demonstrate that human gesture cues, even without\npredefined semantics, improve the object-goal navigation for an embodied agent,\noutperforming various state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cheng-Ju Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1\">Jungseock Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation in Speech Recognition using Phonetic Features. (arXiv:2108.02850v1 [eess.AS])","link":"http://arxiv.org/abs/2108.02850","description":"<p>Automatic speech recognition is a difficult problem in pattern recognition\nbecause several sources of variability exist in the speech input like the\nchannel variations, the input might be clean or noisy, the speakers may have\ndifferent accent and variations in the gender, etc. As a result, domain\nadaptation is important in speech recognition where we train the model for a\nparticular source domain and test it on a different target domain. In this\npaper, we propose a technique to perform unsupervised gender-based domain\nadaptation in speech recognition using phonetic features. The experiments are\nperformed on the TIMIT dataset and there is a considerable decrease in the\nphoneme error rate using the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ojha_R/0/1/0/all/0/1\">Rupam Ojha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sekhar_C/0/1/0/all/0/1\">C Chandra Sekhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Neural Networks for Illiquid Alternative Asset Cash Flow Forecasting. (arXiv:2108.02853v1 [q-fin.GN])","link":"http://arxiv.org/abs/2108.02853","description":"<p>Institutional investors have been increasing the allocation of the illiquid\nalternative assets such as private equity funds in their portfolios, yet there\nexists a very limited literature on cash flow forecasting of illiquid\nalternative assets. The net cash flow of private equity funds typically follow\na J-curve pattern, however the timing and the size of the contributions and\ndistributions depend on the investment opportunities. In this paper, we develop\na benchmark model and present two novel approaches (direct vs. indirect) to\npredict the cash flows of private equity funds. We introduce a sliding window\napproach to apply on our cash flow data because different vintage year funds\ncontain different lengths of cash flow information. We then pass the data to an\nLSTM/ GRU model to predict the future cash flows either directly or indirectly\n(based on the benchmark model). We further integrate macroeconomic indicators\ninto our data, which allows us to consider the impact of market environment on\ncash flows and to apply stress testing. Our results indicate that the direct\nmodel is easier to implement compared to the benchmark model and the indirect\nmodel, but still the predicted cash flows align better with the actual cash\nflows. We also show that macroeconomic variables improve the performance of the\ndirect model whereas the impact is not obvious on the indirect model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1\">Tugce Karatas</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Klinkert_F/0/1/0/all/0/1\">Federico Klinkert</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1\">Ali Hirsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enterprise Analytics using Graph Database and Graph-based Deep Learning. (arXiv:2108.02867v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02867","description":"<p>In a business-to-business (B2B) customer relationship management (CRM) use\ncase, each client is a potential business organization/company with a solid\nbusiness strategy and focused and rational decisions. This paper introduces a\ngraph-based analytics approach to improve CRM within a B2B environment. In our\napproach, in the first instance, we have designed a graph database using the\nNeo4j platform. Secondly, the graph database has been investigated by using\ndata mining and exploratory analysis coupled with cypher graph query language.\nSpecifically, we have applied the graph convolution network (GCN) to enable CRM\nanalytics to forecast sales. This is the first step towards a GCN-based binary\nclassification based on graph databases in the domain of B2B CRM. We evaluate\nthe performance of the proposed GCN model on graph databases and compare it\nwith Random Forest (RF), Convolutional Neural Network (CNN), and Artificial\nNeural Network (ANN). The proposed GCN approach is further augmented with the\nshortest path and eigenvector centrality attribute to significantly improve the\naccuracy of sales prediction. Experimental results reveal that the proposed\ngraph-based deep learning approach outperforms the Random Forests (RsF) and two\ndeep learning models, i.e., CNN and ANN under different combinations of graph\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalliadan_S/0/1/0/all/0/1\">Shyam Krishnan Kalliadan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02870","description":"<p>Covid-19 detection at an early stage can aid in an effective treatment and\nisolation plan to prevent its spread. Recently, transfer learning has been used\nfor Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major\nlimitations inherent to these proposed methods is limited labeled dataset size\nthat affects the reliability of Covid-19 diagnosis and disease progression. In\nthis work, we demonstrate that how we can augment limited X-ray images data by\nusing Contrast limited adaptive histogram equalization (CLAHE) to train the\nlast layer of the pre-trained deep learning models to mitigate the bias of\ntransfer learning for Covid-19 detection. We transfer learned various\npre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,\nand GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.\nThe experiment results reveal that the CLAHE-based augmentation to various\npre-trained deep learning models significantly improves the model efficiency.\nThe pre-trained VCG-16 model with CLAHEbased augmented images achieves a\nsensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when\ntrained on non-augmented data. Other models demonstrate a value of less than\n60% when trained on non-augmented data. Our results reveal that the sample bias\ncan negatively impact the performance of transfer learning which is\nsignificantly improved by using CLAHE-based augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1\">Aparna Reji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Human Innate Immune System Dependencies using Graph Neural Networks. (arXiv:2108.02872v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02872","description":"<p>Since the rapid outbreak of Covid-19 and with no approved vaccines to date,\nprofound research interest has emerged to understand the innate immune response\nto viruses. This understanding can help to inhibit virus replication, prolong\nadaptive immune response, accelerated virus clearance, and tissue recovery, a\nkey milestone to propose a vaccine to combat coronaviruses (CoVs), e.g.,\nCovid-19. Although an innate immune system triggers inflammatory responses\nagainst CoVs upon recognition of viruses, however, a vaccine is the ultimate\nprotection against CoV spread. The development of this vaccine is\ntime-consuming and requires a deep understanding of the innate immune response\nsystem. In this work, we propose a graph neural network-based model that\nexploits the interactions between pattern recognition receptors (PRRs), i.e.,\nthe human immune response system. These interactions can help to recognize\npathogen-associated molecular patterns (PAMPs) to predict the activation\nrequirements of each PRR. The immune response information of each PRR is\nderived from combining its historical PAMPs activation coupled with the modeled\neffect on the same from PRRs in its neighborhood. On one hand, this work can\nhelp to understand how long Covid-19 can confer immunity where a strong immune\nresponse means people already been infected can safely return to work. On the\nother hand, this GNN-based understanding can also abode well for vaccine\ndevelopment efforts. Our proposal has been evaluated using CoVs immune response\ndataset, with results showing an average IFNs activation prediction accuracy of\n90%, compared to 85% using feed-forward neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1\">Shagufta Henna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpolation can hurt robust generalization even when there is no noise. (arXiv:2108.02883v1 [stat.ML])","link":"http://arxiv.org/abs/2108.02883","description":"<p>Numerous recent works show that overparameterization implicitly reduces\nvariance for min-norm interpolators and max-margin classifiers. These findings\nsuggest that ridge regularization has vanishing benefits in high dimensions. We\nchallenge this narrative by showing that, even in the absence of noise,\navoiding interpolation through ridge regularization can significantly improve\ngeneralization. We prove this phenomenon for the robust risk of both linear\nregression and classification and hence provide the first theoretical result on\nrobust overfitting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Donhauser_K/0/1/0/all/0/1\">Konstantin Donhauser</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tifrea_A/0/1/0/all/0/1\">Alexandru &#x162;ifrea</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aerni_M/0/1/0/all/0/1\">Michael Aerni</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heckel_R/0/1/0/all/0/1\">Reinhard Heckel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_F/0/1/0/all/0/1\">Fanny Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RIS-assisted UAV Communications for IoT with Wireless Power Transfer Using Deep Reinforcement Learning. (arXiv:2108.02889v1 [eess.SP])","link":"http://arxiv.org/abs/2108.02889","description":"<p>Many of the devices used in Internet-of-Things (IoT) applications are\nenergy-limited, and thus supplying energy while maintaining seamless\nconnectivity for IoT devices is of considerable importance. In this context, we\npropose a simultaneous wireless power transfer and information transmission\nscheme for IoT devices with support from reconfigurable intelligent surface\n(RIS)-aided unmanned aerial vehicle (UAV) communications. In particular, in a\nfirst phase, IoT devices harvest energy from the UAV through wireless power\ntransfer; and then in a second phase, the UAV collects data from the IoT\ndevices through information transmission. To characterise the agility of the\nUAV, we consider two scenarios: a hovering UAV and a mobile UAV. Aiming at\nmaximizing the total network sum-rate, we jointly optimize the trajectory of\nthe UAV, the energy harvesting scheduling of IoT devices, and the phaseshift\nmatrix of the RIS. We formulate a Markov decision process and propose two deep\nreinforcement learning algorithms to solve the optimization problem of\nmaximizing the total network sum-rate. Numerical results illustrate the\neffectiveness of the UAV's flying path optimization and the network's\nthroughput of our proposed techniques compared with other benchmark schemes.\nGiven the strict requirements of the RIS and UAV, the significant improvement\nin processing time and throughput performance demonstrates that our proposed\nscheme is well applicable for practical IoT applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Khac Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1\">Antonino Masaracchia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Do_Duy_T/0/1/0/all/0/1\">Tan Do-Duy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1\">Trung Q. Duong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Scheduling for Federated Learning Through Over-the-Air Computation. (arXiv:2108.02891v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02891","description":"<p>A new machine learning (ML) technique termed as federated learning (FL) aims\nto preserve data at the edge devices and to only exchange ML model parameters\nin the learning process. FL not only reduces the communication needs but also\nhelps to protect the local privacy. Although FL has these advantages, it can\nstill experience large communication latency when there are massive edge\ndevices connected to the central parameter server (PS) and/or millions of model\nparameters involved in the learning process. Over-the-air computation (AirComp)\nis capable of computing while transmitting data by allowing multiple devices to\nsend data simultaneously by using analog modulation. To achieve good\nperformance in FL through AirComp, user scheduling plays a critical role. In\nthis paper, we investigate and compare different user scheduling policies,\nwhich are based on various criteria such as wireless channel conditions and the\nsignificance of model updates. Receiver beamforming is applied to minimize the\nmean-square-error (MSE) of the distortion of function aggregation result via\nAirComp. Simulation results show that scheduling based on the significance of\nmodel updates has smaller fluctuations in the training process while scheduling\nbased on channel condition has the advantage on energy efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haijian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Rose Qingyang Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Reinforcement Learning for Intelligent Reflecting Surface-assisted D2D Communications. (arXiv:2108.02892v1 [eess.SP])","link":"http://arxiv.org/abs/2108.02892","description":"<p>In this paper, we propose a deep reinforcement learning (DRL) approach for\nsolving the optimisation problem of the network's sum-rate in device-to-device\n(D2D) communications supported by an intelligent reflecting surface (IRS). The\nIRS is deployed to mitigate the interference and enhance the signal between the\nD2D transmitter and the associated D2D receiver. Our objective is to jointly\noptimise the transmit power at the D2D transmitter and the phase shift matrix\nat the IRS to maximise the network sum-rate. We formulate a Markov decision\nprocess and then propose the proximal policy optimisation for solving the\nmaximisation game. Simulation results show impressive performance in terms of\nthe achievable rate and processing time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1\">Khoi Khac Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1\">Antonino Masaracchia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yin_C/0/1/0/all/0/1\">Cheng Yin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1\">Long D. Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dobre_O/0/1/0/all/0/1\">Octavia A. Dobre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1\">Trung Q. Duong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02899","description":"<p>Document digitization is essential for the digital transformation of our\nsocieties, yet a crucial step in the process, Optical Character Recognition\n(OCR), is still not perfect. Even commercial OCR systems can produce\nquestionable output depending on the fidelity of the scanned documents. In this\npaper, we demonstrate an effective framework for mitigating OCR errors for any\ndownstream NLP task, using Named Entity Recognition (NER) as an example. We\nfirst address the data scarcity problem for model training by constructing a\ndocument synthesis pipeline, generating realistic but degraded data with NER\nlabels. We measure the NER accuracy drop at various degradation levels and show\nthat a text restoration model, trained on the degraded data, significantly\ncloses the NER accuracy gaps caused by OCR errors, including on an\nout-of-domain dataset. For the benefit of the community, we have made the\ndocument synthesis pipeline available as an open-source project.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1\">Amit Gupte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1\">Alexey Romanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1\">Sahitya Mantravadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1\">Dalitso Banda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Raza Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1\">Lakshmanan Ramu Meenal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Soundar Srinivasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist. (arXiv:2108.02904v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02904","description":"<p>Optimizing economic and public policy is critical to address socioeconomic\nissues and trade-offs, e.g., improving equality, productivity, or wellness, and\nposes a complex mechanism design problem. A policy designer needs to consider\nmultiple objectives, policy levers, and behavioral responses from strategic\nactors who optimize for their individual objectives. Moreover, real-world\npolicies should be explainable and robust to simulation-to-reality gaps, e.g.,\ndue to calibration issues. Existing approaches are often limited to a narrow\nset of policy levers or objectives that are hard to measure, do not yield\nexplicit optimal policies, or do not consider strategic behavior, for example.\nHence, it remains challenging to optimize policy in real-world scenarios. Here\nwe show that the AI Economist framework enables effective, flexible, and\ninterpretable policy design using two-level reinforcement learning (RL) and\ndata-driven simulations. We validate our framework on optimizing the stringency\nof US state policies and Federal subsidies during a pandemic, e.g., COVID-19,\nusing a simulation fitted to real data. We find that log-linear policies\ntrained using RL significantly improve social welfare, based on both public\nhealth and economic outcomes, compared to past outcomes. Their behavior can be\nexplained, e.g., well-performing policies respond strongly to changes in\nrecovery and vaccination rates. They are also robust to calibration errors,\ne.g., infection rates that are over or underestimated. As of yet, real-world\npolicymaking has not seen adoption of machine learning methods at large,\nincluding RL and AI-driven simulations. Our results show the potential of AI to\nguide policy design and improve social welfare amidst the complexity of the\nreal world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1\">Alexander Trott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1\">Sunil Srinivasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wal_D/0/1/0/all/0/1\">Douwe van der Wal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haneuse_S/0/1/0/all/0/1\">Sebastien Haneuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Stephan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating dataset harms requires stewardship: Lessons from 1000 papers. (arXiv:2108.02922v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02922","description":"<p>Concerns about privacy, bias, and harmful applications have shone a light on\nthe ethics of machine learning datasets, even leading to the retraction of\nprominent datasets including DukeMTMC, MS-Celeb-1M, TinyImages, and VGGFace2.\nIn response, the machine learning community has called for higher ethical\nstandards, transparency efforts, and technical fixes in the dataset creation\nprocess. The premise of our work is that these efforts can be more effective if\ninformed by an understanding of how datasets are used in practice in the\nresearch community. We study three influential face and person recognition\ndatasets - DukeMTMC, MS-Celeb-1M, and Labeled Faces in the Wild (LFW) - by\nanalyzing nearly 1000 papers that cite them. We found that the creation of\nderivative datasets and models, broader technological and social change, the\nlack of clarity of licenses, and dataset management practices can introduce a\nwide range of ethical concerns. We conclude by suggesting a distributed\napproach that can mitigate these harms, making recommendations to dataset\ncreators, conference program committees, dataset users, and the broader\nresearch community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kenny Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Arunesh Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1\">Arvind Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Feature Learning For Infinite Data. (arXiv:2108.02932v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02932","description":"<p>This study addresses the actual behavior of the credit-card fraud detection\nenvironment where financial transactions containing sensitive data must not be\namassed in an enormous amount to conduct learning. We introduce a new adaptive\nlearning approach that adjusts frequently and efficiently to new transaction\nchunks; each chunk is discarded after each incremental training step. Our\napproach combines transfer learning and incremental feature learning. The\nformer improves the feature relevancy for subsequent chunks, and the latter, a\nnew paradigm, increases accuracy during training by determining the optimal\nnetwork architecture dynamically for each new chunk. The architectures of past\nincremental approaches are fixed; thus, the accuracy may not improve with new\nchunks. We show the effectiveness and superiority of our approach\nexperimentally on an actual fraud dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadreddin_A/0/1/0/all/0/1\">Armin Sadreddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1\">Samira Sadaoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])","link":"http://arxiv.org/abs/2108.02941","description":"<p>Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1\">Harm de Wet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of Debiased Representations with Pseudo-Attributes. (arXiv:2108.02943v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02943","description":"<p>Dataset bias is a critical challenge in machine learning, and its negative\nimpact is aggravated when models capture unintended decision rules with\nspurious correlations. Although existing works often handle this issue using\nhuman supervision, the availability of the proper annotations is impractical\nand even unrealistic. To better tackle this challenge, we propose a simple but\neffective debiasing technique in an unsupervised manner. Specifically, we\nperform clustering on the feature embedding space and identify pseudoattributes\nby taking advantage of the clustering results even without an explicit\nattribute supervision. Then, we employ a novel cluster-based reweighting scheme\nfor learning debiased representation; this prevents minority groups from being\ndiscounted for minimizing the overall loss, which is desirable for worst-case\ngeneralization. The extensive experiments demonstrate the outstanding\nperformance of our approach on multiple standard benchmarks, which is even as\ncompetitive as the supervised counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seonguk Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auxiliary Class Based Multiple Choice Learning. (arXiv:2108.02949v1 [cs.LG])","link":"http://arxiv.org/abs/2108.02949","description":"<p>The merit of ensemble learning lies in having different outputs from many\nindividual models on a single input, i.e., the diversity of the base models.\nThe high quality of diversity can be achieved when each model is specialized to\ndifferent subsets of the whole dataset. Moreover, when each model explicitly\nknows to which subsets it is specialized, more opportunities arise to improve\ndiversity. In this paper, we propose an advanced ensemble method, called\nAuxiliary class based Multiple Choice Learning (AMCL), to ultimately specialize\neach model under the framework of multiple choice learning (MCL). The\nadvancement of AMCL is originated from three novel techniques which control the\nframework from different directions: 1) the concept of auxiliary class to\nprovide more distinct information through the labels, 2) the strategy, named\nmemory-based assignment, to determine the association between the inputs and\nthe models, and 3) the feature fusion module to achieve generalized features.\nTo demonstrate the performance of our method compared to all variants of MCL\nmethods, we conduct extensive experiments on the image classification and\nsegmentation tasks. Overall, the performance of AMCL exceeds all others in most\nof the public datasets trained with various networks as members of the\nensembles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sihwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dae Yon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Taejang Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])","link":"http://arxiv.org/abs/2108.02998","description":"<p>The aortic vessel tree is composed of the aorta and its branching arteries,\nand plays a key role in supplying the whole body with blood. Aortic diseases,\nlike aneurysms or dissections, can lead to an aortic rupture, whose treatment\nwith open surgery is highly risky. Therefore, patients commonly undergo drug\ntreatment under constant monitoring, which requires regular inspections of the\nvessels through imaging. The standard imaging modality for diagnosis and\nmonitoring is computed tomography (CT), which can provide a detailed picture of\nthe aorta and its branching vessels if combined with a contrast agent,\nresulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree\ngeometry from consecutive CTAs, are overlaid and compared. This allows to not\nonly detect changes in the aorta, but also more peripheral vessel tree changes,\ncaused by the primary pathology or newly developed. When performed manually,\nthis reconstruction requires slice by slice contouring, which could easily take\na whole day for a single aortic vessel tree and, hence, is not feasible in\nclinical practice. Automatic or semi-automatic vessel tree segmentation\nalgorithms, on the other hand, can complete this task in a fraction of the\nmanual execution time and run in parallel to the clinical routine of the\nclinicians. In this paper, we systematically review computing techniques for\nthe automatic and semi-automatic segmentation of the aortic vessel tree. The\nreview concludes with an in-depth discussion on how close these\nstate-of-the-art approaches are to an application in clinical practice and how\nactive this research field is, taking into account the number of publications,\ndatasets and challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fen-hua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])","link":"http://arxiv.org/abs/2108.03002","description":"<p>More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition\n(CSVD-QR) method for matrix complete problem is presented, whose computational\ncomplexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than\n$\\min\\{m,n\\}$, where $r$ represents the largest number of singular values of\nmatrix $X$. What is particularly interesting is that after replacing the\nnuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as\nthe upper bound of the nuclear norm, when the intermediate matrix $D$ in its\ndecomposition is close to the diagonal matrix, it will converge to the nuclear\nnorm, and is exactly equal, when the $D$ matrix is equal to the diagonal\nmatrix, to the nuclear norm, which ingeniously avoids the calculation of the\nsingular value of the matrix. To the best of our knowledge, there is no\nliterature to generalize and apply it to solve tensor complete problems.\nInspired by this, in this paper we propose a class of tensor minimization model\nbased on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,\nwhich is convex and therefore has a global minimum solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1\">HongBing Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1\">XinYi Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1\">HongTao Fan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1\">YaJing Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1\">Yinlin Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inspecting the Process of Bank Credit Rating via Visual Analytics. (arXiv:2108.03011v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03011","description":"<p>Bank credit rating classifies banks into different levels based on publicly\ndisclosed and internal information, serving as an important input in financial\nrisk management. However, domain experts have a vague idea of exploring and\ncomparing different bank credit rating schemes. A loose connection between\nsubjective and quantitative analysis and difficulties in determining\nappropriate indicator weights obscure understanding of bank credit ratings.\nFurthermore, existing models fail to consider bank types by just applying a\nunified indicator weight set to all banks. We propose RatingVis to assist\nexperts in exploring and comparing different bank credit rating schemes. It\nsupports interactively inferring indicator weights for banks by involving\ndomain knowledge and considers bank types in the analysis loop. We conduct a\ncase study with real-world bank data to verify the efficacy of RatingVis.\nExpert feedback suggests that our approach helps them better understand\ndifferent rating schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiangqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Quan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tangzhi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojuan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Summaries of Black Box Incident Triaging with Subgroup Discovery. (arXiv:2108.03013v1 [cs.AI])","link":"http://arxiv.org/abs/2108.03013","description":"<p>The need of predictive maintenance comes with an increasing number of\nincidents reported by monitoring systems and equipment/software users. In the\nfront line, on-call engineers (OCEs) have to quickly assess the degree of\nseverity of an incident and decide which service to contact for corrective\nactions. To automate these decisions, several predictive models have been\nproposed, but the most efficient models are opaque (say, black box), strongly\nlimiting their adoption. In this paper, we propose an efficient black box model\nbased on 170K incidents reported to our company over the last 7 years and\nemphasize on the need of automating triage when incidents are massively\nreported on thousands of servers running our product, an ERP. Recent\ndevelopments in eXplainable Artificial Intelligence (XAI) help in providing\nglobal explanations to the model, but also, and most importantly, with local\nexplanations for each model prediction/outcome. Sadly, providing a human with\nan explanation for each outcome is not conceivable when dealing with an\nimportant number of daily predictions. To address this problem, we propose an\noriginal data-mining method rooted in Subgroup Discovery, a pattern mining\ntechnique with the natural ability to group objects that share similar\nexplanations of their black box predictions and provide a description for each\ngroup. We evaluate this approach and present our preliminary results which give\nus good hope towards an effective OCE's adoption. We believe that this approach\nprovides a new way to address the problem of model agnostic outcome\nexplanation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Remil_Y/0/1/0/all/0/1\">Youcef Remil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendimerad_A/0/1/0/all/0/1\">Anes Bendimerad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plantevit_M/0/1/0/all/0/1\">Marc Plantevit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robardet_C/0/1/0/all/0/1\">C&#xe9;line Robardet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaytoue_M/0/1/0/all/0/1\">Mehdi Kaytoue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifiable Energy-based Representations: An Application to Estimating Heterogeneous Causal Effects. (arXiv:2108.03039v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03039","description":"<p>Conditional average treatment effects (CATEs) allow us to understand the\neffect heterogeneity across a large population of individuals. However, typical\nCATE learners assume all confounding variables are measured in order for the\nCATE to be identifiable. Often, this requirement is satisfied by simply\ncollecting many variables, at the expense of increased sample complexity for\nestimating CATEs. To combat this, we propose an energy-based model (EBM) that\nlearns a low-dimensional representation of the variables by employing a noise\ncontrastive loss function. With our EBM we introduce a preprocessing step that\nalleviates the dimensionality curse for any existing model and learner\ndeveloped for estimating CATE. We prove that our EBM keeps the representations\npartially identifiable up to some universal constant, as well as having\nuniversal approximation capability to avoid excessive information loss from\nmodel misspecification; these properties combined with our loss function,\nenable the representations to converge and keep the CATE estimation consistent.\nExperiments demonstrate the convergence of the representations, as well as show\nthat estimating CATEs on our representations performs better than on the\nvariables or the representations obtained via various benchmark dimensionality\nreduction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrevoets_J/0/1/0/all/0/1\">Jeroen Berrevoets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03064","description":"<p>We propose a self-supervised contrastive learning approach for facial\nexpression recognition (FER) in videos. We propose a novel temporal\nsampling-based augmentation scheme to be utilized in addition to standard\nspatial augmentations used for contrastive learning. Our proposed temporal\naugmentation scheme randomly picks from one of three temporal sampling\ntechniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential\nsampling. This is followed by a combination of up to three standard spatial\naugmentations. We then use a deep R(2+1)D network for FER, which we train in a\nself-supervised fashion based on the augmentations and subsequently fine-tune.\nExperiments are performed on the Oulu-CASIA dataset and the performance is\ncompared to other works in FER. The results indicate that our method achieves\nan accuracy of 89.4%, setting a new state-of-the-art by outperforming other\nworks. Additional experiments and analysis confirm the considerable\ncontribution of the proposed temporal augmentation versus the existing spatial\nones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shuvendu Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03067","description":"<p>This paper demonstrates a two-stage method for deriving insights from social\nmedia data relating to disinformation by applying a combination of geospatial\nclassification and embedding-based language modelling across multiple\nlanguages. In particular, the analysis in centered on Twitter and\ndisinformation for three European languages: English, French and Spanish.\nFirstly, Twitter data is classified into European and non-European sets using\nBERT. Secondly, Word2vec is applied to the classified texts resulting in\nEurocentric, non-Eurocentric and global representations of the data for the\nthree target languages. This comparative analysis demonstrates not only the\nefficacy of the classification method but also highlights geographic, temporal\nand linguistic differences in the disinformation-related media. Thus, the\ncontributions of the work are threefold: (i) a novel language-independent\ntransformer-based geolocation method; (ii) an analytical approach that exploits\nlexical specificity and word embeddings to interrogate user-generated content;\nand (iii) a dataset of 36 million disinformation related tweets in English,\nFrench and Spanish.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1\">David Tuxworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1\">Luis Espinosa-Anke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1\">Alun Preece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1\">David Rogers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine learning for surface prediction in ACTS. (arXiv:2108.03068v1 [physics.ins-det])","link":"http://arxiv.org/abs/2108.03068","description":"<p>We present an ongoing R&amp;D activity for machine-learning-assisted navigation\nthrough detectors to be used for track reconstruction. We investigate different\napproaches of training neural networks for surface prediction and compare their\nresults. This work is carried out in the context of the ACTS tracking toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Huth_B/0/1/0/all/0/1\">Benjamin Huth</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Salzburger_A/0/1/0/all/0/1\">Andreas Salzburger</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wettig_T/0/1/0/all/0/1\">Tilo Wettig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rectified Euler k-means and Beyond. (arXiv:2108.03081v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03081","description":"<p>Euler k-means (EulerK) first maps data onto the unit hyper-sphere surface of\nequi-dimensional space via a complex mapping which induces the robust Euler\nkernel and next employs the popular $k$-means. Consequently, besides enjoying\nthe virtues of k-means such as simplicity and scalability to large data sets,\nEulerK is also robust to noises and outliers. Although so, the centroids\ncaptured by EulerK deviate from the unit hyper-sphere surface and thus in\nstrict distributional sense, actually are outliers. This weird phenomenon also\noccurs in some generic kernel clustering methods. Intuitively, using such\noutlier-like centroids should not be quite reasonable but it is still seldom\nattended. To eliminate the deviation, we propose two Rectified Euler k-means\nmethods, i.e., REK1 and REK2, which retain the merits of EulerK while acquire\nreal centroids residing on the mapped space to better characterize the data\nstructures. Specifically, REK1 rectifies EulerK by imposing the constraint on\nthe centroids while REK2 views each centroid as the mapped image from a\npre-image in the original space and optimizes these pre-images in Euler kernel\ninduced space. Undoubtedly, our proposed REKs can methodologically be extended\nto solve problems of such a category. Finally, the experiments validate the\neffectiveness of REK1 and REK2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yunxia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+chen_S/0/1/0/all/0/1\">Songcan chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Knowledge Distillation for Multilingual Social Event Detection. (arXiv:2108.03084v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03084","description":"<p>Recently published graph neural networks (GNNs) show promising performance at\nsocial event detection tasks. However, most studies are oriented toward\nmonolingual data in languages with abundant training samples. This has left the\nmore common multilingual settings and lesser-spoken languages relatively\nunexplored. Thus, we present a GNN that incorporates cross-lingual word\nembeddings for detecting events in multilingual data streams. The first exploit\nis to make the GNN work with multilingual data. For this, we outline a\nconstruction strategy that aligns messages in different languages at both the\nnode and semantic levels. Relationships between messages are established by\nmerging entities that are the same but are referred to in different languages.\nNon-English message representations are converted into English semantic space\nvia the cross-lingual word embeddings. The resulting message graph is then\nuniformly encoded by a GNN model. In special cases where a lesser-spoken\nlanguage needs to be detected, a novel cross-lingual knowledge distillation\nframework, called CLKD, exploits prior knowledge learned from similar threads\nin English to make up for the paucity of annotated data. Experiments on both\nsynthetic and real-world datasets show the framework to be highly effective at\ndetection in both multilingual data and in languages where training samples are\nscarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiaqian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yongxin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Requirements Smells With Deep Learning: Experiences, Challenges and Future Work. (arXiv:2108.03087v1 [cs.SE])","link":"http://arxiv.org/abs/2108.03087","description":"<p>Requirements Engineering (RE) is the initial step towards building a software\nsystem. The success or failure of a software project is firmly tied to this\nphase, based on communication among stakeholders using natural language. The\nproblem with natural language is that it can easily lead to different\nunderstandings if it is not expressed precisely by the stakeholders involved,\nwhich results in building a product different from the expected one. Previous\nwork proposed to enhance the quality of the software requirements detecting\nlanguage errors based on ISO 29148 requirements language criteria. The existing\nsolutions apply classical Natural Language Processing (NLP) to detect them. NLP\nhas some limitations, such as domain dependability which results in poor\ngeneralization capability. Therefore, this work aims to improve the previous\nwork by creating a manually labeled dataset and using ensemble learning, Deep\nLearning (DL), and techniques such as word embeddings and transfer learning to\novercome the generalization problem that is tied with classical NLP and improve\nprecision and recall metrics using a manually labeled dataset. The current\nfindings show that the dataset is unbalanced and which class examples should be\nadded more. It is tempting to train algorithms even if the dataset is not\nconsiderably representative. Whence, the results show that models are\noverfitting; in Machine Learning this issue is solved by adding more instances\nto the dataset, improving label quality, removing noise, and reducing the\nlearning algorithms complexity, which is planned for this research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habib_M/0/1/0/all/0/1\">Mohammad Kasra Habib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1\">Stefan Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graziotin_D/0/1/0/all/0/1\">Daniel Graziotin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Path classification by stochastic linear recurrent neural networks. (arXiv:2108.03090v1 [stat.ML])","link":"http://arxiv.org/abs/2108.03090","description":"<p>We investigate the functioning of a classifying biological neural network\nfrom the perspective of statistical learning theory, modelled, in a simplified\nsetting, as a continuous-time stochastic recurrent neural network (RNN) with\nidentity activation function. In the purely stochastic (robust) regime, we give\na generalisation error bound that holds with high probability, thus showing\nthat the empirical risk minimiser is the best-in-class hypothesis. We show that\nRNNs retain a partial signature of the paths they are fed as the unique\ninformation exploited for training and classification tasks. We argue that\nthese RNNs are easy to train and robust and back these observations with\nnumerical experiments on both synthetic and real data. We also exhibit a\ntrade-off phenomenon between accuracy and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Bartolomaeus_W/0/1/0/all/0/1\">Wiebke Bartolomaeus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Boutaib_Y/0/1/0/all/0/1\">Youness Boutaib</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nestler_S/0/1/0/all/0/1\">Sandra Nestler</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rauhut_H/0/1/0/all/0/1\">Holger Rauhut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03117","description":"<p>In recent years, deep learning based methods have shown success in essential\nmedical image analysis tasks such as segmentation. Post-processing and refining\nthe results of segmentation is a common practice to decrease the\nmisclassifications originating from the segmentation network. In addition to\nwidely used methods like Conditional Random Fields (CRFs) which focus on the\nstructure of the segmented volume/area, a graph-based recent approach makes use\nof certain and uncertain points in a graph and refines the segmentation\naccording to a small graph convolutional network (GCN). However, there are two\ndrawbacks of the approach: most of the edges in the graph are assigned randomly\nand the GCN is trained independently from the segmentation network. To address\nthese issues, we define a new neighbor-selection mechanism according to feature\ndistances and combine the two networks in the training procedure. According to\nthe experimental results on pancreas segmentation from Computed Tomography (CT)\nimages, we demonstrate improvement in the quantitative measures. Also,\nexamining the dynamic neighbors created by our method, edges between\nsemantically similar image parts are observed. The proposed method also shows\nqualitative enhancements in the segmentation maps, as demonstrated in the\nvisual results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">Ufuk Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1\">Atahan Ozer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1\">Yusuf H. Sahin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Deep Model Reference Adaptive Control. (arXiv:2108.03120v1 [eess.SY])","link":"http://arxiv.org/abs/2108.03120","description":"<p>In this paper, we present a Stochastic Deep Neural Network-based Model\nReference Adaptive Control. Building on our work \"Deep Model Reference Adaptive\nControl\", we extend the controller capability by using Bayesian deep neural\nnetworks (DNN) to represent uncertainties and model non-linearities. Stochastic\nDeep Model Reference Adaptive Control uses a Lyapunov-based method to adapt the\noutput-layer weights of the DNN model in real-time, while a data-driven\nsupervised learning algorithm is used to update the inner-layers parameters.\nThis asynchronous network update ensures boundedness and guaranteed tracking\nperformance with a learning-based real-time feedback controller. A Bayesian\napproach to DNN learning helped avoid over-fitting the data and provide\nconfidence intervals over the predictions. The controller's stochastic nature\nalso ensured \"Induced Persistency of excitation,\" leading to convergence of the\noverall system signal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Joshi_G/0/1/0/all/0/1\">Girish Joshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhary_G/0/1/0/all/0/1\">Girish Chowdhary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])","link":"http://arxiv.org/abs/2108.03131","description":"<p>The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of\nlife globally, and a critical factor in mitigating its effects is screening\nindividuals for infections, thereby allowing for both proper treatment for\nthose individuals as well as action to be taken to prevent further spread of\nthe virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a\nscreening tool as it is a much cheaper and easier to apply imaging modality\nthan others that are traditionally used for pulmonary examinations, namely\nchest x-ray and computed tomography. Given the scarcity of expert radiologists\nfor interpreting POCUS examinations in many highly affected regions around the\nworld, low-cost deep learning-driven clinical decision support solutions can\nhave a large impact during the on-going pandemic. Motivated by this, we\nintroduce COVID-Net US, a highly efficient, self-attention deep convolutional\nneural network design tailored for COVID-19 screening from lung POCUS images.\nExperimental results show that the proposed COVID-Net US can achieve an AUC of\nover 0.98 while achieving 353X lower architectural complexity, 62X lower\ncomputational complexity, and 14.3X faster inference times on a Raspberry Pi.\nClinical validation was also conducted, where select cases were reviewed and\nreported on by a practicing clinician (20 years of clinical practice)\nspecializing in intensive care (ICU) and 15 years of expertise in POCUS\ninterpretation. To advocate affordable healthcare and artificial intelligence\nfor resource-constrained environments, we have made COVID-Net US open source\nand publicly available as part of the COVID-Net open source initiative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1\">Alexander MacLean</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1\">Andy Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1\">Hayden Gunraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1\">Sonny Kohli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RockGPT: Reconstructing three-dimensional digital rocks from single two-dimensional slice from the perspective of video generation. (arXiv:2108.03132v1 [eess.IV])","link":"http://arxiv.org/abs/2108.03132","description":"<p>Random reconstruction of three-dimensional (3D) digital rocks from\ntwo-dimensional (2D) slices is crucial for elucidating the microstructure of\nrocks and its effects on pore-scale flow in terms of numerical modeling, since\nmassive samples are usually required to handle intrinsic uncertainties. Despite\nremarkable advances achieved by traditional process-based methods, statistical\napproaches and recently famous deep learning-based models, few works have\nfocused on producing several kinds of rocks with one trained model and allowing\nthe reconstructed samples to satisfy certain given properties, such as\nporosity. To fill this gap, we propose a new framework, named RockGPT, which is\ncomposed of VQ-VAE and conditional GPT, to synthesize 3D samples based on a\nsingle 2D slice from the perspective of video generation. The VQ-VAE is\nutilized to compress high-dimensional input video, i.e., the sequence of\ncontinuous rock slices, to discrete latent codes and reconstruct them. In order\nto obtain diverse reconstructions, the discrete latent codes are modeled using\nconditional GPT in an autoregressive manner, while incorporating conditional\ninformation from a given slice, rock type, and porosity. We conduct two\nexperiments on five kinds of rocks, and the results demonstrate that RockGPT\ncan produce different kinds of rocks with the same model, and the reconstructed\nsamples can successfully meet certain specified porosities. In a broader sense,\nthrough leveraging the proposed conditioning scheme, RockGPT constitutes an\neffective way to build a general model to produce multiple kinds of rocks\nsimultaneously that also satisfy user-defined properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Q/0/1/0/all/0/1\">Qiang Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03140","description":"<p>Extreme Learning Machine is a powerful classification method very competitive\nexisting classification methods. It is extremely fast at training.\nNevertheless, it cannot perform face verification tasks properly because face\nverification tasks require comparison of facial images of two individuals at\nthe same time and decide whether the two faces identify the same person. The\nstructure of Extreme Leaning Machine was not designed to feed two input data\nstreams simultaneously, thus, in 2-input scenarios Extreme Learning Machine\nmethods are normally applied using concatenated inputs. However, this setup\nconsumes two times more computational resources and it is not optimized for\nrecognition tasks where learning a separable distance metric is critical. For\nthese reasons, we propose and develop a Siamese Extreme Learning Machine\n(SELM). SELM was designed to be fed with two data streams in parallel\nsimultaneously. It utilizes a dual-stream Siamese condition in the extra\nSiamese layer to transform the data before passing it along to the hidden\nlayer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature\nexclusively trained on a variety of specific demographic groups. This feature\nenables learning and extracting of useful facial features of each group.\nExperiments were conducted to evaluate and compare the performances of SELM,\nExtreme Learning Machine, and DCNN. The experimental results showed that the\nproposed feature was able to perform correct classification at 97.87% accuracy\nand 99.45% AUC. They also showed that using SELM in conjunction with the\nproposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the\nwell-known DCNN and Extreme Leaning Machine methods by a wide margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1\">Wasu Kudisthalert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1\">Kitsuchart Pasupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attainment Regions in Feature-Parameter Space for High-Level Debugging in Autonomous Robots. (arXiv:2108.03150v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03150","description":"<p>Understanding a controller's performance in different scenarios is crucial\nfor robots that are going to be deployed in safety-critical tasks. If we do not\nhave a model of the dynamics of the world, which is often the case in complex\ndomains, we may need to approximate a performance function of the robot based\non its interaction with the environment. Such a performance function gives us\ninsights into the behaviour of the robot, allowing us to fine-tune the\ncontroller with manual interventions. In high-dimensionality systems, where the\nactionstate space is large, fine-tuning a controller is non-trivial. To\novercome this problem, we propose a performance function whose domain is\ndefined by external features and parameters of the controller. Attainment\nregions are defined over such a domain defined by feature-parameter pairs, and\nserve the purpose of enabling prediction of successful execution of the task.\nThe use of the feature-parameter space -in contrast to the action-state space-\nallows us to adapt, explain and finetune the controller over a simpler (i.e.,\nlower dimensional space). When the robot successfully executes the task, we use\nthe attainment regions to gain insights into the limits of the controller, and\nits robustness. When the robot fails to execute the task, we use the regions to\ndebug the controller and find adaptive and counterfactual changes to the\nsolutions. Another advantage of this approach is that we can generalise through\nthe use of Gaussian processes regression of the performance function in the\nhigh-dimensional space. To test our approach, we demonstrate learning an\napproximation to the performance function in simulation, with a mobile robot\ntraversing different terrain conditions. Then, with a sample-efficient method,\nwe propagate the attainment regions to a physical robot in a similar\nenvironment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Sim&#xf3;n C. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Augmented Hybrid CNN for Stress Recognition Using Wrist-based Photoplethysmography Sensor. (arXiv:2108.03166v1 [eess.SP])","link":"http://arxiv.org/abs/2108.03166","description":"<p>Stress is a physiological state that hampers mental health and has serious\nconsequences to physical health. Moreover, the COVID-19 pandemic has increased\nstress levels among people across the globe. Therefore, continuous monitoring\nand detection of stress are necessary. The recent advances in wearable devices\nhave allowed the monitoring of several physiological signals related to stress.\nAmong them, wrist-worn wearable devices like smartwatches are most popular due\nto their convenient usage. And the photoplethysmography (PPG) sensor is the\nmost prevalent sensor in almost all consumer-grade wrist-worn smartwatches.\nTherefore, this paper focuses on using a wrist-based PPG sensor that collects\nBlood Volume Pulse (BVP) signals to detect stress which may be applicable for\nconsumer-grade wristwatches. Moreover, state-of-the-art works have used either\nclassical machine learning algorithms to detect stress using hand-crafted\nfeatures or have used deep learning algorithms like Convolutional Neural\nNetwork (CNN) which automatically extracts features. This paper proposes a\nnovel hybrid CNN (H-CNN) classifier that uses both the hand-crafted features\nand the automatically extracted features by CNN to detect stress using the BVP\nsignal. Evaluation on the benchmark WESAD dataset shows that, for 3-class\nclassification (Baseline vs. Stress vs. Amusement), our proposed H-CNN\noutperforms traditional classifiers and normal CNN by 5% and 7% accuracy, and\n10% and 7% macro F1 score, respectively. Also for 2-class classification\n(Stress vs. Non-stress), our proposed H-CNN outperforms traditional classifiers\nand normal CNN by 3% and ~5% accuracy, and ~3% and ~7% macro F1 score,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rashid_N/0/1/0/all/0/1\">Nafiul Rashid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Luke Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dautta_M/0/1/0/all/0/1\">Manik Dautta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jimenez_A/0/1/0/all/0/1\">Abel Jimenez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tseng_P/0/1/0/all/0/1\">Peter Tseng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faruque_M/0/1/0/all/0/1\">Mohammad Abdullah Al Faruque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Tensor Summation Compressive Sensing Network (GTSNET): An Easy to Learn Compressive Sensing Operation. (arXiv:2108.03167v1 [eess.SP])","link":"http://arxiv.org/abs/2108.03167","description":"<p>In CS literature, the efforts can be divided into two groups: finding a\nmeasurement matrix that preserves the compressed information at the maximum\nlevel, and finding a reconstruction algorithm for the compressed information.\nIn the traditional CS setup, the measurement matrices are selected as random\nmatrices, and optimization-based iterative solutions are used to recover the\nsignals. However, when we handle large signals, using random matrices become\ncumbersome especially when it comes to iterative optimization-based solutions.\nEven though recent deep learning-based solutions boost the reconstruction\naccuracy performance while speeding up the recovery, still jointly learning the\nwhole measurement matrix is a difficult process. In this work, we introduce a\nseparable multi-linear learning of the CS matrix by representing it as the\nsummation of arbitrary number of tensors. For a special case where the CS\noperation is set as a single tensor multiplication, the model is reduced to the\nlearning-based separable CS; while a dense CS matrix can be approximated and\nlearned as the summation of multiple tensors. Both cases can be used in CS of\ntwo or multi-dimensional signals e.g., images, multi-spectral images, videos,\netc. Structural CS matrices can also be easily approximated and learned in our\nmulti-linear separable learning setup with structural tensor sum\nrepresentation. Hence, our learnable generalized tensor summation CS operation\nencapsulates most CS setups including separable CS, non-separable CS\n(traditional vector-matrix multiplication), structural CS, and CS of the\nmulti-dimensional signals. For both gray-scale and RGB images, the proposed\nscheme surpasses most state-of-the-art solutions, especially in lower\nmeasurement rates. Although the performance gain remains limited from tensor to\nthe sum of tensor representation for gray-scale images, it becomes significant\nin the RGB case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yamac_M/0/1/0/all/0/1\">Mehmet Yamac</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akpinar_U/0/1/0/all/0/1\">Ugur Akpinar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahin_E/0/1/0/all/0/1\">Erdem Sahin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Responding to Illegal Activities Along the Canadian Coastlines Using Reinforcement Learning. (arXiv:2108.03169v1 [eess.SP])","link":"http://arxiv.org/abs/2108.03169","description":"<p>This article elaborates on how machine learning (ML) can leverage the\nsolution of a contemporary problem related to the security of maritime domains.\nThe worldwide ``Illegal, Unreported, and Unregulated'' (IUU) fishing incidents\nhave led to serious environmental and economic consequences which involve\ndrastic changes in our ecosystems in addition to financial losses caused by the\ndepletion of natural resources. The Fisheries and Aquatic Department (FAD) of\nthe United Nation's Food and Agriculture Organization (FAO) issued a report\nwhich indicated that the annual losses due to IUU fishing reached $25 Billion.\nThis imposes negative impacts on the future-biodiversity of the marine\necosystem and domestic Gross National Product (GNP). Hence, robust interception\nmechanisms are increasingly needed for detecting and pursuing the unrelenting\nillegal fishing incidents in maritime territories. This article addresses the\nproblem of coordinating the motion of a fleet of marine vessels (pursuers) to\ncatch an IUU vessel while still in local waters. The problem is formulated as a\npursuer-evader problem that is tackled within an ML framework. One or more\npursuers, such as law enforcement vessels, intercept an evader (i.e., the\nillegal fishing ship) using an online reinforcement learning mechanism that is\nbased on a value iteration process. It employs real-time navigation\nmeasurements of the evader ship as well as those of the pursuing vessels and\nreturns back model-free interception strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1\">Mohammed Abouheaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qu_S/0/1/0/all/0/1\">Shuzheng Qu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1\">Wail Gueaieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abielmona_R/0/1/0/all/0/1\">Rami Abielmona</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harb_M/0/1/0/all/0/1\">Moufid Harb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shift-invariant waveform learning on epileptic ECoG. (arXiv:2108.03177v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03177","description":"<p>Seizure detection algorithms must discriminate abnormal neuronal activity\nassociated with a seizure from normal neural activity in a variety of\nconditions. Our approach is to seek spatiotemporal waveforms with distinct\nmorphology in electrocorticographic (ECoG) recordings of epileptic patients\nthat are indicative of a subsequent seizure (preictal) versus non-seizure\nsegments (interictal). To find these waveforms we apply a shift-invariant\nk-means algorithm to segments of spatially filtered signals to learn codebooks\nof prototypical waveforms. The frequency of the cluster labels from the\ncodebooks is then used to train a binary classifier that predicts the class\n(preictal or interictal) of a test ECoG segment. We use the Matthews\ncorrelation coefficient to evaluate the performance of the classifier and the\nquality of the codebooks. We found that our method finds recurrent\nnon-sinusoidal waveforms that could be used to build interpretable features for\nseizure prediction and that are also physiologically meaningful.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_Cardenas_C/0/1/0/all/0/1\">Carlos H. Mendoza-Cardenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockmeier_A/0/1/0/all/0/1\">Austin J. Brockmeier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporally Abstract Partial Models. (arXiv:2108.03213v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03213","description":"<p>Humans and animals have the ability to reason and make predictions about\ndifferent courses of action at many time scales. In reinforcement learning,\noption models (Sutton, Precup \\&amp; Singh, 1999; Precup, 2000) provide the\nframework for this kind of temporally abstract prediction and reasoning.\nNatural intelligent agents are also able to focus their attention on courses of\naction that are relevant or feasible in a given situation, sometimes termed\naffordable actions. In this paper, we define a notion of affordances for\noptions, and develop temporally abstract partial option models, that take into\naccount the fact that an option might be affordable only in certain situations.\nWe analyze the trade-offs between estimation and approximation error in\nplanning and learning when using such models, and identify some interesting\nspecial cases. Additionally, we demonstrate empirically the potential impact of\npartial option models on the efficiency of planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1\">Khimya Khetarpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1\">Zafarali Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1\">Gheorghe Comanici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Modifications to Improve Tabular Neural Networks. (arXiv:2108.03214v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03214","description":"<p>There is growing interest in neural network architectures for tabular data.\nMany general-purpose tabular deep learning models have been introduced\nrecently, with performance sometimes rivaling gradient boosted decision trees\n(GBDTs). These recent models draw inspiration from various sources, including\nGBDTs, factorization machines, and neural networks from other application\ndomains. Previous tabular neural networks are also drawn upon, but are possibly\nunder-considered, especially models associated with specific tabular problems.\nThis paper focuses on several such models, and proposes modifications for\nimproving their performance. When modified, these models are shown to be\ncompetitive with leading general-purpose tabular models, including GBDTs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fiedler_J/0/1/0/all/0/1\">James Fiedler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Driving Scenario Trajectories with Active Learning. (arXiv:2108.03217v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03217","description":"<p>Annotating the driving scenario trajectories based only on explicit rules\n(i.e., knowledge-based methods) can be subject to errors, such as false\npositive/negative classification of scenarios that lie on the border of two\nscenario classes, missing unknown scenario classes, and also anomalies. On the\nother side, verifying the labels by the annotators is not cost-efficient. For\nthis purpose, active learning (AL) could potentially improve the annotation\nprocedure by inclusion of an annotator/expert in an efficient way. In this\nstudy, we develop an active learning framework to annotate driving trajectory\ntime-series data. At the first step, we compute an embedding of the time-series\ntrajectories into a latent space in order to extract the temporal nature. For\nthis purpose, we study three different latent space representations:\nmultivariate Time Series t-Distributed Stochastic Neighbor Embedding (mTSNE),\nRecurrent Auto-Encoder (RAE) and Variational Recurrent Auto-Encoder (VRAE). We\nthen apply different active learning paradigms with different classification\nmodels to the embedded data. In particular, we study the two classifiers Neural\nNetwork (NN) and Support Vector Machines (SVM), with three active learning\nquery strategies (i.e., entropy, margin and random). In the following, we\nexplore the possibilities of the framework to discover unknown classes and\ndemonstrate how it can be used to identify the out-of-class trajectories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarl_S/0/1/0/all/0/1\">Sanna Jarl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahrovani_S/0/1/0/all/0/1\">Sadegh Rahrovani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1\">Morteza Haghir Chehreghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study on Dense and Sparse (Visual) Rewards in Robot Policy Learning. (arXiv:2108.03222v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03222","description":"<p>Deep Reinforcement Learning (DRL) is a promising approach for teaching robots\nnew behaviour. However, one of its main limitations is the need for carefully\nhand-coded reward signals by an expert. We argue that it is crucial to automate\nthe reward learning process so that new skills can be taught to robots by their\nusers. To address such automation, we consider task success classifiers using\nvisual observations to estimate the rewards in terms of task success. In this\nwork, we study the performance of multiple state-of-the-art deep reinforcement\nlearning algorithms under different types of reward: Dense, Sparse, Visual\nDense, and Visual Sparse rewards. Our experiments in various simulation tasks\n(Pendulum, Reacher, Pusher, and Fetch Reach) show that while DRL agents can\nlearn successful behaviours using visual rewards when the goal targets are\ndistinguishable, their performance may decrease if the task goal is not clearly\nvisible. Our results also show that visual dense rewards are more successful\nthan visual sparse rewards and that there is no single best algorithm for all\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohtasib_A/0/1/0/all/0/1\">Abdalkarim Mohtasib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1\">Heriberto Cuayahuitl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Label Correction for Noisy Label Learning. (arXiv:1911.03809v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1911.03809","description":"<p>Leveraging weak or noisy supervision for building effective machine learning\nmodels has long been an important research problem. Its importance has further\nincreased recently due to the growing need for large-scale datasets to train\ndeep learning models. Weak or noisy supervision could originate from multiple\nsources including non-expert annotators or automatic labeling based on\nheuristics or user interaction signals. There is an extensive amount of\nprevious work focusing on leveraging noisy labels. Most notably, recent work\nhas shown impressive gains by using a meta-learned instance re-weighting\napproach where a meta-learning framework is used to assign instance weights to\nnoisy labels. In this paper, we extend this approach via posing the problem as\nlabel correction problem within a meta-learning framework. We view the label\ncorrection procedure as a meta-process and propose a new meta-learning based\nframework termed MLC (Meta Label Correction) for learning with noisy labels.\nSpecifically, a label correction network is adopted as a meta-model to produce\ncorrected labels for noisy labels while the main model is trained to leverage\nthe corrected labeled. Both models are jointly trained by solving a bi-level\noptimization problem. We run extensive experiments with different label noise\nlevels and types on both image recognition and text classification tasks. We\ncompare the reweighing and correction approaches showing that the correction\nframing addresses some of the limitation of reweighting. We also show that the\nproposed MLC approach achieves large improvements over previous methods in many\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumais_S/0/1/0/all/0/1\">Susan Dumais</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1912.07942","description":"<p>To continuously improve quality and reflect changes in data, machine learning\napplications have to regularly retrain and update their core models. We show\nthat a differential analysis of language model snapshots before and after an\nupdate can reveal a surprising amount of detailed information about changes in\nthe training data. We propose two new metrics---\\emph{differential score} and\n\\emph{differential rank}---for analyzing the leakage due to updates of natural\nlanguage models. We perform leakage analysis using these metrics across models\ntrained on several different datasets using different methods and\nconfigurations. We discuss the privacy implications of our findings, propose\nmitigation strategies and evaluate their effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1\">Santiago Zanella-B&#xe9;guelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1\">Victor R&#xfc;hle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1\">Andrew Paverd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1\">Olga Ohrimenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1\">Boris K&#xf6;pf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1\">Marc Brockschmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DriveML: An R Package for Driverless Machine Learning. (arXiv:2005.00478v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2005.00478","description":"<p>In recent years, the concept of automated machine learning has become very\npopular. Automated Machine Learning (AutoML) mainly refers to the automated\nmethods for model selection and hyper-parameter optimization of various\nalgorithms such as random forests, gradient boosting, neural networks, etc. In\nthis paper, we introduce a new package i.e. DriveML for automated machine\nlearning. DriveML helps in implementing some of the pillars of an automated\nmachine learning pipeline such as automated data preparation, feature\nengineering, model building and model explanation by running the function\ninstead of writing lengthy R codes. The DriveML package is available in CRAN.\nWe compare the DriveML package with other relevant packages in CRAN/Github and\nfind that DriveML performs the best across different parameters. We also\nprovide an illustration by applying the DriveML package with default\nconfiguration on a real world dataset. Overall, the main benefits of DriveML\nare in development time savings, reduce developer's errors, optimal tuning of\nmachine learning models and reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Putatunda_S/0/1/0/all/0/1\">Sayan Putatunda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ubrangala_D/0/1/0/all/0/1\">Dayananda Ubrangala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rama_K/0/1/0/all/0/1\">Kiran Rama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1\">Ravi Kondapalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximate Gradient Coding with Optimal Decoding. (arXiv:2006.09638v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2006.09638","description":"<p>In distributed optimization problems, a technique called gradient coding,\nwhich involves replicating data points, has been used to mitigate the effect of\nstraggling machines. Recent work has studied approximate gradient coding, which\nconcerns coding schemes where the replication factor of the data is too low to\nrecover the full gradient exactly. Our work is motivated by the challenge of\ncreating approximate gradient coding schemes that simultaneously work well in\nboth the adversarial and stochastic models. To that end, we introduce novel\napproximate gradient codes based on expander graphs, in which each machine\nreceives exactly two blocks of data points. We analyze the decoding error both\nin the random and adversarial straggler setting, when optimal decoding\ncoefficients are used. We show that in the random setting, our schemes achieve\nan error to the gradient that decays exponentially in the replication factor.\nIn the adversarial setting, the error is nearly a factor of two smaller than\nany existing code with similar performance in the random setting. We show\nconvergence bounds both in the random and adversarial setting for gradient\ndescent under standard assumptions using our codes. In the random setting, our\nconvergence rate improves upon block-box bounds. In the adversarial setting, we\nshow that gradient descent can converge down to a noise floor that scales\nlinearly with the adversarial error to the gradient. We demonstrate empirically\nthat our schemes achieve near-optimal error in the random setting and converge\nfaster than algorithms which do not use the optimal decoding coefficients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Glasgow_M/0/1/0/all/0/1\">Margalit Glasgow</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wootters_M/0/1/0/all/0/1\">Mary Wootters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.08428","description":"<p>Adversarial robustness of deep learning models has gained much traction in\nthe last few years. Various attacks and defenses are proposed to improve the\nadversarial robustness of modern-day deep learning architectures. While all\nthese approaches help improve the robustness, one promising direction for\nimproving adversarial robustness is un-explored, i.e., the complex topology of\nthe neural network architecture. In this work, we answer the following\nquestion: \"Can the complex topology of a neural network give adversarial\nrobustness without any form of adversarial training?\" empirically by\nexperimenting with different hand-crafted and NAS based architectures. Our\nfindings show that, for small-scale attacks, NAS-based architectures are more\nrobust for small-scale datasets and simple tasks than hand-crafted\narchitectures. However, as the dataset's size or the task's complexity\nincrease, hand-crafted architectures are more robust than NAS-based\narchitectures. We perform the first large scale study to understand adversarial\nrobustness purely from an architectural perspective. Our results show that\nrandom sampling in the search space of DARTS (a popular NAS method) with simple\nensembling can improve the robustness to PGD attack by nearly ~12\\%. We show\nthat NAS, which is popular for SoTA accuracy, can provide adversarial accuracy\nas a free add-on without any form of adversarial training. Our results show\nthat leveraging the power of neural network topology with methods like\nensembles can be an excellent way to achieve adversarial robustness without any\nform of adversarial training. We also introduce a metric that can be used to\ncalculate the trade-off between clean accuracy and adversarial robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1\">Chaitanya Devaguptapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Devansh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.08637","description":"<p>Coronaviruses constitute a family of viruses that gives rise to respiratory\ndiseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is\ncrucial for an effective treatment strategy. However, the RT-PCR test which is\nconsidered to be a gold standard in the diagnosis of COVID-19 suffers from a\nhigh false-negative rate. Chest X-ray (CXR) image analysis has emerged as a\nfeasible and effective diagnostic technique towards this objective. In this\nwork, we propose the COVID-19 classification problem as a three-class\nclassification problem to distinguish between COVID-19, normal, and pneumonia\nclasses. We propose a three-stage framework, named COV-ELM. Stage one deals\nwith preprocessing and transformation while stage two deals with feature\nextraction. These extracted features are passed as an input to the ELM at the\nthird stage, resulting in the identification of COVID-19. The choice of ELM in\nthis work has been motivated by its faster convergence, better generalization\ncapability, and shorter training time in comparison to the conventional\ngradient-based learning algorithms. As bigger and diverse datasets become\navailable, ELM can be quickly retrained as compared to its gradient-based\ncompetitor models. The proposed model achieved a macro average F1-score of 0.95\nand the overall sensitivity of ${0.94 \\pm 0.02} at a 95% confidence interval.\nWhen compared to state-of-the-art machine learning algorithms, the COV-ELM is\nfound to outperform its competitors in this three-class classification\nscenario. Further, LIME has been integrated with the proposed COV-ELM model to\ngenerate annotated CXR images. The annotations are based on the superpixels\nthat have contributed to distinguish between the different classes. It was\nobserved that the superpixels correspond to the regions of the human lungs that\nare clinically observed in COVID-19 and Pneumonia cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1\">Sheetal Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1\">Manoj Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1\">Ankit Rajpal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1\">Navin Lakhyani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1\">Arpita Saggar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1\">Naveen Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v3 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2009.13370","description":"<p>This paper estimates free energy, average mutual information, and minimum\nmean square error (MMSE) of a linear model under two assumptions: (1) the\nsource is generated by a Markov chain, (2) the source is generated via a hidden\nMarkov model. Our estimates are based on the replica method in statistical\nphysics. We show that under the posterior mean estimator, the linear model with\nMarkov sources or hidden Markov sources is decoupled into single-input AWGN\nchannels with state information available at both encoder and decoder where the\nstate distribution follows the left Perron-Frobenius eigenvector with unit\nManhattan norm of the stochastic matrix of Markov chains. Numerical results\nshow that the free energies and MSEs obtained via the replica method are\nclosely approximate to their counterparts achieved by the Metropolis-Hastings\nalgorithm or some well-known approximate message passing algorithms in the\nresearch literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1\">Lan V. Truong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network. (arXiv:2011.06175v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.06175","description":"<p>We propose a novel approach to optimize fleet management by combining\nmulti-agent reinforcement learning with graph neural network. To provide\nride-hailing service, one needs to optimize dynamic resources and demands over\nspatial domain. While the spatial structure was previously approximated with a\nregular grid, our approach represents the road network with a graph, which\nbetter reflects the underlying geometric structure. Dynamic resource allocation\nis formulated as multi-agent reinforcement learning, whose action-value\nfunction (Q function) is approximated with graph neural networks. We use\nstochastic policy update rule over the graph with deep Q-networks (DQN), and\nachieve superior results over the greedy policy update. We design a realistic\nsimulator that emulates the empirical taxi call data, and confirm the\neffectiveness of the proposed model under various conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juhyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kihyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSD2 Explainable AI Model for Credit Scoring. (arXiv:2011.10367v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.10367","description":"<p>The aim of this project is to develop and test advanced analytical methods to\nimprove the prediction accuracy of Credit Risk Models, preserving at the same\ntime the model interpretability. In particular, the project focuses on applying\nan explainable machine learning model to bank-related databases. The input data\nwere obtained from open data. Over the total proven models, CatBoost has shown\nthe highest performance. The algorithm implementation produces a GINI of 0.68\nafter tuning the hyper-parameters. SHAP package is used to provide a global and\nlocal interpretation of the model predictions to formulate a\nhuman-comprehensive approach to understanding the decision-maker algorithm. The\n20 most important features are selected using the Shapley values to present a\nfull human-understandable model that reveals how the attributes of an\nindividual are related to its model prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Torrent_N/0/1/0/all/0/1\">Neus Llop Torrent</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Visani_G/0/1/0/all/0/1\">Giorgio Visani</a> (2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Bagli_E/0/1/0/all/0/1\">Enrico Bagli</a> (2) ((1) Politecnico di Milano Graduate School of Business, (2) CRIF S.p.A, (3) University of Bologna School of Informatics and Engineering)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Programmer: Discrete Latent Codes for Program Synthesis. (arXiv:2012.00377v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.00377","description":"<p>In many sequence learning tasks, such as program synthesis and document\nsummarization, a key problem is searching over a large space of possible output\nsequences. We propose to learn representations of the outputs that are\nspecifically meant for search: rich enough to specify the desired output but\ncompact enough to make search more efficient. Discrete latent codes are\nappealing for this purpose, as they naturally allow sophisticated combinatorial\nsearch strategies. The latent codes are learned using a self-supervised\nlearning principle, in which first a discrete autoencoder is trained on the\noutput sequences, and then the resulting latent codes are used as intermediate\ntargets for the end-to-end sequence prediction task. Based on these insights,\nwe introduce the \\emph{Latent Programmer}, a program synthesis method that\nfirst predicts a discrete latent code from input/output examples, and then\ngenerates the program in the target language. We evaluate the Latent Programmer\non two domains: synthesis of string transformation programs, and generation of\nprograms from natural language descriptions. We demonstrate that the discrete\nlatent representation significantly improves synthesis accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Joey Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Occam's Razor in System Identification: Double-Descent when Modeling Dynamics. (arXiv:2012.06341v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.06341","description":"<p>System identification aims to build models of dynamical systems from data.\nTraditionally, choosing the model requires the designer to balance between two\ngoals of conflicting nature; the model must be rich enough to capture the\nsystem dynamics, but not so flexible that it learns spurious random effects\nfrom the dataset. It is typically observed that the model validation\nperformance follows a U-shaped curve as the model complexity increases. Recent\ndevelopments in machine learning and statistics, however, have observed\nsituations where a \"double-descent\" curve subsumes this U-shaped\nmodel-performance curve. With a second decrease in performance occurring beyond\nthe point where the model has reached the capacity of interpolating - i.e.,\n(near) perfectly fitting - the training data. To the best of our knowledge,\nsuch phenomena have not been studied within the context of dynamic systems. The\npresent paper aims to answer the question: \"Can such a phenomenon also be\nobserved when estimating parameters of dynamic systems?\" We show that the\nanswer is yes, verifying such behavior experimentally both for artificially\ngenerated and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Ant&#xf4;nio H. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendriks_J/0/1/0/all/0/1\">Johannes N. Hendriks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wills_A/0/1/0/all/0/1\">Adrian G. Wills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1\">Thomas B. Sch&#xf6;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introduction to Normalizing Flows for Lattice Field Theory. (arXiv:2101.08176v3 [hep-lat] UPDATED)","link":"http://arxiv.org/abs/2101.08176","description":"<p>This notebook tutorial demonstrates a method for sampling Boltzmann\ndistributions of lattice field theories using a class of machine learning\nmodels known as normalizing flows. The ideas and approaches proposed in\n<a href=\"/abs/1904.12072\">arXiv:1904.12072</a>, <a href=\"/abs/2002.02428\">arXiv:2002.02428</a>, and <a href=\"/abs/2003.06413\">arXiv:2003.06413</a> are reviewed and a\nconcrete implementation of the framework is presented. We apply this framework\nto a lattice scalar field theory and to U(1) gauge theory, explicitly encoding\ngauge symmetries in the flow-based approach to the latter. This presentation is\nintended to be interactive and working with the attached Jupyter notebook is\nrecommended.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1\">Michael S. Albergo</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1\">Denis Boyda</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1\">Daniel C. Hackett</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1\">Gurtej Kanwar</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1\">Kyle Cranmer</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Racaniere_S/0/1/0/all/0/1\">S&#xe9;bastien Racani&#xe8;re</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Rezende_D/0/1/0/all/0/1\">Danilo Jimenez Rezende</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1\">Phiala E. Shanahan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.08862","description":"<p>The deadly triad refers to the instability of a reinforcement learning\nalgorithm when it employs off-policy learning, function approximation, and\nbootstrapping simultaneously. In this paper, we investigate the target network\nas a tool for breaking the deadly triad, providing theoretical support for the\nconventional wisdom that a target network stabilizes training. We first propose\nand analyze a novel target network update rule which augments the commonly used\nPolyak-averaging style update with two projections. We then apply the target\nnetwork and ridge regularization in several divergent algorithms and show their\nconvergence to regularized TD fixed points. Those algorithms are off-policy\nwith linear function approximation and bootstrapping, spanning both policy\nevaluation and control, as well as both discounted and average-reward settings.\nIn particular, we provide the first convergent linear $Q$-learning algorithms\nunder nonrestrictive and changing behavior policies without bi-level\noptimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hengshuai Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1\">Shimon Whiteson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural networks for Anatomical Therapeutic Chemical (ATC) classification. (arXiv:2101.11713v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2101.11713","description":"<p>Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is\na critical and highly competitive area of research in bioinformatics because of\nits potential for expediting drug develop-ment and research. Predicting an\nunknown compound's therapeutic and chemical characteristics ac-cording to how\nthese characteristics affect multiple organs/systems makes automatic ATC\nclassifica-tion a challenging multi-label problem. Results: In this work, we\npropose combining multiple multi-label classifiers trained on distinct sets of\nfeatures, including sets extracted from a Bidirectional Long Short-Term Memory\nNetwork (BiLSTM). Experiments demonstrate the power of this approach, which is\nshown to outperform the best methods reported in the literature, including the\nstate-of-the-art developed by the fast.ai research group. Availability: All\nsource code developed for this study is available at\nhttps://github.com/LorisNanni. Contact: loris.nanni@unipd.it\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lumini_A/0/1/0/all/0/1\">Alessandra Lumini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-strategy for Learning Tuning Parameters with Guarantees. (arXiv:2102.02504v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2102.02504","description":"<p>Online learning methods, like the online gradient algorithm (OGA) and\nexponentially weighted aggregation (EWA), often depend on tuning parameters\nthat are difficult to set in practice. We consider an online meta-learning\nscenario, and we propose a meta-strategy to learn these parameters from past\ntasks. Our strategy is based on the minimization of a regret bound. It allows\nto learn the initialization and the step size in OGA with guarantees. It also\nallows to learn the prior or the learning rate in EWA. We provide a regret\nanalysis of the strategy. It allows to identify settings where meta-learning\nindeed improves on learning each task in isolation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1\">Dimitri Meunier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1\">Pierre Alquier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise Reduction in X-ray Photon Correlation Spectroscopy with Convolutional Neural Networks Encoder-Decoder Models. (arXiv:2102.03877v2 [cond-mat.mtrl-sci] UPDATED)","link":"http://arxiv.org/abs/2102.03877","description":"<p>Like other experimental techniques, X-ray Photon Correlation Spectroscopy is\nsubject to various kinds of noise. Random and correlated fluctuations and\nheterogeneities can be present in a two-time correlation function and obscure\nthe information about the intrinsic dynamics of a sample. Simultaneously\naddressing the disparate origins of noise in the experimental data is\nchallenging. We propose a computational approach for improving the\nsignal-to-noise ratio in two-time correlation functions that is based on\nConvolutional Neural Network Encoder-Decoder (CNN-ED) models. Such models\nextract features from an image via convolutional layers, project them to a low\ndimensional space and then reconstruct a clean image from this reduced\nrepresentation via transposed convolutional layers. Not only are ED models a\ngeneral tool for random noise removal, but their application to low\nsignal-to-noise data can enhance the data quantitative usage since they are\nable to learn the functional form of the signal. We demonstrate that the CNN-ED\nmodels trained on real-world experimental data help to effectively extract\nequilibrium dynamics parameters from two-time correlation functions, containing\nstatistical noise and dynamic heterogeneities. Strategies for optimizing the\nmodels performance and their applicability limits are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Konstantinova_T/0/1/0/all/0/1\">Tatiana Konstantinova</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Wiegart_L/0/1/0/all/0/1\">Lutz Wiegart</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rakitin_M/0/1/0/all/0/1\">Maksim Rakitin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+DeGennaro_A/0/1/0/all/0/1\">Anthony M. DeGennaro</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Barbour_A/0/1/0/all/0/1\">Andi M. Barbour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Federated Learning with Attack-Adaptive Aggregation. (arXiv:2102.05257v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.05257","description":"<p>Federated learning is vulnerable to various attacks, such as model poisoning\nand backdoor attacks, even if some existing defense strategies are used. To\naddress this challenge, we propose an attack-adaptive aggregation strategy to\ndefend against various attacks for robust federated learning. The proposed\napproach is based on training a neural network with an attention mechanism that\nlearns the vulnerability of federated learning models from a set of plausible\nattacks. To the best of our knowledge, our aggregation strategy is the first\none that can be adapted to defend against various attacks in a data-driven\nfashion. Our approach has achieved competitive performance in defending model\npoisoning and backdoor attacks in federated learning tasks on image and text\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Ching Pui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization in Quantum Machine Learning: a Quantum Information Perspective. (arXiv:2102.08991v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2102.08991","description":"<p>Quantum classification and hypothesis testing are two tightly related\nsubjects, the main difference being that the former is data driven: how to\nassign to quantum states $\\rho(x)$ the corresponding class $c$ (or hypothesis)\nis learnt from examples during training, where $x$ can be either tunable\nexperimental parameters or classical data \"embedded\" into quantum states. Does\nthe model generalize? This is the main question in any data-driven strategy,\nnamely the ability to predict the correct class even of previously unseen\nstates. Here we establish a link between quantum machine learning\nclassification and quantum hypothesis testing (state and channel\ndiscrimination) and then show that the accuracy and generalization capability\nof quantum classifiers depend on the (R\\'enyi) mutual informations $I(C{:}Q)$\nand $I_2(X{:}Q)$ between the quantum state space $Q$ and the classical\nparameter space $X$ or class space $C$. Based on the above characterization, we\nthen show how different properties of $Q$ affect classification accuracy and\ngeneralization, such as the dimension of the Hilbert space, the amount of\nnoise, and the amount of neglected information from $X$ via, e.g., pooling\nlayers. Moreover, we introduce a quantum version of the Information Bottleneck\nprinciple that allows us to explore the various tradeoffs between accuracy and\ngeneralization. Finally, in order to check our theoretical predictions, we\nstudy the classification of the quantum phases of an Ising spin chain, and we\npropose the Variational Quantum Information Bottleneck (VQIB) method to\noptimize quantum embeddings of classical data to favor generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Banchi_L/0/1/0/all/0/1\">Leonardo Banchi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pereira_J/0/1/0/all/0/1\">Jason Pereira</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pirandola_S/0/1/0/all/0/1\">Stefano Pirandola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning for Physical Layer Design. (arXiv:2102.11777v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2102.11777","description":"<p>Model-free techniques, such as machine learning (ML), have recently attracted\nmuch interest towards the physical layer design, e.g., symbol detection,\nchannel estimation, and beamforming. Most of these ML techniques employ\ncentralized learning (CL) schemes and assume the availability of datasets at a\nparameter server (PS), demanding the transmission of data from edge devices,\nsuch as mobile phones, to the PS. Exploiting the data generated at the edge,\nfederated learning (FL) has been proposed recently as a distributed learning\nscheme, in which each device computes the model parameters and sends them to\nthe PS for model aggregation while the datasets are kept intact at the edge.\nThus, FL is more communication-efficient and privacy-preserving than CL and\napplicable to the wireless communication scenarios, wherein the data are\ngenerated at the edge devices. This article presents the recent advances in\nFL-based training for physical layer design problems. Compared to CL, the\neffectiveness of FL is presented in terms of communication overhead with a\nslight performance loss in the learning accuracy. The design challenges, such\nas model, data, and hardware complexity, are also discussed in detail along\nwith possible solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Elbir_A/0/1/0/all/0/1\">Ahmet M. Elbir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Papazafeiropoulos_A/0/1/0/all/0/1\">Anastasios K. Papazafeiropoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatzinotas_S/0/1/0/all/0/1\">Symeon Chatzinotas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sample Complexity and Overparameterization Bounds for Temporal Difference Learning with Neural Network Approximation. (arXiv:2103.01391v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.01391","description":"<p>In this paper, we study the dynamics of temporal difference learning with\nneural network-based value function approximation over a general state space,\nnamely, \\emph{Neural TD learning}. We consider two practically used algorithms,\nprojection-free and max-norm regularized Neural TD learning, and establish the\nfirst convergence bounds for these algorithms. An interesting observation from\nour results is that max-norm regularization can dramatically improve the\nperformance of TD learning algorithms, both in terms of sample complexity and\noverparameterization. In particular, we prove that max-norm regularization\nimproves state-of-the-art sample complexity and overparameterization bounds.\nThe results in this work rely on a novel Lyapunov drift analysis of the network\nparameters as a stopped and controlled random process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1\">Semih Cayci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satpathi_S/0/1/0/all/0/1\">Siddhartha Satpathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1\">Niao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A rigorous introduction for linear models. (arXiv:2105.04240v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.04240","description":"<p>This survey is meant to provide an introduction to linear models and the\ntheories behind them. Our goal is to give a rigorous introduction to the\nreaders with prior exposure to ordinary least squares. In machine learning, the\noutput is usually a nonlinear function of the input. Deep learning even aims to\nfind a nonlinear dependence with many layers which require a large amount of\ncomputation. However, most of these algorithms build upon simple linear models.\nWe then describe linear models from different views and find the properties and\ntheories behind the models. The linear model is the main technique in\nregression problems and the primary tool for it is the least squares\napproximation which minimizes a sum of squared errors. This is a natural choice\nwhen we're interested in finding the regression function which minimizes the\ncorresponding expected squared error. This survey is primarily a summary of\npurpose, significance of important theories behind linear models, e.g.,\ndistribution theory, minimum variance estimator. We first describe ordinary\nleast squares from three different points of view upon which we disturb the\nmodel with random noise and Gaussian noise. By Gaussian noise, the model gives\nrise to the likelihood so that we introduce a maximum likelihood estimator. It\nalso develops some distribution theories via this Gaussian disturbance. The\ndistribution theory of least squares will help us answer various questions and\nintroduce related applications. We then prove least squares is the best\nunbiased linear model in the sense of mean squared error and most importantly,\nit actually approaches the theoretical limit. We end up with linear models with\nthe Bayesian approach and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2105.06073","description":"<p>A basic requirement for a mathematical model is often that its solution\n(output) shouldn't change much if the model's parameters (input) are perturbed.\nThis is important because the exact values of parameters may not be known and\none would like to avoid being mislead by an output obtained using incorrect\nvalues. Thus, it's rarely enough to address an application by formulating a\nmodel, solving the resulting optimization problem and presenting the solution\nas the answer. One would need to confirm that the model is suitable, i.e.,\n\"good,\" and this can, at least in part, be achieved by considering a family of\noptimization problems constructed by perturbing parameters of concern. The\nresulting sensitivity analysis uncovers troubling situations with unstable\nsolutions, which we referred to as \"bad\" models, and indicates better model\nformulations. Embedding an actual problem of interest within a family of\nproblems is also a primary path to optimality conditions as well as\ncomputationally attractive, alternative problems, which under ideal\ncircumstances, and when properly tuned, may even furnish the minimum value of\nthe actual problem. The tuning of these alternative problems turns out to be\nintimately tied to finding multipliers in optimality conditions and thus\nemerges as a main component of several optimization algorithms. In fact, the\ntuning amounts to solving certain dual optimization problems. In this tutorial,\nwe'll discuss the opportunities and insights afforded by this broad\nperspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1\">Johannes O. Royset</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.04156","description":"<p>Recent works in self-supervised learning have advanced the state-of-the-art\nby relying on the contrastive learning paradigm, which learns representations\nby pushing positive pairs, or similar examples from the same class, closer\ntogether while keeping negative pairs far apart. Despite the empirical\nsuccesses, theoretical foundations are limited -- prior analyses assume\nconditional independence of the positive pairs given the same class label, but\nrecent empirical applications use heavily correlated positive pairs (i.e., data\naugmentations of the same image). Our work analyzes contrastive learning\nwithout assuming conditional independence of positive pairs using a novel\nconcept of the augmentation graph on data. Edges in this graph connect\naugmentations of the same data, and ground-truth classes naturally form\nconnected sub-graphs. We propose a loss that performs spectral decomposition on\nthe population augmentation graph and can be succinctly written as a\ncontrastive learning objective on neural net representations. Minimizing this\nobjective leads to features with provable accuracy guarantees under linear\nprobe evaluation. By standard generalization bounds, these accuracy guarantees\nalso hold when minimizing the training contrastive loss. Empirically, the\nfeatures learned by our objective can match or outperform several strong\nbaselines on benchmark vision datasets. In all, this work provides the first\nprovable analysis for contrastive learning where guarantees for linear probe\nevaluation can apply to realistic empirical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1\">Jeff Z. HaoChen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Colin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Feedback Learning for Contact-Rich Manipulation Tasks with Uncertainty. (arXiv:2106.04306v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2106.04306","description":"<p>While classic control theory offers state of the art solutions in many\nproblem scenarios, it is often desired to improve beyond the structure of such\nsolutions and surpass their limitations. To this end, residual policy learning\n(RPL) offers a formulation to improve existing controllers with reinforcement\nlearning (RL) by learning an additive \"residual\" to the output of a given\ncontroller. However, the applicability of such an approach highly depends on\nthe structure of the controller. Often, internal feedback signals of the\ncontroller limit an RL algorithm to adequately change the policy and, hence,\nlearn the task. We propose a new formulation that addresses these limitations\nby also modifying the feedback signals to the controller with an RL policy and\nshow superior performance of our approach on a contact-rich peg-insertion task\nunder position and orientation uncertainty. In addition, we use a recent\nCartesian impedance control architecture as the control framework which can be\navailable to us as a black-box while assuming no knowledge about its\ninput/output structure, and show the difficulties of standard RPL. Furthermore,\nwe introduce an adaptive curriculum for the given task to gradually increase\nthe task difficulty in terms of position and orientation uncertainty. A video\nshowing the results can be found at https://youtu.be/SAZm_Krze7U .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1\">Alireza Ranjbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1\">Ngo Anh Vien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1\">Hanna Ziesche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1\">Joschka Boedecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Based Proximity Matrix Factorization for Node Embedding. (arXiv:2106.05476v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05476","description":"<p>Node embedding learns a low-dimensional representation for each node in the\ngraph. Recent progress on node embedding shows that proximity matrix\nfactorization methods gain superb performance and scale to large graphs with\nmillions of nodes. Existing approaches first define a proximity matrix and then\nlearn the embeddings that fit the proximity by matrix factorization. Most\nexisting matrix factorization methods adopt the same proximity for different\ntasks, while it is observed that different tasks and datasets may require\ndifferent proximity, limiting their representation power.\n</p>\n<p>Motivated by this, we propose {\\em Lemane}, a framework with trainable\nproximity measures, which can be learned to best suit the datasets and tasks at\nhand automatically. Our method is end-to-end, which incorporates differentiable\nSVD in the pipeline so that the parameters can be trained via backpropagation.\nHowever, this learning process is still expensive on large graphs. To improve\nthe scalability, we train proximity measures only on carefully subsampled\ngraphs, and then apply standard proximity matrix factorization on the original\ngraph using the learned proximity. Note that, computing the learned proximities\nfor each pair is still expensive for large graphs, and existing techniques for\ncomputing proximities are not applicable to the learned proximities. Thus, we\npresent generalized push techniques to make our solution scalable to large\ngraphs with millions of nodes. Extensive experiments show that our proposed\nsolution outperforms existing solutions on both link prediction and node\nclassification tasks on almost all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zengfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2106.05825","description":"<p>Deep Neural Networks (DNNs) are employed in an increasing number of\napplications, some of which are safety critical. Unfortunately, DNNs are known\nto be vulnerable to so-called adversarial attacks that manipulate inputs to\ncause incorrect results that can be beneficial to an attacker or damaging to\nthe victim. Multiple defenses have been proposed to increase the robustness of\nDNNs. In general, these defenses have high overhead, some require\nattack-specific re-training of the model or careful tuning to adapt to\ndifferent attacks.\n</p>\n<p>This paper presents HASI, a hardware-accelerated defense that uses a process\nwe call stochastic inference to detect adversarial inputs. We show that by\ncarefully injecting noise into the model at inference time, we can\ndifferentiate adversarial inputs from benign ones. HASI uses the output\ndistribution characteristics of noisy inference compared to a non-noisy\nreference to detect adversarial inputs. We show an adversarial detection rate\nof 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds\nthe detection rate of the state of the art approaches, with a much lower\noverhead. We demonstrate two software/hardware-accelerated co-designs, which\nreduces the performance impact of stochastic inference to 1.58X-2X relative to\nthe unprotected baseline, compared to 15X-20X overhead for a software-only GPU\nimplementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States. (arXiv:2106.08334v2 [hep-ph] UPDATED)","link":"http://arxiv.org/abs/2106.08334","description":"<p>Tensor Networks are non-trivial representations of high-dimensional tensors,\noriginally designed to describe quantum many-body systems. We show that Tensor\nNetworks are ideal vehicles to connect quantum mechanical concepts to machine\nlearning techniques, thereby facilitating an improved interpretability of\nneural networks. This study presents the discrimination of top quark signal\nover QCD background processes using a Matrix Product State classifier. We show\nthat entanglement entropy can be used to interpret what a network learns, which\ncan be used to reduce the complexity of the network and feature space without\nloss of generality or performance. For the optimisation of the network, we\ncompare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic\ngradient descent (SGD) and propose a joined training algorithm to harness the\nexplainability of DMRG with the efficiency of SGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/hep-ph/1/au:+Araz_J/0/1/0/all/0/1\">Jack Y. Araz</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Spannowsky_M/0/1/0/all/0/1\">Michael Spannowsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.12543","description":"<p>As machine learning models grow more complex and their applications become\nmore high-stakes, tools for explaining model predictions have become\nincreasingly important. This has spurred a flurry of research in model\nexplainability and has given rise to feature attribution methods such as LIME\nand SHAP. Despite their widespread use, evaluating and comparing different\nfeature attribution methods remains challenging: evaluations ideally require\nhuman studies, and empirical evaluation metrics are often data-intensive or\ncomputationally prohibitive on real-world datasets. In this work, we address\nthis issue by releasing XAI-Bench: a suite of synthetic datasets along with a\nlibrary for benchmarking feature attribution algorithms. Unlike real-world\ndatasets, synthetic datasets allow the efficient computation of conditional\nexpected values that are needed to evaluate ground-truth Shapley values and\nother metrics. The synthetic datasets we release offer a wide variety of\nparameters that can be configured to simulate real-world data. We demonstrate\nthe power of our library by benchmarking popular explainability techniques\nacross several evaluation metrics and across a variety of settings. The\nversatility and efficiency of our library will help researchers bring their\nexplainability methods from development to deployment. Our code is available at\nhttps://github.com/abacusai/xai-bench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1\">Sujay Khandagale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1\">Colin White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1\">Willie Neiswanger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control with Model-assisted Learning. (arXiv:2106.14144v2 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2106.14144","description":"<p>As people spend up to 87% of their time indoors, intelligent Heating,\nVentilation, and Air Conditioning (HVAC) systems in buildings are essential for\nmaintaining occupant comfort and reducing energy consumption. These HVAC\nsystems in smart buildings rely on real-time sensor readings, which in practice\noften suffer from various faults and could also be vulnerable to malicious\nattacks. Such faulty sensor inputs may lead to the violation of indoor\nenvironment requirements (e.g., temperature, humidity, etc.) and the increase\nof energy consumption. While many model-based approaches have been proposed in\nthe literature for building HVAC control, it is costly to develop accurate\nphysical models for ensuring their performance and even more challenging to\naddress the impact of sensor faults. In this work, we present a novel\nlearning-based framework for sensor fault-tolerant HVAC control, which includes\nthree deep learning based components for 1) generating temperature proposals\nwith the consideration of possible sensor faults, 2) selecting one of the\nproposals based on the assessment of their accuracy, and 3) applying\nreinforcement learning with the selected temperature proposal. Moreover, to\naddress the challenge of training data insufficiency in building-related tasks,\nwe propose a model-assisted learning method leveraging an abstract model of\nbuilding physical dynamics. Through extensive experiments, we demonstrate that\nthe proposed fault-tolerant HVAC control framework can significantly reduce\nbuilding temperature violations under a variety of sensor fault patterns while\nmaintaining energy efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yangyang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1\">Zheng O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.05948","description":"<p>The goal of representation learning is different from the ultimate objective\nof machine learning such as decision making, it is therefore very difficult to\nestablish clear and direct objectives for training representation learning\nmodels. It has been argued that a good representation should disentangle the\nunderlying variation factors, yet how to translate this into training\nobjectives remains unknown. This paper presents an attempt to establish direct\ntraining criterions and design principles for developing good representation\nlearning models. We propose that a good representation learning model should be\nmaximally expressive, i.e., capable of distinguishing the maximum number of\ninput configurations. We formally define expressiveness and introduce the\nmaximum expressiveness (MEXS) theorem of a general learning model. We propose\nto train a model by maximizing its expressiveness while at the same time\nincorporating general priors such as model smoothness. We present a conscience\ncompetitive learning algorithm which encourages the model to reach its MEXS\nwhilst at the same time adheres to model smoothness prior. We also introduce a\nlabel consistent training (LCT) technique to boost model smoothness by\nencouraging it to assign consistent labels to similar samples. We present\nextensive experimental results to show that our method can indeed design\nrepresentation learning models capable of developing representations that are\nas good as or better than state of the art. We also show that our technique is\ncomputationally efficient, robust against different parameter settings and can\nwork effectively on a variety of datasets. Code available at\nhttps://github.com/qlilx/odgrlm.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1\">Jonathan M Garibaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1\">Guoping Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning. (arXiv:2107.06344v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.06344","description":"<p>Drivers have unique and rich driving behaviors when operating vehicles in\ntraffic. This paper presents a novel driver behavior learning approach that\ncaptures the uniqueness and richness of human driver behavior in realistic\ndriving scenarios. A stochastic inverse reinforcement learning (SIRL) approach\nis proposed to learn a distribution of cost function, which represents the\nrichness of the human driver behavior with a given set of driver-specific\ndemonstrations. Evaluations are conducted on the realistic driving data\ncollected from the 3D driver-in-the-loop driving simulation. The results show\nthat the learned stochastic driver model is capable of expressing the richness\nof the human driving strategies under different realistic driving scenarios.\nCompared to the deterministic baseline driver behavior model, the results\nreveal that the proposed stochastic driver behavior model can better replicate\nthe driver's unique and rich driving strategies in a variety of traffic\nconditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ozkan_M/0/1/0/all/0/1\">Mehmet Fatih Ozkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocque_A/0/1/0/all/0/1\">Abishek Joseph Rocque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yao Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Round Active Learning. (arXiv:2107.06703v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.06703","description":"<p>Active learning (AL) aims at reducing labeling effort by identifying the most\nvaluable unlabeled data points from a large pool. Traditional AL frameworks\nhave two limitations: First, they perform data selection in a multi-round\nmanner, which is time-consuming and impractical. Second, they usually assume\nthat there are a small amount of labeled data points available in the same\ndomain as the data in the unlabeled pool. Recent work proposes a solution for\none-round active learning based on data utility learning and optimization,\nwhich fixes the first issue but still requires the initially labeled data\npoints in the same domain. In this paper, we propose $\\mathrm{D^2ULO}$ as a\nsolution that solves both issues. Specifically, $\\mathrm{D^2ULO}$ leverages the\nidea of domain adaptation (DA) to train a data utility model which can\neffectively predict the utility for any given unlabeled data in the target\ndomain once labeled. The trained data utility model can then be used to select\nhigh-utility data and at the same time, provide an estimate for the utility of\nthe selected data. Our algorithm does not rely on any feedback from annotators\nin the target domain and hence, can be used to perform zero-round active\nlearning or warm-start existing multi-round active learning strategies. Our\nexperiments show that $\\mathrm{D^2ULO}$ outperforms the existing\nstate-of-the-art AL strategies equipped with domain adaptation over various\ndomain shift settings (e.g., real-to-real data and synthetic-to-real data).\nParticularly, $\\mathrm{D^2ULO}$ is applicable to the scenario where source and\ntarget labels have mismatches, which is not supported by the existing works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Si Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoxi Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering and attention model based for intelligent trading. (arXiv:2107.06782v2 [q-fin.ST] UPDATED)","link":"http://arxiv.org/abs/2107.06782","description":"<p>The foreign exchange market has taken an important role in the global\nfinancial market. While foreign exchange trading brings high-yield\nopportunities to investors, it also brings certain risks. Since the\nestablishment of the foreign exchange market in the 20th century, foreign\nexchange rate forecasting has become a hot issue studied by scholars from all\nover the world. Due to the complexity and number of factors affecting the\nforeign exchange market, technical analysis cannot respond to administrative\nintervention or unexpected events. Our team chose several pairs of foreign\ncurrency historical data and derived technical indicators from 2005 to 2021 as\nthe dataset and established different machine learning models for event-driven\nprice prediction for oversold scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Rana_M/0/1/0/all/0/1\">Mimansa Rana</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mao_N/0/1/0/all/0/1\">Nanxiang Mao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Ao_M/0/1/0/all/0/1\">Ming Ao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Wu_X/0/1/0/all/0/1\">Xiaohui Wu</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Liang_P/0/1/0/all/0/1\">Poning Liang</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Khushi_M/0/1/0/all/0/1\">Matloob Khushi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zooming Into the Darknet: Characterizing Internet Background Radiation and its Structural Changes. (arXiv:2108.00079v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2108.00079","description":"<p>Network telescopes or \"Darknets\" provide a unique window into Internet-wide\nmalicious activities associated with malware propagation, denial of service\nattacks, scanning performed for network reconnaissance, and others. Analyses of\nthe resulting data can provide actionable insights to security analysts that\ncan be used to prevent or mitigate cyber-threats. Large Darknets, however,\nobserve millions of nefarious events on a daily basis which makes the\ntransformation of the captured information into meaningful insights\nchallenging. We present a novel framework for characterizing Darknet behavior\nand its temporal evolution aiming to address this challenge. The proposed\nframework: (i) Extracts a high dimensional representation of Darknet events\ncomposed of features distilled from Darknet data and other external sources;\n(ii) Learns, in an unsupervised fashion, an information-preserving\nlow-dimensional representation of these events (using deep representation\nlearning) that is amenable to clustering; (iv) Performs clustering of the\nscanner data in the resulting representation space and provides interpretable\ninsights using optimal decision trees; and (v) Utilizes the clustering outcomes\nas \"signatures\" that can be used to detect structural changes in the Darknet\nactivities. We evaluate the proposed system on a large operational Network\nTelescope and demonstrate its ability to detect real-world, high-impact\ncybersecurity incidents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kallitsis_M/0/1/0/all/0/1\">Michalis Kallitsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1\">Vasant Honavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prajapati_R/0/1/0/all/0/1\">Rupesh Prajapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dinghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_J/0/1/0/all/0/1\">John Yen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-encoder based Model for High-dimensional Imbalanced Industrial Data. (arXiv:2108.02083v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2108.02083","description":"<p>With the proliferation of IoT devices, the distributed control systems are\nnow capturing and processing more sensors at higher frequency than ever before.\nThese new data, due to their volume and novelty, cannot be effectively consumed\nwithout the help of data-driven techniques. Deep learning is emerging as a\npromising technique to analyze these data, particularly in soft sensor\nmodeling. The strong representational capabilities of complex data and the\nflexibility it offers from an architectural perspective make it a topic of\nactive applied research in industrial settings. However, the successful\napplications of deep learning in soft sensing are still not widely integrated\nin factory control systems, because most of the research on soft sensing do not\nhave access to large scale industrial data which are varied, noisy and\nincomplete. The results published in most research papers are therefore not\neasily reproduced when applied to the variety of data in industrial settings.\nHere we provide manufacturing data sets that are much larger and more complex\nthan public open soft sensor data. Moreover, the data sets are from Seagate\nfactories on active service with only necessary anonymization, so that they\nreflect the complex and noisy nature of real-world data. We introduce a\nvariance weighted multi-headed auto-encoder classification model that fits well\ninto the high-dimensional and highly imbalanced data. Besides the use of\nweighting or sampling methods to handle the highly imbalanced data, the model\nalso simultaneously predicts multiple outputs by exploiting output-supervised\nrepresentation learning and multi-task weighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bom_S/0/1/0/all/0/1\">Sthitie Bom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Reinforcement Learning over MDPs. (arXiv:2108.02323v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.02323","description":"<p>The past decade has seen the rapid development of Reinforcement Learning,\nwhich acquires impressive performance with numerous training resources.\nHowever, one of the greatest challenges in RL is generalization efficiency\n(i.e., generalization performance in a unit time). This paper proposes a\nframework of Active Reinforcement Learning (ARL) over MDPs to improve\ngeneralization efficiency in a limited resource by instance selection. Given a\nnumber of instances, the algorithm chooses out valuable instances as training\nsets while training the policy, thereby costing fewer resources. Unlike\nexisting approaches, we attempt to actively select and use training data rather\nthan train on all the given data, thereby costing fewer resources. Furthermore,\nwe introduce a general instance evaluation metrics and selection mechanism into\nthe framework. Experiments results reveal that the proposed framework with\nProximal Policy Optimization as policy optimizer can effectively improve\ngeneralization efficiency than unselect-ed and unbiased selected methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DNN-HMM based Speaker Adaptive Emotion Recognition using Proposed Epoch and MFCC Features. (arXiv:1806.00984v1 [cs.SD] CROSS LISTED)","link":"http://arxiv.org/abs/1806.00984","description":"<p>Speech is produced when time varying vocal tract system is excited with time\nvarying excitation source. Therefore, the information present in a speech such\nas message, emotion, language, speaker is due to the combined effect of both\nexcitation source and vocal tract system. However, there is very less\nutilization of excitation source features to recognize emotion. In our earlier\nwork, we have proposed a novel method to extract glottal closure instants\n(GCIs) known as epochs. In this paper, we have explored epoch features namely\ninstantaneous pitch, phase and strength of epochs for discriminating emotions.\nWe have combined the excitation source features and the well known\nMale-frequency cepstral coefficient (MFCC) features to develop an emotion\nrecognition system with improved performance. DNN-HMM speaker adaptive models\nhave been developed using MFCC, epoch and combined features. IEMOCAP emotional\ndatabase has been used to evaluate the models. The average accuracy for emotion\nrecognition system when using MFCC and epoch features separately is 59.25% and\n54.52% respectively. The recognition performance improves to 64.2% when MFCC\nand epoch features are combined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fahad_M/0/1/0/all/0/1\">Md. Shah Fahad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_J/0/1/0/all/0/1\">Jainath Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_G/0/1/0/all/0/1\">Gyadhar Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deepak_A/0/1/0/all/0/1\">Akshay Deepak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Machine Learning"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}