{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Decision-Focused Summarization. (arXiv:2109.06896v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06896","description":"<p>Relevance in summarization is typically defined based on textual information\nalone, without incorporating insights about a particular decision. As a result,\nto support risk analysis of pancreatic cancer, summaries of medical notes may\ninclude irrelevant information such as a knee injury. We propose a novel\nproblem, decision-focused summarization, where the goal is to summarize\nrelevant information for a decision. We leverage a predictive model that makes\nthe decision based on the full text to provide valuable insights on how a\ndecision can be inferred from text. To build a summary, we then select\nrepresentative sentences that lead to similar model decisions as using the full\ntext while accounting for textual non-redundancy. To evaluate our method\n(DecSum), we build a testbed where the task is to summarize the first ten\nreviews of a restaurant in support of predicting its future rating on Yelp.\nDecSum substantially outperforms text-only summarization methods and\nmodel-based explanation methods in decision faithfulness and\nrepresentativeness. We further demonstrate that DecSum is the only method that\nenables humans to outperform random chance in predicting which restaurant will\nbe better rated in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chao-Chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit. (arXiv:2109.06912v1 [eess.AS])","link":"http://arxiv.org/abs/2109.06912","description":"<p>This paper presents fairseq S^2, a fairseq extension for speech synthesis. We\nimplement a number of autoregressive (AR) and non-AR text-to-speech models, and\ntheir multi-speaker variants. To enable training speech synthesis models with\nless curated data, a number of preprocessing tools are built and their\nimportance is shown empirically. To facilitate faster iteration of development\nand analysis, a suite of automatic metrics is included. Apart from the features\nadded specifically for this extension, fairseq S^2 also benefits from the\nscalability offered by fairseq and can be easily integrated with other\nstate-of-the-art systems provided in this framework. The code, documentation,\nand pre-trained models are available at\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1\">Jiatao Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning. (arXiv:2109.06935v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06935","description":"<p>Recent work has shown evidence that the knowledge acquired by multilingual\nBERT (mBERT) has two components: a language-specific and a language-neutral\none. This paper analyses the relationship between them, in the context of\nfine-tuning on two tasks -- POS tagging and natural language inference -- which\nrequire the model to bring to bear different degrees of language-specific\nknowledge. Visualisations reveal that mBERT loses the ability to cluster\nrepresentations by language after fine-tuning, a result that is supported by\nevidence from language identification experiments. However, further experiments\non 'unlearning' language-specific representations using gradient reversal and\niterative adversarial learning are shown not to add further improvement to the\nlanguage-independent component over and above the effect of fine-tuning. The\nresults presented here suggest that the process of fine-tuning causes a\nreorganisation of the model's limited representational capacity, enhancing\nlanguage-independent representations at the expense of language-specific ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanti_M/0/1/0/all/0/1\">Marc Tanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plas_L/0/1/0/all/0/1\">Lonneke van der Plas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borg_C/0/1/0/all/0/1\">Claudia Borg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders. (arXiv:2109.06939v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06939","description":"<p>Multi-task learning with transformer encoders (MTL) has emerged as a powerful\ntechnique to improve performance on closely-related tasks for both accuracy and\nefficiency while a question still remains whether or not it would perform as\nwell on tasks that are distinct in nature. We first present MTL results on five\nNLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over\nsingle-task learning. We then conduct an extensive pruning analysis to show\nthat a certain set of attention heads get claimed by most tasks during MTL, who\ninterfere with one another to fine-tune those heads for their own objectives.\nBased on this finding, we propose the Stem Cell Hypothesis to reveal the\nexistence of attention heads naturally talented for many tasks that cannot be\njointly trained to create adequate embeddings for all of those tasks. Finally,\nwe design novel parameter-free probes to justify our hypothesis and demonstrate\nhow attention heads are transformed across the five tasks during MTL through\nlabel analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Han He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Exposing Problems with Neural Dialog Models. (arXiv:2109.06950v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06950","description":"<p>Neural dialog models are known to suffer from problems such as generating\nunsafe and inconsistent responses. Even though these problems are crucial and\nprevalent, they are mostly manually identified by model designers through\ninteractions. Recently, some research instructs crowdworkers to goad the bots\ninto triggering such problems. However, humans leverage superficial clues such\nas hate speech, while leaving systematic problems undercover. In this paper, we\npropose two methods including reinforcement learning to automatically trigger a\ndialog model into generating problematic responses. We show the effect of our\nmethods in exposing safety and contradiction issues with state-of-the-art\ndialog models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagae_K/0/1/0/all/0/1\">Kenji Sagae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech. (arXiv:2109.06952v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06952","description":"<p>Automatic Speech Recognition (ASR) systems are often optimized to work best\nfor speakers with canonical speech patterns. Unfortunately, these systems\nperform poorly when tested on atypical speech and heavily accented speech. It\nhas previously been shown that personalization through model fine-tuning\nsubstantially improves performance. However, maintaining such large models per\nspeaker is costly and difficult to scale. We show that by adding a relatively\nsmall number of extra parameters to the encoder layers via so-called residual\nadapter, we can achieve similar adaptation gains compared to model fine-tuning,\nwhile only updating a tiny fraction (less than 0.5%) of the model parameters.\nWe demonstrate this on two speech adaptation tasks (atypical and accented\nspeech) and for two state-of-the-art ASR architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zayats_V/0/1/0/all/0/1\">Vicky Zayats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padfield_D/0/1/0/all/0/1\">Dirk Padfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaillancourt_K/0/1/0/all/0/1\">Kara Vaillancourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biadsy_F/0/1/0/all/0/1\">Fadi Biadsy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for More Efficient Dynamic Programs. (arXiv:2109.06966v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06966","description":"<p>Computational models of human language often involve combinatorial problems.\nFor instance, a probabilistic parser may marginalize over exponentially many\ntrees to make predictions. Algorithms for such problems often employ dynamic\nprogramming and are not always unique. Finding one with optimal asymptotic\nruntime can be unintuitive, time-consuming, and error-prone. Our work aims to\nautomate this laborious process. Given an initial correct declarative program,\nwe search for a sequence of semantics-preserving transformations to improve its\nrunning time as much as possible. To this end, we describe a set of program\ntransformations, a simple metric for assessing the efficiency of a transformed\nprogram, and a heuristic search procedure to improve this metric. We show that\nin practice, automated search -- like the mental search performed by human\nprogrammers -- can find substantial improvements to the initial program.\nEmpirically, we show that many common speed-ups described in the NLP literature\ncould have been discovered automatically by our system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1\">Jason Eisner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Identification of Dementia from Transcripts using Transformer Networks. (arXiv:2109.06980v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06980","description":"<p>Alzheimer's disease (AD) is the main cause of dementia which is accompanied\nby loss of memory and may lead to severe consequences in peoples' everyday life\nif not diagnosed on time. Very few works have exploited transformer-based\nnetworks and despite the high accuracy achieved, little work has been done in\nterms of model interpretability. In addition, although Mini-Mental State Exam\n(MMSE) scores are inextricably linked with the identification of dementia,\nresearch works face the task of dementia identification and the task of the\nprediction of MMSE scores as two separate tasks. In order to address these\nlimitations, we employ several transformer-based models, with BERT achieving\nthe highest accuracy accounting for 85.56%. Concurrently, we propose an\ninterpretable method to detect AD patients based on siamese networks reaching\naccuracy up to 81.18%. Next, we introduce two multi-task learning models, where\nthe main task refers to the identification of dementia (binary classification),\nwhile the auxiliary one corresponds to the identification of the severity of\ndementia (multiclass classification). Our model obtains accuracy equal to\n84.99% on the detection of AD patients in the multi-task learning setting.\nFinally, we present some new methods to identify the linguistic patterns used\nby AD patients and non-AD ones, including text statistics, vocabulary\nuniqueness, word usage, correlations via a detailed linguistic analysis, and\nexplainability techniques (LIME). Findings indicate significant differences in\nlanguage between AD and non-AD patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ilias_L/0/1/0/all/0/1\">Loukas Ilias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1\">Dimitris Askounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOPE: A Corpus of Naturally-Occurring Presuppositions in English. (arXiv:2109.06987v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06987","description":"<p>Understanding language requires grasping not only the overtly stated content,\nbut also making inferences about things that were left unsaid. These inferences\ninclude presuppositions, a phenomenon by which a listener learns about new\ninformation through reasoning about what a speaker takes as given.\nPresuppositions require complex understanding of the lexical and syntactic\nproperties that trigger them as well as the broader conversational context. In\nthis work, we introduce the Naturally-Occurring Presuppositions in English\n(NOPE) Corpus to investigate the context-sensitivity of 10 different types of\npresupposition triggers and to evaluate machine learning models' ability to\npredict human inferences. We find that most of the triggers we investigate\nexhibit moderate variability. We further find that transformer-based models\ndraw correct inferences in simple cases involving presuppositions, but they\nfail to capture the minority of exceptional cases in which human judgments\nreveal complex interactions between context and triggers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_S/0/1/0/all/0/1\">Sebastian Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agha_O/0/1/0/all/0/1\">Omar Agha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Soo-Hwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuoye Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Three Step Training Approach with Data Augmentation for Morphological Inflection. (arXiv:2109.07006v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07006","description":"<p>We present the BME submission for the SIGMORPHON 2021 Task 0 Part 1,\nGeneralization Across Typologically Diverse Languages shared task. We use an\nLSTM encoder-decoder model with three step training that is first trained on\nall languages, then fine-tuned on each language families and finally finetuned\non individual languages. We use a different type of data augmentation technique\nin the first two steps. Our system outperformed the only other submission.\nAlthough it remains worse than the Transformer baseline released by the\norganizers, our model is simpler and our data augmentation techniques are\neasily applicable to new languages. We perform ablation studies and show that\nthe augmentation techniques and the three training steps often help but\nsometimes have a negative effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szolnok_G/0/1/0/all/0/1\">Gabor Szolnok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barta_B/0/1/0/all/0/1\">Botond Barta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakatos_D/0/1/0/all/0/1\">Dorina Lakatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acs_J/0/1/0/all/0/1\">Judit Acs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering. (arXiv:2109.07009v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07009","description":"<p>In this paper we propose a novel approach towards improving the efficiency of\nQuestion Answering (QA) systems by filtering out questions that will not be\nanswered by them. This is based on an interesting new finding: the answer\nconfidence scores of state-of-the-art QA systems can be approximated well by\nmodels solely using the input question text. This enables preemptive filtering\nof questions that are not answered by the system due to their answer confidence\nscores being lower than the system threshold. Specifically, we learn\nTransformer-based question models by distilling Transformer-based answering\nmodels. Our experiments on three popular QA datasets and one industrial QA\nbenchmark demonstrate the ability of our question models to approximate the\nPrecision/Recall curves of the target QA system well. These question models,\nwhen used as filters, can effectively trade off lower computation cost of QA\nsystems for lower Recall, e.g., reducing computation by ~60%, while only losing\n~3-4% of Recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Siddhant Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Written Justifications are Key to Aggregate Crowdsourced Forecasts. (arXiv:2109.07017v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07017","description":"<p>This paper demonstrates that aggregating crowdsourced forecasts benefits from\nmodeling the written justifications provided by forecasters. Our experiments\nshow that the majority and weighted vote baselines are competitive, and that\nthe written justifications are beneficial to call a question throughout its\nlife except in the last quarter. We also conduct an error analysis shedding\nlight into the characteristics that make a justification unreliable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotamraju_S/0/1/0/all/0/1\">Saketh Kotamraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_E/0/1/0/all/0/1\">Eduardo Blanco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency Effects on Syntactic Rule Learning in Transformers. (arXiv:2109.07020v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07020","description":"<p>Pre-trained language models perform well on a variety of linguistic tasks\nthat require symbolic reasoning, raising the question of whether such models\nimplicitly represent abstract symbols and rules. We investigate this question\nusing the case study of BERT's performance on English subject-verb agreement.\nUnlike prior work, we train multiple instances of BERT from scratch, allowing\nus to perform a series of controlled interventions at pre-training time. We\nshow that BERT often generalizes well to subject-verb pairs that never occurred\nin training, suggesting a degree of rule-governed behavior. We also find,\nhowever, that performance is heavily influenced by word frequency, with\nexperiments showing that both the absolute frequency of a verb form, as well as\nthe frequency relative to the alternate inflection, are causally implicated in\nthe predictions BERT makes at inference time. Closer analysis of these\nfrequency effects reveals that BERT's behavior is consistent with a system that\ncorrectly applies the SVA rule in general but struggles to overcome strong\ntraining priors and to estimate agreement features (singular vs. plural) on\ninfrequent lexical items.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text NLG. (arXiv:2109.07043v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07043","description":"<p>Ever since neural models were adopted in data-to-text language generation,\nthey have invariably been reliant on extrinsic components to improve their\nsemantic accuracy, because the models normally do not exhibit the ability to\ngenerate text that reliably mentions all of the information provided in the\ninput. In this paper, we propose a novel decoding method that extracts\ninterpretable information from encoder-decoder models' cross-attention, and\nuses it to infer which attributes are mentioned in the generated text, which is\nsubsequently used to rescore beam hypotheses. Using this decoding method with\nT5 and BART, we show on three datasets its ability to dramatically reduce\nsemantic errors in the generated outputs, while maintaining their\nstate-of-the-art quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Juraska_J/0/1/0/all/0/1\">Juraj Juraska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conditional Generative Matching Model for Multi-lingual Reply Suggestion. (arXiv:2109.07046v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07046","description":"<p>We study the problem of multilingual automated reply suggestions (RS) model\nserving many languages simultaneously. Multilingual models are often challenged\nby model capacity and severe data distribution skew across languages. While\nprior works largely focus on monolingual models, we propose Conditional\nGenerative Matching models (CGM), optimized within a Variational Autoencoder\nframework to address challenges arising from multi-lingual RS. CGM does so with\nexpressive message conditional priors, mixture densities to enhance\nmulti-lingual data representation, latent alignment for language\ndiscrimination, and effective variational optimization techniques for training\nmulti-lingual RS. The enhancements result in performance that exceed\ncompetitive baselines in relevance (ROUGE score) by more than 10\\% on average,\nand 16\\% for low resource languages. CGM also shows remarkable improvements in\ndiversity (80\\%) illustrating its expressiveness in representation of\nmulti-lingual data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARCH: Efficient Adversarial Regularized Training with Caching. (arXiv:2109.07048v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07048","description":"<p>Adversarial regularization can improve model generalization in many natural\nlanguage processing tasks. However, conventional approaches are computationally\nexpensive since they need to generate a perturbation for each sample in each\nepoch. We propose a new adversarial regularization method ARCH (adversarial\nregularization with caching), where perturbations are generated and cached once\nevery several epochs. As caching all the perturbations imposes memory usage\nconcerns, we adopt a K-nearest neighbors-based strategy to tackle this issue.\nThe strategy only requires caching a small amount of perturbations, without\nintroducing additional training time. We evaluate our proposed method on a set\nof neural machine translation and natural language understanding tasks. We\nobserve that ARCH significantly eases the computational burden (saves up to\n70\\% of computational time in comparison with conventional approaches). More\nsurprisingly, by reducing the variance of stochastic gradients, ARCH produces a\nnotably better (in most of the tasks) or comparable model generalization. Our\ncode is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Training with Differentiable Teacher. (arXiv:2109.07049v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07049","description":"<p>Self-training achieves enormous success in various semi-supervised and\nweakly-supervised learning tasks. The method can be interpreted as a\nteacher-student framework, where the teacher generates pseudo-labels, and the\nstudent makes predictions. The two models are updated alternatingly. However,\nsuch a straightforward alternating update rule leads to training instability.\nThis is because a small change in the teacher may result in a significant\nchange in the student. To address this issue, we propose {\\ours}, short for\ndifferentiable self-training, that treats teacher-student as a Stackelberg\ngame. In this game, a leader is always in a more advantageous position than a\nfollower. In self-training, the student contributes to the prediction\nperformance, and the teacher controls the training process by generating\npseudo-labels. Therefore, we treat the student as the leader and the teacher as\nthe follower. The leader procures its advantage by acknowledging the follower's\nstrategy, which involves differentiable pseudo-labels and differentiable sample\nweights. Consequently, the leader-follower interaction can be effectively\ncaptured via Stackelberg gradient, obtained by differentiating the follower's\nstrategy. Experimental results on semi- and weakly-supervised classification\nand named entity recognition tasks show that our model outperforms existing\napproaches by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_S/0/1/0/all/0/1\">Siawpeng Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongyuan Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Text Auto-Completion with Next Phrase Prediction. (arXiv:2109.07067v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07067","description":"<p>Language models such as GPT-2 have performed well on constructing\nsyntactically sound sentences for text auto-completion task. However, such\nmodels often require considerable training effort to adapt to specific writing\ndomains (e.g., medical). In this paper, we propose an intermediate training\nstrategy to enhance pre-trained language models' performance in the text\nauto-completion task and fastly adapt them to specific domains. Our strategy\nincludes a novel self-supervised training objective called Next Phrase\nPrediction (NPP), which encourages a language model to complete the partial\nquery with enriched phrases and eventually improve the model's text\nauto-completion performance. Preliminary experiments have shown that our\napproach is able to outperform the baselines in auto-completion for email and\nacademic writing domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Lexically Constrained Headline Generation. (arXiv:2109.07080v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07080","description":"<p>This paper explores a variant of automatic headline generation methods, where\na generated headline is required to include a given phrase such as a company or\na product name. Previous methods using Transformer-based models generate a\nheadline including a given phrase by providing the encoder with additional\ninformation corresponding to the given phrase. However, these methods cannot\nalways include the phrase in the generated headline. Inspired by previous\nRNN-based methods generating token sequences in backward and forward directions\nfrom the given phrase, we propose a simple Transformer-based method that\nguarantees to include the given phrase in the high-quality generated headline.\nWe also consider a new headline generation strategy that takes advantage of the\ncontrollable generation order of Transformer. Our experiments with the Japanese\nNews Corpus demonstrate that our methods, which are guaranteed to include the\nphrase in the generated headline, achieve ROUGE scores comparable to previous\nTransformer-based methods. We also show that our generation strategy performs\nbetter than previous strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamada_K/0/1/0/all/0/1\">Kosuke Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hitomi_Y/0/1/0/all/0/1\">Yuta Hitomi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamori_H/0/1/0/all/0/1\">Hideaki Tamori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1\">Ryohei Sasano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koichi Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Extraction of Word Embedding from Q-contexts. (arXiv:2109.07084v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07084","description":"<p>The notion of word embedding plays a fundamental role in natural language\nprocessing (NLP). However, pre-training word embedding for very large-scale\nvocabulary is computationally challenging for most existing methods. In this\nwork, we show that with merely a small fraction of contexts (Q-contexts)which\nare typical in the whole corpus (and their mutual information with words), one\ncan construct high-quality word embedding with negligible errors. Mutual\ninformation between contexts and words can be encoded canonically as a sampling\nstate, thus, Q-contexts can be fast constructed. Furthermore, we present an\nefficient and effective WEQ method, which is capable of extracting word\nembedding directly from these typical contexts. In practical scenarios, our\nalgorithm runs 11$\\sim$13 times faster than well-established methods. By\ncomparing with well-known methods such as matrix factorization, word2vec,\nGloVeand fasttext, we demonstrate that our method achieves comparable\nperformance on a variety of downstream NLP tasks, and in the meanwhile\nmaintains run-time and resource advantages over all these baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1\">Junsheng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weizhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Ben Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chang-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Document-Level Paraphrase Generation with Sentence Rewriting and Reordering. (arXiv:2109.07095v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07095","description":"<p>Paraphrase generation is an important task in natural language processing.\nPrevious works focus on sentence-level paraphrase generation, while ignoring\ndocument-level paraphrase generation, which is a more challenging and valuable\ntask. In this paper, we explore the task of document-level paraphrase\ngeneration for the first time and focus on the inter-sentence diversity by\nconsidering sentence rewriting and reordering. We propose CoRPG (Coherence\nRelationship guided Paraphrase Generation), which leverages graph GRU to encode\nthe coherence relationship graph and get the coherence-aware representation for\neach sentence, which can be used for re-arranging the multiple (possibly\nmodified) input sentences. We create a pseudo document-level paraphrase dataset\nfor training CoRPG. Automatic evaluation results show CoRPG outperforms several\nstrong baseline models on the BERTScore and diversity scores. Human evaluation\nalso shows our model can generate document paraphrase with more diversity and\nsemantic preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yitao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. (arXiv:2109.07102v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07102","description":"<p>There have been many efforts to try to understand what grammatical knowledge\n(e.g., ability to understand the part of speech of a token) is encoded in large\npre-trained language models (LM). This is done through `Edge Probing' (EP)\ntests: simple ML models that predict the grammatical properties of a span\n(whether it has a particular part of speech) using \\textit{only} the LM's token\nrepresentations. However, most NLP applications use \\finetuned\\ LMs. Here, we\nask: if a LM is \\finetuned, does the encoding of linguistic information in it\nchange, as measured by EP tests? Conducting experiments on multiple\nquestion-answering (QA) datasets, we answer that question negatively: the EP\ntest results do not change significantly when the fine-tuned QA model performs\nwell or in adversarial situations where the model is forced to learn wrong\ncorrelations. However, a critical analysis of the EP task datasets reveals that\nEP models may rely on spurious correlations to make predictions. This indicates\neven if \\finetuning\\ changes the encoding of such knowledge, the EP tests might\nfail to measure it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sagnik Ray Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Named Entity Recognition Based on Multi-hop Dependency Trigger. (arXiv:2109.07118v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07118","description":"<p>This paper presents a simple and effective approach in low-resource named\nentity recognition (NER) based on multi-hop dependency trigger. Dependency\ntrigger refer to salient nodes relative to a entity in the dependency graph of\na context sentence. Our main observation is that there often exists trigger\nwhich play an important role to recognize the location and type of entity in\nsentence. Previous research has used manual labelling of trigger. Our main\ncontribution is to propose use a syntactic parser to automatically annotate\ntrigger. Experiments on two English datasets (CONLL 2003 and BC5CDR) show that\nthe proposed method is comparable to the previous trigger-based NER model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiangxu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Does The User Want? Information Gain for Hierarchical Dialogue Policy Optimisation. (arXiv:2109.07129v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07129","description":"<p>The dialogue management component of a task-oriented dialogue system is\ntypically optimised via reinforcement learning (RL). Optimisation via RL is\nhighly susceptible to sample inefficiency and instability. The hierarchical\napproach called Feudal Dialogue Management takes a step towards more efficient\nlearning by decomposing the action space. However, it still suffers from\ninstability due to the reward only being provided at the end of the dialogue.\nWe propose the usage of an intrinsic reward based on information gain to\naddress this issue. Our proposed reward favours actions that resolve\nuncertainty or query the user whenever necessary. It enables the policy to\nlearn how to retrieve the users' needs efficiently, which is an integral aspect\nin every task-oriented conversation. Our algorithm, which we call FeudalGain,\nachieves state-of-the-art results in most environments of the PyDial framework,\noutperforming much more complex approaches. We confirm the sample efficiency\nand stability of our algorithm through experiments in simulation and a human\ntrial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Universality of Deep COntextual Language Models. (arXiv:2109.07140v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07140","description":"<p>Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors\ndominate the landscape of Natural Language Processing due to their ability to\nscale across multiple tasks rapidly by pre-training a single model, followed by\ntask-specific fine-tuning. Furthermore, multilingual versions of such models\nlike XLM-R and mBERT have given promising results in zero-shot cross-lingual\ntransfer, potentially enabling NLP applications in many under-served and\nunder-resourced languages. Due to this initial success, pre-trained models are\nbeing used as `Universal Language Models' as the starting point across diverse\ntasks, domains, and languages. This work explores the notion of `Universality'\nby identifying seven dimensions across which a universal model should be able\nto scale, that is, perform equally well or reasonably well, to be useful across\ndiverse settings. We outline the current theoretical and empirical results that\nsupport model performance across these dimensions, along with extensions that\nmay help address some of their current limitations. Through this survey, we lay\nthe foundation for understanding the capabilities and limitations of massive\ncontextual language models and help discern research gaps and directions for\nfuture work to make these LMs inclusive and fair to diverse applications,\nusers, and linguistic phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Poonam Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation. (arXiv:2109.07141v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07141","description":"<p>Quality Estimation (QE) plays an essential role in applications of Machine\nTranslation (MT). Traditionally, a QE system accepts the original source text\nand translation from a black-box MT system as input. Recently, a few studies\nindicate that as a by-product of translation, QE benefits from the model and\ntraining data's information of the MT system where the translations come from,\nand it is called the \"glass-box QE\". In this paper, we extend the definition of\n\"glass-box QE\" generally to uncertainty quantification with both \"black-box\"\nand \"glass-box\" approaches and design several features deduced from them to\nblaze a new trial in improving QE's performance. We propose a framework to fuse\nthe feature engineering of uncertainty quantification into a pre-trained\ncross-lingual language model to predict the translation quality. Experiment\nresults show that our method achieves state-of-the-art performances on the\ndatasets of WMT 2020 QE shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangbin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaolin Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics of European poetry is shaped by conservative forces: The relationship between poetic meter and meaning in accentual-syllabic verse. (arXiv:2109.07148v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07148","description":"<p>Recent advances in cultural analytics and large-scale computational studies\nof art, literature and film often show that long-term change in the features of\nartistic works happens gradually. These findings suggest that conservative\nforces that shape creative domains might be underestimated. To this end, we\nprovide the first large-scale formal evidence of the persistent association\nbetween poetic meter and semantics in 18-19th European literatures, using\nCzech, German and Russian collections with additional data from English poetry\nand early modern Dutch songs. Our study traces this association through a\nseries of clustering experiments using the abstracted semantic features of\n150,000 poems. With the aid of topic modeling we infer semantic features for\nindividual poems. Texts were also lexically simplified across collections to\nincrease generalizability and decrease the sparseness of word frequency\ndistributions. Topics alone enable recognition of the meters in each observed\nlanguage, as may be seen from highly robust clustering of same-meter samples\n(median Adjusted Rand Index between 0.48 and 1). In addition, this study shows\nthat the strength of the association between form and meaning tends to decrease\nover time. This may reflect a shift in aesthetic conventions between the 18th\nand 19th centuries as individual innovation was increasingly favored in\nliterature. Despite this decline, it remains possible to recognize semantics of\nthe meters from past or future, which suggests the continuity of semantic\ntraditions while also revealing the historical variability of conditions across\nlanguages. This paper argues that distinct metrical forms, which are often\ncopied in a language over centuries, also maintain long-term semantic inertia\nin poetry. Our findings, thus, highlight the role of the formal features of\ncultural items in influencing the pace and shape of cultural evolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1\">Artjoms &#x160;e&#x13c;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plechac_P/0/1/0/all/0/1\">Petr Plech&#xe1;&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassche_A/0/1/0/all/0/1\">Alie Lassche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Residual and Normalization Layers into Analysis of Masked Language Models. (arXiv:2109.07152v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07152","description":"<p>Transformer architecture has become ubiquitous in the natural language\nprocessing field. To interpret the Transformer-based models, their attention\npatterns have been extensively analyzed. However, the Transformer architecture\nis not only composed of the multi-head attention; other components can also\ncontribute to Transformers' progressive performance. In this study, we extended\nthe scope of the analysis of Transformers from solely the attention patterns to\nthe whole attention block, i.e., multi-head attention, residual connection, and\nlayer normalization. Our analysis of Transformer-based masked language models\nshows that the token-to-token interaction performed via attention has less\nimpact on the intermediate representations than previously assumed. These\nresults provide new intuitive explanations of existing reports; for example,\ndiscarding the learned attention patterns tends not to adversely affect the\nperformance. The codes of our experiments are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_G/0/1/0/all/0/1\">Goro Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models be Biomedical Knowledge Bases?. (arXiv:2109.07154v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07154","description":"<p>Pre-trained language models (LMs) have become ubiquitous in solving various\nnatural language processing (NLP) tasks. There has been increasing interest in\nwhat knowledge these LMs contain and how we can extract that knowledge,\ntreating LMs as knowledge bases (KBs). While there has been much work on\nprobing LMs in the general domain, there has been little attention to whether\nthese powerful LMs can be used as domain-specific KBs. To this end, we create\nthe BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge\ntriples for probing biomedical LMs. We find that biomedical LMs with recently\nproposed probing methods can achieve up to 18.51% Acc@5 on retrieving\nbiomedical knowledge. Although this seems promising given the task difficulty,\nour detailed analyses reveal that most predictions are highly correlated with\nprompt templates without any subjects, hence producing similar results on each\nrelation and hindering their capabilities to be used as domain-specific KBs. We\nhope that BioLAMA can serve as a challenging benchmark for biomedical factual\nprobing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Mujeen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Sean Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Minji Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungdong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Match Job Candidates Using Multilingual Bi-Encoder BERT. (arXiv:2109.07157v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07157","description":"<p>In this talk, we will show how we used Randstad history of candidate\nplacements to generate labeled CV-vacancy pairs dataset. Afterwards we\nfine-tune a multilingual BERT with bi encoder structure over this dataset, by\nadding a cosine similarity log loss layer. We will explain how using the\nmentioned structure helps us overcome most of the challenges described above,\nand how it enables us to build a maintainable and scalable pipeline to match\nCVs and vacancies. In addition, we show how we gain a better semantic\nunderstanding, and learn to bridge the vocabulary gap. Finally, we highlight\nhow multilingual transformers help us handle cross language barrier and might\nreduce discrimination.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavi_D/0/1/0/all/0/1\">Dor Lavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Generative Factors in Natural Language with Discrete Variational Autoencoders. (arXiv:2109.07169v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07169","description":"<p>The ability of learning disentangled representations represents a major step\nfor interpretable NLP systems as it allows latent linguistic features to be\ncontrolled. Most approaches to disentanglement rely on continuous variables,\nboth for images and text. We argue that despite being suitable for image\ndatasets, continuous variables may not be ideal to model features of textual\ndata, due to the fact that most generative factors in text are discrete. We\npropose a Variational Autoencoder based method which models language features\nas discrete variables and encourages independence between variables for\nlearning disentangled representations. The proposed model outperforms\ncontinuous and discrete baselines on several qualitative and quantitative\nbenchmarks for disentanglement as well as on a text style transfer downstream\napplication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mercatali_G/0/1/0/all/0/1\">Giangiacomo Mercatali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup. (arXiv:2109.07177v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07177","description":"<p>Mixup is a recent regularizer for current deep classification networks.\nThrough training a neural network on convex combinations of pairs of examples\nand their labels, it imposes locally linear constraints on the model's input\nspace. However, such strict linear constraints often lead to under-fitting\nwhich degrades the effects of regularization. Noticeably, this issue is getting\nmore serious when the resource is extremely limited. To address these issues,\nwe propose the Adversarial Mixing Policy (AMP), organized in a min-max-rand\nformulation, to relax the Locally Linear Constraints in Mixup. Specifically,\nAMP adds a small adversarial perturbation to the mixing coefficients rather\nthan the examples. Thus, slight non-linearity is injected in-between the\nsynthetic examples and synthetic labels. By training on these data, the deep\nnetworks are further regularized, and thus achieve a lower predictive error\nrate. Experiments on five text classification benchmarks and five backbone\nmodels have empirically shown that our methods reduce the error rate over Mixup\nvariants in a significant margin (up to 31.3%), especially in low-resource\nconditions (up to 17.5%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuzhao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weiguo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Language Models for Factoid Question Answering at BioASQ9b. (arXiv:2109.07185v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07185","description":"<p>In this work, we describe our experiments and participating systems in the\nBioASQ Task 9b Phase B challenge of biomedical question answering. We have\nfocused on finding the ideal answers and investigated multi-task fine-tuning\nand gradual unfreezing techniques on transformer-based language models. For\nfactoid questions, our ALBERT-based systems ranked first in test batch 1 and\nfourth in test batch 2. Our DistilBERT systems outperformed the ALBERT variants\nin test batches 4 and 5 despite having 81% fewer parameters than ALBERT.\nHowever, we observed that gradual unfreezing had no significant impact on the\nmodel's accuracy compared to standard fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanna_U/0/1/0/all/0/1\">Urvashi Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molla_D/0/1/0/all/0/1\">Diego Moll&#xe1;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiagent Multimodal Categorization for Symbol Emergence: Emergent Communication via Interpersonal Cross-modal Inference. (arXiv:2109.07194v1 [cs.AI])","link":"http://arxiv.org/abs/2109.07194","description":"<p>This paper describes a computational model of multiagent multimodal\ncategorization that realizes emergent communication. We clarify whether the\ncomputational model can reproduce the following functions in a symbol emergence\nsystem, comprising two agents with different sensory modalities playing a\nnaming game. (1) Function for forming a shared lexical system that comprises\nperceptual categories and corresponding signs, formed by agents through\nindividual learning and semiotic communication between agents. (2) Function to\nimprove the categorization accuracy in an agent via semiotic communication with\nanother agent, even when some sensory modalities of each agent are missing. (3)\nFunction that an agent infers unobserved sensory information based on a sign\nsampled from another agent in the same manner as cross-modal inference. We\npropose an interpersonal multimodal Dirichlet mixture (Inter-MDM), which is\nderived by dividing an integrative probabilistic generative model, which is\nobtained by integrating two Dirichlet mixtures (DMs). The Markov chain Monte\nCarlo algorithm realizes emergent communication. The experimental results\ndemonstrated that Inter-MDM enables agents to form multimodal categories and\nappropriately share signs between agents. It is shown that emergent\ncommunication improves categorization accuracy, even when some sensory\nmodalities are missing. Inter-MDM enables an agent to predict unobserved\ninformation based on a shared sign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hagiwara_Y/0/1/0/all/0/1\">Yoshinobu Hagiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_K/0/1/0/all/0/1\">Kazuma Furukawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1\">Akira Taniguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis in Poems in Misurata Sub-dialect -- A Sentiment Detection in an Arabic Sub-dialect. (arXiv:2109.07203v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07203","description":"<p>Over the recent decades, there has been a significant increase and\ndevelopment of resources for Arabic natural language processing. This includes\nthe task of exploring Arabic Language Sentiment Analysis (ALSA) from Arabic\nutterances in both Modern Standard Arabic (MSA) and different Arabic dialects.\nThis study focuses on detecting sentiment in poems written in Misurata Arabic\nsub-dialect spoken in Misurata, Libya. The tools used to detect sentiment from\nthe dataset are Sklearn as well as Mazajak sentiment tool 1. Logistic\nRegression, Random Forest, Naive Bayes (NB), and Support Vector Machines (SVM)\nclassifiers are used with Sklearn, while the Convolutional Neural Network (CNN)\nis implemented with Mazajak. The results show that the traditional classifiers\nscore a higher level of accuracy as compared to Mazajak which is built on an\nalgorithm that includes deep learning techniques. More research is suggested to\nanalyze Arabic sub-dialect poetry in order to investigate the aspects that\ncontribute to sentiments in these multi-line texts; for example, the use of\nfigurative language such as metaphors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abugharsa_A/0/1/0/all/0/1\">Azza Abugharsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Relation-Oriented Clustering Method for Open Relation Extraction. (arXiv:2109.07205v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07205","description":"<p>The clustering-based unsupervised relation discovery method has gradually\nbecome one of the important methods of open relation extraction (OpenRE).\nHowever, high-dimensional vectors can encode complex linguistic information\nwhich leads to the problem that the derived clusters cannot explicitly align\nwith the relational semantic classes. In this work, we propose a\nrelation-oriented clustering model and use it to identify the novel relations\nin the unlabeled data. Specifically, to enable the model to learn to cluster\nrelational data, our method leverages the readily available labeled data of\npre-defined relations to learn a relation-oriented representation. We minimize\ndistance between the instance with same relation by gathering the instances\ntowards their corresponding relation centroids to form a cluster structure, so\nthat the learned representation is cluster-friendly. To reduce the clustering\nbias on predefined classes, we optimize the model by minimizing a joint\nobjective on both labeled and unlabeled data. Experimental results show that\nour method reduces the error rate by 29.2% and 15.7%, on two datasets\nrespectively, compared with current SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yaqian Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"{E}fficient{BERT}: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation. (arXiv:2109.07222v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07222","description":"<p>Pre-trained language models have shown remarkable results on various NLP\ntasks. Nevertheless, due to their bulky size and slow inference speed, it is\nhard to deploy them on edge devices. In this paper, we have a critical insight\nthat improving the feed-forward network (FFN) in BERT has a higher gain than\nimproving the multi-head attention (MHA) since the computational cost of FFN is\n2$\\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to\ndesigning efficient FFN as opposed to previous works that pay attention to MHA.\nSince FFN comprises a multilayer perceptron (MLP) that is essential in BERT\noptimization, we further design a thorough search space towards an advanced MLP\nand perform a coarse-to-fine mechanism to search for an efficient BERT\narchitecture. Moreover, to accelerate searching and enhance model\ntransferability, we employ a novel warm-up knowledge distillation strategy at\neach search stage. Extensive experiments show our searched EfficientBERT is\n6.9$\\times$ smaller and 4.4$\\times$ faster than BERT$\\rm_{BASE}$, and has\ncompetitive performances on GLUE and SQuAD Benchmarks. Concretely,\nEfficientBERT attains a 77.7 average score on GLUE \\emph{test}, 0.7 higher than\nMobileBERT$\\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0\n\\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.\nThe code is released at https://github.com/cheneydon/efficient-bert.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chenhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangrun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiefeng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Much do Lyrics Matter? Analysing Lyrical Simplicity Preferences for Individuals At Risk of Depression. (arXiv:2109.07227v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07227","description":"<p>Music affects and in some cases reflects one's emotional state. Key to this\ninfluence is lyrics and their meaning in conjunction with the acoustic\nproperties of the track. Recent work has focused on analysing these acoustic\nproperties and showing that individuals prone to depression primarily consume\nlow valence and low energy music. However, no studies yet have explored lyrical\ncontent preferences in relation to online music consumption of such\nindividuals. In the current study, we examine lyrical simplicity, measured as\nthe Compressibility and Absolute Information Content of the text, associated\nwith preferences of individuals at risk for depression. Using the six-month\nlistening history of 541 Last.fm users, we compare lyrical simplicity trends\nfor users grouped as being at risk (At-Risk) of depression from those that are\nnot (No-Risk). Our findings reveal that At-Risk individuals prefer songs with\ngreater information content (lower Compressibility) on average, especially for\nsongs characterised as Sad. Furthermore, we found that At-Risk individuals also\nhave greater variability of Absolute Information Content across their listening\nhistory. We discuss the results in light of existing socio-psychological\nlab-based research on music habits associated with depression and their\nrelevance to naturally occurring online music listening behaviour.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shriram_J/0/1/0/all/0/1\">Jaidev Shriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paruchuri_S/0/1/0/all/0/1\">Sreeharsha Paruchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alluri_V/0/1/0/all/0/1\">Vinoo Alluri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialog speech sentiment classification for imbalanced datasets. (arXiv:2109.07228v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07228","description":"<p>Speech is the most common way humans express their feelings, and sentiment\nanalysis is the use of tools such as natural language processing and\ncomputational algorithms to identify the polarity of these feelings. Even\nthough this field has seen tremendous advancements in the last two decades, the\ntask of effectively detecting under represented sentiments in different kinds\nof datasets is still a challenging task. In this paper, we use single and\nbi-modal analysis of short dialog utterances and gain insights on the main\nfactors that aid in sentiment detection, particularly in the underrepresented\nclasses, in datasets with and without inherent sentiment component.\nFurthermore, we propose an architecture which uses a learning rate scheduler\nand different monitoring criteria and provides state-of-the-art results for the\nSWITCHBOARD imbalanced sentiment dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nicolaou_S/0/1/0/all/0/1\">Sergis Nicolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavrides_L/0/1/0/all/0/1\">Lambros Mavrides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tryfou_G/0/1/0/all/0/1\">Georgina Tryfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_K/0/1/0/all/0/1\">Kyriakos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1\">Sergios Theodoridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Mathematical Properties of Integers. (arXiv:2109.07230v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07230","description":"<p>Embedding words in high-dimensional vector spaces has proven valuable in many\nnatural language applications. In this work, we investigate whether\nsimilarly-trained embeddings of integers can capture concepts that are useful\nfor mathematical applications. We probe the integer embeddings for mathematical\nknowledge, apply them to a set of numerical reasoning tasks, and show that by\nlearning the representations from mathematical sequence data, we can\nsubstantially improve over number embeddings learned from English text corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1\">Maria Ryskina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_K/0/1/0/all/0/1\">Kevin Knight</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWEAT: Scoring Polarization of Topics across Different Corpora. (arXiv:2109.07231v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07231","description":"<p>Understanding differences of viewpoints across corpora is a fundamental task\nfor computational social sciences. In this paper, we propose the Sliced Word\nEmbedding Association Test (SWEAT), a novel statistical measure to compute the\nrelative polarization of a topical wordset across two distributional\nrepresentations. To this end, SWEAT uses two additional wordsets, deemed to\nhave opposite valence, to represent two different poles. We validate our\napproach and illustrate a case study to show the usefulness of the introduced\nmeasure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marelli_M/0/1/0/all/0/1\">Marco Marelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicoli_P/0/1/0/all/0/1\">Paolo Nicoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palmonari_M/0/1/0/all/0/1\">Matteo Palmonari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal Text Classification. (arXiv:2109.07234v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07234","description":"<p>We aim to highlight an interesting trend to contribute to the ongoing debate\naround advances within legal Natural Language Processing. Recently, the focus\nfor most legal text classification tasks has shifted towards large pre-trained\ndeep learning models such as BERT. In this paper, we show that a more\ntraditional approach based on Support Vector Machine classifiers reaches\ncompetitive performance with deep learning models. We also highlight that error\nreduction obtained by using specialised BERT-based models over baselines is\nnoticeably smaller in the legal domain when compared to general language tasks.\nWe discuss some hypotheses for these results to support future discussions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonsus_M/0/1/0/all/0/1\">Marc Alphonsus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regressive Ensemble for Machine Translation Quality Evaluation. (arXiv:2109.07242v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07242","description":"<p>This work introduces a simple regressive ensemble for evaluating machine\ntranslation quality based on a set of novel and established metrics. We\nevaluate the ensemble using a correlation to expert-based MQM scores of the WMT\n2021 Metrics workshop. In both monolingual and zero-shot cross-lingual\nsettings, we show a significant performance improvement over single metrics. In\nthe cross-lingual settings, we also demonstrate that an ensemble approach is\nwell-applicable to unseen languages. Furthermore, we identify a strong\nreference-free baseline that consistently outperforms the commonly-used BLEU\nand METEOR measures and significantly improves our ensemble's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Clinical Information Extraction with Transferred Contextual Embeddings. (arXiv:2109.07243v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07243","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) model has\nachieved the state-of-the-art performance for many natural language processing\n(NLP) tasks. Yet, limited research has been contributed to studying its\neffectiveness when the target domain is shifted from the pre-training corpora,\nfor example, for biomedical or clinical NLP applications. In this paper, we\napplied it to a widely studied a hospital information extraction (IE) task and\nanalyzed its performance under the transfer learning setting. Our application\nbecame the new state-of-the-art result by a clear margin, compared with a range\nof existing IE models. Specifically, on this nursing handover data set, the\nmacro-average F1 score from our model was 0.438, whilst the previous best deep\nlearning models had 0.416. In conclusion, we showed that BERT based\npre-training models can be transferred to health-related documents under mild\nconditions and with a proper fine-tuning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zimin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suominen_H/0/1/0/all/0/1\">Hanna Suominen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs. (arXiv:2109.07263v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07263","description":"<p>We propose a novel problem within end-to-end learning of task-oriented\ndialogs (TOD), in which the dialog system mimics a troubleshooting agent who\nhelps a user by diagnosing their problem (e.g., car not starting). Such dialogs\nare grounded in domain-specific flowcharts, which the agent is supposed to\nfollow during the conversation. Our task exposes novel technical challenges for\nneural TOD, such as grounding an utterance to the flowchart without explicit\nannotation, referring to additional manual pages when user asks a clarification\nquestion, and ability to follow unseen flowcharts at test time. We release a\ndataset (FloDial) consisting of 2,738 dialogs grounded on 12 different\ntroubleshooting flowcharts. We also design a neural model, FloNet, which uses a\nretrieval-augmented generation architecture to train the dialog agent. Our\nexperiments find that FloNet can do zero-shot transfer to unseen flowcharts,\nand sets a strong baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1\">Dinesh Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shantanu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scope resolution of predicted negation cues: A two-step neural network-based approach. (arXiv:2109.07264v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07264","description":"<p>Neural network-based methods are the state of the art in negation scope\nresolution. However, they often use the unrealistic assumption that cue\ninformation is completely accurate. Even if this assumption holds, there\nremains a dependency on engineered features from state-of-the-art machine\nlearning methods. The current study adopted a two-step negation resolving\napporach to assess whether a Bidirectional Long Short-Term Memory-based method\ncan be used for cue detection as well, and how inaccurate cue predictions would\naffect the scope resolution performance. Results suggest that this method is\nnot suitable for negation detection. Scope resolution performance is most\nrobust against inaccurate information for models with a recurrent layer only,\ncompared to extensions with a Conditional Random Fields layer or a\npost-processing algorithm. We advocate for more research into the application\nof deep learning on negation detection and the effect of imperfect information\non scope resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_D/0/1/0/all/0/1\">Daan de Jong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Length is a Domain: Length-based Overfitting in Transformer Models. (arXiv:2109.07276v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07276","description":"<p>Transformer-based sequence-to-sequence architectures, while achieving\nstate-of-the-art results on a large number of NLP tasks, can still suffer from\noverfitting during training. In practice, this is usually countered either by\napplying regularization methods (e.g. dropout, L2-regularization) or by\nproviding huge amounts of training data. Additionally, Transformer and other\narchitectures are known to struggle when generating very long sequences. For\nexample, in machine translation, the neural-based systems perform worse on very\nlong sequences when compared to the preceding phrase-based translation\napproaches (Koehn and Knowles, 2017).\n</p>\n<p>We present results which suggest that the issue might also be in the mismatch\nbetween the length distributions of the training and validation data combined\nwith the aforementioned tendency of the neural networks to overfit to the\ntraining data. We demonstrate on a simple string editing task and a machine\ntranslation task that the Transformer model performance drops significantly\nwhen facing sequences of length diverging from the length distribution in the\ntraining data. Additionally, we show that the observed drop in performance is\ndue to the hypothesis length corresponding to the lengths seen by the model\nduring training rather than the length of the input sequence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varis_D/0/1/0/all/0/1\">Du&#x161;an Vari&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context. (arXiv:2109.07293v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07293","description":"<p>Embedding based methods are widely used for unsupervised keyphrase extraction\n(UKE) tasks. Generally, these methods simply calculate similarities between\nphrase embeddings and document embedding, which is insufficient to capture\ndifferent context for a more effective UKE model. In this paper, we propose a\nnovel method for UKE, where local and global contexts are jointly modeled. From\na global view, we calculate the similarity between a certain phrase and the\nwhole document in the vector space as transitional embedding based models do.\nIn terms of the local view, we first build a graph structure based on the\ndocument where phrases are regarded as vertices and the edges are similarities\nbetween vertices. Then, we proposed a new centrality computation method to\ncapture local salient information based on the graph structure. Finally, we\nfurther combine the modeling of global and local context for ranking. We\nevaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010)\nand compare with existing state-of-the-art models. The results show that our\nmodel outperforms most models while generalizing better on input documents with\ndifferent domains and length. Additional ablation study shows that both the\nlocal and global information is crucial for unsupervised keyphrase extraction\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinnian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Vision-Language Models `See' when they See Scenes. (arXiv:2109.07301v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07301","description":"<p>Images can be described in terms of the objects they contain, or in terms of\nthe types of scene or place that they instantiate. In this paper we address to\nwhat extent pretrained Vision and Language models can learn to align\ndescriptions of both types with images. We compare 3 state-of-the-art models,\nVisualBERT, LXMERT and CLIP. We find that (i) V&amp;L models are susceptible to\nstylistic biases acquired during pretraining; (ii) only CLIP performs\nconsistently well on both object- and scene-level descriptions. A follow-up\nablation study shows that CLIP uses object-level information in the visual\nmodality to align with scene-level textual descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cafagna_M/0/1/0/all/0/1\">Michele Cafagna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deemter_K/0/1/0/all/0/1\">Kees van Deemter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training. (arXiv:2109.07306v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07306","description":"<p>Compared to monolingual models, cross-lingual models usually require a more\nexpressive vocabulary to represent all languages adequately. We find that many\nlanguages are under-represented in recent cross-lingual language models due to\nthe limited vocabulary capacity. To this end, we propose an algorithm VoCap to\ndetermine the desired vocabulary capacity of each language. However, increasing\nthe vocabulary size significantly slows down the pre-training speed. In order\nto address the issues, we propose k-NN-based target sampling to accelerate the\nexpensive softmax. Our experiments show that the multilingual vocabulary\nlearned with VoCap benefits cross-lingual language model pre-training.\nMoreover, k-NN-based target sampling mitigates the side-effects of increasing\nthe vocabulary size while achieving comparable performance and faster\npre-training speed. The code and the pretrained multilingual vocabularies are\navailable at https://github.com/bozheng-hit/VoCapXLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedding Convolutions for Short Text Extreme Classification with Millions of Labels. (arXiv:2109.07319v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07319","description":"<p>Automatic annotation of short-text data to a large number of target labels,\nreferred to as Short Text Extreme Classification, has recently found numerous\napplications in prediction of related searches and product recommendation\ntasks. The conventional usage of Convolutional Neural Network (CNN) to capture\nn-grams in text-classification relies heavily on uniformity in word-ordering\nand the presence of long input sequences to convolve over. However, this is\nmissing in short and unstructured text sequences encountered in search and\nrecommendation. In order to tackle this, we propose an orthogonal approach by\nrecasting the convolution operation to capture coupled semantics along the\nembedding dimensions, and develop a word-order agnostic embedding enhancement\nmodule to deal with the lack of structure in such queries. Benefitting from the\ncomputational efficiency of the convolution operation, Embedding Convolutions,\nwhen applied on the enriched word embeddings, result in a light-weight and yet\npowerful encoder (InceptionXML) that is robust to the inherent lack of\nstructure in short-text extreme classification.\n</p>\n<p>Towards scaling our model to problems with millions of labels, we also\npropose InceptionXML+, which addresses the shortcomings of the dynamic\nhard-negative mining framework in the recently proposed LightXML by improving\nthe alignment between the label-shortlister and extreme classifier. On popular\nbenchmark datasets, we empirically demonstrate that the proposed method\noutperforms state-of-the-art deep extreme classifiers such as Astec by an\naverage of 5% and 8% on the P@k and propensity-scored PSP@k metrics\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharbanda_S/0/1/0/all/0/1\">Siddhant Kharbanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Atmadeep Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palrecha_A/0/1/0/all/0/1\">Akash Palrecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babbar_R/0/1/0/all/0/1\">Rohit Babbar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mi{\\dh}eind's WMT 2021 submission. (arXiv:2109.07343v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07343","description":"<p>We present Mi{\\dh}eind's submission for the English$\\to$Icelandic and\nIcelandic$\\to$English subsets of the 2021 WMT news translation task.\nTransformer-base models are trained for translation on parallel data to\ngenerate backtranslations iteratively. A pretrained mBART-25 model is then\nadapted for translation using parallel data as well as the last backtranslation\niteration. This adapted pretrained model is then used to re-generate\nbacktranslations, and the training of the adapted model is continued.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simonarson_H/0/1/0/all/0/1\">Haukur Barri S&#xed;monarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1\">V&#xe9;steinn Sn&#xe6;bjarnarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragnarsson_P/0/1/0/all/0/1\">P&#xe9;tur Orri Ragnarsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_H/0/1/0/all/0/1\">Haukur P&#xe1;ll J&#xf3;nsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7B%5CTH%7Dorsteinsson_V/0/1/0/all/0/1\">Vilhj&#xe1;lmur &#xde;orsteinsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing an Abusive Language Classification Framework for Telegram to Investigate the German Hater Community. (arXiv:2109.07346v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07346","description":"<p>Since traditional social media platforms ban more and more actors that\ndistribute hate speech or other forms of abusive language (deplatforming),\nthese actors migrate to alternative platforms that do not moderate the users'\ncontent. One known platform that is relevant for the German hater community is\nTelegram, for which there have only been made limited research efforts so far.\n</p>\n<p>The goal of this study is to develop a broad framework that consists of (i)\nan abusive language classification model for German Telegram messages and (ii)\na classification model for the hatefulness of Telegram channels. For the first\npart, we employ existing abusive language datasets containing posts from other\nplatforms to build our classification models. For the channel classification\nmodel, we develop a method that combines channel specific content information\ncoming from a topic model with a social graph to predict the hatefulness of\nchannels. Furthermore, we complement these two approaches for hate speech\ndetection with insightful results on the evolution of the hater community on\nTelegram in Germany. Moreover, we propose methods to the hate speech research\ncommunity for scalable network analyses for social media platforms. As an\nadditional output of the study, we release an annotated abusive language\ndataset containing 1,149 annotated Telegram messages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wich_M/0/1/0/all/0/1\">Maximilian Wich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorniak_A/0/1/0/all/0/1\">Adrian Gorniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eder_T/0/1/0/all/0/1\">Tobias Eder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartmann_D/0/1/0/all/0/1\">Daniel Bartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakici_B/0/1/0/all/0/1\">Burak Enes &#xc7;akici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer of Monolingual Models. (arXiv:2109.07348v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07348","description":"<p>Recent studies in zero-shot cross-lingual learning using multilingual models\nhave falsified the previous hypothesis that shared vocabulary and joint\npre-training are the keys to cross-lingual generalization. Inspired by this\nadvancement, we introduce a cross-lingual transfer method for monolingual\nmodels based on domain adaptation. We study the effects of such transfer from\nfour different languages to English. Our experimental results on GLUE show that\nthe transferred models outperform the native English model independently of the\nsource language. After probing the English linguistic knowledge encoded in the\nrepresentations before and after transfer, we find that semantic information is\nretained from the source language, while syntactic information is learned\nduring transfer. Additionally, the results of evaluating the transferred models\nin source language tasks reveal that their performance in the source domain\ndeteriorates after transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gogoulou_E/0/1/0/all/0/1\">Evangelia Gogoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekgren_A/0/1/0/all/0/1\">Ariel Ekgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isbister_T/0/1/0/all/0/1\">Tim Isbister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahlgren_M/0/1/0/all/0/1\">Magnus Sahlgren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ELITR ECA Corpus. (arXiv:2109.07351v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07351","description":"<p>We present the ELITR ECA corpus, a multilingual corpus derived from\npublications of the European Court of Auditors. We use automatic translation\ntogether with Bleualign to identify parallel sentence pairs in all 506\ntranslation directions. The result is a corpus comprising 264k document pairs\nand 41.9M sentence pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Williams_P/0/1/0/all/0/1\">Philip Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU. (arXiv:2109.07364v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07364","description":"<p>Incremental processing allows interactive systems to respond based on partial\ninputs, which is a desirable property e.g. in dialogue agents. The currently\npopular Transformer architecture inherently processes sequences as a whole,\nabstracting away the notion of time. Recent work attempts to apply Transformers\nincrementally via restart-incrementality by repeatedly feeding, to an unchanged\nmodel, increasingly longer input prefixes to produce partial outputs. However,\nthis approach is computationally costly and does not scale efficiently for long\nsequences. In parallel, we witness efforts to make Transformers more efficient,\ne.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we\nexamine the feasibility of LT for incremental NLU in English. Our results show\nthat the recurrent LT model has better incremental performance and faster\ninference speed compared to the standard Transformer and LT with\nrestart-incrementality, at the cost of part of the non-incremental (full\nsequence) quality. We show that the performance drop can be mitigated by\ntraining the model to wait for right context before committing to an output and\nthat training with input prefixes is beneficial for delivering correct partial\noutputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahardipraja_P/0/1/0/all/0/1\">Patrick Kahardipraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madureira_B/0/1/0/all/0/1\">Brielen Madureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniST: Unified End-to-end Model for Streaming and Non-streaming Speech Translation. (arXiv:2109.07368v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07368","description":"<p>This paper presents a unified end-to-end frame-work for both streaming and\nnon-streamingspeech translation. While the training recipes for non-streaming\nspeech translation have been mature, the recipes for streaming\nspeechtranslation are yet to be built. In this work, wefocus on developing a\nunified model (UniST) which supports streaming and non-streaming ST from the\nperspective of fundamental components, including training objective, attention\nmechanism and decoding policy. Experiments on the most popular speech-to-text\ntranslation benchmark dataset, MuST-C, show that UniST achieves significant\nimprovement for non-streaming ST, and a better-learned trade-off for BLEU score\nand latency metrics for streaming ST, compared with end-to-end baselines and\nthe cascaded models. We will make our codes and evaluation tools publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Transferable Table Question Answering. (arXiv:2109.07377v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07377","description":"<p>Weakly-supervised table question-answering(TableQA) models have achieved\nstate-of-art performance by using pre-trained BERT transformer to jointly\nencoding a question and a table to produce structured query for the question.\nHowever, in practical settings TableQA systems are deployed over table corpora\nhaving topic and word distributions quite distinct from BERT's pretraining\ncorpus. In this work we simulate the practical topic shift scenario by\ndesigning novel challenge benchmarks WikiSQL-TS and WikiTQ-TS, consisting of\ntrain-dev-test splits in five distinct topic groups, based on the popular\nWikiSQL and WikiTableQuestions datasets. We empirically show that, despite\npre-training on large open-domain text, performance of models degrades\nsignificantly when they are evaluated on unseen topics. In response, we propose\nT3QA (Topic Transferable Table Question Answering) a pragmatic adaptation\nframework for TableQA comprising of: (1) topic-specific vocabulary injection\ninto BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2)\nbased natural language question generation pipeline focused on generating topic\nspecific training data, and (3) a logical form reranker. We show that T3QA\nprovides a reasonably good baseline for our topic shift benchmarks. We believe\nour topic split benchmarks will lead to robust TableQA solutions that are\nbetter suited for practical deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Ahmed Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_J/0/1/0/all/0/1\">Jaydeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canim_M/0/1/0/all/0/1\">Mustafa Canim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1\">Karthik Sankaranarayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. (arXiv:2109.07383v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07383","description":"<p>This paper addresses the efficiency challenge of Neural Architecture Search\n(NAS) by formulating the task as a ranking problem. Previous methods require\nnumerous training examples to estimate the accurate performance of\narchitectures, although the actual goal is to find the distinction between\n\"good\" and \"bad\" candidates. Here we do not resort to performance predictors.\nInstead, we propose a performance ranking method (RankNAS) via pairwise\nranking. It enables efficient architecture search using much fewer training\nexamples. Moreover, we develop an architecture selection method to prune the\nsearch space and concentrate on more promising candidates. Extensive\nexperiments on machine translation and language modeling tasks show that\nRankNAS can design high-performance architectures while being orders of\nmagnitude faster than state-of-the-art NAS systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiangnan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xia Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changliang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs. (arXiv:2109.07396v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07396","description":"<p>End-to-End task-oriented dialogue systems generate responses based on dialog\nhistory and an accompanying knowledge base (KB). Inferring those KB entities\nthat are most relevant for an utterance is crucial for response generation.\nExisting state of the art scales to large KBs by softly filtering over\nirrelevant KB information. In this paper, we propose a novel filtering\ntechnique that consists of (1) a pairwise similarity based filter that\nidentifies relevant information by respecting the n-ary structure in a KB\nrecord. and, (2) an auxiliary loss that helps in separating contextually\nunrelated KB information. We also propose a new metric -- multiset entity F1\nwhich fixes a correctness issue in the existing entity F1 metric. Experimental\nresults on three publicly available task-oriented dialog datasets show that our\nproposed approach outperforms existing state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1\">Dinesh Raghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Atishya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching with Transformers in MELT. (arXiv:2109.07401v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07401","description":"<p>One of the strongest signals for automated matching of ontologies and\nknowledge graphs are the textual descriptions of the concepts. The methods that\nare typically applied (such as character- or token-based comparisons) are\nrelatively simple, and therefore do not capture the actual meaning of the\ntexts. With the rise of transformer-based language models, text comparison\nbased on meaning (rather than lexical features) is possible. In this paper, we\nmodel the ontology matching task as classification problem and present\napproaches based on transformer models. We further provide an easy to use\nimplementation in the MELT framework which is suited for ontology and knowledge\ngraph matching. We show that a transformer-based filter helps to choose the\ncorrect correspondences given a high-recall alignment and already achieves a\ngood result with simple alignment post-processing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hertling_S/0/1/0/all/0/1\">Sven Hertling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1\">Jan Portisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT is Robust! A Case Against Synonym-Based Adversarial Examples in Text Classification. (arXiv:2109.07403v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07403","description":"<p>Deep Neural Networks have taken Natural Language Processing by storm. While\nthis led to incredible improvements across many tasks, it also initiated a new\nresearch field, questioning the robustness of these neural networks by\nattacking them. In this paper, we investigate four word substitution-based\nattacks on BERT. We combine a human evaluation of individual word substitutions\nand a probabilistic analysis to show that between 96% and 99% of the analyzed\nattacks do not preserve semantics, indicating that their success is mainly\nbased on feeding poor data to the model. To further confirm that, we introduce\nan efficient data augmentation procedure and show that many adversarial\nexamples can be prevented by including data similar to the attacks during\ntraining. An additional post-processing step reduces the success rates of\nstate-of-the-art attacks below 5%. Finally, by looking at more reasonable\nthresholds on constraints for word substitutions, we conclude that BERT is a\nlot more robust than research on attacks suggests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauser_J/0/1/0/all/0/1\">Jens Hauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascual_D/0/1/0/all/0/1\">Dami&#xe1;n Pascual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document. (arXiv:2109.07410v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07410","description":"<p>Given the recent proliferation of false claims online, there has been a lot\nof manual fact-checking effort. As this is very time-consuming, human\nfact-checkers can benefit from tools that can support them and make them more\nefficient. Here, we focus on building a system that could provide such support.\nGiven an input document, it aims to detect all sentences that contain a claim\nthat can be verified by some previously fact-checked claims (from a given\ndatabase). The output is a re-ranked list of the document sentences, so that\nthose that can be verified are ranked as high as possible, together with\ncorresponding evidence. Unlike previous work, which has looked into claim\nretrieval, here we take a document-level perspective. We create a new manually\nannotated dataset for the task, and we propose suitable evaluation measures. We\nfurther experiment with a learning-to-rank approach, achieving sizable\nperformance gains over several strong baselines. Our analysis demonstrates the\nimportance of modeling text similarity and stance, while also taking into\naccount the veracity of the retrieved previously fact-checked claims. We\nbelieve that this research would be of interest to fact-checkers, journalists,\nmedia, and regulatory authorities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations. (arXiv:2109.07424v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07424","description":"<p>While contrastive learning is proven to be an effective training strategy in\ncomputer vision, Natural Language Processing (NLP) is only recently adopting it\nas a self-supervised alternative to Masked Language Modeling (MLM) for\nimproving sequence representations. This paper introduces SupCL-Seq, which\nextends the supervised contrastive learning from computer vision to the\noptimization of sequence representations in NLP. By altering the dropout mask\nprobability in standard Transformer architectures, for every representation\n(anchor), we generate augmented altered views. A supervised contrastive loss is\nthen utilized to maximize the system's capability of pulling together similar\nsamples (e.g., anchors and their altered views) and pushing apart the samples\nbelonging to the other classes. Despite its simplicity, SupCLSeq leads to large\ngains in many sequence classification tasks on the GLUE benchmark compared to a\nstandard BERTbase, including 6% absolute improvement on CoLA, 5.4% on MRPC,\n4.7% on RTE and 2.6% on STSB. We also show consistent gains over self\nsupervised contrastively learned representations, especially in non-semantic\ntasks. Finally we show that these gains are not solely due to augmentation, but\nrather to a downstream optimized sequence representation. Code:\nhttps://github.com/hooman650/SupCL-Seq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sedghamiz_H/0/1/0/all/0/1\">Hooman Sedghamiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raval_S/0/1/0/all/0/1\">Shivam Raval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhanai_T/0/1/0/all/0/1\">Tuka Alhanai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1\">Mohammad Ghassemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative and Generative Transformer-based Models For Situation Entity Classification. (arXiv:2109.07434v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07434","description":"<p>We re-examine the situation entity (SE) classification task with varying\namounts of available training data. We exploit a Transformer-based variational\nautoencoder to encode sentences into a lower dimensional latent space, which is\nused to generate the text and learn a SE classifier. Test set and cross-genre\nevaluations show that when training data is plentiful, the proposed model can\nimprove over the previous discriminative state-of-the-art models. Our approach\nperforms disproportionately better with smaller amounts of training data, but\nwhen faced with extremely small sets (4 instances per label), generative RNN\nmethods outperform transformers. Our work provides guidance for future efforts\non SE and semantic prediction tasks, and low-label training regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaee_M/0/1/0/all/0/1\">Mehdi Rezaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darvish_K/0/1/0/all/0/1\">Kasra Darvish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kebe_G/0/1/0/all/0/1\">Gaoussou Youssouf Kebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative. (arXiv:2109.07437v1 [cs.LG])","link":"http://arxiv.org/abs/2109.07437","description":"<p>Pre-training, where models are trained on an auxiliary objective with\nabundant data before being fine-tuned on data from the downstream task, is now\nthe dominant paradigm in NLP. In general, the pre-training step relies on\nlittle to no direct knowledge of the task on which the model will be\nfine-tuned, even when the end-task is known in advance. Our work challenges\nthis status-quo of end-task agnostic pre-training. First, on three different\nlow-resource NLP tasks from two domains, we demonstrate that multi-tasking the\nend-task and auxiliary objectives results in significantly better downstream\ntask performance than the widely-used task-agnostic continued pre-training\nparadigm of Gururangan et al. (2020). We next introduce an online meta-learning\nalgorithm that learns a set of multi-task weights to better balance among our\nmultiple auxiliary objectives, achieving further improvements on end task\nperformance and data efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dery_L/0/1/0/all/0/1\">Lucio M. Dery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is \"moby dick\" a Whale or a Bird? Named Entities and Terminology in Speech Translation. (arXiv:2109.07439v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07439","description":"<p>Automatic translation systems are known to struggle with rare words. Among\nthese, named entities (NEs) and domain-specific terms are crucial, since errors\nin their translation can lead to severe meaning distortions. Despite their\nimportance, previous speech translation (ST) studies have neglected them, also\ndue to the dearth of publicly available resources tailored to their specific\nevaluation. To fill this gap, we i) present the first systematic analysis of\nthe behavior of state-of-the-art ST systems in translating NEs and terminology,\nand ii) release NEuRoparl-ST, a novel benchmark built from European Parliament\nspeeches annotated with NEs and terminology. Our experiments on the three\nlanguage directions covered by our benchmark (en-&gt;es/fr/it) show that ST\nsystems correctly translate 75-80% of terms and 65-70% of NEs, with very low\nperformance (37-40%) on person names.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_S/0/1/0/all/0/1\">Susana Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1\">Luisa Bentivogli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges in Detoxifying Language Models. (arXiv:2109.07445v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07445","description":"<p>Large language models (LM) generate remarkably fluent text and can be\nefficiently adapted across NLP tasks. Measuring and guaranteeing the quality of\ngenerated text in terms of safety is imperative for deploying LMs in the real\nworld; to this end, prior work often relies on automatic evaluation of LM\ntoxicity. We critically discuss this approach, evaluate several toxicity\nmitigation strategies with respect to both automatic and human evaluation, and\nanalyze consequences of toxicity mitigation in terms of model bias and LM\nquality. We demonstrate that while basic intervention strategies can\neffectively optimize previously established automatic metrics on the\nRealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for\nboth texts about, and dialects of, marginalized groups. Additionally, we find\nthat human raters often disagree with high automatic toxicity scores after\nstrong toxicity reduction interventions -- highlighting further the nuances\ninvolved in careful evaluation of LM toxicity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welbl_J/0/1/0/all/0/1\">Johannes Welbl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1\">Amelia Glaese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uesato_J/0/1/0/all/0/1\">Jonathan Uesato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dathathri_S/0/1/0/all/0/1\">Sumanth Dathathri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1\">John Mellor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_K/0/1/0/all/0/1\">Kirsty Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohli_P/0/1/0/all/0/1\">Pushmeet Kohli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coppin_B/0/1/0/all/0/1\">Ben Coppin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Sen Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Does Translation Require Context? A Data-driven, Multilingual Exploration. (arXiv:2109.07446v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07446","description":"<p>Although proper handling of discourse phenomena significantly contributes to\nthe quality of machine translation (MT), common translation quality metrics do\nnot adequately capture them. Recent works in context-aware MT attempt to target\na small set of these phenomena during evaluation. In this paper, we propose a\nnew metric, P-CXMI, which allows us to identify translations that require\ncontext systematically and confirm the difficulty of previously studied\nphenomena as well as uncover new ones that have not been addressed in previous\nwork. We then develop the Multilingual Discourse-Aware (MuDA) benchmark, a\nseries of taggers for these phenomena in 14 different language pairs, which we\nuse to evaluate context-aware MT. We find that state-of-the-art context-aware\nMT models find marginal improvements over context-agnostic models on our\nbenchmark, which suggests current models do not handle these ambiguities\neffectively. We release code and data to invite the MT research community to\nincrease efforts on context-aware translation on discourse phenomena and\nlanguages that are currently overlooked.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WikiGUM: Exhaustive Entity Linking for Wikification in 12 Genres. (arXiv:2109.07449v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07449","description":"<p>Previous work on Entity Linking has focused on resources targeting non-nested\nproper named entity mentions, often in data from Wikipedia, i.e. Wikification.\nIn this paper, we present and evaluate WikiGUM, a fully wikified dataset,\ncovering all mentions of named entities, including their non-named and\npronominal mentions, as well as mentions nested within other mentions. The\ndataset covers a broad range of 12 written and spoken genres, most of which\nhave not been included in Entity Linking efforts to date, leading to poor\nperformance by a pretrained SOTA system in our evaluation. The availability of\na variety of other annotations for the same data also enables further research\non entities in context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jessica Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Machines Read Coding Manuals Yet? -- A Benchmark for Building Better Language Models for Code Understanding. (arXiv:2109.07452v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07452","description":"<p>Code understanding is an increasingly important application of Artificial\nIntelligence. A fundamental aspect of understanding code is understanding text\nabout code, e.g., documentation and forum discussions. Pre-trained language\nmodels (e.g., BERT) are a popular approach for various NLP tasks, and there are\nnow a variety of benchmarks, such as GLUE, to help improve the development of\nsuch models for natural language understanding. However, little is known about\nhow well such models work on textual artifacts about code, and we are unaware\nof any systematic set of downstream tasks for such an evaluation. In this\npaper, we derive a set of benchmarks (BLANCA - Benchmarks for LANguage models\non Coding Artifacts) that assess code understanding based on tasks such as\npredicting the best answer to a question in a forum post, finding related forum\nposts, or predicting classes related in a hierarchy from class documentation.\nWe evaluate the performance of current state-of-the-art language models on\nthese tasks and show that there is a significant improvement on each task from\nfine tuning. We also show that multi-task training over BLANCA tasks helps\nbuild better language models for code understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolby_J/0/1/0/all/0/1\">Julian Dolby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCusker_J/0/1/0/all/0/1\">Jamie McCusker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivas_K/0/1/0/all/0/1\">Kavitha Srinivas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Text Representations: A Theory-Driven Approach. (arXiv:2109.07458v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07458","description":"<p>Much of the progress in contemporary NLP has come from learning\nrepresentations, such as masked language model (MLM) contextual embeddings,\nthat turn challenging problems into simple classification tasks. But how do we\nquantify and explain this effect? We adapt general tools from computational\nlearning theory to fit the specific characteristics of text datasets and\npresent a method to evaluate the compatibility between representations and\ntasks. Even though many tasks can be easily solved with simple bag-of-words\n(BOW) representations, BOW does poorly on hard natural language inference\ntasks. For one such task we find that BOW cannot distinguish between real and\nrandomized labelings, while pre-trained MLM representations show 72x greater\ndistinction between real and random labelings than BOW. This method provides a\ncalibrated, quantitative measure of the difficulty of a classification-based\nNLP task, enabling comparisons between representations without requiring\nempirical evaluations that may be sensitive to initializations and\nhyperparameters. The method provides a fresh perspective on the patterns in a\ndataset and the alignment of those patterns with specific labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yauney_G/0/1/0/all/0/1\">Gregory Yauney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Domain Adaptation of Language Models via Adaptive Tokenization. (arXiv:2109.07460v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07460","description":"<p>Contextual embedding-based language models trained on large data sets, such\nas BERT and RoBERTa, provide strong performance across a wide range of tasks\nand are ubiquitous in modern NLP. It has been observed that fine-tuning these\nmodels on tasks involving data from domains different from that on which they\nwere pretrained can lead to suboptimal performance. Recent work has explored\napproaches to adapt pretrained language models to new domains by incorporating\nadditional pretraining using domain-specific corpora and task data. We propose\nan alternative approach for transferring pretrained language models to new\ndomains by adapting their tokenizers. We show that domain-specific subword\nsequences can be efficiently determined directly from divergences in the\nconditional token distributions of the base and domain-specific corpora. In\ndatasets from four disparate domains, we find adaptive tokenization on a\npretrained RoBERTa model provides &gt;97% of the performance benefits of domain\nspecific pretraining. Our approach produces smaller models and less training\nand inference time than other approaches using tokenizer augmentation. While\nadaptive tokenization incurs a 6% increase in model parameters in our\nexperimentation, due to the introduction of 10k new domain-specific tokens, our\napproach, using 64 vCPUs, is 72x faster than further pretraining the language\nmodel on domain-specific corpora on 8 TPUs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachidananda_V/0/1/0/all/0/1\">Vin Sachidananda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessler_J/0/1/0/all/0/1\">Jason S. Kessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-an Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnnIE: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark. (arXiv:2109.07464v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07464","description":"<p>Open Information Extraction (OIE) is the task of extracting facts from\nsentences in the form of relations and their corresponding arguments in\nschema-free manner. Intrinsic performance of OIE systems is difficult to\nmeasure due to the incompleteness of existing OIE benchmarks: the ground truth\nextractions do not group all acceptable surface realizations of the same fact\nthat can be extracted from a sentence. To measure performance of OIE systems\nmore realistically, it is necessary to manually annotate complete facts (i.e.,\nclusters of all acceptable surface realizations of the same fact) from input\nsentences. We propose AnnIE: an interactive annotation platform that\nfacilitates such challenging annotation tasks and supports creation of complete\nfact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order\nto support different use case scenarios (i.e., benchmarks covering different\ntypes of facts). We use AnnIE to build two complete OIE benchmarks: one with\nverb-mediated facts and another with facts encompassing named entities.\nFinally, we evaluate several OIE systems on our complete benchmarks created\nwith AnnIE. Our results suggest that existing incomplete benchmarks are overly\nlenient, and that OIE systems are not as robust as previously reported. We\npublicly release AnnIE under non-restrictive license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_N/0/1/0/all/0/1\">Niklas Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingying Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Limits of Minimal Pairs in Contrastive Evaluation. (arXiv:2109.07465v1 [cs.CL])","link":"http://arxiv.org/abs/2109.07465","description":"<p>Minimal sentence pairs are frequently used to analyze the behavior of\nlanguage models. It is often assumed that model behavior on contrastive pairs\nis predictive of model behavior at large. We argue that two conditions are\nnecessary for this assumption to hold: First, a tested hypothesis should be\nwell-motivated, since experiments show that contrastive evaluation can lead to\nfalse positives. Secondly, test data should be chosen such as to minimize\ndistributional discrepancy between evaluation time and deployment time. For a\ngood approximation of deployment-time decoding, we recommend that minimal pairs\nare created based on machine-generated text, as opposed to human-written\nreferences. We present a contrastive evaluation suite for English-German MT\nthat implements this recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1\">Jannis Vamvas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Statements Considered Useful. (arXiv:2001.04425v5 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2001.04425","description":"<p>Knowledge bases (KBs) about notable entities and their properties are an\nimportant asset in applications such as search, question answering and\ndialogue. All popular KBs capture virtually only positive statements, and\nabstain from taking any stance on statements not stored in the KB. This paper\nmakes the case for explicitly stating salient statements that do not hold.\nNegative statements are useful to overcome limitations of question answering\nsystems that are mainly geared for positive questions; they can also contribute\nto informative summaries of entities. Due to the abundance of such invalid\nstatements, any effort to compile them needs to address ranking by saliency. We\npresent a statisticalinference method for compiling and ranking negative\nstatements, based on expectations from positive statements of related entities\nin peer groups. Experimental results, with a variety of datasets, show that the\nmethod can effectively discover notable negative statements, and extrinsic\nstudies underline their usefulness for entity summarization. Datasets and code\nare released as resources for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1\">Hiba Arnaout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Neural Program Synthesis from Multimodal Specifications. (arXiv:2010.01678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01678","description":"<p>Multimodal program synthesis, which leverages different types of user input\nto synthesize a desired program, is an attractive way to scale program\nsynthesis to challenging settings; however, it requires integrating noisy\nsignals from the user, like natural language, with hard constraints on the\nprogram's behavior. This paper proposes an optimal neural synthesis approach\nwhere the goal is to find a program that satisfies user-provided constraints\nwhile also maximizing the program's score with respect to a neural model.\nSpecifically, we focus on multimodal synthesis tasks in which the user intent\nis expressed using a combination of natural language (NL) and input-output\nexamples. At the core of our method is a top-down recurrent neural model that\nplaces distributions over abstract syntax trees conditioned on the NL input.\nThis model not only allows for efficient search over the space of syntactically\nvalid programs, but it allows us to leverage automated program analysis\ntechniques for pruning the search space based on infeasibility of partial\nprograms with respect to the user's constraints. The experimental results on a\nmultimodal synthesis dataset (StructuredRegex) show that our method\nsubstantially outperforms prior state-of-the-art techniques in terms of\naccuracy and efficiency, and finds model-optimal programs more frequently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiaochu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillig_I/0/1/0/all/0/1\">Isil Dillig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation. (arXiv:2012.04882v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04882","description":"<p>The success of emotional conversation systems depends on sufficient\nperception and appropriate expression of emotions. In a real-world\nconversation, we firstly instinctively perceive emotions from multi-source\ninformation, including the emotion flow of dialogue history, facial\nexpressions, and personalities of speakers, and then express suitable emotions\naccording to our personalities, but these multiple types of information are\ninsufficiently exploited in emotional conversation fields. To address this\nissue, we propose a heterogeneous graph-based model for emotional conversation\ngeneration. Specifically, we design a Heterogeneous Graph-Based Encoder to\nrepresent the conversation content (i.e., the dialogue history, its emotion\nflow, facial expressions, and speakers' personalities) with a heterogeneous\ngraph neural network, and then predict suitable emotions for feedback. After\nthat, we employ an Emotion-Personality-Aware Decoder to generate a response not\nonly relevant to the conversation context but also with appropriate emotions,\nby taking the encoded graph representations, the predicted emotions from the\nencoder and the personality of the current speaker as inputs. Experimental\nresults show that our model can effectively perceive emotions from multi-source\nknowledge and generate a satisfactory response, which significantly outperforms\nprevious state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Synthetic Data Improves Neural Machine Translation with Knowledge Distillation. (arXiv:2012.15455v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15455","description":"<p>This paper explores augmenting monolingual data for knowledge distillation in\nneural machine translation. Source language monolingual text can be\nincorporated as a forward translation. Interestingly, we find the best way to\nincorporate target language monolingual text is to translate it to the source\nlanguage and round-trip translate it back to the target language, resulting in\na fully synthetic corpus. We find that combining monolingual data from both\nsource and target languages yields better performance than a corpus twice as\nlarge only in one language. Moreover, experiments reveal that the improvement\ndepends upon the provenance of the test set. If the test set was originally in\nthe source language (with the target side written by translators), then forward\ntranslating source monolingual data matters. If the test set was originally in\nthe target language (with the source written by translators), then\nincorporating target monolingual data matters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-to-text Generation by Splicing Together Nearest Neighbors. (arXiv:2101.08248v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.08248","description":"<p>We propose to tackle data-to-text generation tasks by directly splicing\ntogether retrieved segments of text from \"neighbor\" source-target pairs. Unlike\nrecent work that conditions on retrieved neighbors but generates text\ntoken-by-token, left-to-right, we learn a policy that directly manipulates\nsegments of neighbor text, by inserting or replacing them in partially\nconstructed generations. Standard techniques for training such a policy require\nan oracle derivation for each generation, and we prove that finding the\nshortest such derivation can be reduced to parsing under a particular weighted\ncontext-free grammar. We find that policies learned in this way perform on par\nwith strong baselines in terms of automatic and human evaluation, but allow for\nmore interpretable and controllable generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute. (arXiv:2102.12459v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12459","description":"<p>Large language models have become increasingly difficult to train because of\nthe growing computation time and cost. In this work, we present SRU++, a\nhighly-efficient architecture that combines fast recurrence and attention for\nsequence modeling. SRU++ exhibits strong modeling capacity and training\nefficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and\nBillion Word datasets, our model obtains better bits-per-character and\nperplexity while using 3x-10x less training cost compared to top-performing\nTransformer models. For instance, our model achieves a state-of-the-art result\non the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We\nfurther demonstrate that SRU++ requires minimal attention for near\nstate-of-the-art performance. Our results suggest jointly leveraging fast\nrecurrence with little attention as a promising direction for accelerating\nmodel training and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute Alignment: Controlling Text Generation from Pre-trained Language Models. (arXiv:2103.11070v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11070","description":"<p>Large language models benefit from training with a large amount of unlabeled\ntext, which gives them increasingly fluent and diverse generation capabilities.\nHowever, using these models for text generation that takes into account target\nattributes, such as sentiment polarity or specific topics, remains a challenge.\nWe propose a simple and flexible method for controlling text generation by\naligning disentangled attribute representations. In contrast to recent efforts\non training a discriminator to perturb the token level distribution for an\nattribute, we use the same data to learn an alignment function to guide the\npre-trained, non-controlled language model to generate texts with the target\nattribute without changing the original language model parameters. We evaluate\nour method on sentiment- and topic-controlled generation, and show large\nperformance gains over previous methods while retaining fluency and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagae_K/0/1/0/all/0/1\">Kenji Sagae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Explanations from Empirical Explainers. (arXiv:2103.15429v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.15429","description":"<p>Amid a discussion about Green AI in which we see explainability neglected, we\nexplore the possibility to efficiently approximate computationally expensive\nexplainers. To this end, we propose feature attribution modelling with\nEmpirical Explainers. Empirical Explainers learn from data to predict the\nattribution maps of expensive explainers. We train and test Empirical\nExplainers in the language domain and find that they model their expensive\ncounterparts surprisingly well, at a fraction of the cost. They could thus\nmitigate the computational burden of neural explanations significantly, in\napplications that tolerate an approximation error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwarzenberg_R/0/1/0/all/0/1\">Robert Schwarzenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldhus_N/0/1/0/all/0/1\">Nils Feldhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach. (arXiv:2104.04886v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.04886","description":"<p>Adversarial regularization has been shown to improve the generalization\nperformance of deep learning models in various natural language processing\ntasks. Existing works usually formulate the method as a zero-sum game, which is\nsolved by alternating gradient descent/ascent algorithms. Such a formulation\ntreats the adversarial and the defending players equally, which is undesirable\nbecause only the defending player contributes to the generalization\nperformance. To address this issue, we propose Stackelberg Adversarial\nRegularization (SALT), which formulates adversarial regularization as a\nStackelberg game. This formulation induces a competition between a leader and a\nfollower, where the follower generates perturbations, and the leader trains the\nmodel subject to the perturbations. Different from conventional approaches, in\nSALT, the leader is in an advantageous position. When the leader moves, it\nrecognizes the strategy of the follower and takes the anticipated follower's\noutcomes into consideration. Such a leader's advantage enables us to improve\nthe model fitting to the unperturbed data. The leader's strategic information\nis captured by the Stackelberg gradient, which is obtained using an unrolling\nalgorithm. Our experimental results on a set of machine translation and natural\nlanguage understanding tasks show that SALT outperforms existing adversarial\nregularization baselines across all tasks. Our code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing. (arXiv:2104.07275v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07275","description":"<p>An effective recipe for building seq2seq, non-autoregressive, task-oriented\nparsers to map utterances to semantic frames proceeds in three steps: encoding\nan utterance $x$, predicting a frame's length |y|, and decoding a |y|-sized\nframe with utterance and ontology tokens. Though empirically strong, these\nmodels are typically bottlenecked by length prediction, as even small\ninaccuracies change the syntactic and semantic characteristics of resulting\nframes. In our work, we propose span pointer networks, non-autoregressive\nparsers which shift the decoding task from text generation to span prediction;\nthat is, when imputing utterance spans into frame slots, our model produces\nendpoints (e.g., [i, j]) as opposed to text (e.g., \"6pm\"). This natural\nquantization of the output space reduces the variability of gold frames,\ntherefore improving length prediction and, ultimately, exact match.\nFurthermore, length prediction is now responsible for frame syntax and the\ndecoder is responsible for frame semantics, resulting in a coarse-to-fine\nmodel. We evaluate our approach on several task-oriented semantic parsing\ndatasets. Notably, we bridge the quality gap between non-autogressive and\nautoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020).\nFurthermore, due to our more consistent gold frames, we show strong\nimprovements in model generalization in both cross-domain and cross-lingual\ntransfer in low-resource settings. Finally, due to our diminished output\nvocabulary, we observe 70% reduction in latency and 83% reduction in memory at\nbeam size 5 compared to prior non-autoregressive parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_P/0/1/0/all/0/1\">Pierce Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Arun Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shrey Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Abhinav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zotov_A/0/1/0/all/0/1\">Alexander Zotov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Open-Vocabulary Translation from Visual Text Representations. (arXiv:2104.08211v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08211","description":"<p>Machine translation models have discrete vocabularies and commonly use\nsubword segmentation techniques to achieve an 'open vocabulary.' This approach\nrelies on consistent and correct underlying unicode sequences, and makes models\nsusceptible to degradation from common types of noise and variation. Motivated\nby the robustness of human language processing, we propose the use of visual\ntext representations, which dispense with a finite set of text embeddings in\nfavor of continuous vocabularies created by processing visually rendered text\nwith sliding windows. We show that models using visual text representations\napproach or match performance of traditional text models on small and larger\ndatasets. More importantly, models with visual embeddings demonstrate\nsignificant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a\ncharacter permuted German-English task where subword models degrade to 1.9.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etter_D/0/1/0/all/0/1\">David Etter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does language help generalization in vision models?. (arXiv:2104.08313v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.08313","description":"<p>Vision models trained on multimodal datasets can benefit from the wide\navailability of large image-caption datasets. A recent model (CLIP) was found\nto generalize well in zero-shot and transfer learning settings. This could\nimply that linguistic or \"semantic grounding\" confers additional generalization\nabilities to the visual feature space. Here, we systematically evaluate various\nmultimodal architectures and vision-only models in terms of unsupervised\nclustering, few-shot learning, transfer learning and adversarial robustness. In\neach setting, multimodal training produced no additional generalization\ncapability compared to standard supervised visual training. We conclude that\nwork is still required for semantic grounding to help improve vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Devillers_B/0/1/0/all/0/1\">Benjamin Devillers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1\">Bhavin Choksi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielawski_R/0/1/0/all/0/1\">Romain Bielawski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning. (arXiv:2104.08718v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08718","description":"<p>Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n</p>\n<p>In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters. (arXiv:2105.06232v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06232","description":"<p>To diversify and enrich generated dialogue responses, knowledge-grounded\ndialogue has been investigated in recent years. The existing methods tackle the\nknowledge grounding challenge by retrieving the relevant sentences over a large\ncorpus and augmenting the dialogues with explicit extra information. Despite\ntheir success, however, the existing works have drawbacks on the inference\nefficiency. This paper proposes KnowExpert, an end-to-end framework to bypass\nthe explicit retrieval process and inject knowledge into the pre-trained\nlanguage models with lightweight adapters and adapt to the knowledge-grounded\ndialogue task. To the best of our knowledge, this is the first attempt to\ntackle this challenge without retrieval in this task under an open-domain\nchit-chat scenario. The experimental results show that KknowExpert performs\ncomparably with some retrieval-based baselines while being time-efficient in\ninference, demonstrating the potential of our proposed direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1\">Etsuko Ishii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. (arXiv:2105.06965v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06965","description":"<p>When language models process syntactically complex sentences, do they use\ntheir representations of syntax in a manner that is consistent with the grammar\nof the language? We propose AlterRep, an intervention-based method to address\nthis question. For any linguistic feature of a given sentence, AlterRep\ngenerates counterfactual representations by altering how the feature is\nencoded, while leaving intact all other aspects of the original representation.\nBy measuring the change in a model's word prediction behavior when these\ncounterfactual representations are substituted for the original ones, we can\ndraw conclusions about the causal effect of the linguistic feature in question\non the model's behavior. We apply this method to study how BERT models of\ndifferent sizes process relative clauses (RCs). We find that BERT variants use\nRC boundary information during word prediction in a manner that is consistent\nwith the rules of English grammar; this RC boundary information generalizes to\na considerable extent across different RC types, suggesting that BERT\nrepresents RCs as an abstract linguistic category.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1\">Grusha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.09601","description":"<p>In recent years, abstractive text summarization with multimodal inputs has\nstarted drawing attention due to its ability to accumulate information from\ndifferent source modalities and generate a fluent textual summary. However,\nexisting methods use short videos as the visual modality and short summary as\nthe ground-truth, therefore, perform poorly on lengthy videos and long\nground-truth summary. Additionally, there exists no benchmark dataset to\ngeneralize this task on videos of varying lengths. In this paper, we introduce\nAVIATE, the first large-scale dataset for abstractive text summarization with\nvideos of diverse duration, compiled from presentations in well-known academic\nconferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding\nresearch papers as the reference summaries, which ensure adequate quality and\nuniformity of the ground-truth. We then propose FLORAL, a factorized\nmulti-modal Transformer based decoder-only language model, which inherently\ncaptures the intra-modal and inter-modal dynamics within various input\nmodalities for the text summarization task. FLORAL utilizes an increasing\nnumber of self-attentions to capture multimodality and performs significantly\nbetter than traditional encoder-decoder based networks. Extensive experiments\nillustrate that FLORAL achieves significant improvement over the baselines in\nboth qualitative and quantitative evaluations on the existing How2 dataset for\nshort videos and newly introduced AVIATE dataset for videos with diverse\nduration, beating the best baseline on the two datasets by $1.39$ and $2.74$\nROUGE-L points respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1\">Vikram Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTR: Prompt Tuning with Rules for Text Classification. (arXiv:2105.11259v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11259","description":"<p>Fine-tuned pre-trained language models (PLMs) have achieved awesome\nperformance on almost all NLP tasks. By using additional prompts to fine-tune\nPLMs, we can further stimulate the rich knowledge distributed in PLMs to better\nserve downstream tasks. Prompt tuning has achieved promising results on some\nfew-class classification tasks such as sentiment classification and natural\nlanguage inference. However, manually designing lots of language prompts is\ncumbersome and fallible. For those auto-generated prompts, it is also expensive\nand time-consuming to verify their effectiveness in non-few-shot scenarios.\nHence, it is still challenging for prompt tuning to address many-class\nclassification tasks. To this end, we propose prompt tuning with rules (PTR)\nfor many-class text classification and apply logic rules to construct prompts\nwith several sub-prompts. In this way, PTR is able to encode prior knowledge of\neach class into prompt tuning. We conduct experiments on relation\nclassification, a typical and complicated many-class classification task, and\nthe results show that PTR can significantly and consistently outperform\nexisting state-of-the-art baselines. This indicates that PTR is a promising\napproach to take advantage of both human prior knowledge and PLMs for those\ncomplicated classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion. (arXiv:2106.03821v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.03821","description":"<p>It is now well established from a variety of studies that there is a\nsignificant benefit from combining video and audio data in detecting active\nspeakers. However, either of the modalities can potentially mislead audiovisual\nfusion by inducing unreliable or deceptive information. This paper outlines\nactive speaker detection as a multi-objective learning problem to leverage best\nof each modalities using a novel self-attention, uncertainty-based multimodal\nfusion scheme. Results obtained show that the proposed multi-objective learning\narchitecture outperforms traditional approaches in improving both mAP and AUC\nscores. We further demonstrate that our fusion strategy surpasses, in active\nspeaker detection, other modality fusion methods reported in various\ndisciplines. We finally show that the proposed method significantly improves\nthe state-of-the-art on the AVA-ActiveSpeaker dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pouthier_B/0/1/0/all/0/1\">Baptiste Pouthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilati_L/0/1/0/all/0/1\">Laurent Pilati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gudupudi_L/0/1/0/all/0/1\">Leela K. Gudupudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouveyron_C/0/1/0/all/0/1\">Charles Bouveyron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precioso_F/0/1/0/all/0/1\">Frederic Precioso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Grounding with 3D Objects. (arXiv:2107.12514v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12514","description":"<p>Seemingly simple natural language requests to a robot are generally\nunderspecified, for example \"Can you bring me the wireless mouse?\" Flat images\nof candidate mice may not provide the discriminative information needed for\n\"wireless.\" The world, and objects in it, are not flat images but complex 3D\nshapes. If a human requests an object based on any of its basic properties,\nsuch as color, shape, or texture, robots should perform the necessary\nexploration to accomplish the task. In particular, while substantial effort and\nprogress has been made on understanding explicitly visual attributes like color\nand category, comparatively little progress has been made on understanding\nlanguage about shapes and contours. In this work, we introduce a novel\nreasoning task that targets both visual and non-visual language about 3D\nobjects. Our new benchmark, ShapeNet Annotated with Referring Expressions\n(SNARE) requires a model to choose which of two objects is being referenced by\na natural language description. We introduce several CLIP-based models for\ndistinguishing objects and demonstrate that while recent advances in jointly\nmodeling vision and language are useful for robotic language understanding, it\nis still the case that these image-based models are weaker at understanding the\n3D nature of objects -- properties which play a key role in manipulation. We\nfind that adding view estimation to language grounding models improves accuracy\non both SNARE and when identifying objects referred to in language on a robot\nplatform, but note that a large gap remains between these models and human\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_M/0/1/0/all/0/1\">Mohit Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual Entity Learning in Language Models for Conversational Agents. (arXiv:2108.00082v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00082","description":"<p>Neural language models (LM) trained on diverse corpora are known to work well\non previously seen entities, however, updating these models with dynamically\nchanging entities such as place names, song titles and shopping items requires\nre-training from scratch and collecting full sentences containing these\nentities. We aim to address this issue, by introducing entity-aware language\nmodels (EALM), where we integrate entity models trained on catalogues of\nentities into the pre-trained LMs. Our combined language model adaptively adds\ninformation from the entity models into the pre-trained LM depending on the\nsentence context. Our entity models can be updated independently of the\npre-trained LM, enabling us to influence the distribution of entities output by\nthe final LM, without any further training of the pre-trained LM. We show\nsignificant perplexity improvements on task-oriented dialogue datasets,\nespecially on long-tailed utterances, with an ability to continually adapt to\nnew entities (to an extent).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing. (arXiv:2108.04990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04990","description":"<p>Interpretability methods like Integrated Gradient and LIME are popular\nchoices for explaining natural language model predictions with relative word\nimportance scores. These interpretations need to be robust for trustworthy NLP\napplications in high-stake areas like medicine or finance. Our paper\ndemonstrates how interpretations can be manipulated by making simple word\nperturbations on an input text. Via a small portion of word-level swaps, these\nadversarial perturbations aim to make the resulting text semantically and\nspatially similar to its seed input (therefore sharing similar\ninterpretations). Simultaneously, the generated examples achieve the same\nprediction label as the seed yet are given a substantially different\nexplanation by the interpretation methods. Our experiments generate fragile\ninterpretations to attack two SOTA interpretation methods, across three popular\nTransformer models and on two different NLP datasets. We observe that the rank\norder correlation drops by over 20% when less than 10% of words are perturbed\non average. Further, rank-order correlation keeps decreasing as more words get\nperturbed. Furthermore, we demonstrate that candidates generated from our\nmethod have good quality metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sanchit Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Fine-tuning for Pre-trained Language Models. (arXiv:2108.12848v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12848","description":"<p>Pre-trained language models (PrLM) have to carefully manage input units when\ntraining on a very large text with a vocabulary consisting of millions of\nwords. Previous works have shown that incorporating span-level information over\nconsecutive words in pre-training could further improve the performance of\nPrLMs. However, given that span-level clues are introduced and fixed in\npre-training, previous methods are time-consuming and lack of flexibility. To\nalleviate the inconvenience, this paper presents a novel span fine-tuning\nmethod for PrLMs, which facilitates the span setting to be adaptively\ndetermined by specific downstream tasks during the fine-tuning phase. In\ndetail, any sentences processed by the PrLM will be segmented into multiple\nspans according to a pre-sampled dictionary. Then the segmentation information\nwill be sent through a hierarchical CNN module together with the representation\noutputs of the PrLM and ultimately generate a span-enhanced representation.\nExperiments on GLUE benchmark show that the proposed span fine-tuning method\nsignificantly enhances the PrLM, and at the same time, offer more flexibility\nin an efficient way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_R/0/1/0/all/0/1\">Rongzhou Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\infty$-former: Infinite Memory Transformer. (arXiv:2109.00301v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00301","description":"<p>Transformers struggle when attending to long contexts, since the amount of\ncomputation grows with the context length, and therefore they cannot model\nlong-term memories effectively. Several variations have been proposed to\nalleviate this problem, but they all have a finite memory capacity, being\nforced to drop old information. In this paper, we propose the $\\infty$-former,\nwhich extends the vanilla transformer with an unbounded long-term memory. By\nmaking use of a continuous-space attention mechanism to attend over the\nlong-term memory, the $\\infty$-former's attention complexity becomes\nindependent of the context length. Thus, it is able to model arbitrarily long\ncontexts and maintain \"sticky memories\" while keeping a fixed computation\nbudget. Experiments on a synthetic sorting task demonstrate the ability of the\n$\\infty$-former to retain information from long sequences. We also perform\nexperiments on language modeling, by training a model from scratch and by\nfine-tuning a pre-trained language model, which show benefits of unbounded\nlong-term memories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training. (arXiv:2109.00993v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00993","description":"<p>Large Transformer-based language models such as BERT have led to broad\nperformance improvements on many NLP tasks. Domain-specific variants of these\nmodels have demonstrated excellent performance on a variety of specialised\ntasks. In legal NLP, BERT-based models have led to new state-of-the-art results\non multiple tasks. The exploration of these models has demonstrated the\nimportance of capturing the specificity of the legal language and its\nvocabulary. However, such approaches suffer from high computational costs,\nleading to a higher ecological impact and lower accessibility. Our findings,\nfocusing on English language legal text, show that lightweight LSTM-based\nLanguage Models are able to capture enough information from a small legal text\npretraining corpus and achieve excellent performance on short legal text\nclassification tasks. This is achieved with a significantly reduced\ncomputational overhead compared to BERT-based models. However, our method also\nshows degraded performance on a more complex task, multi-label classification\nof longer documents, highlighting the limitations of this lightweight approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheewala_A/0/1/0/all/0/1\">Akshita Gheewala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briton_P/0/1/0/all/0/1\">Paul Briton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonsus_M/0/1/0/all/0/1\">Marc Alphonsus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laabiyad_R/0/1/0/all/0/1\">Rym Laabiyad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_F/0/1/0/all/0/1\">Francesco Piccoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment. (arXiv:2109.02363v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02363","description":"<p>Cross-lingual entity alignment (EA) aims to find the equivalent entities\nbetween crosslingual KGs, which is a crucial step for integrating KGs.\nRecently, many GNN-based EA methods are proposed and show decent performance\nimprovements on several public datasets. Meanwhile, existing GNN-based EA\nmethods inevitably inherit poor interpretability and low efficiency from neural\nnetworks. Motivated by the isomorphic assumption of GNNbased methods, we\nsuccessfully transform the cross-lingual EA problem into the assignment\nproblem. Based on this finding, we propose a frustratingly Simple but Effective\nUnsupervised entity alignment method (SEU) without neural networks. Extensive\nexperiments show that our proposed unsupervised method even beats advanced\nsupervised methods across all public datasets and has high efficiency,\ninterpretability, and stability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Attention Transformer for Leveraging Word-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.02789","description":"<p>Pretrained contextualized representations offer great success for many\ndownstream tasks, including document ranking. The multilingual versions of such\npretrained representations provide a possibility of jointly learning many\nlanguages with the same model. Although it is expected to gain big with such\njoint training, in the case of cross lingual information retrieval (CLIR), the\nmodels under a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize that the\nperformance drop is due to the translation gap between query and documents. In\nthe monolingual retrieval task, because of the same lexical inputs, it is\neasier for model to identify the query terms that occurred in documents.\nHowever, in the multilingual pretrained models that the words in different\nlanguages are projected into the same hyperspace, the model tends to translate\nquery terms into related terms, i.e., terms that appear in a similar context,\nin addition to or sometimes rather than synonyms in the target language. This\nproperty is creating difficulties for the model to connect terms that cooccur\nin both query and document. To address this issue, we propose a novel Mixed\nAttention Transformer (MAT) that incorporates external word level knowledge,\nsuch as a dictionary or translation table. We design a sandwich like\narchitecture to embed MAT into the recent transformer based deep neural models.\nBy encoding the translation knowledge into an attention matrix, the model with\nMAT is able to focus on the mutually translated words in the input sequence.\nExperimental results demonstrate the effectiveness of the external knowledge\nand the significant improvement of MAT embedded neural reranking model on CLIR\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05186","description":"<p>This paper investigates continual learning for semantic parsing. In this\nsetting, a neural semantic parser learns tasks sequentially without accessing\nfull training data from previous tasks. Direct application of the SOTA\ncontinual learning algorithms to this problem fails to achieve comparable\nperformance with re-training models with all seen tasks because they have not\nconsidered the special properties of structured outputs yielded by semantic\nparsers. Therefore, we propose TotalRecall, a continual learning method\ndesigned for neural semantic parsers from two aspects: i) a sampling method for\nmemory replay that diversifies logical form templates and balances\ndistributions of parse actions in a memory; ii) a two-stage training method\nthat significantly improves generalization capability of the parsers across\ntasks. We conduct extensive experiments to study the research problems involved\nin continual semantic parsing and demonstrate that a neural semantic parser\ntrained with TotalRecall achieves superior performance than the one trained\ndirectly with the SOTA continual learning algorithms and achieve a 3-6 times\nspeedup compared to re-training from scratch. Code and datasets are available\nat: https://github.com/zhuang-li/cl_nsp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good-Enough Example Extrapolation. (arXiv:2109.05602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05602","description":"<p>This paper asks whether extrapolating the hidden space distribution of text\nexamples from one class onto another is a valid inductive bias for data\naugmentation. To operationalize this question, I propose a simple data\naugmentation protocol called \"good-enough example extrapolation\" (GE3). GE3 is\nlightweight and has no hyperparameters. Applied to three text classification\ndatasets for various data imbalance scenarios, GE3 improves performance more\nthan upsampling and other hidden-space data augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids' Representations. (arXiv:2109.05958v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05958","description":"<p>Most of the recent works on probing representations have focused on BERT,\nwith the presumption that the findings might be similar to the other models. In\nthis work, we extend the probing studies to two other models in the family,\nnamely ELECTRA and XLNet, showing that variations in the pre-training\nobjectives or architectural choices can result in different behaviors in\nencoding linguistic information in the representations. Most notably, we\nobserve that ELECTRA tends to encode linguistic knowledge in the deeper layers,\nwhereas XLNet instead concentrates that in the earlier layers. Also, the former\nmodel undergoes a slight change during fine-tuning, whereas the latter\nexperiences significant adjustments. Moreover, we show that drawing conclusions\nbased on the weight mixing evaluation strategy -- which is widely used in the\ncontext of layer-wise probing -- can be misleading given the norm disparity of\nthe representations across different layers. Instead, we adopt an alternative\ninformation-theoretic probing with minimum description length, which has\nrecently been proven to provide more reliable and informative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fayyaz_M/0/1/0/all/0/1\">Mohsen Fayyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghazadeh_E/0/1/0/all/0/1\">Ehsan Aghazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohebbi_H/0/1/0/all/0/1\">Hosein Mohebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Emergence of the Shape Bias Results from Communicative Efficiency. (arXiv:2109.06232v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06232","description":"<p>By the age of two, children tend to assume that new word categories are based\non objects' shape, rather than their color or texture; this assumption is\ncalled the shape bias. They are thought to learn this bias by observing that\ntheir caregiver's language is biased towards shape based categories. This\npresents a chicken and egg problem: if the shape bias must be present in the\nlanguage in order for children to learn it, how did it arise in language in the\nfirst place? In this paper, we propose that communicative efficiency explains\nboth how the shape bias emerged and why it persists across generations. We\nmodel this process with neural emergent language agents that learn to\ncommunicate about raw pixelated images. First, we show that the shape bias\nemerges as a result of efficient communication strategies employed by agents.\nSecond, we show that pressure brought on by communicative need is also\nnecessary for it to persist across generations; simply having a shape bias in\nan agent's input language is insufficient. These results suggest that, over and\nabove the operation of other learning strategies, the shape bias in human\nlearners may emerge and be sustained by communicative pressures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portelance_E/0/1/0/all/0/1\">Eva Portelance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1\">Michael C. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-OCR Document Correction with large Ensembles of Character Sequence Models. (arXiv:2109.06264v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06264","description":"<p>In this paper, we propose a novel method based on character\nsequence-to-sequence models to correct documents already processed with Optical\nCharacter Recognition (OCR) systems. The main contribution of this paper is a\nset of strategies to accurately process strings much longer than the ones used\nto train the sequence model while being sample- and resource-efficient,\nsupported by thorough experimentation. The strategy with the best performance\ninvolves splitting the input document in character n-grams and combining their\nindividual corrections into the final output using a voting scheme that is\nequivalent to an ensemble of a large number of sequence models. We further\ninvestigate how to weigh the contributions from each one of the members of this\nensemble. We test our method on nine languages of the ICDAR 2019 competition on\npost-OCR text correction and achieve a new state-of-the-art performance in five\nof them. Our code for post-OCR correction is shared at\nhttps://github.com/jarobyte91/post_ocr_correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_Orta_J/0/1/0/all/0/1\">Juan Ramirez-Orta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xamena_E/0/1/0/all/0/1\">Eduardo Xamena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguitman_A/0/1/0/all/0/1\">Ana Maguitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expert Knowledge-Guided Length-Variant Hierarchical Label Generation for Proposal Classification. (arXiv:2109.06661v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.06661","description":"<p>To advance the development of science and technology, research proposals are\nsubmitted to open-court competitive programs developed by government agencies\n(e.g., NSF). Proposal classification is one of the most important tasks to\nachieve effective and fair review assignments. Proposal classification aims to\nclassify a proposal into a length-variant sequence of labels. In this paper, we\nformulate the proposal classification problem into a hierarchical multi-label\nclassification task. Although there are certain prior studies, proposal\nclassification exhibit unique features: 1) the classification result of a\nproposal is in a hierarchical discipline structure with different levels of\ngranularity; 2) proposals contain multiple types of documents; 3) domain\nexperts can empirically provide partial labels that can be leveraged to improve\ntask performances. In this paper, we focus on developing a new deep proposal\nclassification framework to jointly model the three features. In particular, to\nsequentially generate labels, we leverage previously-generated labels to\npredict the label of next level; to integrate partial labels from experts, we\nuse the embedding of these empirical partial labels to initialize the state of\nneural networks. Our model can automatically identify the best length of label\nsequence to stop next label prediction. Finally, we present extensive results\nto demonstrate that our method can jointly model partial labels, textual\ninformation, and semantic dependencies in label sequences, and, thus, achieve\nadvanced performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fuzzy Attention for Structured Sentiment Analysis. (arXiv:2109.06719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06719","description":"<p>Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructured sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structured sentiment analysis. We further explore the\nparsing modeling on structured sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letain Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding. (arXiv:2109.06838v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06838","description":"<p>While large language models have shown exciting progress on several NLP\nbenchmarks, evaluating their ability for complex analogical reasoning remains\nunder-explored. Here, we introduce a high-quality crowdsourced dataset of\nnarratives for employing proverbs in context as a benchmark for abstract\nlanguage understanding. The dataset provides fine-grained annotation of aligned\nspans between proverbs and narratives, and contains minimal lexical overlaps\nbetween narratives and proverbs, ensuring that models need to go beyond\nsurface-level reasoning to succeed. We explore three tasks: (1) proverb\nrecommendation and alignment prediction, (2) narrative generation for a given\nproverb and topic, and (3) identifying narratives with similar motifs. Our\nexperiments show that neural language models struggle in our tasks compared to\nhumans, and the tasks pose multiple learning challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal Transformer Models May Not Always Help. (arXiv:2109.06862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06862","description":"<p>Deep learning-based Natural Language Processing methods, especially\ntransformers, have achieved impressive performance in the last few years.\nApplying those state-of-the-art NLP methods to legal activities to automate or\nsimplify some simple work is of great value. This work investigates the value\nof domain adaptive pre-training and language adapters in legal NLP tasks. By\ncomparing the performance of language models with domain adaptive pre-training\non different tasks and different dataset splits, we show that domain adaptive\npre-training is only helpful with low-resource downstream tasks, thus far from\nbeing a panacea. We also benchmark the performance of adapters in a typical\nlegal NLP task and show that they can yield similar performance to full model\ntuning with much smaller training costs. As an additional result, we release\nLegalRoBERTa, a RoBERTa model further pre-trained on legal corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Saibo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">R&#xe9;mi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}