{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ALLWAS: Active Learning on Language models in WASserstein space. (arXiv:2109.01691v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01691","description":"<p>Active learning has emerged as a standard paradigm in areas with scarcity of\nlabeled training data, such as in the medical domain. Language models have\nemerged as the prevalent choice of several natural language tasks due to the\nperformance boost offered by these models. However, in several domains, such as\nmedicine, the scarcity of labeled training data is a common issue. Also, these\nmodels may not work well in cases where class imbalance is prevalent. Active\nlearning may prove helpful in these cases to boost the performance with a\nlimited label budget. To this end, we propose a novel method using sampling\ntechniques based on submodular optimization and optimal transport for active\nlearning in language models, dubbed ALLWAS. We construct a sampling strategy\nbased on submodular optimization of the designed objective in the gradient\ndomain. Furthermore, to enable learning from few samples, we propose a novel\nstrategy for sampling from the Wasserstein barycenters. Our empirical\nevaluations on standard benchmark datasets for text classification show that\nour methods perform significantly better (&gt;20% relative increase in some cases)\nthan existing approaches for active learning on language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1\">Anson Bastos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_M/0/1/0/all/0/1\">Manohar Kaul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models. (arXiv:2109.01754v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01754","description":"<p>Large-scale conversational assistants like Alexa, Siri, Cortana and Google\nAssistant process every utterance using multiple models for domain, intent and\nnamed entity recognition. Given the decoupled nature of model development and\nlarge traffic volumes, it is extremely difficult to identify utterances\nprocessed erroneously by such systems. We address this challenge to detect\ndomain classification errors using offline Transformer models. We combine\nutterance encodings from a RoBERTa model with the Nbest hypothesis produced by\nthe production system. We then fine-tune end-to-end in a multitask setting\nusing a small dataset of humanannotated utterances with domain classification\nerrors. We tested our approach for detecting misclassifications from one domain\nthat accounts for &lt;0.5% of the traffic in a large-scale conversational AI\nsystem. Our approach achieves an F1 score of 30% outperforming a bi- LSTM\nbaseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this\nfurther by 2.2% to 32.2% by ensembling multiple models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fofadiya_D/0/1/0/all/0/1\">Darshan Fofadiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_P/0/1/0/all/0/1\">Prathap Ramachandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Cross-Domain Named Entity Recognition. (arXiv:2109.01758v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01758","description":"<p>Current work in named entity recognition (NER) shows that data augmentation\ntechniques can produce more robust models. However, most existing techniques\nfocus on augmenting in-domain data in low-resource scenarios where annotated\ndata is quite limited. In contrast, we study cross-domain data augmentation for\nthe NER task. We investigate the possibility of leveraging data from\nhigh-resource domains by projecting it into the low-resource domains.\nSpecifically, we propose a novel neural architecture to transform the data\nrepresentation from a high-resource to a low-resource domain by learning the\npatterns (e.g. style, noise, abbreviations, etc.) in the text that\ndifferentiate them and a shared feature space where both domains are aligned.\nWe experiment with diverse datasets and show that transforming the data to the\nlow-resource domain representation achieves significant improvements over only\nusing data from high-resource domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])","link":"http://arxiv.org/abs/2109.01815","description":"<p>How data is represented and operationalized is critical for building\ncomputational solutions that are both effective and efficient. A common\napproach is to represent data objects as binary vectors, denoted \\textit{hash\ncodes}, which require little storage and enable efficient similarity search\nthrough direct indexing into a hash table or through similarity computations in\nan appropriate space. Due to the limited expressibility of hash codes, compared\nto real-valued representations, a core open challenge is how to generate hash\ncodes that well capture semantic content or latent properties using a small\nnumber of bits, while ensuring that the hash codes are distributed in a way\nthat does not reduce their search efficiency. State of the art methods use\nrepresentation learning for generating such hash codes, focusing on neural\nautoencoder architectures where semantics are encoded into the hash codes by\nlearning to reconstruct the original inputs of the hash codes. This thesis\naddresses the above challenge and makes a number of contributions to\nrepresentation learning that (i) improve effectiveness of hash codes through\nmore expressive representations and a more effective similarity measure than\nthe current state of the art, namely the Hamming distance, and (ii) improve\nefficiency of hash codes by learning representations that are especially suited\nto the choice of search method. The contributions are empirically validated on\nseveral tasks related to similarity search and recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Casper Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frustratingly Simple Pretraining Alternatives to Masked Language Modeling. (arXiv:2109.01819v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01819","description":"<p>Masked language modeling (MLM), a self-supervised pretraining objective, is\nwidely used in natural language processing for learning text representations.\nMLM trains a model to predict a random sample of input tokens that have been\nreplaced by a [MASK] placeholder in a multi-class setting over the entire\nvocabulary. When pretraining, it is common to use alongside MLM other auxiliary\nobjectives on the token or sequence level to improve downstream performance\n(e.g. next sentence prediction). However, no previous work so far has attempted\nin examining whether other simpler linguistically intuitive or not objectives\ncan be used standalone as main pretraining objectives. In this paper, we\nexplore five simple pretraining objectives based on token-level classification\ntasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our\nproposed methods achieve comparable or better performance to MLM using a\nBERT-BASE architecture. We further validate our methods using smaller models,\nshowing that pretraining a model with 41% of the BERT-BASE's parameters,\nBERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1\">George Chrysostomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01839","description":"<p>As a kind of new expression elements, Internet memes are popular and\nextensively used in online chatting scenarios since they manage to make\ndialogues vivid, moving, and interesting. However, most current dialogue\nresearches focus on text-only dialogue tasks. In this paper, we propose a new\ntask named as \\textbf{M}eme incorporated \\textbf{O}pen-domain \\textbf{D}ialogue\n(MOD). Compared to previous dialogue tasks, MOD is much more challenging since\nit requires the model to understand the multimodal elements as well as the\nemotions behind them. To facilitate the MOD research, we construct a\nlarge-scale open-domain multimodal dialogue dataset incorporating abundant\nInternet memes into utterances. The dataset consists of $\\sim$45K Chinese\nconversations with $\\sim$606K utterances. Each conversation contains about $13$\nutterances with about $4$ Internet memes on average and each utterance equipped\nwith an Internet meme is annotated with the corresponding emotion. In addition,\nwe present a simple and effective method, which utilizes a unified generation\nnetwork to solve the MOD task. Experimental results demonstrate that our method\ntrained on the proposed corpus is able to achieve expressive communication\nincluding texts and memes. The corpus and models have been publicly available\nat https://github.com/lizekang/DSTC10-MOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhengcong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01850","description":"<p>As the digital news industry becomes the main channel of information\ndissemination, the adverse impact of fake news is explosively magnified. The\ncredibility of a news report should not be considered in isolation. Rather,\npreviously published news articles on the similar event could be used to assess\nthe credibility of a news report. Inspired by this, we propose a BERT-based\nmultimodal unreliable news detection framework, which captures both textual and\nvisual information from unreliable articles utilising the contrastive learning\nstrategy. The contrastive learner interacts with the unreliable news classifier\nto push similar credible news (or similar unreliable news) closer while moving\nnews articles with similar content but opposite credibility labels away from\neach other in the multimodal embedding space. Experimental results on a\nCOVID-19 related dataset, ReCOVery, show that our model outperforms a number of\ncompetitive baseline in unreliable news detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing Paraphrase Away from Original Sentence: A Multi-Round Paraphrase Generation Approach. (arXiv:2109.01862v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01862","description":"<p>In recent years, neural paraphrase generation based on Seq2Seq has achieved\nsuperior performance, however, the generated paraphrase still has the problem\nof lack of diversity. In this paper, we focus on improving the diversity\nbetween the generated paraphrase and the original sentence, i.e., making\ngenerated paraphrase different from the original sentence as much as possible.\nWe propose BTmPG (Back-Translation guided multi-round Paraphrase Generation),\nwhich leverages multi-round paraphrase generation to improve diversity and\nemploys back-translation to preserve semantic information. We evaluate BTmPG on\ntwo benchmark datasets. Both automatic and human evaluation show BTmPG can\nimprove the diversity of paraphrase while preserving the semantics of the\noriginal sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01900","description":"<p>Identifying emotions from text is crucial for a variety of real world tasks.\nWe consider the two largest now-available corpora for emotion classification:\nGoEmotions, with 58k messages labelled by readers, and Vent, with 33M\nwriter-labelled messages. We design a benchmark and evaluate several feature\nspaces and learning algorithms, including two simple yet novel models on top of\nBERT that outperform previous strong baselines on GoEmotions. Through an\nexperiment with human participants, we also analyze the differences between how\nwriters express emotions and how readers perceive them. Our results suggest\nthat emotions expressed by writers are harder to identify than emotions that\nreaders perceive. We share a public web interface for researchers to explore\nour models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Gonzalez_N/0/1/0/all/0/1\">Nurudin Alvarez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaltenbrunner_A/0/1/0/all/0/1\">Andreas Kaltenbrunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations. (arXiv:2109.01924v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01924","description":"<p>Linguistic entrainment is a phenomenon where people tend to mimic each other\nin conversation. The core instrument to quantify entrainment is a linguistic\nsimilarity measure between conversational partners. Most of the current\nsimilarity measures are based on bag-of-words approaches that rely on\nlinguistic markers, ignoring the overall language structure and dialogue\ncontext. To address this issue, we propose to use a neural network model to\nperform the similarity measure for entrainment. Our model is context-aware, and\nit further leverages a novel component to learn the shared high-level\nlinguistic features across dialogues. We first investigate the effectiveness of\nour novel component. Then we use the model to perform similarity measure in a\ncorpus-based entrainment analysis. We observe promising results for both\nevaluation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])","link":"http://arxiv.org/abs/2109.01934","description":"<p>Vision-and-language (V\\&amp;L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&amp;L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&amp;L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case. (arXiv:2109.01935v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01935","description":"<p>Contextualised word embeddings is a powerful tool to detect contextual\nsynonyms. However, most of the current state-of-the-art (SOTA) deep learning\nconcept extraction methods remain supervised and underexploit the potential of\nthe context. In this paper, we propose a self-supervised pre-training approach\nwhich is able to detect contextual synonyms of concepts being training on the\ndata created by shallow matching. We apply our methodology in the sparse\nmulti-class setting (over 15,000 concepts) to extract phenotype information\nfrom electronic health records. We further investigate data augmentation\ntechniques to address the problem of the class sparsity. Our approach achieves\na new SOTA for the unsupervised phenotype concept annotation on clinical text\non F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and\n4.0 absolute points, respectively. After fine-tuning with as little as 20\\% of\nthe labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic\nevaluation on three ICU benchmarks also shows the benefit of using the\nphenotypes annotated by our model as features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolanos_L/0/1/0/all/0/1\">Luis Bolanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_A/0/1/0/all/0/1\">Ashwani Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freire_G/0/1/0/all/0/1\">Guilherme Freire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the ability of monolingual models to learn language-agnostic representations. (arXiv:2109.01942v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01942","description":"<p>Pretrained multilingual models have become a de facto default approach for\nzero-shot cross-lingual transfer. Previous work has shown that these models are\nable to achieve cross-lingual representations when pretrained on two or more\nlanguages with shared parameters. In this work, we provide evidence that a\nmodel can achieve language-agnostic representations even when pretrained on a\nsingle language. That is, we find that monolingual models pretrained and\nfinetuned on different languages achieve competitive performance compared to\nthe ones that use the same target language. Surprisingly, the models show a\nsimilar performance on a same task regardless of the pretraining language. For\nexample, models pretrained on distant languages such as German and Portuguese\nperform similarly on English tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_L/0/1/0/all/0/1\">Leandro Rodrigues de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])","link":"http://arxiv.org/abs/2109.01949","description":"<p>Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01951","description":"<p>The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (&lt; 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks. (arXiv:2109.01958v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01958","description":"<p>Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Evaluation for Explainable AI. (arXiv:2109.01962v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01962","description":"<p>While recent years have witnessed the emergence of various explainable\nmethods in machine learning, to what degree the explanations really represent\nthe reasoning process behind the model prediction -- namely, the faithfulness\nof explanation -- is still an open problem. One commonly used way to measure\nfaithfulness is \\textit{erasure-based} criteria. Though conceptually simple,\nerasure-based criterion could inevitably introduce biases and artifacts. We\npropose a new methodology to evaluate the faithfulness of explanations from the\n\\textit{counterfactual reasoning} perspective: the model should produce\nsubstantially different outputs for the original input and its corresponding\ncounterfactual edited on a faithful feature. Specially, we introduce two\nalgorithms to find the proper counterfactuals in both discrete and continuous\nscenarios and then use the acquired counterfactuals to measure faithfulness.\nEmpirical results on several datasets show that compared with existing metrics,\nour proposed counterfactual evaluation method can achieve top correlation with\nthe ground truth under diffe\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchical Structures with Differentiable Nondeterministic Stacks. (arXiv:2109.01982v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01982","description":"<p>Learning hierarchical structures in sequential data -- from simple\nalgorithmic patterns to natural language -- in a reliable, generalizable way\nremains a challenging problem for neural language models. Past work has shown\nthat recurrent neural networks (RNNs) struggle to generalize on held-out\nalgorithmic or syntactic patterns without supervision or some inductive bias.\nTo remedy this, many papers have explored augmenting RNNs with various\ndifferentiable stacks, by analogy with finite automata and pushdown automata.\nIn this paper, we present a stack RNN model based on the recently proposed\nNondeterministic Stack RNN (NS-RNN) that achieves lower cross-entropy than all\nprevious stack RNNs on five context-free language modeling tasks (within 0.05\nnats of the information-theoretic lower bound), including a task in which the\nNS-RNN previously failed to outperform a deterministic stack RNN baseline. Our\nmodel assigns arbitrary positive weights instead of probabilities to stack\nactions, and we provide an analysis of why this improves training. We also\npropose a restricted version of the NS-RNN that makes it practical to use for\nlanguage modeling on natural language and present results on the Penn Treebank\ncorpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-entry Prediction for Online Conversations via Self-Supervised Learning. (arXiv:2109.02020v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02020","description":"<p>In recent years, world business in online discussions and opinion sharing on\nsocial media is booming. Re-entry prediction task is thus proposed to help\npeople keep track of the discussions which they wish to continue. Nevertheless,\nexisting works only focus on exploiting chatting history and context\ninformation, and ignore the potential useful learning signals underlying\nconversation data, such as conversation thread patterns and repeated engagement\nof target users, which help better understand the behavior of target users in\nconversations. In this paper, we propose three interesting and well-founded\nauxiliary tasks, namely, Spread Pattern, Repeated Target user, and Turn\nAuthorship, as the self-supervised signals for re-entry prediction. These\nauxiliary tasks are trained together with the main task in a multi-task manner.\nExperimental results on two datasets newly collected from Twitter and Reddit\nshow that our method outperforms the previous state-of-the-arts with fewer\nparameters and faster convergence. Extensive experiments and analysis show the\neffectiveness of our proposed models and also point out some key ideas in\ndesigning self-supervised tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02040","description":"<p>Masked language modeling (MLM) is one of the key sub-tasks in vision-language\npretraining. In the cross-modal setting, tokens in the sentence are masked at\nrandom, and the model predicts the masked tokens given the image and the text.\nIn this paper, we observe several key disadvantages of MLM in this setting.\nFirst, as captions tend to be short, in a third of the sentences no token is\nsampled. Second, the majority of masked tokens are stop-words and punctuation,\nleading to under-utilization of the image. We investigate a range of\nalternative masking strategies specific to the cross-modal setting that address\nthese shortcomings, aiming for better fusion of text and image in the learned\nrepresentation. When pre-training the LXMERT model, our alternative masking\nstrategies consistently improve over the original masking strategy on three\ndownstream tasks, especially in low resource settings. Further, our\npre-training approach substantially outperforms the baseline model on a\nprompt-based probing task designed to elicit image objects. These results and\nour analysis indicate that our method allows for better utilization of the\ntraining data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_M/0/1/0/all/0/1\">Michael Elhadad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v1 [cs.SD])","link":"http://arxiv.org/abs/2109.02051","description":"<p>Many endeavors have sought to develop countermeasure techniques as\nenhancements on Automatic Speaker Verification (ASV) systems, in order to make\nthem more robust against spoof attacks. As evidenced by the latest ASVspoof\n2019 countermeasure challenge, models currently deployed for the task of ASV\nare, at their best, devoid of suitable degrees of generalization to unseen\nattacks. Upon further investigation of the proposed methods, it appears that a\nbroader three-tiered view of the proposed systems. comprised of the classifier,\nfeature extraction phase, and model loss function, may to some extent lessen\nthe problem. Accordingly, the present study proposes the Efficient Attention\nBranch Network (EABN) modular architecture with a combined loss function to\naddress the generalization problem...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_A/0/1/0/all/0/1\">Amir Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayounpour_M/0/1/0/all/0/1\">Mohammad Mehdi Homayounpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Self-Debiasing Framework for Robust NLU Training. (arXiv:2109.02071v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02071","description":"<p>Existing Natural Language Understanding (NLU) models have been shown to\nincorporate dataset biases leading to strong performance on in-distribution\n(ID) test sets but poor performance on out-of-distribution (OOD) ones. We\nintroduce a simple yet effective debiasing framework whereby the shallow\nrepresentations of the main model are used to derive a bias model and both\nmodels are trained simultaneously. We demonstrate on three well studied NLU\ntasks that despite its simplicity, our method leads to competitive OOD results.\nIt significantly outperforms other debiasing approaches on two tasks, while\nstill delivering high in-distribution performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction. (arXiv:2109.02099v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02099","description":"<p>Distantly supervised relation extraction (RE) automatically aligns\nunstructured text with relation instances in a knowledge base (KB). Due to the\nincompleteness of current KBs, sentences implying certain relations may be\nannotated as N/A instances, which causes the so-called false negative (FN)\nproblem. Current RE methods usually overlook this problem, inducing improper\nbiases in both training and testing procedures. To address this issue, we\npropose a two-stage approach. First, it finds out possible FN samples by\nheuristically leveraging the memory mechanism of deep neural networks. Then, it\naligns those unlabeled data with the training data into a unified feature space\nby adversarial training to assign pseudo labels and further utilize the\ninformation contained in them. Experiments on two wildly-used benchmark\ndatasets demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_K/0/1/0/all/0/1\">Kailong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Botao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02102","description":"<p>This paper demonstrates that by fine-tuning an autoregressive language model\n(GPT-Neo) on appropriately structured step-by-step demonstrations, it is\npossible to teach it to execute a mathematical task that has previously proved\ndifficult for Transformers - longhand modulo operations - with a relatively\nsmall number of examples. Specifically, we fine-tune GPT-Neo to solve the\nnumbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et\nal. (<a href=\"/abs/1904.01557\">arXiv:1904.01557</a>) reported below 40% accuracy on this task with 2 million\ntraining examples. We show that after fine-tuning on 200 appropriately\nstructured demonstrations of solving long division problems and reporting the\nremainders, the smallest available GPT-Neo model achieves over 80% accuracy.\nThis is achieved by constructing an appropriate dataset for fine-tuning, with\nno changes to the learning algorithm. These results suggest that fine-tuning\nautoregressive language models on small sets of well-crafted demonstrations may\nbe a useful paradigm for enabling individuals without training in machine\nlearning to coax such models to perform some kinds of complex multi-step tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Recchia_G/0/1/0/all/0/1\">Gabriel Recchia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Models for Text Coherence Assessment. (arXiv:2109.02176v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02176","description":"<p>Coherence is an important aspect of text quality and is crucial for ensuring\nits readability. It is essential desirable for outputs from text generation\nsystems like summarization, question answering, machine translation, question\ngeneration, table-to-text, etc. An automated coherence scoring model is also\nhelpful in essay scoring or providing writing feedback. A large body of\nprevious work has leveraged entity-based methods, syntactic patterns, discourse\nrelations, and more recently traditional deep learning architectures for text\ncoherence assessment. Previous work suffers from drawbacks like the inability\nto handle long-range dependencies, out-of-vocabulary words, or model sequence\ninformation. We hypothesize that coherence assessment is a cognitively complex\ntask that requires deeper models and can benefit from other related tasks.\nAccordingly, in this paper, we propose four different Transformer-based\narchitectures for the task: vanilla Transformer, hierarchical Transformer,\nmulti-task learning-based model, and a model with fact-based input\nrepresentation. Our experiments with popular benchmark datasets across multiple\ndomains on four different coherence assessment tasks demonstrate that our\nmodels achieve state-of-the-art results outperforming existing models by a good\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_T/0/1/0/all/0/1\">Tushar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_D/0/1/0/all/0/1\">Daksh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nearest Neighbour Few-Shot Learning for Cross-lingual Classification. (arXiv:2109.02221v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02221","description":"<p>Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have\nled to significant performance gains on a wide range of cross-lingual NLP\ntasks, success on many downstream tasks still relies on the availability of\nsufficient annotated data. Traditional fine-tuning of pre-trained models using\nonly a few target samples can cause over-fitting. This can be quite limiting as\nmost languages in the world are under-resourced. In this work, we investigate\ncross-lingual adaptation using a simple nearest neighbor few-shot (&lt;15 samples)\ninference technique for classification tasks. We experiment using a total of 16\ndistinct languages across two NLP tasks- XNLI and PAWS-X. Our approach\nconsistently improves traditional fine-tuning using only a handful of labeled\nsamples in target locales. We also demonstrate its generalization capability\nacross tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_B/0/1/0/all/0/1\">Batool Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack. (arXiv:2109.02229v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02229","description":"<p>Over the past few years, various word-level textual attack approaches have\nbeen proposed to reveal the vulnerability of deep neural networks used in\nnatural language processing. Typically, these approaches involve an important\noptimization step to determine which substitute to be used for each word in the\noriginal input. However, current research on this step is still rather limited,\nfrom the perspectives of both problem-understanding and problem-solving. In\nthis paper, we address these issues by uncovering the theoretical properties of\nthe problem and proposing an efficient local search algorithm (LS) to solve it.\nWe establish the first provable approximation guarantee on solving the problem\nin general cases. Notably, for adversarial textual attack, it is even better\nthan the previous bound which only holds in special case. Extensive experiments\ninvolving five NLP tasks, six datasets and eleven NLP models show that LS can\nlargely reduce the number of queries usually by an order of magnitude to\nachieve high attack success rates. Further experiments show that the\nadversarial examples crafted by LS usually have higher quality, exhibit better\ntransferability, and can bring more robustness improvement to victim models by\nadversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks. (arXiv:2109.02237v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02237","description":"<p>Biomedical entity linking is the task of linking entity mentions in a\nbiomedical document to referent entities in a knowledge base. Recently, many\nBERT-based models have been introduced for the task. While these models have\nachieved competitive results on many datasets, they are computationally\nexpensive and contain about 110M parameters. Little is known about the factors\ncontributing to their impressive performance and whether the\nover-parameterization is needed. In this work, we shed some light on the inner\nworking mechanisms of these large BERT-based models. Through a set of probing\nexperiments, we have found that the entity linking performance only changes\nslightly when the input word order is shuffled or when the attention scope is\nlimited to a fixed window size. From these observations, we propose an\nefficient convolutional neural network with residual connections for biomedical\nentity linking. Because of the sparse connectivity and weight sharing\nproperties, our model has a small number of parameters and is highly efficient.\nOn five public datasets, our model achieves comparable or even better linking\naccuracy than the state-of-the-art BERT-based models while having about 60\ntimes fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STaCK: Sentence Ordering with Temporal Commonsense Knowledge. (arXiv:2109.02247v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02247","description":"<p>Sentence order prediction is the task of finding the correct order of\nsentences in a randomly ordered document. Correctly ordering the sentences\nrequires an understanding of coherence with respect to the chronological\nsequence of events described in the text. Document-level contextual\nunderstanding and commonsense knowledge centered around these events are often\nessential in uncovering this coherence and predicting the exact chronological\norder. In this paper, we introduce STaCK -- a framework based on graph neural\nnetworks and temporal commonsense knowledge to model global information and\npredict the relative order of sentences. Our graph network accumulates temporal\nevidence using knowledge of `past' and `future' and formulates sentence\nordering as a constrained edge classification problem. We report results on\nfive different datasets, and empirically show that the proposed method is\nnaturally suitable for order prediction. The implementation of this work is\npublicly available at: https://github.com/declare-lab/sentence-ordering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations. (arXiv:2109.02254v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02254","description":"<p>The rapid growth in published clinical trials makes it difficult to maintain\nup-to-date systematic reviews, which requires finding all relevant trials. This\nleads to policy and practice decisions based on out-of-date, incomplete, and\nbiased subsets of available clinical evidence. Extracting and then normalising\nPopulation, Intervention, Comparator, and Outcome (PICO) information from\nclinical trial articles may be an effective way to automatically assign trials\nto systematic reviews and avoid searching and screening - the two most\ntime-consuming systematic review processes. We propose and test a novel\napproach to PICO span detection. The major difference between our proposed\nmethod and previous approaches comes from detecting spans without needing\nannotated span data and using only crowdsourced sentence-level annotations.\nExperiments on two datasets show that PICO span detection results achieve much\nhigher results for recall when compared to fully supervised methods with PICO\nsentence detection at least as good as human annotations. By removing the\nreliance on expert annotations for span detection, this work could be used in\nhuman-machine pipeline for turning low-quality crowdsourced, and sentence-level\nPICO annotations into structured information that can be used to quickly assign\ntrials to relevant systematic reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourgeois_F/0/1/0/all/0/1\">Florence T. Bourgeois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1\">Adam G. Dunn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training. (arXiv:2109.02284v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02284","description":"<p>Learning multilingual and multi-domain translation model is challenging as\nthe heterogeneous and imbalanced data make the model converge inconsistently\nover different corpora in real world. One common practice is to adjust the\nshare of each corpus in the training, so that the learning process is balanced\nand low-resource cases can benefit from the high resource ones. However,\nautomatic balancing methods usually depend on the intra- and inter-dataset\ncharacteristics, which is usually agnostic or requires human priors. In this\nwork, we propose an approach, MultiUAT, that dynamically adjusts the training\ndata usage based on the model's uncertainty on a small set of trusted clean\ndata for multi-corpus machine translation. We experiments with two classes of\nuncertainty measures on multilingual (16 languages with 4 settings) and\nmulti-domain settings (4 for in-domain and 2 for out-of-domain on\nEnglish-German translation) and demonstrate our approach MultiUAT substantially\noutperforms its baselines, including both static and dynamic strategies. We\nanalyze the cross-domain transfer and show the deficiency of static and\nsimilarity based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text. (arXiv:2109.02289v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02289","description":"<p>Numerical reasoning skills are essential for complex question answering (CQA)\nover text. It requires opertaions including counting, comparison, addition and\nsubtraction. A successful approach to CQA on text, Neural Module Networks\n(NMNs), follows the programmer-interpreter paradigm and leverages specialised\nmodules to perform compositional reasoning. However, the NMNs framework does\nnot consider the relationship between numbers and entities in both questions\nand paragraphs. We propose effective techniques to improve NMNs' numerical\nreasoning capabilities by making the interpreter question-aware and capturing\nthe relationship between entities and numbers. On the same subset of the DROP\ndataset for CQA on text, experimental results show that our additions\noutperform the original NMNs by 3.0 points for the overall F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiao-Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser. (arXiv:2109.02297v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02297","description":"<p>Considering the importance of building a good Visual Dialog (VD) Questioner,\nmany researchers study the topic under a Q-Bot-A-Bot image-guessing game\nsetting, where the Questioner needs to raise a series of questions to collect\ninformation of an undisclosed image. Despite progress has been made in\nSupervised Learning (SL) and Reinforcement Learning (RL), issues still exist.\nFirstly, previous methods do not provide explicit and effective guidance for\nQuestioner to generate visually related and informative questions. Secondly,\nthe effect of RL is hampered by an incompetent component, i.e., the Guesser,\nwho makes image predictions based on the generated dialogs and assigns rewards\naccordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced\nQuestioner (ReeQ) that generates questions under the guidance of related\nentities and learns entity-based questioning strategy from human dialogs; 2) we\npropose an Augmented Guesser (AugG) that is strong and is optimized for the VD\nsetting especially. Experimental results on the VisDial v1.0 dataset show that\nour approach achieves state-of-theart performance on both image-guessing task\nand question diversity. Human study further proves that our model generates\nmore visually related, informative and coherent questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightTag: Text Annotation Platform. (arXiv:2109.02320v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02320","description":"<p>Text annotation tools assume that their user's goal is to create a labeled\ncorpus. However, users view annotation as a necessary evil on the way to\ndeliver business value through NLP. Thus an annotation tool should optimize for\nthe throughput of the global NLP process, not only the productivity of\nindividual annotators. LightTag is a text annotation tool designed and built on\nthat principle. This paper shares our design rationale, data modeling choices,\nand user interface decisions then illustrates how those choices serve the full\nNLP lifecycle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perry_T/0/1/0/all/0/1\">Tal Perry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hocalarim: Mining Turkish Student Reviews. (arXiv:2109.02325v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02325","description":"<p>We introduce Hocalarim (MyProfessors), the largest student review dataset\navailable for the Turkish language. It consists of over 5000 professor reviews\nleft online by students, with different aspects of education rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset and present its\nstatistics. We examine the impact of students' institution type on their\nratings and the correlation of students' bias to give positive or negative\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1\">Ibrahim Faruk Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calik_N/0/1/0/all/0/1\">Necmettin Bera Calik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yapucuoglu_M/0/1/0/all/0/1\">Mert Yapucuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting a Spin on Language: A Quantum Interpretation of Unary Connectives for Linguistic Applications. (arXiv:2004.04128v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.04128","description":"<p>Extended versions of the Lambek Calculus currently used in computational\nlinguistics rely on unary modalities to allow for the controlled application of\nstructural rules affecting word order and phrase structure. These controlled\nstructural operations give rise to derivational ambiguities that are missed by\nthe original Lambek Calculus or its pregroup simplification. Proposals for\ncompositional interpretation of extended Lambek Calculus in the compact closed\ncategory of FVect and linear maps have been made, but in these proposals the\nsyntax-semantics mapping ignores the control modalities, effectively\nrestricting their role to the syntax. Our aim is to turn the modalities into\nfirst-class citizens of the vectorial interpretation. Building on the\ndirectional density matrix semantics, we extend the interpretation of the type\nsystem with an extra spin density matrix space. The interpretation of proofs\nthen results in ambiguous derivations being tensored with orthogonal spin\nstates. Our method introduces a way of simultaneously representing co-existing\ninterpretations of ambiguous utterances, and provides a uniform framework for\nthe integration of lexical and derivational ambiguity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Adriana D. Correia</a> (Utrecht University), <a href=\"http://arxiv.org/find/cs/1/au:+Stoof_H/0/1/0/all/0/1\">Henk T. C. Stoof</a> (Utrecht University), <a href=\"http://arxiv.org/find/cs/1/au:+Moortgat_M/0/1/0/all/0/1\">Michael Moortgat</a> (Utrecht University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Natural Language Processing on Near-Term Quantum Computers. (arXiv:2005.04147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.04147","description":"<p>In this work, we describe a full-stack pipeline for natural language\nprocessing on near-term quantum computers, aka QNLP. The language-modelling\nframework we employ is that of compositional distributional semantics\n(DisCoCat), which extends and complements the compositional structure of\npregroup grammars. Within this model, the grammatical reduction of a sentence\nis interpreted as a diagram, encoding a specific interaction of words according\nto the grammar. It is this interaction which, together with a specific choice\nof word embedding, realises the meaning (or \"semantics\") of a sentence.\nBuilding on the formal quantum-like nature of such interactions, we present a\nmethod for mapping DisCoCat diagrams to quantum circuits. Our methodology is\ncompatible both with NISQ devices and with established Quantum Machine Learning\ntechniques, paving the way to near-term applications of quantum technology to\nnatural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Gogioso_S/0/1/0/all/0/1\">Stefano Gogioso</a> (Hashberg), <a href=\"http://arxiv.org/find/cs/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Chiappori_N/0/1/0/all/0/1\">Nicol&#xf2; Chiappori</a> (Hashberg), <a href=\"http://arxiv.org/find/cs/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a> (University of Oxford and Cambridge Quantum Computing Ltd.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact and dynamics of hate and counter speech online. (arXiv:2009.08392v3 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2009.08392","description":"<p>Citizen-generated counter speech is a promising way to fight hate speech and\npromote peaceful, non-polarized discourse. However, there is a lack of\nlarge-scale longitudinal studies of its effectiveness for reducing hate speech.\nTo this end, we perform an exploratory analysis of the effectiveness of counter\nspeech using several different macro- and micro-level measures to analyze\n180,000 political conversations that took place on German Twitter over four\nyears. We report on the dynamic interactions of hate and counter speech over\ntime and provide insights into whether, as in `classic' bullying situations,\norganized efforts are more effective than independent individuals in steering\nonline discourse. Taken together, our results build a multifaceted picture of\nthe dynamics of hate and counter speech online. While we make no causal claims\ndue to the complexity of discourse dynamics, our findings suggest that\norganized hate speech is associated with changes in public discourse and that\ncounter speech -- especially when organized -- may help curb hateful rhetoric\nin online discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garland_J/0/1/0/all/0/1\">Joshua Garland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazi_Zahedi_K/0/1/0/all/0/1\">Keyan Ghazi-Zahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_J/0/1/0/all/0/1\">Jean-Gabriel Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_Dufresne_L/0/1/0/all/0/1\">Laurent H&#xe9;bert-Dufresne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galesic_M/0/1/0/all/0/1\">Mirta Galesic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.16413","description":"<p>The COVID-19 pandemic has had a significant impact on society, both because\nof the serious health effects of COVID-19 and because of public health measures\nimplemented to slow its spread. Many of these difficulties are fundamentally\ninformation needs; attempts to address these needs have caused an information\noverload for both researchers and the public. Natural language processing\n(NLP), the branch of artificial intelligence that interprets human language,\ncan be applied to address many of the information needs made urgent by the\nCOVID-19 pandemic. This review surveys approximately 150 NLP studies and more\nthan 50 systems and datasets addressing the COVID-19 pandemic. We detail work\non four core NLP tasks: information retrieval, named entity recognition,\nliterature-based discovery, and question answering. We also describe work that\ndirectly addresses aspects of the pandemic through four additional tasks: topic\nmodeling, sentiment and emotion analysis, caseload forecasting, and\nmisinformation detection. We conclude by discussing observable trends and\nremaining challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shankai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference. (arXiv:2011.14203v5 [cs.AR] UPDATED)","link":"http://arxiv.org/abs/2011.14203","description":"<p>Transformer-based language models such as BERT provide significant accuracy\nimprovement for a multitude of natural language processing (NLP) tasks.\nHowever, their hefty computational and memory demands make them challenging to\ndeploy to resource-constrained edge platforms with strict latency requirements.\nWe present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware\nenergy optimization for multi-task NLP. EdgeBERT employs entropy-based early\nexit predication in order to perform dynamic voltage-frequency scaling (DVFS),\nat a sentence granularity, for minimal energy consumption while adhering to a\nprescribed target latency. Computation and memory footprint overheads are\nfurther alleviated by employing a calibrated combination of adaptive attention\nspan, selective network pruning, and floating-point quantization. Furthermore,\nin order to maximize the synergistic benefits of these algorithms in always-on\nand intermediate edge computing settings, we specialize a 12nm scalable\nhardware accelerator system, integrating a fast-switching low-dropout voltage\nregulator (LDO), an all-digital phase-locked loop (ADPLL), as well as,\nhigh-density embedded non-volatile memories (eNVMs) wherein the sparse\nfloating-point bit encodings of the shared multi-task parameters are carefully\nstored. Altogether, latency-aware multi-task NLP inference acceleration on the\nEdgeBERT hardware system generates up to 7x, 2.5x, and 53x lower energy\ncompared to the conventional inference without early stopping, the\nlatency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson\nTegra X2 mobile GPU, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tambe_T/0/1/0/all/0/1\">Thierry Tambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooper_C/0/1/0/all/0/1\">Coleman Hooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pentecost_L/0/1/0/all/0/1\">Lillian Pentecost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_T/0/1/0/all/0/1\">Tianyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">En-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donato_M/0/1/0/all/0/1\">Marco Donato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul N. Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1\">David Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gu-Yeon Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer Feed-Forward Layers Are Key-Value Memories. (arXiv:2012.14913v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14913","description":"<p>Feed-forward layers constitute two-thirds of a transformer model's\nparameters, yet their role in the network remains under-explored. We show that\nfeed-forward layers in transformer-based language models operate as key-value\nmemories, where each key correlates with textual patterns in the training\nexamples, and each value induces a distribution over the output vocabulary. Our\nexperiments show that the learned patterns are human-interpretable, and that\nlower layers tend to capture shallow patterns, while upper layers learn more\nsemantic ones. The values complement the keys' input patterns by inducing\noutput distributions that concentrate probability mass on tokens likely to\nappear immediately after each pattern, particularly in the upper layers.\nFinally, we demonstrate that the output of a feed-forward layer is a\ncomposition of its memories, which is subsequently refined throughout the\nmodel's layers via residual connections to produce the final output\ndistribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1\">Roei Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01345","description":"<p>Emotion dynamics is a framework for measuring how an individual's emotions\nchange over time. It is a powerful tool for understanding how we behave and\ninteract with the world. In this paper, we introduce a framework to track\nemotion dynamics through one's utterances. Specifically we introduce a number\nof utterance emotion dynamics (UED) metrics inspired by work in Psychology. We\nuse this approach to trace emotional arcs of movie characters. We analyze\nthousands of such character arcs to test hypotheses that inform our broader\nunderstanding of stories. Notably, we show that there is a tendency for\ncharacters to use increasingly more negative words and become increasingly\nemotionally discordant with each other until about 90 percent of the narrative\nlength. UED also has applications in behavior studies, social sciences, and\npublic health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hipson_W/0/1/0/all/0/1\">Will E. Hipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models. (arXiv:2103.06678v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.06678","description":"<p>In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baimukan_N/0/1/0/all/0/1\">Nurpeiis Baimukan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructive and Toxic Speech Detection for Open-domain Social Media Comments in Vietnamese. (arXiv:2103.10069v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.10069","description":"<p>The rise of social media has led to the increasing of comments on online\nforums. However, there still exists invalid comments which are not informative\nfor users. Moreover, those comments are also quite toxic and harmful to people.\nIn this paper, we create a dataset for constructive and toxic speech detection,\nnamed UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset)\nwith 10,000 human-annotated comments. For these tasks, we propose a system for\nconstructive and toxic speech detection with the state-of-the-art transfer\nlearning model in Vietnamese NLP as PhoBERT. With this system, we obtain\nF1-scores of 78.59% and 59.40% for classifying constructive and toxic comments,\nrespectively. Besides, we implement various baseline models as traditional\nMachine Learning and Deep Neural Network-Based models to evaluate the dataset.\nWith the results, we can solve several tasks on the online discussions and\ndevelop the framework for identifying constructiveness and toxicity of\nVietnamese social media comments automatically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in your Head? Emergent Behaviour in Multi-Task Transformer Models. (arXiv:2104.06129v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06129","description":"<p>The primary paradigm for multi-task training in natural language processing\nis to represent the input with a shared pre-trained language model, and add a\nsmall, thin network (head) per task. Given an input, a target head is the head\nthat is selected for outputting the final prediction. In this work, we examine\nthe behaviour of non-target heads, that is, the output of heads when given\ninput that belongs to a different task than the one they were trained for. We\nfind that non-target heads exhibit emergent behaviour, which may either explain\nthe target task, or generalize beyond their original task. For example, in a\nnumerical reasoning task, a span extraction head extracts from the input the\narguments to a computation that results in a number generated by a target\ngenerative head. In addition, a summarization head that is trained with a\ntarget question answering head, outputs query-based summaries when given a\nquestion and a context from which the answer is to be extracted. This emergent\nbehaviour suggests that multi-task training leads to non-trivial extrapolation\nof skills, which can be harnessed for interpretability and generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_U/0/1/0/all/0/1\">Uri Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Arie_A/0/1/0/all/0/1\">Aviv Ben-Arie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planning with Learned Entity Prompts for Abstractive Summarization. (arXiv:2104.07606v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07606","description":"<p>We introduce a simple but flexible mechanism to learn an intermediate plan to\nground the generation of abstractive summaries. Specifically, we prepend (or\nprompt) target summaries with entity chains -- ordered sequences of entities\nmentioned in the summary. Transformer-based sequence-to-sequence models are\nthen trained to generate the entity chain and then continue generating the\nsummary conditioned on the entity chain and the input. We experimented with\nboth pretraining and finetuning with this content planning objective. When\nevaluated on CNN/DailyMail, XSum, SAMSum and BillSum, we demonstrate\nempirically that the grounded generation with the planning objective improves\nentity specificity and planning in summaries for all datasets, and achieves\nstate-of-the-art performance on XSum and SAMSum in terms of Rouge. Moreover, we\ndemonstrate empirically that planning with entity chains provides a mechanism\nto control hallucinations in abstractive summaries. By prompting the decoder\nwith a modified content plan that drops hallucinated entities, we outperform\nstate-of-the-art approaches for faithfulness when evaluated automatically and\nby humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_G/0/1/0/all/0/1\">Gon&#xe7;alo Simoes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_V/0/1/0/all/0/1\">Vitaly Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Ryan McDonald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Masked Segmental Language Model for Unsupervised Natural Language Segmentation. (arXiv:2104.07829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07829","description":"<p>Segmentation remains an important preprocessing step both in languages where\n\"words\" or other important syntactic/semantic units (like morphemes) are not\nclearly delineated by white space, as well as when dealing with continuous\nspeech data, where there is often no meaningful pause between words.\nNear-perfect supervised methods have been developed for use in resource-rich\nlanguages such as Chinese, but many of the world's languages are both\nmorphologically complex, and have no large dataset of \"gold\" segmentations into\nmeaningful units. To solve this problem, we propose a new type of Segmental\nLanguage Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021)\nfor use in both unsupervised and lightly supervised segmentation tasks. We\nintroduce a Masked Segmental Language Model (MSLM) built on a span-masking\ntransformer architecture, harnessing the power of a bi-directional masked\nmodeling context and attention. In a series of experiments, our model\nconsistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation\nquality, and performs similarly to the Recurrent model on English (PTB). We\nconclude by discussing the different challenges posed in segmenting\nphonemic-type writing systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1\">C.M. Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levow_G/0/1/0/all/0/1\">Gina-Anne Levow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1\">Shane Steinert-Threlkeld</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07858","description":"<p>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.\nRecent studies propose supervised PQ, where the embedding and quantization\nmodels can be jointly trained with supervised learning. However, there is a\nlack of appropriate formulation of the joint training objective; thus, the\nimprovements over previous non-supervised baselines are limited in reality. In\nthis work, we propose the Matching-oriented Product Quantization (MoPQ), where\na novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the\nminimization of MCL, we are able to maximize the matching probability of query\nand ground-truth key, which contributes to the optimal retrieval accuracy.\nGiven that the exact computation of MCL is intractable due to the demand of\nvast contrastive samples, we further propose the Differentiable Cross-device\nSampling (DCS), which significantly augments the contrastive samples for\nprecise approximation of MCL. We conduct extensive experimental studies on four\nreal-world datasets, whose results verify the effectiveness of MoPQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08773","description":"<p>Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. NLP models built with the conventional paradigm, however, often\nstruggle with generalization across tasks (e.g., a question-answering system\ncannot solve classification tasks). A long-standing challenge in AI is to build\na model that is equipped with the understanding of human-readable instructions\nthat define the tasks, and can generalize to new tasks. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions and 193k task instances. The instructions are\nobtained from crowdsourcing instructions used to collect existing NLP datasets\nand mapped to a unified schema. We adopt generative pre-trained language models\nto encode task-specific instructions along with input and generate task output.\nOur results indicate that models can benefit from instructions to generalize\nacross tasks. These models, however, are far behind supervised task-specific\nmodels, indicating significant room for more progress in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14638","description":"<p>It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). A corpus of 7,873 scientific articles dealing with natural products\n(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine\nautomatically extracts categorical and numerical parameters such as (i) the\nplant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it can potentially make obsolete the current article\nreviewing process in some areas, especially those in which machine learning\nmodels capture texts structure, text semantics, and latent knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Amauri J Paula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00270","description":"<p>Opinion prediction is an emerging research area with diverse real-world\napplications, such as market research and situational awareness. We identify\ntwo lines of approaches to the problem of opinion prediction. One uses\ntopic-based sentiment analysis with time-series modeling, while the other uses\nstatic embedding of text. The latter approaches seek user-specific solutions by\ngenerating user fingerprints. Such approaches are useful in predicting user's\nreactions to unseen content. In this work, we propose a novel dynamic\nfingerprinting method that leverages contextual embedding of user's comments\nconditioned on relevant user's reading history. We integrate BERT variants with\na recurrent neural network to generate predictions. The results show up to 13\\%\nimprovement in micro F1-score compared to previous approaches. Experimental\nresults show novel insights that were previously unknown such as better\npredictions for an increase in dynamic history length, the impact of the nature\nof the article on performance, thereby laying the foundation for further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1\">Kishore Tumarada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dr. Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dr. Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_D/0/1/0/all/0/1\">Dr. Eduard Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnawali_D/0/1/0/all/0/1\">Dr. Omprakash Gnawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1\">Dr. Arjun Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07790","description":"<p>Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08468","description":"<p>We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10750","description":"<p>Relation Extraction (RE) from tables is the task of identifying relations\nbetween pairs of columns of a table. Generally, RE models for this task require\nlabelled tables for training. These labelled tables can also be generated\nartificially from a Knowledge Graph (KG), which makes the cost to acquire them\nmuch lower in comparison to manual annotations. However, unlike real tables,\nthese synthetic tables lack associated metadata, such as, column-headers,\ncaptions, etc; this is because synthetic tables are created out of KGs that do\nnot store such metadata. Meanwhile, previous works have shown that metadata is\nimportant for accurate RE from tables. To address this issue, we propose\nmethods to artificially create some of this metadata for synthetic tables.\nAfterward, we experiment with a BERT-based model, in line with recently\npublished works, that takes as input a combination of proposed artificial\nmetadata and table content. Our empirical results show that this leads to an\nimprovement of 9\\%-45\\% in F1 score, in absolute terms, over 2 tabular\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gaurav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Joshua Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work claims, our auxiliary experiments suggest that relation\nprediction is contributory to named entity prediction in a non-negligible way.\nThe source code can be found at https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization. (arXiv:2108.13139v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13139","description":"<p>Dialogue summarization has drawn much attention recently. Especially in the\ncustomer service domain, agents could use dialogue summaries to help boost\ntheir works by quickly knowing customer's issues and service progress. These\napplications require summaries to contain the perspective of a single speaker\nand have a clear topic flow structure, while neither are available in existing\ndatasets. Therefore, in this paper, we introduce a novel Chinese dataset for\nCustomer Service Dialogue Summarization (CSDS). CSDS improves the abstractive\nsummaries in two aspects: (1) In addition to the overall summary for the whole\ndialogue, role-oriented summaries are also provided to acquire different\nspeakers' viewpoints. (2) All the summaries sum up each topic separately, thus\ncontaining the topic-level structure of the dialogue. We define tasks in CSDS\nas generating the overall summary and different role-oriented summaries for a\ngiven dialogue. Next, we compare various summarization methods on CSDS, and\nexperiment results show that existing methods are prone to generate redundant\nand incoherent summaries. Besides, the performance becomes much worse when\nanalyzing the performance on role-oriented summaries and topic structures. We\nhope that this study could benchmark Chinese dialogue summarization and benefit\nfurther studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqun Ma</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junnan Zhu</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a> (1 and 2) ((1) National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China, (2) School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China, (3) Fanyu AI Laboratory, Zhongke Fanyu Technology Co., Ltd, Beijing, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13875","description":"<p>Scenario-based question answering (SQA) requires retrieving and reading\nparagraphs from a large corpus to answer a question which is contextualized by\na long scenario description. Since a scenario contains both keyphrases for\nretrieval and much noise, retrieval for SQA is extremely difficult. Moreover,\nit can hardly be supervised due to the lack of relevance labels of paragraphs\nfor SQA. To meet the challenge, in this paper we propose a joint\nretriever-reader model called JEEVES where the retriever is implicitly\nsupervised only using QA labels via a novel word weighting mechanism. JEEVES\nsignificantly outperforms a variety of strong baselines on multiple-choice\nquestions in three SQA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00430","description":"<p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guojun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning. (arXiv:2109.00840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00840","description":"<p>Though language model text embeddings have revolutionized NLP research, their\nability to capture high-level semantic information, such as relations between\nentities in text, is limited. In this paper, we propose a novel contrastive\nlearning framework that trains sentence embeddings to encode the relations in a\ngraph structure. Given a sentence (unstructured text) and its graph, we use\ncontrastive learning to impose relation-related structure on the token-level\nrepresentations of the sentence obtained with a CharacterBERT (El Boukkouri et\nal.,2020) model. The resulting relation-aware sentence embeddings achieve\nstate-of-the-art results on the relation extraction task using only a simple\nKNN classifier, thereby demonstrating the success of the proposed method.\nAdditional visualization by a tSNE analysis shows the effectiveness of the\nlearned representation space compared to baselines. Furthermore, we show that\nwe can learn a different space for named entity recognition, again using a\ncontrastive learning objective, and demonstrate how to successfully combine\nboth representation spaces in an entity-relation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coman_A/0/1/0/all/0/1\">Andrei C. Coman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation. (arXiv:2109.01048v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01048","description":"<p>Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(HKLM), a unified pre-trained language model (PLM) for all forms of text,\nincluding unstructured text, semi-structured text and well-structured text. To\ncapture the corresponding relations among these multi-format knowledge, our\napproach uses masked language model objective to learn word knowledge, uses\ntriple classification objective and title matching objective to learn entity\nknowledge and topic knowledge respectively. To obtain the aforementioned\nmulti-format text, we construct a corpus in the tourism domain and conduct\nexperiments on 5 tourism NLP datasets. The results show that our approach\noutperforms the pre-training of plain text using only 1/4 of the data. The\ncode, datasets, corpus and knowledge graph will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualized Embeddings based Convolutional Neural Networks for Duplicate Question Identification. (arXiv:2109.01560v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01560","description":"<p>Question Paraphrase Identification (QPI) is a critical task for large-scale\nQuestion-Answering forums. The purpose of QPI is to determine whether a given\npair of questions are semantically identical or not. Previous approaches for\nthis task have yielded promising results, but have often relied on complex\nrecurrence mechanisms that are expensive and time-consuming in nature. In this\npaper, we propose a novel architecture combining a Bidirectional Transformer\nEncoder with Convolutional Neural Networks for the QPI task. We produce the\npredictions from the proposed architecture using two different inference\nsetups: Siamese and Matched Aggregation. Experimental results demonstrate that\nour model achieves state-of-the-art performance on the Quora Question Pairs\ndataset. We empirically prove that the addition of convolution layers to the\nmodel architecture improves the results in both inference setups. We also\ninvestigate the impact of partial and complete fine-tuning and analyze the\ntrade-off between computational power and accuracy in the process. Based on the\nobtained results, we conclude that the Matched-Aggregation setup consistently\noutperforms the Siamese setup. Our work provides insights into what\narchitecture combinations and setups are likely to produce better results for\nthe QPI task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakhrani_H/0/1/0/all/0/1\">Harsh Sakhrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Saloni Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratadiya_P/0/1/0/all/0/1\">Pratik Ratadiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/1106.0107","description":"<p>Handwritten character recognition is always a frontier area of research in\nthe field of pattern recognition and image processing and there is a large\ndemand for OCR on hand written documents. Even though, sufficient studies have\nperformed in foreign scripts like Chinese, Japanese and Arabic characters, only\na very few work can be traced for handwritten character recognition of Indian\nscripts especially for the South Indian scripts. This paper provides an\noverview of offline handwritten character recognition in South Indian Scripts,\nnamely Malayalam, Tamil, Kannada and Telungu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jomy_J/0/1/0/all/0/1\">John Jomy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramod_K/0/1/0/all/0/1\">K. V. Pramod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_B/0/1/0/all/0/1\">Balakrishnan Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}