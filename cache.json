{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Text Simplification for Comprehension-based Question-Answering. (arXiv:2109.13984v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13984","description":"<p>Text simplification is the process of splitting and rephrasing a sentence to\na sequence of sentences making it easier to read and understand while\npreserving the content and approximating the original meaning. Text\nsimplification has been exploited in NLP applications like machine translation,\nsummarization, semantic role labeling, and information extraction, opening a\nbroad avenue for its exploitation in comprehension-based question-answering\ndownstream tasks. In this work, we investigate the effect of text\nsimplification in the task of question-answering using a comprehension context.\nWe release Simple-SQuAD, a simplified version of the widely-used SQuAD dataset.\n</p>\n<p>Firstly, we outline each step in the dataset creation pipeline, including\nstyle transfer, thresholding of sentences showing correct transfer, and offset\nfinding for each answer. Secondly, we verify the quality of the transferred\nsentences through various methodologies involving both automated and human\nevaluation. Thirdly, we benchmark the newly created corpus and perform an\nablation study for examining the effect of the simplification process in the\nSQuAD-based question answering task. Our experiments show that simplification\nleads to up to 2.04% and 1.74% increase in Exact Match and F1, respectively.\nFinally, we conclude with an analysis of the transfer process, investigating\nthe types of edits made by the model, and the effect of sentence length on the\ntransfer model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dadu_T/0/1/0/all/0/1\">Tanvi Dadu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pant_K/0/1/0/all/0/1\">Kartikey Pant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagar_S/0/1/0/all/0/1\">Seema Nagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbhuiya_F/0/1/0/all/0/1\">Ferdous Ahmed Barbhuiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_K/0/1/0/all/0/1\">Kuntal Dey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations. (arXiv:2109.14017v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14017","description":"<p>Recent research has adopted a new experimental field centered around the\nconcept of text perturbations which has revealed that shuffled word order has\nlittle to no impact on the downstream performance of Transformer-based language\nmodels across many NLP tasks. These findings contradict the common\nunderstanding of how the models encode hierarchical and structural information\nand even question if the word order is modeled with position embeddings. To\nthis end, this paper proposes nine probing datasets organized by the type of\n\\emph{controllable} text perturbation for three Indo-European languages with a\nvarying degree of word order flexibility: English, Swedish and Russian. Based\non the probing analysis of the M-BERT and M-BART models, we report that the\nsyntactic sensitivity depends on the language and model pre-training\nobjectives. We also find that the sensitivity grows across layers together with\nthe increase of the perturbation granularity. Last but not least, we show that\nthe models barely use the positional information to induce syntactic trees from\ntheir intermediate self-attention and contextualized representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taktasheva_E/0/1/0/all/0/1\">Ekaterina Taktasheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marked Attribute Bias in Natural Language Inference. (arXiv:2109.14039v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14039","description":"<p>Reporting and providing test sets for harmful bias in NLP applications is\nessential for building a robust understanding of the current problem. We\npresent a new observation of gender bias in a downstream NLP application:\nmarked attribute bias in natural language inference. Bias in downstream\napplications can stem from training data, word embeddings, or be amplified by\nthe model in use. However, focusing on biased word embeddings is potentially\nthe most impactful first step due to their universal nature. Here we seek to\nunderstand how the intrinsic properties of word embeddings contribute to this\nobserved marked attribute effect, and whether current post-processing methods\naddress the bias successfully. An investigation of the current debiasing\nlandscape reveals two open problems: none of the current debiased embeddings\nmitigate the marked attribute error, and none of the intrinsic bias measures\nare predictive of the marked attribute effect. By noticing that a new type of\nintrinsic bias measure correlates meaningfully with the marked attribute\neffect, we propose a new postprocessing debiasing scheme for static word\nembeddings. The proposed method applied to existing embeddings achieves new\nbest results on the marked attribute bias test set. See\nhttps://github.com/hillary-dawkins/MAB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dawkins_H/0/1/0/all/0/1\">Hillary Dawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Second Order WinoBias (SoWinoBias) Test Set for Latent Gender Bias Detection in Coreference Resolution. (arXiv:2109.14047v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14047","description":"<p>We observe an instance of gender-induced bias in a downstream application,\ndespite the absence of explicit gender words in the test cases. We provide a\ntest set, SoWinoBias, for the purpose of measuring such latent gender bias in\ncoreference resolution systems. We evaluate the performance of current\ndebiasing methods on the SoWinoBias test set, especially in reference to the\nmethod's design and altered embedding space properties. See\nhttps://github.com/hillarydawkins/SoWinoBias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dawkins_H/0/1/0/all/0/1\">Hillary Dawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Summaries for Scientific Paper Review. (arXiv:2109.14059v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14059","description":"<p>The review process is essential to ensure the quality of publications.\nRecently, the increase of submissions for top venues in machine learning and\nNLP has caused a problem of excessive burden on reviewers and has often caused\nconcerns regarding how this may not only overload reviewers, but also may\naffect the quality of the reviews. An automatic system for assisting with the\nreviewing process could be a solution for ameliorating the problem. In this\npaper, we explore automatic review summary generation for scientific papers. We\nposit that neural language models have the potential to be valuable candidates\nfor this task. In order to test this hypothesis, we release a new dataset of\nscientific papers and their reviews, collected from papers published in the\nNeurIPS conference from 2013 to 2020. We evaluate state of the art neural\nsummarization models, present initial results on the feasibility of automatic\nreview summary generation, and propose directions for the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uban_A/0/1/0/all/0/1\">Ana Sabina Uban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAFT: A Real-World Few-Shot Text Classification Benchmark. (arXiv:2109.14076v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14076","description":"<p>Large pre-trained language models have shown promise for few-shot learning,\ncompleting text-based tasks given only a few task-specific examples. Will\nmodels soon solve classification tasks that have so far been reserved for human\nresearch assistants? Existing benchmarks are not designed to measure progress\nin applied settings, and so don't directly answer this question. The RAFT\nbenchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring\ntasks and uses an evaluation setup that mirrors deployment. Baseline\nevaluations on RAFT reveal areas current techniques struggle with: reasoning\nover long texts and tasks with many classes. Human baselines show that some\nclassification tasks are difficult for non-expert humans, reflecting that\nreal-world value sometimes depends on domain expertise. Yet even non-expert\nhuman baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets\nand leaderboard will track which model improvements translate into real-world\nbenefits at https://raft.elicit.org .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alex_N/0/1/0/all/0/1\">Neel Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lifland_E/0/1/0/all/0/1\">Eli Lifland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhishek Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maham_P/0/1/0/all/0/1\">Pegah Maham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_C/0/1/0/all/0/1\">C. Jess Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hine_E/0/1/0/all/0/1\">Emmie Hine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashurst_C/0/1/0/all/0/1\">Carolyn Ashurst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedille_P/0/1/0/all/0/1\">Paul Sedille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlier_A/0/1/0/all/0/1\">Alexis Carlier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noetel_M/0/1/0/all/0/1\">Michael Noetel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhlmuller_A/0/1/0/all/0/1\">Andreas Stuhlm&#xfc;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14084","description":"<p>We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_F/0/1/0/all/0/1\">Florian Metze Luke Zettlemoyer Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Video-Language Segmentation. (arXiv:2109.14131v1 [cs.CV])","link":"http://arxiv.org/abs/2109.14131","description":"<p>We focus on the problem of segmenting a certain object referred by a natural\nlanguage sentence in video content, at the core of formulating a pinpoint\nvision-language relation. While existing attempts mainly construct such\nrelation in an implicit way, i.e., grid-level multi-modal feature fusion, it\nhas been proven problematic to distinguish semantically similar objects under\nthis paradigm. In this work, we propose to interwind the visual and linguistic\nmodalities in an explicit way via the contrastive learning objective, which\ndirectly aligns the referred object and the language description and separates\nthe unreferred content apart across frames. Moreover, to remedy for the\ndegradation problem, we present two complementary hard instance mining\nstrategies, i.e., Language-relevant Channel Filter and Relative Hard Instance\nConstruction. They encourage the network to exclude visual-distinguishable\nfeature and to focus on easy-confused objects during the contrastive training.\nExtensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB\nSentences, quantitatively demonstrate the state-of-the-arts performance of our\nmethod and qualitatively show the more accurate distinguishment between\nsemantically similar objects over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Dialogue State Tracking by Joint Slot Modeling. (arXiv:2109.14144v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14144","description":"<p>Dialogue state tracking models play an important role in a task-oriented\ndialogue system. However, most of them model the slot types conditionally\nindependently given the input. We discover that it may cause the model to be\nconfused by slot types that share the same data type. To mitigate this issue,\nwe propose TripPy-MRF and TripPy-LSTM that models the slots jointly. Our\nresults show that they are able to alleviate the confusion mentioned above, and\nthey push the state-of-the-art on dataset MultiWoZ 2.1 from 58.7 to 61.3. Our\nimplementation is available at https://github.com/CTinRay/Trippy-Joint.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Arabic Diacritization by Learning to Diacritize and Translate. (arXiv:2109.14150v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14150","description":"<p>We propose a novel multitask learning method for diacritization which trains\na model to both diacritize and translate. Our method addresses data sparsity by\nexploiting large, readily available bitext corpora. Furthermore, translation\nrequires implicit linguistic and semantic knowledge, which is helpful for\nresolving ambiguities in the diacritization task. We apply our method to the\nPenn Arabic Treebank and report a new state-of-the-art word error rate of\n4.79%. We also conduct manual and automatic analysis to better understand our\nmethod and highlight some of the remaining challenges in diacritization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">Brian Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshehri_A/0/1/0/all/0/1\">Ali Alshehri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflexivity in Issues of Scale and Representation in a Digital Humanities Project. (arXiv:2109.14184v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14184","description":"<p>In this paper, we explore issues that we have encountered in developing a\npipeline that combines natural language processing with data analysis and\nvisualization techniques. The characteristics of the corpus - being comprised\nof diaries of a single person spanning several decades - present both\nconceptual challenges in terms of issues of representation, and affordances as\na source for historical research. We consider these issues in a team context\nwith a particular focus on the generation and interpretation of visualizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Annie T. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_C/0/1/0/all/0/1\">Camille Lyans Cole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context based Roman-Urdu to Urdu Script Transliteration System. (arXiv:2109.14197v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14197","description":"<p>Now a day computer is necessary for human being and it is very useful in many\nfields like search engine, text processing, short messaging services, voice\nchatting and text recognition. Since last many years there are many tools and\ntechniques that have been developed to support the writing of language script.\nMost of the Asian languages like Arabic, Urdu, Persian, Chains and Korean are\nwritten in Roman alphabets. Roman alphabets are the most commonly used for\ntransliteration of languages, which have non-Latin scripts. For writing Urdu\ncharacters as an input, there are many layouts which are already exist. Mostly\nUrdu speaker prefer to use Roman-Urdu for different applications, because\nmostly user is not familiar with Urdu language keyboard. The objective of this\nwork is to improve the context base transliteration of Roman-Urdu to Urdu\nscript. In this paper, we propose an algorithm which effectively solve the\ntransliteration issues. The algorithm work like, convert the encoding roman\nwords into the words in the standard Urdu script and match it with the lexicon.\nIf match found, then display the word in the text editor. The highest frequency\nwords are displayed if more than one match found in the lexicon. Display the\nfirst encoded and converted instance and set it to the default if there is not\na single instance of the match is found and then adjust the given ambiguous\nword to their desire location according to their context. The outcome of this\nalgorithm proved the efficiency and significance as compare to other models and\nalgorithms which work for transliteration of Raman-Urdu to Urdu on context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shakeel_H/0/1/0/all/0/1\">H Muhammad Shakeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Rashid Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waheed_M/0/1/0/all/0/1\">Muhammad Waheed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who says like a style of Vitamin: Towards Syntax-Aware DialogueSummarization using Multi-task Learning. (arXiv:2109.14199v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14199","description":"<p>Abstractive dialogue summarization is a challenging task for several reasons.\nFirst, most of the important pieces of information in a conversation are\nscattered across utterances through multi-party interactions with different\ntextual styles. Second, dialogues are often informal structures, wherein\ndifferent individuals express personal perspectives, unlike text summarization,\ntasks that usually target formal documents such as news articles. To address\nthese issues, we focused on the association between utterances from individual\nspeakers and unique syntactic structures. Speakers have unique textual styles\nthat can contain linguistic information, such as voiceprint. Therefore, we\nconstructed a syntax-aware model by leveraging linguistic information (i.e.,\nPOS tagging), which alleviates the above issues by inherently distinguishing\nsentences uttered from individual speakers. We employed multi-task learning of\nboth syntax-aware information and dialogue summarization. To the best of our\nknowledge, our approach is the first method to apply multi-task learning to the\ndialogue summarization task. Experiments on a SAMSum corpus (a large-scale\ndialogue summarization corpus) demonstrated that our method improved upon the\nvanilla model. We further analyze the costs and benefits of our approach\nrelative to baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seolhwa Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kisu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation. (arXiv:2109.14200v1 [eess.AS])","link":"http://arxiv.org/abs/2109.14200","description":"<p>Decades of research has studied how language learning infants learn to\ndiscriminate speech sounds, segment words, and associate words with their\nmeanings. While gradual development of such capabilities is unquestionable, the\nexact nature of these skills and the underlying mental representations yet\nremains unclear. In parallel, computational studies have shown that basic\ncomprehension of speech can be achieved by statistical learning between speech\nand concurrent referentially ambiguous visual input. These models can operate\nwithout prior linguistic knowledge such as representations of linguistic units,\nand without learning mechanisms specifically targeted at such units. This has\nraised the question of to what extent knowledge of linguistic units, such as\nphone(me)s, syllables, and words, could actually emerge as latent\nrepresentations supporting the translation between speech and representations\nin other modalities, and without the units being proximal learning targets for\nthe learner. In this study, we formulate this idea as the so-called latent\nlanguage hypothesis (LLH), connecting linguistic representation learning to\ngeneral predictive processing within and across sensory modalities. We review\nthe extent that the audiovisual aspect of LLH is supported by the existing\ncomputational studies. We then explore LLH further in extensive learning\nsimulations with different neural network models for audiovisual\ncross-situational learning, and comparing learning from both synthetic and real\nspeech data. We investigate whether the latent representations learned by the\nnetworks reflect phonetic, syllabic, or lexical structure of input speech by\nutilizing an array of complementary evaluation metrics related to linguistic\nselectivity and temporal characteristics of the representations. As a result,\nwe find that representations associated...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khorrami_K/0/1/0/all/0/1\">Khazar Khorrami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLEU, METEOR, BERTScore: Evaluation of Metrics Performance in Assessing Critical Translation Errors in Sentiment-oriented Text. (arXiv:2109.14250v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14250","description":"<p>Social media companies as well as authorities make extensive use of\nartificial intelligence (AI) tools to monitor postings of hate speech,\ncelebrations of violence or profanity. Since AI software requires massive\nvolumes of data to train computers, Machine Translation (MT) of the online\ncontent is commonly used to process posts written in several languages and\nhence augment the data needed for training. However, MT mistakes are a regular\noccurrence when translating sentiment-oriented user-generated content (UGC),\nespecially when a low-resource language is involved. The adequacy of the whole\nprocess relies on the assumption that the evaluation metrics used give a\nreliable indication of the quality of the translation. In this paper, we assess\nthe ability of automatic quality metrics to detect critical machine translation\nerrors which can cause serious misunderstanding of the affect message. We\ncompare the performance of three canonical metrics on meaningless translations\nwhere the semantic content is seriously impaired as compared to meaningful\ntranslations with a critical error which exclusively distorts the sentiment of\nthe source text. We conclude that there is a need for fine-tuning of automatic\nmetrics to make them more robust in detecting sentiment critical errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Orasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Character Tagger for Short Text Spelling Error Correction. (arXiv:2109.14259v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14259","description":"<p>State-of-the-art approaches to spelling error correction problem include\nTransformer-based Seq2Seq models, which require large training sets and suffer\nfrom slow inference time; and sequence labeling models based on Transformer\nencoders like BERT, which involve token-level label space and therefore a large\npre-defined vocabulary dictionary. In this paper we present a Hierarchical\nCharacter Tagger model, or HCTagger, for short text spelling error correction.\nWe use a pre-trained language model at the character level as a text encoder,\nand then predict character-level edits to transform the original text into its\nerror-free form with a much smaller label space. For decoding, we propose a\nhierarchical multi-task approach to alleviate the issue of long-tail label\ndistribution without introducing extra model parameters. Experiments on two\npublic misspelling correction datasets demonstrate that HCTagger is an accurate\nand much faster approach than many existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mengyi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Call Larisa Ivanovna: Code-Switching Fools Multilingual NLU Models. (arXiv:2109.14350v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14350","description":"<p>Practical needs of developing task-oriented dialogue assistants require the\nability to understand many languages. Novel benchmarks for multilingual natural\nlanguage understanding (NLU) include monolingual sentences in several\nlanguages, annotated with intents and slots. In such setup models for\ncross-lingual transfer show remarkable performance in joint intent recognition\nand slot filling. However, existing benchmarks lack of code-switched\nutterances, which are difficult to gather and label due to complexity in the\ngrammatical structure. The evaluation of NLU models seems biased and limited,\nsince code-switching is being left out of scope.\n</p>\n<p>Our work adopts recognized methods to generate plausible and\nnaturally-sounding code-switched utterances and uses them to create a synthetic\ncode-switched test set. Based on experiments, we report that the\nstate-of-the-art NLU models are unable to handle code-switching. At worst, the\nperformance, evaluated by semantic accuracy, drops as low as 15\\% from 80\\%\nacross languages. Further we show, that pre-training on synthetic code-mixed\ndata helps to maintain performance on the proposed test set at a comparable\nlevel with monolingual data. Finally, we analyze different language pairs and\nshow that the closer the languages are, the better the NLU model handles their\nalternation. This is in line with the common understanding of how multilingual\nmodels conduct transferring between languages\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Birshert_A/0/1/0/all/0/1\">Alexey Birshert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Fact Linking. (arXiv:2109.14364v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14364","description":"<p>Knowledge-intensive NLP tasks can benefit from linking natural language text\nwith facts from a Knowledge Graph (KG). Although facts themselves are\nlanguage-agnostic, the fact labels (i.e., language-specific representation of\nthe fact) in the KG are often present only in a few languages. This makes it\nchallenging to link KG facts to sentences in languages other than the limited\nset of languages. To address this problem, we introduce the task of\nMultilingual Fact Linking (MFL) where the goal is to link fact expressed in a\nsentence to corresponding fact in the KG, even when the fact label in the KG is\nnot available in the language of the sentence. To facilitate research in this\narea, we present a new evaluation dataset, IndicLink. This dataset contains\n11,293 linked WikiData facts and 6,429 sentences spanning English and six\nIndian languages. We propose a Retrieval+Generation model, ReFCoG, that can\nscale to millions of KG facts by combining Dual Encoder based retrieval with a\nSeq2Seq based generation model which is constrained to output only valid KG\nfacts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in\nPrecision@1. In spite of this gain, the model achieves an overall score of\n52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink\ndata are available at https://github.com/SaiKeshav/mfl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolluru_K/0/1/0/all/0/1\">Keshav Kolluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezk_M/0/1/0/all/0/1\">Martin Rezk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EdinSaar@WMT21: North-Germanic Low-Resource Multilingual NMT. (arXiv:2109.14368v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14368","description":"<p>We describe the EdinSaar submission to the shared task of Multilingual\nLow-Resource Translation for North Germanic Languages at the Sixth Conference\non Machine Translation (WMT2021). We submit multilingual translation models for\ntranslations to/from Icelandic (is), Norwegian-Bokmal (nb), and Swedish (sv).\nWe employ various experimental approaches, including multilingual pre-training,\nback-translation, fine-tuning, and ensembling. In most translation directions,\nour models outperform other submitted systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tchistiakova_S/0/1/0/all/0/1\">Svetlana Tchistiakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_K/0/1/0/all/0/1\">Koel Dutta Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDGAR-CORPUS: Billions of Tokens Make The World Go Round. (arXiv:2109.14394v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14394","description":"<p>We release EDGAR-CORPUS, a novel corpus comprising annual reports from all\nthe publicly traded companies in the US spanning a period of more than 25\nyears. To the best of our knowledge, EDGAR-CORPUSis the largest financial NLP\ncorpus available to date. All the reports are downloaded, split into their\ncorresponding items (sections), and provided in a clean, easy-to-use JSON\nformat. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC\nembeddings for the financial domain. We employ these embeddings in a battery of\nfinancial NLP tasks and showcase their superiority over generic GloVe\nembeddings and other existing financial word embeddings. We also open-source\nEDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future\nannual reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1\">Lefteris Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1\">Prodromos Malakasiotis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StoryDB: Broad Multi-language Narrative Dataset. (arXiv:2109.14396v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14396","description":"<p>This paper presents StoryDB - a broad multi-language dataset of narratives.\nStoryDB is a corpus of texts that includes stories in 42 different languages.\nEvery language includes 500+ stories. Some of the languages include more than\n20 000 stories. Every story is indexed across languages and labeled with tags\nsuch as a genre or a topic. The corpus shows rich topical and language\nvariation and can serve as a resource for the study of the role of narrative in\nnatural language processing across various languages including low resource\nones. We also demonstrate how the dataset could be used to benchmark three\nmodern multilanguage models, namely, mDistillBERT, mBERT, and XLM-RoBERTa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiQUE: Biquaternionic Embeddings of Knowledge Graphs. (arXiv:2109.14401v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14401","description":"<p>Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge\ngraphs (KGs). Existing KGE models rely on geometric operations to model\nrelational patterns. Euclidean (circular) rotation is useful for modeling\npatterns such as symmetry, but cannot represent hierarchical semantics. In\ncontrast, hyperbolic models are effective at modeling hierarchical relations,\nbut do not perform as well on patterns on which circular rotation excels. It is\ncrucial for KGE models to unify multiple geometric transformations so as to\nfully cover the multifarious relations in KGs. To do so, we propose BiQUE, a\nnovel model that employs biquaternions to integrate multiple geometric\ntransformations, viz., scaling, translation, Euclidean rotation, and hyperbolic\nrotation. BiQUE makes the best trade-offs among geometric operators during\ntraining, picking the best one (or their best combination) for each relation.\nExperiments on five datasets show BiQUE's effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kok_S/0/1/0/all/0/1\">Stanley Kok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14420","description":"<p>Error correction is widely used in automatic speech recognition (ASR) to\npost-process the generated sentence, and can further reduce the word error rate\n(WER). Although multiple candidates are generated by an ASR system through beam\nsearch, current error correction approaches can only correct one sentence at a\ntime, failing to leverage the voting effect from multiple candidates to better\ndetect and correct error tokens. In this work, we propose FastCorrect 2, an\nerror correction model that takes multiple ASR candidates as input for better\ncorrection accuracy. FastCorrect 2 adopts non-autoregressive generation for\nfast inference, which consists of an encoder that processes multiple source\nsentences and a decoder that generates the target sentence in parallel from the\nadjusted source sentence, where the adjustment is based on the predicted\nduration of each source token. However, there are some issues when handling\nmultiple source sentences. First, it is non-trivial to leverage the voting\neffect from multiple source sentences since they usually vary in length. Thus,\nwe propose a novel alignment algorithm to maximize the degree of token\nalignment among multiple sentences in terms of token and pronunciation\nsimilarity. Second, the decoder can only take one adjusted source sentence as\ninput, while there are multiple source sentences. Thus, we develop a candidate\npredictor to detect the most suitable candidate for the decoder. Experiments on\nour inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce\nthe WER over the previous correction model with single candidate by 3.2% and\n2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR\nerror correction. FastCorrect 2 achieves better performance than the cascaded\nre-scoring and correction pipeline and can serve as a unified post-processing\nmodule for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the Arabic Sentiment Analysis 2021 Competition at KAUST. (arXiv:2109.14456v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14456","description":"<p>This paper provides an overview of the Arabic Sentiment Analysis Challenge\norganized by King Abdullah University of Science and Technology (KAUST). The\ntask in this challenge is to develop machine learning models to classify a\ngiven tweet into one of the three categories Positive, Negative, or Neutral.\nFrom our recently released ASAD dataset, we provide the competitors with 55K\ntweets for training, and 20K tweets for validation, based on which the\nperformance of participating teams are ranked on a leaderboard,\nhttps://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust. The competition\nreceived in total 1247 submissions from 74 teams (99 team members). The final\nwinners are determined by another private set of 20K tweets that have the same\ndistribution as the training and validation set. In this paper, we present the\nmain findings in the competition and summarize the methods and tools used by\nthe top ranked teams. The full dataset of 100K labeled tweets is also released\nfor public usage, at\nhttps://www.kaggle.com/c/arabic-sentiment-analysis-2021-kaust/data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alamro_H/0/1/0/all/0/1\">Hind Alamro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshehri_M/0/1/0/all/0/1\">Manal Alshehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alharbi_B/0/1/0/all/0/1\">Basma Alharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khayyat_Z/0/1/0/all/0/1\">Zuhair Khayyat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalkatawi_M/0/1/0/all/0/1\">Manal Kalkatawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaber_I/0/1/0/all/0/1\">Inji Ibrahim Jaber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition. (arXiv:2004.10663v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.10663","description":"<p>We present a fast and scalable architecture called Explicit Modular\nDecomposition (EMD), in which we incorporate both classification-based and\nextraction-based methods and design four modules (for classification and\nsequence labelling) to jointly extract dialogue states. Experimental results\nbased on the MultiWoz 2.0 dataset validates the superiority of our proposed\nmodel in terms of both complexity and scalability when compared to the\nstate-of-the-art methods, especially in the scenario of multi-domain dialogues\nentangled with many turns of utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems. (arXiv:2010.05740v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.05740","description":"<p>Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue\nsystems is challenging, since it requires to properly represent the entity of\nKB, which is associated with its KB context and dialogue context. The existing\nworks represent the entity with only perceiving a part of its KB context, which\ncan lead to the less effective representation due to the information loss, and\nadversely favor KB reasoning and response generation. To tackle this issue, we\nexplore to fully contextualize the entity representation by dynamically\nperceiving all the relevant entities} and dialogue history. To achieve this, we\npropose a COntext-aware Memory Enhanced Transformer framework (COMET), which\ntreats the KB as a sequence and leverages a novel Memory Mask to enforce the\nentity to only focus on its relevant entities and dialogue history, while\navoiding the distraction from the irrelevant entities. Through extensive\nexperiments, we show that our COMET framework can achieve superior performance\nover the state of the arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1\">Yanjie Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunxu Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence Patterns for Explaining Information Flow in BERT. (arXiv:2011.00740v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.00740","description":"<p>While attention is all you need may be proving true, we do not know why:\nattention-based transformer models such as BERT are superior but how\ninformation flows from input tokens to output predictions are unclear. We\nintroduce influence patterns, abstractions of sets of paths through a\ntransformer model. Patterns quantify and localize the flow of information to\npaths passing through a sequence of model nodes. Experimentally, we find that\nsignificant portion of information flow in BERT goes through skip connections\ninstead of attention heads. We further show that consistency of patterns across\ninstances is an indicator of BERT's performance. Finally, We demonstrate that\npatterns account for far more model performance than previous attention-based\nand layer-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kaiji Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardziel_P/0/1/0/all/0/1\">Piotr Mardziel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Anupam Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing and Recomposing Event Structure. (arXiv:2103.10387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.10387","description":"<p>We present an event structure classification empirically derived from\ninferential properties annotated on sentence- and document-level Universal\nDecompositional Semantics (UDS) graphs. We induce this classification jointly\nwith semantic role, entity, and event-event relation classifications using a\ndocument-level generative model structured by these graphs. To support this\ninduction, we augment existing annotations found in the UDS1.0 dataset, which\ncovers the entirety of the English Web Treebank, with an array of inferential\nproperties capturing fine-grained aspects of the temporal and aspectual\nstructure of events. The resulting dataset (available at decomp.io) is the\nlargest annotation of event structure and (partial) event coreference to date.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gantt_W/0/1/0/all/0/1\">William Gantt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_L/0/1/0/all/0/1\">Lelia Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09996","description":"<p>We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training. Code is\nmade available at https://github.com/pytorch/fairseq/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1\">Prahal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1\">Masoumeh Aminzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Various Tokenizers for Arabic Text Classification. (arXiv:2106.07540v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07540","description":"<p>The first step in any NLP pipeline is to split the text into individual\ntokens. The most obvious and straightforward approach is to use words as\ntokens. However, given a large text corpus, representing all the words is not\nefficient in terms of vocabulary size. In the literature, many tokenization\nalgorithms have emerged to tackle this problem by creating subwords which in\nturn limits the vocabulary size in a given text corpus. Most tokenization\ntechniques are language-agnostic i.e they don't incorporate the linguistic\nfeatures of a given language. Not to mention the difficulty of evaluating such\ntechniques in practice. In this paper, we introduce three new tokenization\nalgorithms for Arabic and compare them to three other baselines using\nunsupervised evaluations. In addition to that, we compare all the six\nalgorithms by evaluating them on three supervised classification tasks which\nare sentiment analysis, news classification and poetry classification using six\npublicly available datasets. Our experiments show that none of the tokenization\ntechnique is the best choice overall and that the performance of a given\ntokenization algorithm depends on the size of the dataset, type of the task,\nand the amount of morphology that exists in the dataset. However, some\ntokenization techniques are better overall as compared to others on various\ntext classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1\">Maged S. Al-shaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaleb_M/0/1/0/all/0/1\">Mustafa Ghaleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Irfan Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark. (arXiv:2107.07498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07498","description":"<p>Pretrained Language Models (PLMs) have achieved tremendous success in natural\nlanguage understanding tasks. While different learning schemes -- fine-tuning,\nzero-shot, and few-shot learning -- have been widely explored and compared for\nlanguages such as English, there is comparatively little work in Chinese to\nfairly and comprehensively evaluate and compare these methods and thus hinders\ncumulative progress. In this paper, we introduce the Chinese Few-shot Learning\nEvaluation Benchmark (FewCLUE), the first comprehensive few-shot evaluation\nbenchmark in Chinese. It includes nine tasks, ranging from single-sentence and\nsentence-pair classification tasks to machine reading comprehension tasks. We\nsystematically evaluate five state-of-the-art (SOTA) few-shot learning methods\n(including PET, ADAPET, LM-BFF, P-tuning and EFL), and compare their\nperformance with fine-tuning and zero-shot learning schemes on the newly\nconstructed FewCLUE benchmark. Experimental results reveal that: 1) The effect\nof different few-shot learning methods is sensitive to the pre-trained model to\nwhich the methods are applied; 2) PET and P-tuning achieve the best overall\nperformance with RoBERTa and ERNIE respectively. Our benchmark is used in the\nfew-shot learning contest of NLPCC 2021. In addition, we provide a\nuser-friendly toolkit, as well as an online leaderboard to help facilitate\nfurther progress on Chinese few-shot learning. We provide a baseline\nperformance on different learning methods, a reference for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaojing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chenyang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huilin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guoao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hai_H/0/1/0/all/0/1\">Hu Hai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter. (arXiv:2108.04816v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2108.04816","description":"<p>The understanding of the public response to COVID-19 vaccines is the key\nsuccess factor to control the COVID-19 pandemic. To understand the public\nresponse, there is a need to explore public opinion. Traditional surveys are\nexpensive and time-consuming, address limited health topics, and obtain\nsmall-scale data. Twitter can provide a great opportunity to understand public\nopinion regarding COVID-19 vaccines. The current study proposes an approach\nusing computational and human coding methods to collect and analyze a large\nnumber of tweets to provide a wider perspective on the COVID-19 vaccine. This\nstudy identifies the sentiment of tweets using a machine learning rule-based\napproach, discovers major topics, explores temporal trend and compares topics\nof negative and non-negative tweets using statistical tests, and discloses top\ntopics of tweets having negative and non-negative sentiment. Our findings show\nthat the negative sentiment regarding the COVID-19 vaccine had a decreasing\ntrend between November 2020 and February 2021. We found Twitter users have\ndiscussed a wide range of topics from vaccination sites to the 2020 U.S.\nelection between November 2020 and February 2021. The findings show that there\nwas a significant difference between tweets having negative and non-negative\nsentiment regarding the weight of most topics. Our results also indicate that\nthe negative and non-negative tweets had different topic priorities and\nfocuses. This research illustrates that Twitter data can be used to explore\npublic opinion regarding the COVID-19 vaccine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1\">Amir Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1\">Bailey Goldschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1\">Hannah R. Boyajieff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafabadi_M/0/1/0/all/0/1\">Mahdi M. Najafabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13602","description":"<p>Adversarial training (AT) is one of the most reliable methods for defending\nagainst adversarial attacks in machine learning. Variants of this method have\nbeen used as regularization mechanisms to achieve SOTA results on NLP\nbenchmarks, and they have been found to be useful for transfer learning and\ncontinual learning. We search for the reasons for the effectiveness of AT by\ncontrasting vanilla and adversarially fine-tuned BERT models. We identify\npartial preservation of BERT's syntactic abilities during fine-tuning as the\nkey to the success of AT. We observe that adversarially fine-tuned models\nremain more faithful to BERT's language modeling behavior and are more\nsensitive to the word order. As concrete examples of syntactic abilities, an\nadversarially fine-tuned model could have an advantage of up to 38% on anaphora\nagreement and up to 11% on dependency parsing. Our analysis demonstrates that\nvanilla fine-tuning oversimplifies the sentence representation by focusing\nheavily on a small subset of words. AT, however, moderates the effect of these\ninfluential words and encourages representational diversity. This allows for a\nmore hierarchical representation of a sentence and leads to the mitigation of\nBERT's loss of syntactic abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_J/0/1/0/all/0/1\">Javid Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13897","description":"<p>The MS MARCO ranking dataset has been widely used for training deep learning\nmodels for IR tasks, achieving considerable effectiveness on diverse zero-shot\nscenarios. However, this type of resource is scarce in other languages than\nEnglish. In this work we present mMARCO, a multilingual version of the MS MARCO\npassage ranking dataset comprising 8 languages that was created using machine\ntranslation. We evaluated mMARCO by fine-tuning mono and multilingual\nre-ranking models on it. Experimental results demonstrate that multilingual\nmodels fine-tuned on our translated dataset achieve superior effectiveness than\nmodels fine-tuned on the original English version alone. Also, our distilled\nmultilingual re-ranker is competitive with non-distilled models while having\n5.4 times fewer parameters. The translated datasets as well as fine-tuned\nmodels are available at https://github.com/unicamp-dl/mMARCO.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Henrique Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campiotti_I/0/1/0/all/0/1\">Israel Campiotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeronymo_V/0/1/0/all/0/1\">Vitor Jeronymo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patterns of Lexical Ambiguity in Contextualised Language Models. (arXiv:2109.13032v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13032","description":"<p>One of the central aspects of contextualised language models is that they\nshould be able to distinguish the meaning of lexically ambiguous words by their\ncontexts. In this paper we investigate the extent to which the contextualised\nembeddings of word forms that display multiplicity of sense reflect traditional\ndistinctions of polysemy and homonymy. To this end, we introduce an extended,\nhuman-annotated dataset of graded word sense similarity and co-predication\nacceptability, and evaluate how well the similarity of embeddings predicts\nsimilarity in meaning. Both types of human judgements indicate that the\nsimilarity of polysemic interpretations falls in a continuum between identity\nof meaning and homonymy. However, we also observe significant differences\nwithin the similarity ratings of polysemes, forming consistent patterns for\ndifferent types of polysemic sense alternation. Our dataset thus appears to\ncapture a substantial part of the complexity of lexical ambiguity, and can\nprovide a realistic test bed for contextualised embeddings. Among the tested\nmodels, BERT Large shows the strongest correlation with the collected word\nsense similarity ratings, but struggles to consistently replicate the observed\nsimilarity patterns. When clustering ambiguous word forms based on their\nembeddings, the model displays high confidence in discerning homonyms and some\ntypes of polysemic alternations, but consistently fails for others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haber_J/0/1/0/all/0/1\">Janosch Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}