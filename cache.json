{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"One Chatbot Per Person: Creating Personalized Chatbots based on Implicit User Profiles. (arXiv:2108.09355v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09355","description":"<p>Personalized chatbots focus on endowing chatbots with a consistent\npersonality to behave like real users, give more informative responses, and\nfurther act as personal assistants. Existing personalized approaches tried to\nincorporate several text descriptions as explicit user profiles. However, the\nacquisition of such explicit profiles is expensive and time-consuming, thus\nbeing impractical for large-scale real-world applications. Moreover, the\nrestricted predefined profile neglects the language behavior of a real user and\ncannot be automatically updated together with the change of user interests. In\nthis paper, we propose to learn implicit user profiles automatically from\nlarge-scale user dialogue history for building personalized chatbots.\nSpecifically, leveraging the benefits of Transformer on language understanding,\nwe train a personalized language model to construct a general user profile from\nthe user's historical responses. To highlight the relevant historical responses\nto the input post, we further establish a key-value memory network of\nhistorical post-response pairs, and build a dynamic post-aware user profile.\nThe dynamic profile mainly describes what and how the user has responded to\nsimilar posts in history. To explicitly utilize users' frequently used words,\nwe design a personalized decoder to fuse two decoding strategies, including\ngenerating a word from the generic vocabulary and copying one word from the\nuser's personalized vocabulary. Experiments on two real-world datasets show the\nsignificant improvement of our model compared with existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hanxun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2020 U.S. Presidential Election: Analysis of Female and Male Users on Twitter. (arXiv:2108.09416v1 [cs.SI])","link":"http://arxiv.org/abs/2108.09416","description":"<p>Social media is commonly used by the public during election campaigns to\nexpress their opinions regarding different issues. Among various social media\nchannels, Twitter provides an efficient platform for researchers and\npoliticians to explore public opinion regarding a wide range of topics such as\neconomy and foreign policy. Current literature mainly focuses on analyzing the\ncontent of tweets without considering the gender of users. This research\ncollects and analyzes a large number of tweets and uses computational, human\ncoding, and statistical analyses to identify topics in more than 300,000 tweets\nposted during the 2020 U.S. presidential election and to compare female and\nmale users regarding the average weight of the topics. Our findings are based\nupon a wide range of topics, such as tax, climate change, and the COVID-19\npandemic. Out of the topics, there exists a significant difference between\nfemale and male users for more than 70% of topics. Our research approach can\ninform studies in the areas of informatics, politics, and communication, and it\ncan be used by political campaigns to obtain a gender-based understanding of\npublic opinion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1\">Amir Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1\">Spring B. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackenzie_A/0/1/0/all/0/1\">Anderson Mackenzie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dorathea Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1\">Hannah R. Boyajieff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1\">Bailey Goldschmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BoundaryNet: An Attentive Deep Network with Fast Marching Distance Maps for Semi-automatic Layout Annotation. (arXiv:2108.09433v1 [cs.CV])","link":"http://arxiv.org/abs/2108.09433","description":"<p>Precise boundary annotations of image regions can be crucial for downstream\napplications which rely on region-class semantics. Some document collections\ncontain densely laid out, highly irregular and overlapping multi-class region\ninstances with large range in aspect ratio. Fully automatic boundary estimation\napproaches tend to be data intensive, cannot handle variable-sized images and\nproduce sub-optimal results for aforementioned images. To address these issues,\nwe propose BoundaryNet, a novel resizing-free approach for high-precision\nsemi-automatic layout annotation. The variable-sized user selected region of\ninterest is first processed by an attention-guided skip network. The network\noptimization is guided via Fast Marching distance maps to obtain a good quality\ninitial boundary estimate and an associated feature representation. These\noutputs are processed by a Residual Graph Convolution Network optimized using\nHausdorff loss to obtain the final region boundary. Results on a challenging\nimage manuscript dataset demonstrate that BoundaryNet outperforms strong\nbaselines and produces high-quality semantic region boundaries. Qualitatively,\nour approach generalizes across multiple document image datasets containing\ndifferent script systems and layouts, all without additional fine-tuning. We\nintegrate BoundaryNet into a document annotation system and show that it\nprovides high annotation throughput compared to manual and fully automatic\nalternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Abhishek Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Palmira: A Deep Deformable Network for Instance Segmentation of Dense and Uneven Layouts in Handwritten Manuscripts. (arXiv:2108.09436v1 [cs.CV])","link":"http://arxiv.org/abs/2108.09436","description":"<p>Handwritten documents are often characterized by dense and uneven layout.\nDespite advances, standard deep network based approaches for semantic layout\nsegmentation are not robust to complex deformations seen across semantic\nregions. This phenomenon is especially pronounced for the low-resource Indic\npalm-leaf manuscript domain. To address the issue, we first introduce\nIndiscapes2, a new large-scale diverse dataset of Indic manuscripts with\nsemantic layout annotations. Indiscapes2 contains documents from four different\nhistorical collections and is 150% larger than its predecessor, Indiscapes. We\nalso propose a novel deep network Palmira for robust, deformation-aware\ninstance segmentation of regions in handwritten manuscripts. We also report\nHausdorff distance and its variants as a boundary-aware performance measure.\nOur experiments demonstrate that Palmira provides robust layouts, outperforms\nstrong baseline approaches and ablative variants. We also include qualitative\nresults on Arabic, South-East Asian and Hebrew historical manuscripts to\nshowcase the generalization capability of Palmira.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharan_P/0/1/0/all/0/1\">Prema Satish Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aitha_S/0/1/0/all/0/1\">Sowmya Aitha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Abhishek Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augustine_A/0/1/0/all/0/1\">Aaron Augustine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training. (arXiv:2108.09479v1 [cs.MM])","link":"http://arxiv.org/abs/2108.09479","description":"<p>Existing approaches to vision-language pre-training (VLP) heavily rely on an\nobject detector based on bounding boxes (regions), where salient objects are\nfirst detected from images and then a Transformer-based model is used for\ncross-modal fusion. Despite their superior performance, these approaches are\nbounded by the capability of the object detector in terms of both effectiveness\nand efficiency. Besides, the presence of object detection imposes unnecessary\nconstraints on model designs and makes it difficult to support end-to-end\ntraining. In this paper, we revisit grid-based convolutional features for\nvision-language pre-training, skipping the expensive region-related steps. We\npropose a simple yet effective grid-based VLP method that works surprisingly\nwell with the grid features. By pre-training only with in-domain datasets, the\nproposed Grid-VLP method can outperform most competitive region-based VLP\nmethods on three examined vision-language understanding tasks. We hope that our\nfindings help to further advance the state of the art of vision-language\npre-training, and provide a new direction towards effective and efficient VLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1\">Bin Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_M/0/1/0/all/0/1\">Min Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CushLEPOR: Customised hLEPOR Metric Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements. (arXiv:2108.09484v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09484","description":"<p>Human evaluation has always been expensive while researchers struggle to\ntrust the automatic metrics. To address this, we propose to customise\ntraditional metrics by taking advantages of the pre-trained language models\n(PLMs) and the limited available human labelled scores. We first re-introduce\nthe hLEPOR metric factors, followed by the Python portable version we developed\nwhich achieved the automatic tuning of the weighting parameters in hLEPOR\nmetric. Then we present the customised hLEPOR (cushLEPOR) which uses LABSE\ndistilled knowledge model to improve the metric agreement with human judgements\nby automatically optimised factor weights regarding the exact MT language pairs\nthat cushLEPOR is deployed to. We also optimise cushLEPOR towards human\nevaluation data based on MQM and pSQM framework on English-German and\nChinese-English language pairs. The experimental investigations show cushLEPOR\nboosts hLEPOR performances towards better agreements to PLMs like LABSE with\nmuch lower cost, and better agreements to human evaluations including MQM and\npSQM scores, and yields much better performances than BLEU (data available at\n\\url{https://github.com/poethan/cushLEPOR}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yseop at FinSim-3 Shared Task 2021: Specializing Financial Domain Learning with Phrase Representations. (arXiv:2108.09485v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09485","description":"<p>In this paper, we present our approaches for the FinSim-3 Shared Task 2021:\nLearning Semantic Similarities for the Financial Domain. The aim of this shared\ntask is to correctly classify a list of given terms from the financial domain\ninto the most relevant hypernym (or top-level) concept in an external ontology.\nFor our system submission, we evaluate two methods: a Sentence-RoBERTa\n(SRoBERTa) embeddings model pre-trained on a custom corpus, and a dual\nword-sentence embeddings model that builds on the first method by improving the\nproposed baseline word embeddings construction using the FastText model to\nboost the classification performance. Our system ranks 2nd overall on both\nmetrics, scoring 0.917 on Average Accuracy and 1.141 on Mean Rank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akl_H/0/1/0/all/0/1\">Hanna Abi Akl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mariko_D/0/1/0/all/0/1\">Dominique Mariko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazancourt_H/0/1/0/all/0/1\">Hugues de Mazancourt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metric Learning in Multilingual Sentence Similarity Measurement for Document Alignment. (arXiv:2108.09495v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09495","description":"<p>Document alignment techniques based on multilingual sentence representations\nhave recently shown state of the art results. However, these techniques rely on\nunsupervised distance measurement techniques, which cannot be fined-tuned to\nthe task at hand. In this paper, instead of these unsupervised distance\nmeasurement techniques, we employ Metric Learning to derive task-specific\ndistance measurements. These measurements are supervised, meaning that the\ndistance measurement metric is trained using a parallel dataset. Using a\ndataset belonging to English, Sinhala, and Tamil, which belong to three\ndifferent language families, we show that these task-specific supervised\ndistance learning metrics outperform their unsupervised counterparts, for\ndocument alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajitha_C/0/1/0/all/0/1\">Charith Rajitha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piyarathne_L/0/1/0/all/0/1\">Lakmali Piyarathne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachintha_D/0/1/0/all/0/1\">Dilan Sachintha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Entity Graph Convolutional Network for Relation Extraction across Documents. (arXiv:2108.09505v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09505","description":"<p>Distantly supervised datasets for relation extraction mostly focus on\nsentence-level extraction, and they cover very few relations. In this work, we\npropose cross-document relation extraction, where the two entities of a\nrelation tuple appear in two different documents that are connected via a chain\nof common entities. Following this idea, we create a dataset for two-hop\nrelation extraction, where each chain contains exactly two documents. Our\nproposed dataset covers a higher number of relations than the publicly\navailable sentence-level datasets. We also propose a hierarchical entity graph\nconvolutional network (HEGCN) model for this task that improves performance by\n1.1\\% F1 score on our two-hop relation extraction dataset, compared to some\nstrong neural baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Cute is Pikachu? Gathering and Ranking Pok\\'emon Properties from Data with Pok\\'emon Word Embeddings. (arXiv:2108.09546v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09546","description":"<p>We present different methods for obtaining descriptive properties\nautomatically for the 151 original Pok\\'emon. We train several different word\nembeddings models on a crawled Pok\\'emon corpus, and use them to rank\nautomatically English adjectives based on how characteristic they are to a\ngiven Pok\\'emon. Based on our experiments, it is better to train a model with\ndomain specific data than to use a pretrained model. Word2Vec produces less\nnoise in the results than fastText model. Furthermore, we expand the list of\nproperties for each Pok\\'emon automatically. However, none of the methods is\nspot on and there is a considerable amount of noise in the different semantic\nmodels. Our models have been released on Zenodo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Partanen_N/0/1/0/all/0/1\">Niko Partanen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Summarization for Longform Spoken Dialog. (arXiv:2108.09597v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09597","description":"<p>Every day we are surrounded by spoken dialog. This medium delivers rich\ndiverse streams of information auditorily; however, systematically\nunderstanding dialog can often be non-trivial. Despite the pervasiveness of\nspoken dialog, automated speech understanding and quality information\nextraction remains markedly poor, especially when compared to written prose.\nFurthermore, compared to understanding text, auditory communication poses many\nadditional challenges such as speaker disfluencies, informal prose styles, and\nlack of structure. These concerns all demonstrate the need for a distinctly\nspeech tailored interactive system to help users understand and navigate the\nspoken language domain. While individual automatic speech recognition (ASR) and\ntext summarization methods already exist, they are imperfect technologies;\nneither consider user purpose and intent nor address spoken language induced\ncomplications. Consequently, we design a two stage ASR and text summarization\npipeline and propose a set of semantic segmentation and merging algorithms to\nresolve these speech modeling challenges. Our system enables users to easily\nbrowse and navigate content as well as recover from errors in these underlying\ntechnologies. Finally, we present an evaluation of the system which highlights\nuser preference for hierarchical summarization as a tool to quickly skim audio\nand identify content of interest to the user.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Daniel Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Thomas Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_A/0/1/0/all/0/1\">Albert Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilton_L/0/1/0/all/0/1\">Lydia Chilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Distantly Supervised Relation Extraction with Self-Ensemble Noise Filtering. (arXiv:2108.09689v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09689","description":"<p>Distantly supervised models are very popular for relation extraction since we\ncan obtain a large amount of training data using the distant supervision method\nwithout human annotation. In distant supervision, a sentence is considered as a\nsource of a tuple if the sentence contains both entities of the tuple. However,\nthis condition is too permissive and does not guarantee the presence of\nrelevant relation-specific information in the sentence. As such, distantly\nsupervised training data contains much noise which adversely affects the\nperformance of the models. In this paper, we propose a self-ensemble filtering\nmechanism to filter out the noisy samples during the training process. We\nevaluate our proposed framework on the New York Times dataset which is obtained\nvia distant supervision. Our experiments with multiple state-of-the-art neural\nrelation extraction models show that our proposed filtering mechanism improves\nthe robustness of the models and increases their F1 scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UzBERT: pretraining a BERT model for Uzbek. (arXiv:2108.09814v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09814","description":"<p>Pretrained language models based on the Transformer architecture have\nachieved state-of-the-art results in various natural language processing tasks\nsuch as part-of-speech tagging, named entity recognition, and question\nanswering. However, no such monolingual model for the Uzbek language is\npublicly available. In this paper, we introduce UzBERT, a pretrained Uzbek\nlanguage model based on the BERT architecture. Our model greatly outperforms\nmultilingual BERT on masked language model accuracy. We make the model publicly\navailable under the MIT open-source license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansurov_B/0/1/0/all/0/1\">B. Mansurov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansurov_A/0/1/0/all/0/1\">A. Mansurov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Transformer-based Framework for Duplex Text Normalization. (arXiv:2108.09889v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09889","description":"<p>Text normalization (TN) and inverse text normalization (ITN) are essential\npreprocessing and postprocessing steps for text-to-speech synthesis and\nautomatic speech recognition, respectively. Many methods have been proposed for\neither TN or ITN, ranging from weighted finite-state transducers to neural\nnetworks. Despite their impressive performance, these methods aim to tackle\nonly one of the two tasks but not both. As a result, in a complete spoken\ndialog system, two separate models for TN and ITN need to be built. This\nheterogeneity increases the technical complexity of the system, which in turn\nincreases the cost of maintenance in a production setting. Motivated by this\nobservation, we propose a unified framework for building a single neural duplex\nsystem that can simultaneously handle TN and ITN. Combined with a simple but\neffective data augmentation method, our systems achieve state-of-the-art\nresults on the Google TN dataset for English and Russian. They can also reach\nover 95% sentence-level accuracy on an internal English TN dataset without any\nadditional fine-tuning. In addition, we also create a cleaned dataset from the\nSpoken Wikipedia Corpora for German and report the performance of our systems\non the dataset. Overall, experimental results demonstrate the proposed duplex\ntext normalization framework is highly effective and applicable to a range of\ndomains and languages\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Manh Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1\">Evelina Bakhturina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Granularity and Cost of Annotation in Clinical Sequence Labeling. (arXiv:2108.09913v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09913","description":"<p>Well-annotated datasets, as shown in recent top studies, are becoming more\nimportant for researchers than ever before in supervised machine learning (ML).\nHowever, the dataset annotation process and its related human labor costs\nremain overlooked. In this work, we analyze the relationship between the\nannotation granularity and ML performance in sequence labeling, using clinical\nrecords from nursing shift-change handover. We first study a model derived from\ntextual language features alone, without additional information based on\nnursing knowledge. We find that this sequence tagger performs well in most\ncategories under this granularity. Then, we further include the additional\nmanual annotations by a nurse, and find the sequence tagging performance\nremaining nearly the same. Finally, we give a guideline and reference to the\ncommunity arguing it is not necessary and even not recommended to annotate in\ndetailed granularity because of a low Return on Investment. Therefore we\nrecommend emphasizing other features, like textual knowledge, for researchers\nand practitioners as a cost-effective source for increasing the sequence\nlabeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haozhan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suominen_H/0/1/0/all/0/1\">Hanna Suominen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fluent: An AI Augmented Writing Tool for People who Stutter. (arXiv:2108.09918v1 [cs.AI])","link":"http://arxiv.org/abs/2108.09918","description":"<p>Stuttering is a speech disorder which impacts the personal and professional\nlives of millions of people worldwide. To save themselves from stigma and\ndiscrimination, people who stutter (PWS) may adopt different strategies to\nconceal their stuttering. One of the common strategies is word substitution\nwhere an individual avoids saying a word they might stutter on and use an\nalternative instead. This process itself can cause stress and add more burden.\nIn this work, we present Fluent, an AI augmented writing tool which assists PWS\nin writing scripts which they can speak more fluently. Fluent embodies a novel\nactive learning based method of identifying words an individual might struggle\npronouncing. Such words are highlighted in the interface. On hovering over any\nsuch word, Fluent presents a set of alternative words which have similar\nmeaning but are easier to speak. The user is free to accept or ignore these\nsuggestions. Based on such user interaction (feedback), Fluent continuously\nevolves its classifier to better suit the personalized needs of each user. We\nevaluated our tool by measuring its ability to identify difficult words for 10\nsimulated users. We found that our tool can identify difficult words with a\nmean accuracy of over 80% in under 20 interactions and it keeps improving with\nmore feedback. Our tool can be beneficial for certain important life situations\nlike giving a talk, presentation, etc. The source code for this tool has been\nmade publicly accessible at github.com/bhavyaghai/Fluent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghai_B/0/1/0/all/0/1\">Bhavya Ghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sarcasm Detection in Twitter -- Performance Impact when using Data Augmentation: Word Embeddings. (arXiv:2108.09924v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09924","description":"<p>Sarcasm is the use of words usually used to either mock or annoy someone, or\nfor humorous purposes. Sarcasm is largely used in social networks and\nmicroblogging websites, where people mock or censure in a way that makes it\ndifficult even for humans to tell if what is said is what is meant. Failure to\nidentify sarcastic utterances in Natural Language Processing applications such\nas sentiment analysis and opinion mining will confuse classification algorithms\nand generate false results. Several studies on sarcasm detection have utilized\ndifferent learning algorithms. However, most of these learning models have\nalways focused on the contents of expression only, leaving the contextual\ninformation in isolation. As a result, they failed to capture the contextual\ninformation in the sarcastic expression. Moreover, some datasets used in\nseveral studies have an unbalanced dataset which impacting the model result. In\nthis paper, we propose a contextual model for sarcasm identification in twitter\nusing RoBERTa, and augmenting the dataset by applying Global Vector\nrepresentation (GloVe) for the construction of word embedding and context\nlearning to generate more data and balancing the dataset. The effectiveness of\nthis technique is tested with various datasets and data augmentation settings.\nIn particular, we achieve performance gain by 3.2% in the iSarcasm dataset when\nusing data augmentation to increase 20% of data labeled as sarcastic, resulting\nF-score of 40.4% compared to 37.2% without data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Handoyo_A/0/1/0/all/0/1\">Alif Tri Handoyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidayaturrahman/0/1/0/all/0/1\">Hidayaturrahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhartono_D/0/1/0/all/0/1\">Derwin Suhartono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Preserving Adversarial Text Attacks. (arXiv:2108.10015v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10015","description":"<p>Deep neural networks (DNNs) are known to be vulnerable to adversarial images,\nwhile their robustness in text classification is rarely studied. Several lines\nof text attack methods have been proposed in the literature, including\ncharacter-level, word-level, and sentence-level attacks. However, it is still a\nchallenge to minimize the number of word changes necessary to induce\nmisclassification, while simultaneously ensuring lexical correctness, syntactic\nsoundness, and semantic similarity. In this paper, we propose a Bigram and\nUnigram based adaptive Semantic Preservation Optimization (BU-SPO) method to\nexamine the vulnerability of deep models. Our method has four major merits.\nFirstly, we propose to attack text documents not only at the unigram word level\nbut also at the bigram level which better keeps semantics and avoids producing\nmeaningless outputs. Secondly, we propose a hybrid method to replace the input\nwords with options among both their synonyms candidates and sememe candidates,\nwhich greatly enriches the potential substitutions compared to only using\nsynonyms. Thirdly, we design an optimization algorithm, i.e., Semantic\nPreservation Optimization (SPO), to determine the priority of word\nreplacements, aiming to reduce the modification cost. Finally, we further\nimprove the SPO with a semantic Filter (named SPOF) to find the adversarial\nexample with the highest semantic similarity. We evaluate the effectiveness of\nour BU-SPO and BU-SPOF on IMDB, AG's News, and Yahoo! Answers text datasets by\nattacking four popular DNNs models. Results show that our methods achieve the\nhighest attack success rates and semantics rates by changing the smallest\nnumber of words compared with existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinghao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1\">James Bailey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tianqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Extraction by Associating Event Types and Argument Roles. (arXiv:2108.10038v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10038","description":"<p>Event extraction (EE), which acquires structural event knowledge from texts,\ncan be divided into two sub-tasks: event type classification and element\nextraction (namely identifying triggers and arguments under different role\npatterns). As different event types always own distinct extraction schemas\n(i.e., role patterns), previous work on EE usually follows an isolated learning\nparadigm, performing element extraction independently for different event\ntypes. It ignores meaningful associations among event types and argument roles,\nleading to relatively poor performance for less frequent types/roles. This\npaper proposes a novel neural association framework for the EE task. Given a\ndocument, it first performs type classification via constructing a\ndocument-level graph to associate sentence nodes of different types, and\nadopting a graph attention network to learn sentence embeddings. Then, element\nextraction is achieved by building a universal schema of argument roles, with a\nparameter inheritance mechanism to enhance role preference for extracted\nelements. As such, our model takes into account type and role associations\nduring EE, enabling implicit information sharing among them. Experimental\nresults show that our approach consistently outperforms most state-of-the-art\nEE methods in both sub-tasks. Particularly, for types/roles with less training\ndata, the performance is superior to the existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaohan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Agnostic Sequence Labeling by Copying Nearest Neighbors. (arXiv:1906.04225v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.04225","description":"<p>Retrieve-and-edit based approaches to structured prediction, where structures\nassociated with retrieved neighbors are edited to form new structures, have\nrecently attracted increased interest. However, much recent work merely\nconditions on retrieved structures (e.g., in a sequence-to-sequence framework),\nrather than explicitly manipulating them. We show we can perform accurate\nsequence labeling by explicitly (and only) copying labels from retrieved\nneighbors. Moreover, because this copying is label-agnostic, we can achieve\nimpressive performance when transferring to new sequence-labeling tasks without\nretraining. We additionally consider a dynamic programming approach to sequence\nlabeling in the presence of retrieved neighbors, which allows for controlling\nthe number of distinct (copied) segments used to form a prediction, and leads\nto both more interpretable and accurate predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Broad-Coverage Medical Entity Linking with Semantic Type Prediction and Large-Scale Datasets. (arXiv:2005.00460v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00460","description":"<p>Medical entity linking is the task of identifying and standardizing medical\nconcepts referred to in an unstructured text. Most of the existing methods\nadopt a three-step approach of (1) detecting mentions, (2) generating a list of\ncandidate concepts, and finally (3) picking the best concept among them. In\nthis paper, we probe into alleviating the problem of overgeneration of\ncandidate concepts in the candidate generation module, the most under-studied\ncomponent of medical entity linking. For this, we present MedType, a fully\nmodular system that prunes out irrelevant candidate concepts based on the\npredicted semantic type of an entity mention. We incorporate MedType into five\noff-the-shelf toolkits for medical entity linking and demonstrate that it\nconsistently improves entity linking performance across several benchmark\ndatasets. To address the dearth of annotated training data for medical entity\nlinking, we present WikiMed and PubMedDS, two large-scale medical entity\nlinking datasets, and demonstrate that pre-training MedType on these datasets\nfurther improves entity linking performance. We make our source code and\ndatasets publicly available for medical entity linking research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1\">Shikhar Vashishth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_Griffis_D/0/1/0/all/0/1\">Denis Newman-Griffis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rishabh Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1\">Ritam Dutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn Rose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composing Answer from Multi-spans for Reading Comprehension. (arXiv:2009.06141v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.06141","description":"<p>This paper presents a novel method to generate answers for non-extraction\nmachine reading comprehension (MRC) tasks whose answers cannot be simply\nextracted as one span from the given passages. Using a pointer network-style\nextractive decoder for such type of MRC may result in unsatisfactory\nperformance when the ground-truth answers are given by human annotators or\nhighly re-paraphrased from parts of the passages. On the other hand, using\ngenerative decoder cannot well guarantee the resulted answers with well-formed\nsyntax and semantics when encountering long sentences. Therefore, to alleviate\nthe obvious drawbacks of both sides, we propose an answer making-up method from\nextracted multi-spans that are learned by our model as highly confident\n$n$-gram candidates in the given passage. That is, the returned answers are\ncomposed of discontinuous multi-spans but not just one consecutive span in the\ngiven passages anymore. The proposed method is simple but effective: empirical\nexperiments on MS MARCO show that the proposed method has a better performance\non accurately generating long answers, and substantially outperforms two\ncompetitive typical one-span and Seq2Seq baseline decoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stabilizing Label Assignment for Speech Separation by Self-supervised Pre-training. (arXiv:2010.15366v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2010.15366","description":"<p>Speech separation has been well developed, with the very successful\npermutation invariant training (PIT) approach, although the frequent label\nassignment switching happening during PIT training remains to be a problem when\nbetter convergence speed and achievable performance are desired. In this paper,\nwe propose to perform self-supervised pre-training to stabilize the label\nassignment in training the speech separation model. Experiments over several\ntypes of self-supervised approaches, several typical speech separation models\nand two different datasets showed that very good improvements are achievable if\na proper self-supervised approach is chosen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sung-Feng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-Po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Da-Rong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gene-Ping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Size Does Not Fit All: Finding the Optimal Subword Sizes for FastText Models across Languages. (arXiv:2102.02585v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.02585","description":"<p>Unsupervised representation learning of words from large multilingual corpora\nis useful for downstream tasks such as word sense disambiguation, semantic text\nsimilarity, and information retrieval. The representation precision of\nlog-bilinear fastText models is mostly due to their use of subword information.\nIn previous work, the optimization of fastText's subword sizes has not been\nfully explored, and non-English fastText models were trained using subword\nsizes optimized for English and German word analogy tasks. In our work, we find\nthe optimal subword sizes on the English, German, Czech, Italian, Spanish,\nFrench, Hindi, Turkish, and Russian word analogy tasks. We then propose a\nsimple n-gram coverage model and we show that it predicts better-than-default\nsubword sizes on the Spanish, French, Hindi, Turkish, and Russian word analogy\ntasks. We show that the optimization of fastText's subword sizes matters and\nresults in a 14% improvement on the Czech word analogy task. We also show that\nexpensive parameter optimization can be replaced by a simple n-gram coverage\nmodel that consistently improves the accuracy of fastText models on the word\nanalogy tasks by up to 3% compared to the default subword sizes, and that it is\nwithin 1% accuracy of the optimal subword sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ayetiran_E/0/1/0/all/0/1\">Eniafe Festus Ayetiran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bacovsky_D/0/1/0/all/0/1\">Dalibor Ba&#x10d;ovsk&#xfd;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1\">D&#xe1;vid Lupt&#xe1;k</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a> (1) ((1) Faculty of Informatics Masaryk University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alignment Knowledge Distillation for Online Streaming Attention-based Speech Recognition. (arXiv:2103.00422v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2103.00422","description":"<p>This article describes an efficient training method for online streaming\nattention-based encoder-decoder (AED) automatic speech recognition (ASR)\nsystems. AED models have achieved competitive performance in offline scenarios\nby jointly optimizing all components. They have recently been extended to an\nonline streaming framework via models such as monotonic chunkwise attention\n(MoChA). However, the elaborate attention calculation process is not robust for\nlong-form speech utterances. Moreover, the sequence-level training objective\nand time-restricted streaming encoder cause a nonnegligible delay in token\nemission during inference. To address these problems, we propose CTC\nsynchronous training (CTC-ST), in which CTC alignments are leveraged as a\nreference for token boundaries to enable a MoChA model to learn optimal\nmonotonic input-output alignments. We formulate a purely end-to-end training\nobjective to synchronize the boundaries of MoChA to those of CTC. The CTC model\nshares an encoder with the MoChA model to enhance the encoder representation.\nMoreover, the proposed method provides alignment information learned in the CTC\nbranch to the attention-based decoder. Therefore, CTC-ST can be regarded as\nself-distillation of alignment knowledge from CTC to MoChA. Experimental\nevaluations on a variety of benchmark datasets show that the proposed method\nsignificantly reduces recognition errors and emission latency simultaneously.\nThe robustness to long-form and noisy speech is also demonstrated. We compare\nCTC-ST with several methods that distill alignment knowledge from a hybrid ASR\nsystem and show that the CTC-ST can achieve a comparable tradeoff of accuracy\nand latency without relying on external alignment information. The best MoChA\nsystem shows recognition accuracy comparable to that of RNN-transducer (RNN-T)\nwhile achieving lower emission latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples for Relation Extraction. (arXiv:2104.00312v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.00312","description":"<p>Recent neural-based relation extraction approaches, though achieving\npromising improvement on benchmark datasets, have reported their vulnerability\ntowards adversarial attacks. Thus far, efforts mostly focused on generating\nadversarial samples or defending adversarial attacks, but little is known about\nthe difference between normal and adversarial samples. In this work, we take\nthe first step to leverage the salience-based method to analyze those\nadversarial samples. We observe that salience tokens have a direct correlation\nwith adversarial perturbations. We further find the adversarial perturbations\nare either those tokens not existing in the training set or superficial cues\nassociated with relation labels. To some extent, our approach unveils the\ncharacters against adversarial samples. We release an open-source testbed,\n\"DiagnoseAdv\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.02284","description":"<p>Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zxlzr/LegalPP} for reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Contrastive Learning for Learning Robust Textual Representations. (arXiv:2104.04907v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04907","description":"<p>Although the self-supervised pre-training of transformer models has resulted\nin the revolutionizing of natural language processing (NLP) applications and\nthe achievement of state-of-the-art results with regard to various benchmarks,\nthis process is still vulnerable to small and imperceptible permutations\noriginating from legitimate inputs. Intuitively, the representations should be\nsimilar in the feature space with subtle input permutations, while large\nvariations occur with different meanings. This motivates us to investigate the\nlearning of robust textual representation in a contrastive manner. However, it\nis non-trivial to obtain opposing semantic instances for textual samples. In\nthis study, we propose a disentangled contrastive learning method that\nseparately optimizes the uniformity and alignment of representations without\nnegative sampling. Specifically, we introduce the concept of momentum\nrepresentation consistency to align features and leverage power normalization\nwhile conforming the uniformity. Our experimental results for the NLP\nbenchmarks demonstrate that our approach can obtain better results compared\nwith the baselines, as well as achieve promising improvements with invariance\ntests and adversarial attacks. The code is available in\nhttps://github.com/zxlzr/DCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InfographicVQA. (arXiv:2104.12756v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12756","description":"<p>Infographics are documents designed to effectively communicate information\nusing a combination of textual, graphical and visual elements. In this work, we\nexplore the automatic understanding of infographic images by using Visual\nQuestion Answering technique.To this end, we present InfographicVQA, a new\ndataset that comprises a diverse collection of infographics along with natural\nlanguage questions and answers annotations. The collected questions require\nmethods to jointly reason over the document layout, textual content, graphical\nelements, and data visualizations. We curate the dataset with emphasis on\nquestions that require elementary reasoning and basic arithmetic skills.\nFinally, we evaluate two strong baselines based on state of the art multi-modal\nVQA models, and establish baseline performance for the new task. The dataset,\ncode and leaderboard will be made available at <a href=\"http://docvqa.org\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Minesh Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagal_V/0/1/0/all/0/1\">Viraj Bagal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1\">Rub&#xe8;n P&#xe9;rez Tito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASQ: Automatically Generating Question-Answer Pairs using AMRs. (arXiv:2105.10023v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.10023","description":"<p>We introduce ASQ, a tool to automatically mine questions and answers from a\nsentence using the Abstract Meaning Representation (AMR). Previous work has\nused question-answer pairs to specify the predicate-argument structure of a\nsentence using natural language, which does not require linguistic expertise or\ntraining, and created datasets such as QA-SRL and QAMR, for which the\nquestion-answer pair annotations were crowdsourced. Our goal is to build a tool\n(ASQ) that maps from the traditional meaning representation AMR to a\nquestion-answer meaning representation (QMR). This enables construction of QMR\ndatasets automatically in various domains using existing high-quality AMR\nparsers, and provides an automatic mapping AMR to QMR for ease of understanding\nby non-experts. A qualitative evaluation of the output generated by ASQ from\nthe AMR 2.0 data shows that the question-answer pairs are natural and valid,\nand demonstrate good coverage of the content. We run ASQ on the sentences from\nthe QAMR dataset, to observe that the semantic roles in QAMR are also captured\nby ASQ. We intend to make this tool and the results publicly available for\nothers to use and build upon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakshit_G/0/1/0/all/0/1\">Geetanjali Rakshit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1\">Jeffrey Flanigan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-level Relation Extraction as Semantic Segmentation. (arXiv:2106.03618v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.03618","description":"<p>Document-level relation extraction aims to extract relations among multiple\nentity pairs from a document. Previously proposed graph-based or\ntransformer-based models utilize the entities independently, regardless of\nglobal information among relational triples. This paper approaches the problem\nby predicting an entity-level relation matrix to capture local and global\ninformation, parallel to the semantic segmentation task in computer vision.\nHerein, we propose a Document U-shaped Network for document-level relation\nextraction. Specifically, we leverage an encoder module to capture the context\ninformation of entities and a U-shaped segmentation module over the image-style\nfeature map to capture global interdependency among triples. Experimental\nresults show that our approach can obtain state-of-the-art performance on three\nbenchmark datasets DocRED, CDR, and GDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Survey on Schema-based Event Extraction with Deep Learning. (arXiv:2107.02126v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02126","description":"<p>Schema-based event extraction is a critical technique to apprehend the\nessential content of events promptly. With the rapid development of deep\nlearning technology, event extraction technology based on deep learning has\nbecome a research hotspot. Numerous methods, datasets, and evaluation metrics\nhave been proposed in the literature, raising the need for a comprehensive and\nupdated survey. This paper fills the gap by reviewing the state-of-the-art\napproaches, focusing on deep learning-based models. We summarize the task\ndefinition, paradigm, and models of schema-based event extraction and then\ndiscuss each of these in detail. We introduce benchmark datasets that support\ntests of predictions and evaluation metrics. A comprehensive comparison between\ndifferent techniques is also provided in this survey. Finally, we conclude by\nsummarizing future research directions facing the research area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hei_Y/0/1/0/all/0/1\">Yiming Hei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Rui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beheshti_A/0/1/0/all/0/1\">Amin Beheshti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sangrahaka: A Tool for Annotating and Querying Knowledge Graphs. (arXiv:2107.02782v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2107.02782","description":"<p>In this work, we present a web-based annotation and querying tool Sangrahaka.\nIt annotates entities and relationships from text corpora and constructs a\nknowledge graph (KG). The KG is queried using templatized natural language\nqueries. The application is language and corpus agnostic, but can be tuned for\nspecial needs of a specific language or a corpus. A customized version of the\nframework has been used in two annotation tasks. The application is available\nfor download and installation. Besides having a user-friendly interface, it is\nfast, supports customization, and is fault tolerant on both client and server\nside. The code is available at https://github.com/hrishikeshrt/sangrahaka and\nthe presentation with a demo is available at https://youtu.be/nw9GFLVZMMo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Terdalkar_H/0/1/0/all/0/1\">Hrishikesh Terdalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12651","description":"<p>Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02180","description":"<p>We address the problem of visual storytelling, i.e., generating a story for a\ngiven sequence of images. While each sentence of the story should describe a\ncorresponding image, a coherent story also needs to be consistent and relate to\nboth future and past images. To achieve this we develop ordered image attention\n(OIA). OIA models interactions between the sentence-corresponding image and\nimportant regions in other images of the sequence. To highlight the important\nobjects, a message-passing-like algorithm collects representations of those\nobjects in an order-aware manner. To generate the story's sentences, we then\nhighlight important image attention vectors with an Image-Sentence Attention\n(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we\nintroduce an adaptive prior. The obtained results improve the METEOR score on\nthe VIST dataset by 1%. In addition, an extensive human study verifies\ncoherency improvements and shows that OIA and ISA generated stories are more\nfocused, shareable, and image-grounded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1\">Tom Braude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offensive Language and Hate Speech Detection with Deep Learning and Transfer Learning. (arXiv:2108.03305v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03305","description":"<p>Toxic online speech has become a crucial problem nowadays due to an\nexponential increase in the use of internet by people from different cultures\nand educational backgrounds. Differentiating if a text message belongs to hate\nspeech and offensive language is a key challenge in automatic detection of\ntoxic text content. In this paper, we propose an approach to automatically\nclassify tweets into three classes: Hate, offensive and Neither. Using public\ntweet data set, we first perform experiments to build BI-LSTM models from empty\nembedding and then we also try the same neural network architecture with\npre-trained Glove embedding. Next, we introduce a transfer learning approach\nfor hate speech detection using an existing pre-trained language model BERT\n(Bidirectional Encoder Representations from Transformers), DistilBert\n(Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform\nhyper parameters tuning analysis of our best model (BI-LSTM) considering\ndifferent neural network architectures, learn-ratings and normalization methods\netc. After tuning the model and with the best combination of parameters, we\nachieve over 92 percent accuracy upon evaluating it on test data. We also\ncreate a class module which contains main functionality including text\nclassification, sentiment checking and text data augmentation. This model could\nserve as an intermediate module between user and Twitter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Bencheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jason Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ajay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umair_H/0/1/0/all/0/1\">Hafiza Umair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vovor_A/0/1/0/all/0/1\">Atsu Vovor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durzynski_N/0/1/0/all/0/1\">Natalie Durzynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model. (arXiv:2108.04556v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04556","description":"<p>Code pre-trained models have shown great success in various code-related\ntasks, such as code search, code clone detection, and code translation. Most\nexisting code pre-trained models often treat a code snippet as a plain sequence\nof tokens. However, the inherent syntax and hierarchy that provide important\nstructure and semantic information are ignored. The native derived sequence\nrepresentations of them are insufficient. To this end, we propose CLSEBERT, a\nContrastive Learning Framework for Syntax Enhanced Code Pre-Trained Model, to\ndeal with various code intelligence tasks. In the pre-training stage, we\nconsider the code syntax and hierarchy contained in the Abstract Syntax Tree\n(AST) and leverage the Contrastive Learning (CL) to learn noise-invariant code\nrepresentations. Besides the original masked language model (MLM) objective, we\nalso introduce two novel pre-training objectives: (1) ``AST Node Edge\nPrediction (NEP)'' to predict edges between nodes in the abstract syntax tree;\n(2) ``Code Token Type Prediction (TTP)'' to predict the types of code tokens.\nExtensive experiments on four code intelligence tasks demonstrate the superior\nperformance of CLSEBERT compared to state-of-the-art at the same pre-training\ncorpus and parameter scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yadao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04657","description":"<p>Multi-head attention, a collection of several attention mechanisms that\nindependently attend to different parts of the input, is the key ingredient in\nthe Transformer. Recent work has shown, however, that a large proportion of the\nheads in a Transformer's multi-head attention mechanism can be safely pruned\naway without significantly harming the performance of the model; such pruning\nleads to models that are noticeably smaller and faster in practice. Our work\nintroduces a new head pruning technique that we term differentiable subset\npruning. Intuitively, our method learns per-head importance variables and then\nenforces a user-specified hard constraint on the number of unpruned heads. The\nimportance variables are learned via stochastic gradient descent. We conduct\nexperiments on natural language inference and machine translation; we show that\ndifferentiable subset pruning performs comparably or better than previous works\nwhile offering precise control of the sparsity level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEMix Layers: Disentangling Domains for Modular Language Modeling. (arXiv:2108.05036v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05036","description":"<p>We introduce a new domain expert mixture (DEMix) layer that enables\nconditioning a language model (LM) on the domain of the input text. A DEMix\nlayer is a collection of expert feedforward networks, each specialized to a\ndomain, that makes the LM modular: experts can be mixed, added or removed after\ninitial training. Extensive experiments with autoregressive transformer LMs (up\nto 1.3B parameters) show that DEMix layers reduce test-time perplexity,\nincrease training efficiency, and enable rapid adaptation with little overhead.\nWe show that mixing experts during inference, using a parameter-free weighted\nensemble, allows the model to better generalize to heterogeneous or unseen\ndomains. We also show that experts can be added to iteratively incorporate new\ndomains without forgetting older ones, and that experts can be removed to\nrestrict access to unwanted domains, without additional training. Overall,\nthese results demonstrate benefits of explicitly conditioning on textual\ndomains during language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparative study of universal quantum computing models: towards a physical unification. (arXiv:2108.07909v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2108.07909","description":"<p>Quantum computing has been a fascinating research field in quantum physics.\nRecent progresses motivate us to study in depth the universal quantum computing\nmodels (UQCM), which lie at the foundation of quantum computing and have tight\nconnections with fundamental physics. Although being developed decades ago, a\nphysically concise principle or picture to formalize and understand UQCM is\nstill lacking. This is challenging given the diversity of still-emerging\nmodels, but important to understand the difference between classical and\nquantum computing. In this work, we carried out a primary attempt to unify UQCM\nby classifying a few of them as two categories, hence making a table of models.\nWith such a table, some known models or schemes appear as hybridization or\ncombination of models, and more importantly, it leads to new schemes that have\nnot been explored yet. Our study of UQCM also leads to some insights into\nquantum algorithms. This work reveals the importance and feasibility of\nsystematic study of computing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_D/0/1/0/all/0/1\">D.-S. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08468","description":"<p>We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Contextualization using Top-k Operators for Question Answering over Knowledge Graphs. (arXiv:2108.08597v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08597","description":"<p>Answering complex questions over knowledge bases (KB-QA) faces huge input\ndata with billions of facts, involving millions of entities and thousands of\npredicates. For efficiency, QA systems first reduce the answer search space by\nidentifying a set of facts that is likely to contain all answers and relevant\ncues. The most common technique is to apply named entity disambiguation (NED)\nsystems to the question, and retrieve KB facts for the disambiguated entities.\nThis work presents ECQA, an efficient method that prunes irrelevant parts of\nthe search space using KB-aware signals. ECQA is based on top-k query\nprocessing over score-ordered lists of KB items that combine signals about\nlexical matching, relevance to the question, coherence among candidate items,\nand connectivity in the KB graph. Experiments with two recent QA benchmarks\ndemonstrate the superiority of ECQA over state-of-the-art baselines with\nrespect to answer presence, size of the search space, and runtimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christmann_P/0/1/0/all/0/1\">Philipp Christmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Czech News Dataset for Semantic Textual Similarity. (arXiv:2108.08708v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08708","description":"<p>This paper describes a novel dataset consisting of sentences with semantic\nsimilarity annotations. The data originate from the journalistic domain in the\nCzech language. We describe the process of collecting and annotating the data\nin detail. The dataset contains 138,556 human annotations divided into train\nand test sets. In total, 485 journalism students participated in the creation\nprocess. To increase the reliability of the test set, we compute the annotation\nas an average of 9 individual annotations. We evaluate the quality of the\ndataset by measuring inter and intra annotation annotators' agreements. Beside\nagreement numbers, we provide detailed statistics of the collected dataset. We\nconclude our paper with a baseline experiment of building a system for\npredicting the semantic similarity of sentences. Due to the massive number of\ntraining annotations (116 956), the model can perform significantly better than\nan average annotator (0,92 versus 0,86 of Person's correlation coefficients).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1\">Michal Sej&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}