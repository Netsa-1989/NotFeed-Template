{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"NoFake at CheckThat! 2021: Fake News Detection Using BERT. (arXiv:2108.05419v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05419","description":"<p>Much research has been done for debunking and analysing fake news. Many\nresearchers study fake news detection in the last year, but many are limited to\nsocial media data. Currently, multiples fact-checkers are publishing their\nresults in various formats. Also, multiple fact-checkers use different labels\nfor the fake news, making it difficult to make a generalisable classifier. With\nthe merge classes, the performance of the machine model can be enhanced. This\ndomain categorisation will help group the article, which will help save the\nmanual effort in assigning the claim verification. In this paper, we have\npresented BERT based classification model to predict the domain and\nclassification. We have also used additional data from fact-checked articles.\nWe have achieved a macro F1 score of 83.76 % for Task 3Aand 85.55 % for Task 3B\nusing the additional training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumari_S/0/1/0/all/0/1\">Sushma Kumari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Semantics from Maintenance Records. (arXiv:2108.05454v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05454","description":"<p>Rapid progress in natural language processing has led to its utilization in a\nvariety of industrial and enterprise settings, including in its use for\ninformation extraction, specifically named entity recognition and relation\nextraction, from documents such as engineering manuals and field maintenance\nreports. While named entity recognition is a well-studied problem, existing\nstate-of-the-art approaches require large labelled datasets which are hard to\nacquire for sensitive data such as maintenance records. Further, industrial\ndomain experts tend to distrust results from black box machine learning models,\nespecially when the extracted information is used in downstream predictive\nmaintenance analytics. We overcome these challenges by developing three\napproaches built on the foundation of domain expert knowledge captured in\ndictionaries and ontologies. We develop a syntactic and semantic rules-based\napproach and an approach leveraging a pre-trained language model, fine-tuned\nfor a question-answering task on top of our base dictionary lookup to extract\nentities of interest from maintenance records. We also develop a preliminary\nontology to represent and capture the semantics of maintenance records. Our\nevaluations on a real-world aviation maintenance records dataset show promising\nresults and help identify challenges specific to named entity recognition in\nthe context of noisy industrial data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dixit_S/0/1/0/all/0/1\">Sharad Dixit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulwad_V/0/1/0/all/0/1\">Varish Mulwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1\">Abhinav Saxena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attacks against Ranking Algorithms with Text Embeddings: a Case Study on Recruitment Algorithms. (arXiv:2108.05490v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05490","description":"<p>Recently, some studies have shown that text classification tasks are\nvulnerable to poisoning and evasion attacks. However, little work has\ninvestigated attacks against decision making algorithms that use text\nembeddings, and their output is a ranking. In this paper, we focus on ranking\nalgorithms for recruitment process, that employ text embeddings for ranking\napplicants resumes when compared to a job description. We demonstrate both\nwhite box and black box attacks that identify text items, that based on their\nlocation in embedding space, have significant contribution in increasing the\nsimilarity score between a resume and a job description. The adversary then\nuses these text items to improve the ranking of their resume among others. We\ntested recruitment algorithms that use the similarity scores obtained from\nUniversal Sentence Encoder (USE) and Term Frequency Inverse Document Frequency\n(TF IDF) vectors. Our results show that in both adversarial settings, on\naverage the attacker is successful. We also found that attacks against TF IDF\nis more successful compared to USE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samadi_A/0/1/0/all/0/1\">Anahita Samadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debapriya Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilizadeh_S/0/1/0/all/0/1\">Shirin Nilizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethereum Data Structures. (arXiv:2108.05513v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05513","description":"<p>Ethereum platform operates with rich spectrum of data structures and hashing\nand coding functions. The main source describing them is the Yellow paper,\ncomplemented by a lot of informal blogs. These sources are somehow limited. In\nparticular, the Yellow paper does not ideally balance brevity and detail, in\nsome parts it is very detail, while too shallow elsewhere. The blogs on the\nother hand are often too vague and in certain cases contain incorrect\ninformation. As a solution, we provide this document, which summarises data\nstructures used in Ethereum. The goal is to provide sufficient detail while\nkeeping brevity. Sufficiently detailed formal view is enriched with examples to\nextend on clarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jezek_K/0/1/0/all/0/1\">Kamil Jezek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. (arXiv:2108.05540v1 [cs.IR])","link":"http://arxiv.org/abs/2108.05540","description":"<p>Recent research demonstrates the effectiveness of using fine-tuned language\nmodels~(LM) for dense retrieval. However, dense retrievers are hard to train,\ntypically requiring heavily engineered fine-tuning pipelines to realize their\nfull potential. In this paper, we identify and address two underlying problems\nof dense retrievers: i)~fragility to training data noise and ii)~requiring\nlarge batches to robustly learn the embedding space. We use the recently\nproposed Condenser pre-training architecture, which learns to condense\ninformation into the dense vector through LM pre-training. On top of it, we\npropose coCondenser, which adds an unsupervised corpus-level contrastive loss\nto warm up the passage embedding space. Retrieval experiments on MS-MARCO,\nNatural Question, and Trivia QA datasets show that coCondenser removes the need\nfor heavy data engineering such as augmentation, synthesis, or filtering, as\nwell as the need for large batch training. It shows comparable performance to\nRocketQA, a state-of-the-art, heavily engineered system, using simple small\nbatch fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing. (arXiv:2108.05542v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05542","description":"<p>Transformer-based pretrained language models (T-PTLMs) have achieved great\nsuccess in almost every NLP task. The evolution of these models started with\nGPT and BERT. These models are built on the top of transformers,\nself-supervised learning and transfer learning. Transformed-based PTLMs learn\nuniversal language representations from large volumes of text data using\nself-supervised learning and transfer this knowledge to downstream tasks. These\nmodels provide good background knowledge to downstream tasks which avoids\ntraining of downstream models from scratch. In this comprehensive survey paper,\nwe initially give a brief overview of self-supervised learning. Next, we\nexplain various core concepts like pretraining, pretraining methods,\npretraining tasks, embeddings and downstream adaptation methods. Next, we\npresent a new taxonomy of T-PTLMs and then give brief overview of various\nbenchmarks including both intrinsic and extrinsic. We present a summary of\nvarious useful libraries to work with T-PTLMs. Finally, we highlight some of\nthe future research directions which will further improve these models. We\nstrongly believe that this comprehensive survey paper will serve as a good\nreference to learn the core concepts as well as to stay updated with the recent\nhappenings in T-PTLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_K/0/1/0/all/0/1\">Katikapalli Subramanyam Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1\">Ajit Rajasekharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangeetha_S/0/1/0/all/0/1\">Sivanesan Sangeetha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kicktionary-LOME: A Domain-Specific Multilingual Frame Semantic Parsing Model for Football Language. (arXiv:2108.05575v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05575","description":"<p>This technical report introduces an adapted version of the LOME frame\nsemantic parsing model (Xia et al., EACL 2021) which is capable of\nautomatically annotating texts according to the \"Kicktionary\" domain-specific\nframenet resource. Several methods for training a model even with limited\navailable training data are proposed. While there are some challenges for\nevaluation related to the nature of the available annotations, preliminary\nresults are very promising, with the best model reaching F1-scores of 0.83\n(frame prediction) and 0.81 (semantic role prediction).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minnema_G/0/1/0/all/0/1\">Gosse Minnema</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation Challenges: Results of the Accuracy Evaluation Shared Task. (arXiv:2108.05644v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05644","description":"<p>The Shared Task on Evaluating Accuracy focused on techniques (both manual and\nautomatic) for evaluating the factual accuracy of texts produced by neural NLG\nsystems, in a sports-reporting domain. Four teams submitted evaluation\ntechniques for this task, using very different approaches and techniques. The\nbest-performing submissions did encouragingly well at this difficult task.\nHowever, all automatic submissions struggled to detect factual errors which are\nsemantically or pragmatically complex (for example, based on incorrect\ncomputation or inference).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1\">Craig Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_E/0/1/0/all/0/1\">Ehud Reiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Relevance Ranking under the Pre-training and Fine-tuning Paradigm. (arXiv:2108.05652v1 [cs.IR])","link":"http://arxiv.org/abs/2108.05652","description":"<p>Recently, pre-trained language models such as BERT have been applied to\ndocument ranking for information retrieval, which first pre-train a general\nlanguage model on an unlabeled large corpus and then conduct ranking-specific\nfine-tuning on expert-labeled relevance datasets. Ideally, an IR system would\nmodel relevance from a user-system dualism: the user's view and the system's\nview. User's view judges the relevance based on the activities of \"real users\"\nwhile the system's view focuses on the relevance signals from the system side,\ne.g., from the experts or algorithms, etc. Inspired by the user-system\nrelevance views and the success of pre-trained language models, in this paper\nwe propose a novel ranking framework called Pre-Rank that takes both user's\nview and system's view into consideration, under the pre-training and\nfine-tuning paradigm. Specifically, to model the user's view of relevance,\nPre-Rank pre-trains the initial query-document representations based on\nlarge-scale user activities data such as the click log. To model the system's\nview of relevance, Pre-Rank further fine-tunes the model on expert-labeled\nrelevance data. More importantly, the pre-trained representations, are\nfine-tuned together with handcrafted learning-to-rank features under a wide and\ndeep network architecture. In this way, Pre-Rank can model the relevance by\nincorporating the relevant knowledge and signals from both real search users\nand the IR experts. To verify the effectiveness of Pre-Rank, we showed two\nimplementations by using BERT and SetRank as the underlying ranking model,\nrespectively. Experimental results base on three publicly available benchmarks\nshowed that in both of the implementations, Pre-Rank can respectively\noutperform the underlying ranking models and achieved state-of-the-art\nperformances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1\">Lin Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">XiuQiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Diverse Descriptions from Semantic Graphs. (arXiv:2108.05659v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05659","description":"<p>Text generation from semantic graphs is traditionally performed with\ndeterministic methods, which generate a unique description given an input\ngraph. However, the generation problem admits a range of acceptable textual\noutputs, exhibiting lexical, syntactic and semantic variation. To address this\ndisconnect, we present two main contributions. First, we propose a stochastic\ngraph-to-text model, incorporating a latent variable in an encoder-decoder\nmodel, and its use in an ensemble. Second, to assess the diversity of the\ngenerated sentences, we propose a new automatic evaluation metric which jointly\nevaluates output diversity and quality in a multi-reference setting. We\nevaluate the models on WebNLG datasets in English and Russian, and show an\nensemble of stochastic models produces diverse sets of generated sentences,\nwhile retaining similar quality to state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiuzhou Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_D/0/1/0/all/0/1\">Daniel Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridger: Toward Bursting Scientific Filter Bubbles and Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v1 [cs.DL])","link":"http://arxiv.org/abs/2108.05669","description":"<p>Scientific silos can hinder innovation. These information \"filter bubbles\"\nand the growing challenge of information overload limit awareness across the\nliterature, making it difficult to keep track of even narrow areas of interest,\nlet alone discover new ones. Algorithmic curation and recommendation, which\noften prioritize relevance, can further reinforce these bubbles. In response,\nwe describe Bridger, a system for facilitating discovery of scholars and their\nwork, to explore design tradeoffs among relevant and novel recommendations. We\nconstruct a faceted representation of authors using information extracted from\ntheir papers and inferred personas. We explore approaches both for recommending\nnew content and for displaying it in a manner that helps researchers to\nunderstand the work of authors who they are unfamiliar with. In studies with\ncomputer science researchers, our approach substantially improves users'\nabilities to do so. We develop an approach that locates commonalities and\ncontrasts between scientists---retrieving partially similar authors, rather\nthan aiming for strict similarity. We find this approach helps users discover\nauthors useful for generating novel research ideas of relevance to their work,\nat a higher rate than a state-of-art neural model. Our analysis reveals that\nBridger connects authors who have different citation profiles, publish in\ndifferent venues, and are more distant in social co-authorship networks,\nraising the prospect of bridging diverse communities and facilitating\ndiscovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portenoy_J/0/1/0/all/0/1\">Jason Portenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radensky_M/0/1/0/all/0/1\">Marissa Radensky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1\">Jevin West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance. (arXiv:2108.05682v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05682","description":"<p>In the domain of Morphology, Inflection is a fundamental and important task\nthat gained a lot of traction in recent years, mostly via SIGMORPHON's\nshared-tasks. With average accuracy above 0.9 over the scores of all languages,\nthe task is considered mostly solved using relatively generic neural\nsequence-to-sequence models, even with little data provided. In this work, we\npropose to re-evaluate morphological inflection models by employing harder\ntrain-test splits that will challenge the generalization capacity of the\nmodels. In particular, as opposed to the na\\\"ive split-by-form, we propose a\nsplit-by-lemma method to challenge the performance on existing benchmarks. Our\nexperiments with the three top-ranked systems on the SIGMORPHON's 2020\nshared-task show that the lemma-split presents an average drop of 30 percentage\npoints in macro-average for the 90 languages included. The effect is most\nsignificant for low-resourced languages with a drop as high as 95 points, but\neven high-resourced languages lose about 10 points on average. Our results\nclearly show that generalizing inflection to unseen lemmas is far from being\nsolved, presenting a simple yet effective means to promote more sophisticated\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guriel_D/0/1/0/all/0/1\">David Guriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HopfE: Knowledge Graph Representation Learning using Inverse Hopf Fibrations. (arXiv:2108.05774v1 [cs.IR])","link":"http://arxiv.org/abs/2108.05774","description":"<p>Recently, several Knowledge Graph Embedding (KGE) approaches have been\ndevised to represent entities and relations in dense vector space and employed\nin downstream tasks such as link prediction. A few KGE techniques address\ninterpretability, i.e., mapping the connectivity patterns of the relations\n(i.e., symmetric/asymmetric, inverse, and composition) to a geometric\ninterpretation such as rotations. Other approaches model the representations in\nhigher dimensional space such as four-dimensional space (4D) to enhance the\nability to infer the connectivity patterns (i.e., expressiveness). However,\nmodeling relation and entity in a 4D space often comes at the cost of\ninterpretability. This paper proposes HopfE, a novel KGE approach aiming to\nachieve the interpretability of inferred relations in the four-dimensional\nspace. We first model the structural embeddings in 3D Euclidean space and view\nthe relation operator as an SO(3) rotation. Next, we map the entity embedding\nvector from a 3D space to a 4D hypersphere using the inverse Hopf Fibration, in\nwhich we embed the semantic information from the KG ontology. Thus, HopfE\nconsiders the structural and semantic properties of the entities without losing\nexpressivity and interpretability. Our empirical results on four well-known\nbenchmarks achieve state-of-the-art performance for the KG completion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1\">Anson Bastos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kuldeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadgeri_A/0/1/0/all/0/1\">Abhishek Nadgeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekarpour_S/0/1/0/all/0/1\">Saeedeh Shekarpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mulang_I/0/1/0/all/0/1\">Isaiah Onando Mulang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffart_J/0/1/0/all/0/1\">Johannes Hoffart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable pragmatic communication via self-supervision. (arXiv:2108.05799v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05799","description":"<p>Models of context-sensitive communication often use the Rational Speech Act\nframework (RSA; Frank &amp; Goodman, 2012), which formulates listeners and speakers\nin a cooperative reasoning process. However, the standard RSA formulation can\nonly be applied to small domains, and large-scale applications have relied on\nimitating human behavior. Here, we propose a new approach to scalable\npragmatics, building upon recent theoretical results (Zaslavsky et al., 2020)\nthat characterize pragmatic reasoning in terms of general information-theoretic\nprinciples. Specifically, we propose an architecture and learning process in\nwhich agents acquire pragmatic policies via self-supervision instead of\nimitating human data. This work suggests a new principled approach for\nequipping artificial agents with pragmatic skills via self-supervision, which\nis grounded both in pragmatic theory and in information theory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jennifer Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaslavsky_N/0/1/0/all/0/1\">Noga Zaslavsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining (second-order) graph-based and headed span-based projective dependency parsing. (arXiv:2108.05838v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05838","description":"<p>Graph-based methods are popular in dependency parsing for decades. Recently,\n\\citet{yang2021headed} propose a headed span-based method. Both of them score\nall possible trees and globally find the highest-scoring tree. In this paper,\nwe combine these two kinds of methods, designing several dynamic programming\nalgorithms for joint inference. Experiments show the effectiveness of our\nproposed methods\\footnote{Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Optimal is Greedy Decoding for Extractive Question Answering?. (arXiv:2108.05857v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05857","description":"<p>Fine-tuned language models use greedy decoding to answer reading\ncomprehension questions with relative success. However, this approach does not\nensure that the answer is a span in the given passage, nor does it guarantee\nthat it is the most probable one. Does greedy decoding actually perform worse\nthan an algorithm that does adhere to these properties? To study the\nperformance and optimality of greedy decoding, we present exact-extract, a\ndecoding algorithm that efficiently finds the most probable answer span in the\ncontext. We compare the performance of T5 with both decoding algorithms on\nzero-shot and few-shot extractive question answering. When no training examples\nare available, exact-extract significantly outperforms greedy decoding.\nHowever, greedy decoding quickly converges towards the performance of\nexact-extract with the introduction of a few training examples, becoming more\nextractive and increasingly likelier to generate the most probable span as the\ntraining set grows. We also show that self-supervised training can bias the\nmodel towards extractive behavior, increasing performance in the zero-shot\nsetting without resorting to annotated examples. Overall, our results suggest\nthat pretrained language models are so good at adapting to extractive question\nanswering, that it is often enough to fine-tune on a small training set for the\ngreedy algorithm to emulate the optimal decoding strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castel_O/0/1/0/all/0/1\">Or Castel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1\">Ori Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntax Matters! Syntax-Controlled in Text Style Transfer. (arXiv:2108.05869v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05869","description":"<p>Existing text style transfer (TST) methods rely on style classifiers to\ndisentangle the text's content and style attributes for text style transfer.\nWhile the style classifier plays a critical role in existing TST methods, there\nis no known investigation on its effect on the TST methods. In this paper, we\nconduct an empirical study on the limitations of the style classifiers used in\nexisting TST methods. We demonstrate that the existing style classifiers cannot\nlearn sentence syntax effectively and ultimately worsen existing TST models'\nperformance. To address this issue, we propose a novel Syntax-Aware\nControllable Generation (SACG) model, which includes a syntax-aware style\nclassifier that ensures learned style latent representations effectively\ncapture the syntax information for TST. Through extensive experiments on two\npopular TST tasks, we show that our proposed method significantly outperforms\nthe state-of-the-art methods. Our case studies have also demonstrated SACG's\nability to generate fluent target-style sentences that preserved the original\ncontent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu C. Aggarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The paradox of the compositionality of natural language: a neural machine translation case study. (arXiv:2108.05885v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05885","description":"<p>Moving towards human-like linguistic performance is often argued to require\ncompositional generalisation. Whether neural networks exhibit this ability is\ntypically studied using artificial languages, for which the compositionality of\ninput fragments can be guaranteed and their meanings algebraically composed.\nHowever, compositionality in natural language is vastly more complex than this\nrigid, arithmetics-like version of compositionality, and as such artificial\ncompositionality tests do not allow us to draw conclusions about how neural\nmodels deal with compositionality in more realistic scenarios. In this work, we\nre-instantiate three compositionality tests from the literature and reformulate\nthem for neural machine translation (NMT). The results highlight two main\nissues: the inconsistent behaviour of NMT models and their inability to\n(correctly) modulate between local and global processing. Aside from an\nempirical study, our work is a call to action: we should rethink the evaluation\nof compositionality in neural networks of natural language, where composing\nmeaning is not as straightforward as doing the math.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1\">Verna Dankers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1\">Elia Bruni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Dialogue Generation with Self-supervised Feature Learning. (arXiv:1903.05759v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1903.05759","description":"<p>Generating responses that are consistent with the dialogue context is one of\nthe central challenges in building engaging conversational agents. We\ndemonstrate that neural conversation models can be geared towards generating\nconsistent responses by maintaining certain features related to topics and\npersonas throughout the conversation. Past work has required external\nsupervision that exploits features such as user identities that are often\nunavailable. In our approach, topic and persona feature extractors are trained\nusing a contrastive training scheme that utilizes the natural structure of\ndialogue data. We further adopt a feature disentangling loss which, paired with\ncontrollable response generation techniques, allows us to promote or demote\ncertain learned topics and persona features. Evaluation results demonstrate the\nmodel's ability to capture meaningful topics and persona features. The\nincorporation of the learned features brings significant improvement in terms\nof the quality of generated responses on two dialogue datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1\">Chris Brockett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1\">Bill Dolan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Neural Information Fusion Architecture for Textual Network Embeddings. (arXiv:1908.11057v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/1908.11057","description":"<p>Textual network embeddings aim to learn a low-dimensional representation for\nevery node in the network so that both the structural and textual information\nfrom the networks can be well preserved in the representations. Traditionally,\nthe structural and textual embeddings were learned by models that rarely take\nthe mutual influences between them into account. In this paper, a deep neural\narchitecture is proposed to effectively fuse the two kinds of informations into\none representation. The novelties of the proposed architecture are manifested\nin the aspects of a newly defined objective function, the complementary\ninformation fusion method for structural and textual features, and the mutual\ngate mechanism for textual feature extraction. Experimental results show that\nthe proposed model outperforms the comparing methods on all three datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qinliang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijia Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ambiguity Hierarchy of Regular Infinite Tree Languages. (arXiv:2009.02985v3 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2009.02985","description":"<p>An automaton is unambiguous if for every input it has at most one accepting\ncomputation. An automaton is k-ambiguous (for k &gt; 0) if for every input it has\nat most k accepting computations. An automaton is boundedly ambiguous if it is\nk-ambiguous for some $k \\in \\mathbb{N}$. An automaton is finitely\n(respectively, countably) ambiguous if for every input it has at most finitely\n(respectively, countably) many accepting computations.\n</p>\n<p>The degree of ambiguity of a regular language is defined in a natural way. A\nlanguage is k-ambiguous (respectively, boundedly, finitely, countably\nambiguous) if it is accepted by a k-ambiguous (respectively, boundedly,\nfinitely, countably ambiguous) automaton. Over finite words every regular\nlanguage is accepted by a deterministic automaton. Over finite trees every\nregular language is accepted by an unambiguous automaton. Over $\\omega$-words\nevery regular language is accepted by an unambiguous B\\\"uchi automaton and by a\ndeterministic parity automaton. Over infinite trees Carayol et al. showed that\nthere are ambiguous languages.\n</p>\n<p>We show that over infinite trees there is a hierarchy of degrees of\nambiguity: For every k &gt; 1 there are k-ambiguous languages that are not k - 1\nambiguous; and there are finitely (respectively countably, uncountably)\nambiguous languages that are not boundedly (respectively finitely, countably)\nambiguous.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_A/0/1/0/all/0/1\">Alexander Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiferet_D/0/1/0/all/0/1\">Doron Tiferet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cyclic Proof System for HFLN. (arXiv:2010.14891v3 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2010.14891","description":"<p>A cyclic proof system allows us to perform inductive reasoning without\nexplicit inductions. We propose a cyclic proof system for HFLN, which is a\nhigher-order predicate logic with natural numbers and alternating fixed-points.\nOurs is the first cyclic proof system for a higher-order logic, to our\nknowledge. Due to the presence of higher-order predicates and alternating\nfixed-points, our cyclic proof system requires a more delicate global condition\non cyclic proofs than the original system of Brotherston and Simpson. We prove\nthe decidability of checking the global condition and soundness of this system,\nand also prove a restricted form of standard completeness for an infinitary\nvariant of our cyclic proof system. A potential application of our cyclic proof\nsystem is semi-automated verification of higher-order programs, based on\nKobayashi et al.'s recent work on reductions from program verification to HFLN\nvalidity checking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kori_M/0/1/0/all/0/1\">Mayuko Kori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_T/0/1/0/all/0/1\">Takeshi Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_N/0/1/0/all/0/1\">Naoki Kobayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Ask: Learning to Answer Questions from Millions of Narrated Videos. (arXiv:2012.00451v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.00451","description":"<p>Recent methods for visual question answering rely on large-scale annotated\ndatasets. Manual annotation of questions and answers for videos, however, is\ntedious, expensive and prevents scalability. In this work, we propose to avoid\nmanual annotation and generate a large-scale training dataset for video\nquestion answering making use of automatic cross-modal supervision. We leverage\na question generation transformer trained on text data and use it to generate\nquestion-answer pairs from transcribed video narrations. Given narrated videos,\nwe then automatically generate the HowToVQA69M dataset with 69M\nvideo-question-answer triplets. To handle the open vocabulary of diverse\nanswers in this dataset, we propose a training procedure based on a contrastive\nloss between a video-question multi-modal transformer and an answer\ntransformer. We introduce the zero-shot VideoQA task and show excellent\nresults, in particular for rare answers. Furthermore, we demonstrate our method\nto significantly outperform the state of the art on MSRVTT-QA, MSVD-QA,\nActivityNet-QA and How2QA. Finally, for a detailed evaluation we introduce\niVQA, a new VideoQA dataset with reduced language biases and high-quality\nredundant manual annotations. Our code, datasets and trained models are\navailable at https://antoyang.github.io/just-ask.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Alignment by Fine-tuning Embeddings on Parallel Corpora. (arXiv:2101.08231v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.08231","description":"<p>Word alignment over parallel corpora has a wide variety of applications,\nincluding learning translation lexicons, cross-lingual transfer of language\nprocessing tools, and automatic evaluation or analysis of translation outputs.\nThe great majority of past work on word alignment has worked by performing\nunsupervised learning on parallel texts. Recently, however, other work has\ndemonstrated that pre-trained contextualized word embeddings derived from\nmultilingually trained language models (LMs) prove an attractive alternative,\nachieving competitive results on the word alignment task even in the absence of\nexplicit training on parallel data. In this paper, we examine methods to marry\nthe two approaches: leveraging pre-trained LMs but fine-tuning them on parallel\ntext with objectives designed to improve alignment quality, and proposing\nmethods to effectively extract alignments from these fine-tuned models. We\nperform experiments on five language pairs and demonstrate that our model can\nconsistently outperform previous state-of-the-art models of all varieties. In\naddition, we demonstrate that we are able to train multilingual word aligners\nthat can obtain robust performance on different language pairs. Our aligner,\nAWESOME (Aligning Word Embedding Spaces of Multilingual Encoders), with\npre-trained models is available at https://github.com/neulab/awesome-align\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting User-Perceived Latency of On-Device E2E Speech Recognition. (arXiv:2104.02207v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2104.02207","description":"<p>As speech-enabled devices such as smartphones and smart speakers become\nincreasingly ubiquitous, there is growing interest in building automatic speech\nrecognition (ASR) systems that can run directly on-device; end-to-end (E2E)\nspeech recognition models such as recurrent neural network transducers and\ntheir variants have recently emerged as prime candidates for this task. Apart\nfrom being accurate and compact, such systems need to decode speech with low\nuser-perceived latency (UPL), producing words as soon as they are spoken. This\nwork examines the impact of various techniques - model architectures, training\ncriteria, decoding hyperparameters, and endpointer parameters - on UPL. Our\nanalyses suggest that measures of model size (parameters, input chunk sizes),\nor measures of computation (e.g., FLOPS, RTF) that reflect the model's ability\nto process input frames are not always strongly correlated with observed UPL.\nThus, conventional algorithmic latency measurements might be inadequate in\naccurately capturing latency observed when models are deployed on embedded\ndevices. Instead, we find that factors affecting token emission latency, and\nendpointing behavior have a larger impact on UPL. We achieve the best trade-off\nbetween latency and word error rate when performing ASR jointly with\nendpointing, while utilizing the recently proposed alignment regularization\nmechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Y/0/1/0/all/0/1\">Yuan Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahadeokar_J/0/1/0/all/0/1\">Jay Mahadeokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yangyang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiatong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunyang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1\">Christian Fuegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction. (arXiv:2105.05498v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05498","description":"<p>Accurate terminology translation is crucial for ensuring the practicality and\nreliability of neural machine translation (NMT) systems. To address this,\nlexically constrained NMT explores various methods to ensure pre-specified\nwords and phrases appear in the translation output. However, in many cases,\nthose methods are studied on general domain corpora, where the terms are mostly\nuni- and bi-grams (&gt;98%). In this paper, we instead tackle a more challenging\nsetup consisting of domain-specific corpora with much longer n-gram and highly\nspecialized terms. Inspired by the recent success of masked span prediction\nmodels, we propose a simple and effective training strategy that achieves\nconsistent improvements on both terminology and sentence-level translation for\nthree domain-specific corpora in two language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gyubok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongjun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Similar Language Translation With Transfer Learning. (arXiv:2108.03533v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.03533","description":"<p>We investigate transfer learning based on pre-trained neural machine\ntranslation models to translate between (low-resource) similar languages. This\nwork is part of our contribution to the WMT 2021 Similar Languages Translation\nShared Task where we submitted models for different language pairs, including\nFrench-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our\nmodels for Catalan-Spanish ($82.79$ BLEU) and Portuguese-Spanish ($87.11$ BLEU)\nrank top 1 in the official shared task evaluation, and we are the only team to\nsubmit models for the French-Bambara pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}