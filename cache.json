{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of \"Subjectivity\" and \"Identity Terms\". (arXiv:2109.02691v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02691","description":"<p>Toxic comment classification models are often found biased toward identity\nterms which are terms characterizing a specific group of people such as\n\"Muslim\" and \"black\". Such bias is commonly reflected in false-positive\npredictions, i.e. non-toxic comments with identity terms. In this work, we\npropose a novel approach to tackle such bias in toxic comment classification,\nleveraging the notion of subjectivity level of a comment and the presence of\nidentity terms. We hypothesize that when a comment is made about a group of\npeople that is characterized by an identity term, the likelihood of that\ncomment being toxic is associated with the subjectivity level of the comment,\ni.e. the extent to which the comment conveys personal feelings and opinions.\nBuilding upon the BERT model, we propose a new structure that is able to\nleverage these features, and thoroughly evaluate our model on 4 datasets of\nvarying sizes and representing different social media platforms. The results\nshow that our model can consistently outperform BERT and a SOTA model devised\nto address identity term bias in a different way, with a maximum improvement in\nF1 of 2.43% and 1.91% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhixue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopfgartner_F/0/1/0/all/0/1\">Frank Hopfgartner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-to-Table: A New Way of Information Extraction. (arXiv:2109.02707v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02707","description":"<p>We study a new problem setting of information extraction (IE), referred to as\ntext-to-table, which can be viewed as an inverse problem of the well-studied\ntable-to-text. In text-to-table, given a text, one creates a table or several\ntables expressing the main content of the text, while the model is learned from\ntext-table pair data. The problem setting differs from those of the existing\nmethods for IE. First, the extraction can be carried out from long texts to\nlarge tables with complex structures. Second, the extraction is entirely\ndata-driven, and there is no need to explicitly define the schemas. As far as\nwe know, there has been no previous work that studies the problem. In this\nwork, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.\nWe first employ a seq2seq model fine-tuned from a pre-trained language model to\nperform the task. We also develop a new method within the seq2seq approach,\nexploiting two additional techniques in table generation: table constraint and\ntable relation embeddings. We make use of four existing table-to-text datasets\nin our experiments on text-to-table. Experimental results show that the vanilla\nseq2seq model can outperform the baseline methods of using relation extraction\nand named entity extraction. The results also show that our method can further\nboost the performances of the vanilla seq2seq model. We further discuss the\nmain challenges of the proposed task. The code and data will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xueqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiacheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Inspiring Content on Social Media. (arXiv:2109.02734v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02734","description":"<p>Inspiration moves a person to see new possibilities and transforms the way\nthey perceive their own potential. Inspiration has received little attention in\npsychology, and has not been researched before in the NLP community. To the\nbest of our knowledge, this work is the first to study inspiration through\nmachine learning methods. We aim to automatically detect inspiring content from\nsocial media data. To this end, we analyze social media posts to tease out what\nmakes a post inspiring and what topics are inspiring. We release a dataset of\n5,800 inspiring and 5,800 non-inspiring English-language public post unique ids\ncollected from a dump of Reddit public posts made available by a third party\nand use linguistic heuristics to automatically detect which social media\nEnglish-language posts are inspiring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jane A. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica. (arXiv:2109.02738v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02738","description":"<p>People convey their intention and attitude through linguistic styles of the\ntext that they write. In this study, we investigate lexicon usages across\nstyles throughout two lenses: human perception and machine word importance,\nsince words differ in the strength of the stylistic cues that they provide. To\ncollect labels of human perception, we curate a new dataset, Hummingbird, on\ntop of benchmarking style datasets. We have crowd workers highlight the\nrepresentative words in the text that makes them think the text has the\nfollowing styles: politeness, sentiment, offensiveness, and five emotion types.\nWe then compare these human word labels with word importance derived from a\npopular fine-tuned style classifier like BERT. Our results show that the BERT\noften finds content words not relevant to the target style as important words\nused in style prediction, but humans do not perceive the same way even though\nfor some styles (e.g., positive sentiment and joy) human- and\nmachine-identified words share significant overlap for some styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1\">Shirley Anugrah Hayati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])","link":"http://arxiv.org/abs/2109.02747","description":"<p>We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the {\\sc WhyAct} dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hanwen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Neural Information Status Classification. (arXiv:2109.02753v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02753","description":"<p>Most previous studies on information status (IS) classification and bridging\nanaphora recognition assume that the gold mention or syntactic tree information\nis given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio,\n2020). In this paper, we propose an end-to-end neural approach for information\nstatus classification. Our approach consists of a mention extraction component\nand an information status assignment component. During the inference time, our\nsystem takes a raw text as the input and generates mentions together with their\ninformation status. On the ISNotes corpus (Markert et al., 2012), we show that\nour information status assignment component achieves new state-of-the-art\nresults on fine-grained IS classification based on gold mentions. Furthermore,\nour system performs significantly better than other baselines for both mention\nextraction and fine-grained IS classification in the end-to-end setting.\nFinally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger,\n2016) to recognize referential bridging anaphora. We find that our end-to-end\nsystem trained on ISNotes achieves competitive results on bridging anaphora\nrecognition compared to the previous state-of-the-art system that relies on\nsyntactic information and is trained on the in-domain datasets (Yu and Poesio,\n2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])","link":"http://arxiv.org/abs/2109.02789","description":"<p>Pretrained contextualized representations offer great success for many\ndownstream tasks, including document ranking. The multilingual versions of such\npretrained representations provide a possibility of jointly learning many\nlanguages with the same model. Although it is expected to gain big with such\njoint training, in the case of cross lingual information retrieval (CLIR), the\nmodels under a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize that the\nperformance drop is due to the translation gap between query and documents. In\nthe monolingual retrieval task, because of the same lexical inputs, it is\neasier for model to identify the query terms that occurred in documents.\nHowever, in the multilingual pretrained models that the words in different\nlanguages are projected into the same hyperspace, the model tends to translate\nquery terms into related terms, i.e., terms that appear in a similar context,\nin addition to or sometimes rather than synonyms in the target language. This\nproperty is creating difficulties for the model to connect terms that cooccur\nin both query and document. To address this issue, we propose a novel Mixed\nAttention Transformer (MAT) that incorporates external word level knowledge,\nsuch as a dictionary or translation table. We design a sandwich like\narchitecture to embed MAT into the recent transformer based deep neural models.\nBy encoding the translation knowledge into an attention matrix, the model with\nMAT is able to focus on the mutually translated words in the input sequence.\nExperimental results demonstrate the effectiveness of the external knowledge\nand the significant improvement of MAT embedded neural reranking model on CLIR\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])","link":"http://arxiv.org/abs/2109.02797","description":"<p>The application of Generative Pre-trained Transformer (GPT-2) to learn\ntext-archived game notation provides a model environment for exploring sparse\nreward gameplay. The transformer architecture proves amenable to training on\nsolved text archives describing mazes, Rubik's Cube, and Sudoku solvers. The\nmethod benefits from fine-tuning the transformer architecture to visualize\nplausible strategies derived outside any guidance from human heuristics or\ndomain expertise. The large search space ($&gt;10^{19}$) for the games provides a\npuzzle environment in which the solution has few intermediate rewards and a\nfinal move that solves the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burdick_R/0/1/0/all/0/1\">Ryerson Burdick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02808","description":"<p>FDA has been promoting enrollment practices that could enhance the diversity\nof clinical trial populations, through broadening eligibility criteria.\nHowever, how to broaden eligibility remains a significant challenge. We propose\nan AI approach to Cohort Optimization (AICO) through transformer-based natural\nlanguage processing of the eligibility criteria and evaluation of the criteria\nusing real-world data. The method can extract common eligibility criteria\nvariables from a large set of relevant trials and measure the generalizability\nof trial designs to real-world patients. It overcomes the scalability limits of\nexisting manual methods and enables rapid simulation of eligibility criteria\ndesign for a disease of interest. A case study on breast cancer trial design\ndemonstrates the utility of the method in improving trial generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Cheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deore_U/0/1/0/all/0/1\">Uday Deore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Myah Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_I/0/1/0/all/0/1\">Iya Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devarakonda_M/0/1/0/all/0/1\">Murthy Devarakonda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models. (arXiv:2109.02837v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02837","description":"<p>Commonsense reasoning benchmarks have been largely solved by fine-tuning\nlanguage models. The downside is that fine-tuning may cause models to overfit\nto task-specific data and thereby forget their knowledge gained during\npre-training. Recent works only propose lightweight model updates as models may\nalready possess useful knowledge from past experience, but a challenge remains\nin understanding what parts and to what extent models should be refined for a\ngiven task. In this paper, we investigate what models learn from commonsense\nreasoning datasets. We measure the impact of three different adaptation methods\non the generalization and accuracy of models. Our experiments with two models\nshow that fine-tuning performs best, by learning both the content and the\nstructure of the task, but suffers from overfitting and limited generalization\nto novel answers. We observe that alternative adaptation methods like\nprefix-tuning have comparable accuracy, but generalize better to unseen answers\nand are more robust to adversarial splits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_S/0/1/0/all/0/1\">Satoru Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1\">Alessandro Oltramari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Datasets: A Community Library for Natural Language Processing. (arXiv:2109.02846v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02846","description":"<p>The scale, variety, and quantity of publicly-available NLP datasets has grown\nrapidly as researchers propose new tasks, larger models, and novel benchmarks.\nDatasets is a community library for contemporary NLP designed to support this\necosystem. Datasets aims to standardize end-user interfaces, versioning, and\ndocumentation, while providing a lightweight front-end that behaves similarly\nfor small datasets as for internet-scale corpora. The design of the library\nincorporates a distributed, community-driven approach to adding datasets and\ndocumenting usage. After a year of development, the library now includes more\nthan 650 unique datasets, has more than 250 contributors, and has helped\nsupport a variety of novel cross-dataset research projects and shared tasks.\nThe library is available at https://github.com/huggingface/datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1\">Quentin Lhoest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhishek Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1\">Patrick von Platen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Suraj Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaumond_J/0/1/0/all/0/1\">Julien Chaumond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drame_M/0/1/0/all/0/1\">Mariama Drame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plu_J/0/1/0/all/0/1\">Julien Plu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_J/0/1/0/all/0/1\">Joe Davison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasko_M/0/1/0/all/0/1\">Mario &#x160;a&#x161;ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_B/0/1/0/all/0/1\">Bhavitvya Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandeis_S/0/1/0/all/0/1\">Simon Brandeis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patry_N/0/1/0/all/0/1\">Nicolas Patry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_P/0/1/0/all/0/1\">Philipp Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gugger_S/0/1/0/all/0/1\">Sylvain Gugger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delangue_C/0/1/0/all/0/1\">Cl&#xe9;ment Delangue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matussiere_T/0/1/0/all/0/1\">Th&#xe9;o Matussi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debut_L/0/1/0/all/0/1\">Lysandre Debut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1\">Stas Bekman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cistac_P/0/1/0/all/0/1\">Pierric Cistac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goehringer_T/0/1/0/all/0/1\">Thibault Goehringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustar_V/0/1/0/all/0/1\">Victor Mustar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_F/0/1/0/all/0/1\">Fran&#xe7;ois Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Regular Expressions with Neural Networks via DFA. (arXiv:2109.02882v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02882","description":"<p>Human-designed rules are widely used to build industry applications. However,\nit is infeasible to maintain thousands of such hand-crafted rules. So it is\nvery important to integrate the rule knowledge into neural networks to build a\nhybrid model that achieves better performance. Specifically, the human-designed\nrules are formulated as Regular Expressions (REs), from which the equivalent\nMinimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to\nuse the MDFA as an intermediate model to capture the matched RE patterns as\nrule-based features for each input sentence and introduce these additional\nfeatures into neural networks. We evaluate the proposed method on the ATIS\nintent classification task. The experiment results show that the proposed\nmethod achieves the best performance compared to neural networks and four other\nmethods that combine REs and neural networks when the training dataset is\nrelatively small.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chengjie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhenzhou Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages. (arXiv:2109.02903v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02903","description":"<p>In this paper we present IndicBART, a multilingual, sequence-to-sequence\npre-trained model focusing on 11 Indic languages and English. Different from\nexisting pre-trained models, IndicBART utilizes the orthographic similarity\nbetween Indic scripts to improve transfer learning between similar Indic\nlanguages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation\n(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs\nand extreme summarization for 7 languages using multilingual fine-tuning show\nthat IndicBART is competitive with or better than mBART50 despite containing\nsignificantly fewer parameters. Our analyses focus on identifying the impact of\nscript unification (to Devanagari), corpora size as well as multilingualism on\nthe final performance. The IndicBART model is available under the MIT license\nat https://indicnlp.ai4bharat.org/indic-bart .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrotriya_H/0/1/0/all/0/1\">Himani Shrotriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Reasoning Chains for Multi-hop Science Question Answering. (arXiv:2109.02905v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02905","description":"<p>We propose a novel Chain Guided Retriever-reader ({\\tt CGR}) framework to\nmodel the reasoning chain for multi-hop Science Question Answering. Our\nframework is capable of performing explainable reasoning without the need of\nany corpus-specific annotations, such as the ground-truth reasoning chain, or\nhuman-annotated entity mentions. Specifically, we first generate reasoning\nchains from a semantic graph constructed by Abstract Meaning Representation of\nretrieved evidence facts. A \\textit{Chain-aware loss}, concerning both local\nand global chain information, is also designed to enable the generated chains\nto serve as distant supervision signals for training the retriever, where\nreinforcement learning is also adopted to maximize the utility of the reasoning\nchains. Our framework allows the retriever to capture step-by-step clues of the\nentire reasoning process, which is not only shown to be effective on two\nchallenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge,\nbut also favors explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Driven Content Creation using Statistical and Natural Language Processing Techniques for Financial Domain. (arXiv:2109.02935v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02935","description":"<p>Over the years customers' expectation of getting information instantaneously\nhas given rise to the increased usage of channels like virtual assistants.\nTypically, customers try to get their questions answered by low-touch channels\nlike search and virtual assistant first, before getting in touch with a live\nchat agent or the phone representative. Higher usage of these low-touch systems\nis a win-win for both customers and the organization since it enables\norganizations to attain a low cost of service while customers get served\nwithout delay. In this paper, we propose a two-part framework where the first\npart describes methods to combine the information from different interaction\nchannels like call, search, and chat. We do this by summarizing (using a\nstacked Bi-LSTM network) the high-touch interaction channel data such as call\nand chat into short searchquery like customer intents and then creating an\norganically grown intent taxonomy from interaction data (using Hierarchical\nAgglomerative Clustering). The second part of the framework focuses on\nextracting customer questions by analyzing interaction data sources. It\ncalculates similarity scores using TF-IDF and BERT(Devlin et al., 2019). It\nalso maps these identified questions to the output of the first part of the\nframework using syntactic and semantic similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagwanshi_P/0/1/0/all/0/1\">Prateek Nagwanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT. (arXiv:2109.02938v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02938","description":"<p>This paper presents an automatic method to evaluate the naturalness of\nnatural language generation in dialogue systems. While this task was previously\nrendered through expensive and time-consuming human labor, we present this\nnovel task of automatic naturalness evaluation of generated language. By\nfine-tuning the BERT model, our proposed naturalness evaluation method shows\nrobust results and outperforms the baselines: support vector machines,\nbi-directional LSTMs, and BLEURT. In addition, the training speed and\nevaluation performance of naturalness model are improved by transfer learning\nfrom quality and informativeness linguistic knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_W/0/1/0/all/0/1\">Wolfgang Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02941","description":"<p>Online hate speech has caught everyone's attention from the news related to\nthe COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -\nan umbrella term for online hateful behavior, manifests itself in forms such as\nonline hate speech. Hate speech is a deliberate attack directed towards an\nindividual or a group motivated by the targeted entity's identity or opinions.\nThe rising mass communication through social media further exacerbates the\nharmful consequences of online hate speech. While there has been significant\nresearch on hate-speech identification using Natural Language Processing (NLP),\nthe work on utilizing NLP for prevention and intervention of online hate speech\nlacks relatively. This paper presents a holistic conceptual framework on\nhate-speech NLP countering methods along with a thorough survey on the current\nprogress of NLP for countering online hate speech. It classifies the countering\ntechniques based on their time of action, and identifies potential future\nresearch areas on this topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Mudit Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrase Generation as Unsupervised Machine Translation. (arXiv:2109.02950v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02950","description":"<p>In this paper, we propose a new paradigm for paraphrase generation by\ntreating the task as unsupervised machine translation (UMT) based on the\nassumption that there must be pairs of sentences expressing the same meaning in\na large-scale unlabeled monolingual corpus. The proposed paradigm first splits\na large unlabeled corpus into multiple clusters, and trains multiple UMT models\nusing pairs of these clusters. Then based on the paraphrase pairs produced by\nthese UMT models, a unified surrogate model can be trained to serve as the\nfinal Seq2Seq model to generate paraphrases, which can be directly used for\ntest in the unsupervised setup, or be finetuned on labeled datasets in the\nsupervised setup. The proposed method offers merits over\nmachine-translation-based paraphrase generation methods, as it avoids reliance\non bilingual sentence pairs. It also allows human intervene with the model so\nthat more diverse paraphrases can be generated using different filtering\ncriteria. Extensive experiments on existing paraphrase dataset for both the\nsupervised and unsupervised setups demonstrate the effectiveness the proposed\nparadigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to Identify Toxic, Engaging, & Fact-Claiming Comments. (arXiv:2109.02966v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02966","description":"<p>In this paper we describe the methods we used for our submissions to the\nGermEval 2021 shared task on the identification of toxic, engaging, and\nfact-claiming comments. For all three subtasks we fine-tuned freely available\ntransformer-based models from the Huggingface model hub. We evaluated the\nperformance of various pre-trained models after fine-tuning on 80% of the\ntraining data with different hyperparameters and submitted predictions of the\ntwo best performing resulting models. We found that this approach worked best\nfor subtask 3, for which we achieved an F1-score of 0.736.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gawron_C/0/1/0/all/0/1\">Christian Gawron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Sebastian Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02972","description":"<p>Despite constant improvements in machine translation quality, automatic\npoetry translation remains a challenging problem due to the lack of\nopen-sourced parallel poetic corpora, and to the intrinsic complexities\ninvolved in preserving the semantics, style, and figurative nature of poetry.\nWe present an empirical investigation for poetry translation along several\ndimensions: 1) size and style of training data (poetic vs. non-poetic),\nincluding a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)\nlanguage-family-specific models vs. mixed-multilingual models. To accomplish\nthis, we contribute a parallel dataset of poetry translations for several\nlanguage pairs. Our results show that multilingual fine-tuning on poetic text\nsignificantly outperforms multilingual fine-tuning on non-poetic text that is\n35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and\nhuman evaluation metrics such as faithfulness (meaning and poetic style).\nMoreover, multilingual fine-tuning on poetic data outperforms \\emph{bilingual}\nfine-tuning on poetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Context Choices for Context-aware Machine Translation. (arXiv:2109.02995v1 [cs.CL])","link":"http://arxiv.org/abs/2109.02995","description":"<p>One of the most popular methods for context-aware machine translation (MT) is\nto use separate encoders for the source sentence and context as multiple\nsources for one target sentence. Recent work has cast doubt on whether these\nmodels actually learn useful signals from the context or are improvements in\nautomatic evaluation metrics just a side-effect. We show that multi-source\ntransformer models improve MT over standard transformer-base models even with\nempty lines provided as context, but the translation quality improves\nsignificantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is\nprovided. We also show that even though randomly shuffling in-domain context\ncan also improve over baselines, the correct context further improves\ntranslation quality and random out-of-domain context further degrades it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rikters_M/0/1/0/all/0/1\">Mat&#x12b;ss Rikters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakazawa_T/0/1/0/all/0/1\">Toshiaki Nakazawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge. (arXiv:2109.03004v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03004","description":"<p>One challenge for dialogue agents is to recognize feelings of the\nconversation partner and respond accordingly. In this work, RoBERTa-GPT2 is\nproposed for empathetic dialogue generation, where the pre-trained\nauto-encoding RoBERTa is utilised as encoder and the pre-trained\nauto-regressive GPT-2 as decoder. With the combination of the pre-trained\nRoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.\nTo enable the empathetic ability of RoBERTa-GPT2 model, we propose a\ncommonsense knowledge and emotional concepts extractor, in which the\ncommonsensible and emotional concepts of dialogue context are extracted for the\nGPT-2 decoder. The experiment results demonstrate that the empathetic dialogue\ngeneration benefits from both pre-trained encoder-decoder architecture and\nexternal knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_W/0/1/0/all/0/1\">Wolfgang Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Attention Module for Natural Language Processing. (arXiv:2109.03009v1 [cs.AI])","link":"http://arxiv.org/abs/2109.03009","description":"<p>Recently, large pre-trained neural language models have attained remarkable\nperformance on many downstream natural language processing (NLP) applications\nvia fine-tuning. In this paper, we target at how to further improve the token\nrepresentations on the language models. We, therefore, propose a simple yet\neffective plug-and-play module, Sequential Attention Module (SAM), on the token\nembeddings learned from a pre-trained language model. Our proposed SAM consists\nof two main attention modules deployed sequentially: Feature-wise Attention\nModule (FAM) and Token-wise Attention Module (TAM). More specifically, FAM can\neffectively identify the importance of features at each dimension and promote\nthe effect via dot-product on the original token embeddings for downstream NLP\napplications. Meanwhile, TAM can further re-weight the features at the\ntoken-wise level. Moreover, we propose an adaptive filter on FAM to prevent\nnoise impact and increase information absorption. Finally, we conduct extensive\nexperiments to demonstrate the advantages and properties of our proposed SAM.\nWe first show how SAM plays a primary role in the champion solution of two\nsubtasks of SemEval'21 Task 7. After that, we apply SAM on sentiment analysis\nand three popular NLP tasks and demonstrate that SAM consistently outperforms\nthe state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lianxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generate & Rank: A Multi-task Framework for Math Word Problems. (arXiv:2109.03034v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03034","description":"<p>Math word problem (MWP) is a challenging and critical task in natural\nlanguage processing. Many recent studies formalize MWP as a generation task and\nhave adopted sequence-to-sequence models to transform problem descriptions to\nmathematical expressions. However, mathematical expressions are prone to minor\nmistakes while the generation objective does not explicitly handle such\nmistakes. To address this limitation, we devise a new ranking task for MWP and\npropose Generate &amp; Rank, a multi-task framework based on a generative\npre-trained language model. By joint training with generation and ranking, the\nmodel learns from its own mistakes and is able to distinguish between correct\nand incorrect expressions. Meanwhile, we perform tree-based disturbance\nspecially designed for MWP and an online update to boost the ranker. We\ndemonstrate the effectiveness of our proposed method on the benchmark and the\nresults show that our method consistently outperforms baselines in all\ndatasets. Particularly, in the classical Math23k, our method is 7% (78.4%\n$\\rightarrow$ 85.4%) higher than the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])","link":"http://arxiv.org/abs/2109.03039","description":"<p>Conversational search systems, such as Google Assistant and Microsoft\nCortana, provide a new search paradigm where users are allowed, via natural\nlanguage dialogues, to communicate with search systems. Evaluating such systems\nis very challenging since search results are presented in the format of natural\nlanguage sentences. Given the unlimited number of possible responses,\ncollecting relevance assessments for all the possible responses is infeasible.\nIn this paper, we propose POSSCORE, a simple yet effective automatic evaluation\nmethod for conversational search. The proposed embedding-based metric takes the\ninfluence of part of speech (POS) of the terms in the response into account. To\nthe best knowledge, our work is the first to systematically demonstrate the\nimportance of incorporating syntactic information, such as POS labels, for\nconversational search evaluation. Experimental results demonstrate that our\nmetrics can correlate with human preference, achieving significant improvements\nover state-of-the-art baseline metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Ke Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiaxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1\">Max L. Wilson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning. (arXiv:2109.03062v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03062","description":"<p>Multitask deep learning has been applied to patient outcome prediction from\ntext, taking clinical notes as input and training deep neural networks with a\njoint loss function of multiple tasks. However, the joint training scheme of\nmultitask learning suffers from inter-task interference, and diagnosis\nprediction among the multiple tasks has the generalizability issue due to rare\ndiseases or unseen diagnoses. To solve these challenges, we propose a\nhypernetwork-based approach that generates task-conditioned parameters and\ncoefficients of multitask prediction heads to learn task-specific prediction\nand balance the multitask learning. We also incorporate semantic task\ninformation to improves the generalizability of our task-conditioned multitask\nmodel. Experiments on early and discharge notes extracted from the real-world\nMIMIC database show our method can achieve better performance on multitask\npatient outcome prediction than strong baselines in most cases. Besides, our\nmethod can effectively handle the scenario with limited information and improve\nzero-shot prediction on unseen diagnosis categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation. (arXiv:2109.03079v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03079","description":"<p>Practical dialogue systems require robust methods of detecting out-of-scope\n(OOS) utterances to avoid conversational breakdowns and related failure modes.\nDirectly training a model with labeled OOS examples yields reasonable\nperformance, but obtaining such data is a resource-intensive process. To tackle\nthis limited-data problem, previous methods focus on better modeling the\ndistribution of in-scope (INS) examples. We introduce GOLD as an orthogonal\ntechnique that augments existing data to train better OOS detectors operating\nin low-data regimes. GOLD generates pseudo-labeled candidates using samples\nfrom an auxiliary dataset and keeps only the most beneficial candidates for\ntraining through a novel filtering mechanism. In experiments across three\ntarget benchmarks, the top GOLD model outperforms all existing methods on all\nkey metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median\nbaseline performance. We also analyze the unique properties of OOS data to\nidentify key factors for optimally applying our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning grounded word meaning representations on similarity graphs. (arXiv:2109.03084v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03084","description":"<p>This paper introduces a novel approach to learn visually grounded meaning\nrepresentations of words as low-dimensional node embeddings on an underlying\ngraph hierarchy. The lower level of the hierarchy models modality-specific word\nrepresentations through dedicated but communicating graphs, while the higher\nlevel puts these representations together on a single graph to learn a\nrepresentation jointly from both modalities. The topology of each graph models\nsimilarity relations among words, and is estimated jointly with the graph\nembedding. The assumption underlying this model is that words sharing similar\nmeaning correspond to communities in an underlying similarity graph in a\nlow-dimensional space. We named this model Hierarchical Multi-Modal Similarity\nGraph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE\nto simulate human similarity judgements and concept categorization,\noutperforming the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1\">Herwig Wendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batlle_P/0/1/0/all/0/1\">Pau Batlle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning. (arXiv:2109.03094v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03094","description":"<p>The availability of language representations learned by large pretrained\nneural network models (such as BERT and ELECTRA) has led to improvements in\nmany downstream Natural Language Processing tasks in recent years. Pretrained\nmodels usually differ in pretraining objectives, architectures, and datasets\nthey are trained on which can affect downstream performance. In this\ncontribution, we fine-tuned German BERT and German ELECTRA models to identify\ntoxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)\nin Facebook data provided by the GermEval 2021 competition. We created\nensembles of these models and investigated whether and how classification\nperformance depends on the number of ensemble members and their composition. On\nout-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for\nall subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bornheim_T/0/1/0/all/0/1\">Tobias Bornheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grieger_N/0/1/0/all/0/1\">Niklas Grieger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bialonski_S/0/1/0/all/0/1\">Stephan Bialonski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infusing Future Information into Monotonic Attention Through Language Models. (arXiv:2109.03121v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03121","description":"<p>Simultaneous neural machine translation(SNMT) models start emitting the\ntarget sequence before they have processed the source sequence. The recent\nadaptive policies for SNMT use monotonic attention to perform read/write\ndecisions based on the partial source and target sequences. The lack of\nsufficient information might cause the monotonic attention to take poor\nread/write decisions, which in turn negatively affects the performance of the\nSNMT model. On the other hand, human translators make better read/write\ndecisions since they can anticipate the immediate future words using linguistic\ninformation and domain knowledge.Motivated by human translators, in this work,\nwe propose a framework to aid monotonic attention with an external language\nmodel to improve its decisions.We conduct experiments on the MuST-C\nEnglish-German and English-French speech-to-text translation tasks to show the\neffectiveness of the proposed framework.The proposed SNMT method improves the\nquality-latency trade-off over the state-of-the-art monotonic multihead\nattention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_M/0/1/0/all/0/1\">Mohd Abbas Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indurthi_S/0/1/0/all/0/1\">Sathish Indurthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Beomseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakumarapu_N/0/1/0/all/0/1\">Nikhil Kumar Lakumarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangha Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rare Words Degenerate All Words. (arXiv:2109.03127v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03127","description":"<p>Despite advances in neural network language model, the representation\ndegeneration problem of embeddings is still challenging. Recent studies have\nfound that the learned output embeddings are degenerated into a narrow-cone\ndistribution which makes the similarity between each embeddings positive. They\nanalyzed the cause of the degeneration problem has been demonstrated as common\nto most embeddings. However, we found that the degeneration problem is\nespecially originated from the training of embeddings of rare words. In this\nstudy, we analyze the intrinsic mechanism of the degeneration of rare word\nembeddings with respect of their gradient about the negative log-likelihood\nloss function. Furthermore, we theoretically and empirically demonstrate that\nthe degeneration of rare word embeddings causes the degeneration of non-rare\nword embeddings, and that the overall degeneration problem can be alleviated by\npreventing the degeneration of rare word embeddings. Based on our analyses, we\npropose a novel method, Adaptive Gradient Partial Scaling(AGPS), to address the\ndegeneration problem. Experimental results demonstrate the effectiveness of the\nproposed method qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sangwon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jongyoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-min Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_W/0/1/0/all/0/1\">Woo-Jong Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03137","description":"<p>Existing generative pre-trained language models (e.g., GPT) focus on modeling\nthe language structure and semantics of general texts. However, those models do\nnot consider the numerical properties of numbers and cannot perform robustly on\nnumerical reasoning tasks (e.g., math word problems and measurement\nestimation). In this paper, we propose NumGPT, a generative pre-trained model\nthat explicitly models the numerical properties of numbers in texts.\nSpecifically, it leverages a prototype-based numeral embedding to encode the\nmantissa of the number and an individual embedding to encode the exponent of\nthe number. A numeral-aware loss function is designed to integrate numerals\ninto the pre-training objective of NumGPT. We conduct extensive experiments on\nfour different datasets to evaluate the numeracy ability of NumGPT. The\nexperiment results show that NumGPT outperforms baseline models (e.g., GPT and\nGPT with DICE) on a range of numerical reasoning tasks such as measurement\nestimation, number comparison, math word problems, and magnitude\nclassification. Ablation studies are also conducted to evaluate the impact of\npre-training and model hyperparameters on the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03155","description":"<p>Sentence embedding refers to a set of effective and versatile techniques for\nconverting raw text into numerical vector representations that can be used in a\nwide range of natural language processing (NLP) applications. The majority of\nthese techniques are either supervised or unsupervised. Compared to the\nunsupervised methods, the supervised ones make less assumptions about\noptimization objectives and usually achieve better results. However, the\ntraining requires a large amount of labeled sentence pairs, which is not\navailable in many industrial scenarios. To that end, we propose a generic and\nend-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence\nEmbedding), capable of learning high-quality sentence embeddings from a\npartially labeled dataset. We experimentally show that PAUSE achieves, and\nsometimes surpasses, state-of-the-art results using only a small fraction of\nlabeled sentence pairs on various benchmark tasks. When applied to a real\nindustrial use case where labeled samples are scarce, PAUSE encourages us to\nextend our dataset without the liability of extensive manual annotation work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lele Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1\">Emil Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehrenheim_V/0/1/0/all/0/1\">Vilhelm von Ehrenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_D/0/1/0/all/0/1\">Dhiana Deva Cavalcanti Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Anna Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_S/0/1/0/all/0/1\">Sonja Horn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03158","description":"<p>An individual's variation in writing style is often a function of both social\nand personal attributes. While structured social variation has been extensively\nstudied, e.g., gender based variation, far less is known about how to\ncharacterize individual styles due to their idiosyncratic nature. We introduce\na new approach to studying idiolects through a massive cross-author comparison\nto identify and encode stylistic features. The neural model achieves strong\nperformance at authorship identification on short texts and through an\nanalogy-based probing task, showing that the learned representations exhibit\nsurprising regularities that encode qualitative and quantitative shifts of\nidiolectal styles. Through text perturbation, we quantify the relative\ncontributions of different linguistic elements to idiolectal variation.\nFurthermore, we provide a description of idiolects through measuring inter- and\nintra-author variation, showing that variation in idiolects is often\ndistinctive yet consistent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03160","description":"<p>Transformers-based pretrained language models achieve outstanding results in\nmany well-known NLU benchmarks. However, while pretraining methods are very\nconvenient, they are expensive in terms of time and resources. This calls for a\nstudy of the impact of pretraining data size on the knowledge of the models. We\nexplore this impact on the syntactic capabilities of RoBERTa, using models\ntrained on incremental sizes of raw text data. First, we use syntactic\nstructural probes to determine whether models pretrained on more data encode a\nhigher amount of syntactic information. Second, we perform a targeted syntactic\nevaluation to analyze the impact of pretraining data size on the syntactic\ngeneralization performance of the models. Third, we compare the performance of\nthe different models on three downstream applications: part-of-speech tagging,\ndependency parsing and paraphrase identification. We complement our study with\nan analysis of the cost-benefit trade-off of training such models. Our\nexperiments show that while models pretrained on more data encode more\nsyntactic knowledge and perform better on downstream applications, they do not\nalways offer a better performance across the different syntactic phenomena and\ncome at a higher financial and environmental cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Mayos_L/0/1/0/all/0/1\">Laura P&#xe9;rez-Mayos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanner_L/0/1/0/all/0/1\">Leo Wanner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-Controllable Opinion Summarization. (arXiv:2109.03171v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03171","description":"<p>Recent work on opinion summarization produces general summaries based on a\nset of input reviews and the popularity of opinions expressed in them. In this\npaper, we propose an approach that allows the generation of customized\nsummaries based on aspect queries (e.g., describing the location and room of a\nhotel). Using a review corpus, we create a synthetic training dataset of\n(review, summary) pairs enriched with aspect controllers which are induced by a\nmulti-instance learning model that predicts the aspects of a document at\ndifferent levels of granularity. We fine-tune a pretrained model using our\nsynthetic dataset and generate aspect-specific summaries by modifying the\naspect controllers. Experiments on two benchmarks show that our model\noutperforms the previous state of the art and generates personalized summaries\nby controlling the number of aspects discussed in them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelidis_S/0/1/0/all/0/1\">Stefanos Angelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When differential privacy meets NLP: The devil is in the detail. (arXiv:2109.03175v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03175","description":"<p>Differential privacy provides a formal approach to privacy of individuals.\nApplications of differential privacy in various scenarios, such as protecting\nusers' original utterances, must satisfy certain mathematical properties. Our\ncontribution is a formal analysis of ADePT, a differentially private\nauto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising\nresults on downstream tasks while providing tight privacy guarantees. Our proof\nreveals that ADePT is not differentially private, thus rendering the\nexperimental results unsubstantiated. We also quantify the impact of the error\nin its private mechanism, showing that the true sensitivity is higher by at\nleast factor 6 in an optimistic case of a very small encoder's dimension and\nthat the amount of utterances that are not privatized could easily reach 100%\nof the entire dataset. Our intention is neither to criticize the authors, nor\nthe peer-reviewing process, but rather point out that if differential privacy\napplications in NLP rely on formal guarantees, these should be outlined in full\nand put under detailed scrutiny.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Conversation Disentanglement through Co-Training. (arXiv:2109.03199v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03199","description":"<p>Conversation disentanglement aims to separate intermingled messages into\ndetached sessions, which is a fundamental task in understanding multi-party\nconversations. Existing work on conversation disentanglement relies heavily\nupon human-annotated datasets, which are expensive to obtain in practice. In\nthis work, we explore to train a conversation disentanglement model without\nreferencing any human annotations. Our method is built upon a deep co-training\nalgorithm, which consists of two neural networks: a message-pair classifier and\na session classifier. The former is responsible for retrieving local relations\nbetween two messages while the latter categorizes a message to a session by\ncapturing context-aware information. Both networks are initialized respectively\nwith pseudo data built from an unannotated corpus. During the deep co-training\nprocess, we use the session classifier as a reinforcement learning component to\nlearn a session assigning policy by maximizing the local rewards given by the\nmessage-pair classifier. For the message-pair classifier, we enrich its\ntraining data by retrieving message pairs with high confidence from the\ndisentangled sessions predicted by the session classifier. Experimental results\non the large Movie Dialogue Dataset demonstrate that our proposed approach\nachieves competitive performance compared to the previous supervised methods.\nFurther experiments show that the predicted disentangled conversations can\npromote the performance on the downstream task of multi-party response\nselection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])","link":"http://arxiv.org/abs/2109.03200","description":"<p>The increasing use of social media sites in countries like India has given\nrise to large volumes of code-mixed data. Sentiment analysis of this data can\nprovide integral insights into people's perspectives and opinions. Developing\nrobust explainability techniques which explain why models make their\npredictions becomes essential. In this paper, we propose an adequate\nmethodology to integrate explainable approaches into code-mixed sentiment\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1\">Sudarshan Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_N/0/1/0/all/0/1\">Nipuna Chhabra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint model for intent and entity recognition. (arXiv:2109.03221v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03221","description":"<p>The semantic understanding of natural dialogues composes of several parts.\nSome of them, like intent classification and entity detection, have a crucial\nrole in deciding the next steps in handling user input. Handling each task as\nan individual problem can be wasting of training resources, and also each\nproblem can benefit from each other. This paper tackles these problems as one.\nOur new model, which combine intent and entity recognition into one system, is\nachieving better metrics in both tasks with lower training requirements than\nsolving each task separately. We also optimize the model based on the inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenc_P/0/1/0/all/0/1\">Petr Lorenc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03228","description":"<p>Recent studies on compression of pretrained language models (e.g., BERT)\nusually use preserved accuracy as the metric for evaluation. In this paper, we\npropose two new metrics, label loyalty and probability loyalty that measure how\nclosely a compressed model (i.e., student) mimics the original model (i.e.,\nteacher). We also explore the effect of compression with regard to robustness\nunder adversarial attacks. We benchmark quantization, pruning, knowledge\ndistillation and progressive module replacing with loyalty and robustness. By\ncombining multiple compression techniques, we provide a practical strategy to\nachieve better accuracy, loyalty and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.06438","description":"<p>Advertising is an important revenue source for many companies. However, it is\nexpensive to manually create advertisements that meet the needs of various\nqueries for massive items. In this paper, we propose the query-variant\nadvertisement text generation task that aims to generate candidate\nadvertisements for different queries with various needs given the item\nkeywords. In this task, for many different queries there is only one general\npurposed advertisement with no predefined query-advertisement pair, which would\ndiscourage traditional End-to-End models from generating query-variant\nadvertisements for different queries with different needs. To deal with the\nproblem, we propose a query-variant advertisement text generation model that\ntakes keywords and associated external knowledge as input during training and\nadds different queries during inference. Adding external knowledge helps the\nmodel adapted to the information besides the item keywords during training,\nwhich makes the transition between training and inference more smoothing when\nthe query is added during inference. Both automatic and human evaluation show\nthat our model can generate more attractive and query-focused advertisements\nthan the strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Siyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_C/0/1/0/all/0/1\">Cai Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.12835","description":"<p>This paper shows that, modern word embeddings contain information that\ndistinguishes synonyms and antonyms despite small cosine similarities between\ncorresponding vectors. This information is encoded in the geometry of the\nembeddings and could be extracted with a straight-forward and intuitive\nmanifold learning procedure or a contrasting map. Such a map is trained on a\nsmall labeled subset of the data and can produce new embeddings that explicitly\nhighlight specific semantic attributes of the word. The new embeddings produced\nby the map are shown to improve the performance on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.13401","description":"<p>Recent successes in deep generative modeling have led to significant advances\nin natural language generation (NLG). Incorporating entities into neural\ngeneration models has demonstrated great improvements by assisting to infer the\nsummary topic and to generate coherent content. To enhance the role of entity\nin NLG, in this paper, we aim to model the entity type in the decoding phase to\ngenerate contextual words accurately. We develop a novel NLG model to produce a\ntarget sequence based on a given list of entities. Our model has a multi-step\ndecoder that injects the entity types into the process of entity mention\ngeneration. Experiments on two public news datasets demonstrate type injection\nperforms better than existing type embedding concatenation baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Synthetic Data Improves Neural Machine Translation withKnowledge Distillation. (arXiv:2012.15455v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15455","description":"<p>This paper explores augmenting monolingual data for knowledge distillation in\nneural machine translation. Source language monolingual text can be\nincorporated as a forward translation. Interestingly, we find the best way to\nincorporate target language monolingual text is to translate it to the source\nlanguage and round-trip translate it back to the target language, resulting in\na fully synthetic corpus. We find that combining monolingual data from both\nsource and target languages yields better performance than a corpus twice as\nlarge only in one language. Moreover, experiments reveal that the improvement\ndepends upon the provenance of the test set. If the test set was originally in\nthe source language (with the target side written by translators), then forward\ntranslating source monolingual data matters. If the test set was originally in\nthe target language (with the source written by translators), then\nincorporating target monolingual data matters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00234","description":"<p>Transformers have shown improved performance when compared to previous\narchitectures for sequence processing such as RNNs. Despite their sizeable\nperformance gains, as recently suggested, the model is computationally\nexpensive to train and with a high parameter budget. In light of this, we\nexplore parameter-sharing methods in Transformers with a specific focus on\ngenerative models. We perform an analysis of different parameter\nsharing/reduction methods and develop the Subformer. Our model combines\nsandwich-style parameter sharing, which overcomes naive cross-layer parameter\nsharing in generative models, and self-attentive embedding factorization\n(SAFE). Experiments on machine translation, abstractive summarization and\nlanguage modeling show that the Subformer can outperform the Transformer even\nwhen using significantly fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrese_Taylor_E/0/1/0/all/0/1\">Edison Marrese-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Improving Coherence and Diversity of Slogan Generation. (arXiv:2102.05924v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.05924","description":"<p>Previous work in slogan generation focused on utilising slogan skeletons\nmined from existing slogans. While some generated slogans can be catchy, they\nare often not coherent with the company's focus or style across their marketing\ncommunications because the skeletons are mined from other companies' slogans.\nWe propose a sequence-to-sequence (seq2seq) transformer model to generate\nslogans from a brief company description. A naive seq2seq model fine-tuned for\nslogan generation is prone to introducing false information. We use company\nname delexicalisation and entity masking to alleviate this problem and improve\nthe generated slogans' quality and truthfulness. Furthermore, we apply\nconditional training based on the first words' POS tag to generate\nsyntactically diverse slogans. Our best model achieved a ROUGE-1/-2/-L F1 score\nof 35.58/18.47/33.32. Besides, automatic and human evaluations indicate that\nour method generates significantly more factual, diverse and catchy slogans\nthan strong LSTM and transformer seq2seq baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Akshay Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanvarie_D/0/1/0/all/0/1\">Dittaya Wanvarie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1\">Phu T. V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings. (arXiv:2103.03598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.03598","description":"<p>Intersectional bias is a bias caused by an overlap of multiple social factors\nlike gender, sexuality, race, disability, religion, etc. A recent study has\nshown that word embedding models can be laden with biases against\nintersectional groups like African American females, etc. The first step\ntowards tackling such intersectional biases is to identify them. However,\ndiscovering biases against different intersectional groups remains a\nchallenging task. In this work, we present WordBias, an interactive visual tool\ndesigned to explore biases against intersectional groups encoded in static word\nembeddings. Given a pretrained static word embedding, WordBias computes the\nassociation of each word along different groups based on race, age, etc. and\nthen visualizes them using a novel interactive interface. Using a case study,\nwe demonstrate how WordBias can help uncover biases against intersectional\ngroups like Black Muslim Males, Poor Females, etc. encoded in word embedding.\nIn addition, we also evaluate our tool using qualitative feedback from expert\ninterviews. The source code for this tool can be publicly accessed for\nreproducibility at github.com/bhavyaghai/WordBias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghai_B/0/1/0/all/0/1\">Bhavya Ghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_M/0/1/0/all/0/1\">Md Naimul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-Permuted Paragraph Generation. (arXiv:2104.07228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07228","description":"<p>Generating paragraphs of diverse contents is important in many applications.\nExisting generation models produce similar contents from homogenized contexts\ndue to the fixed left-to-right sentence order. Our idea is permuting the\nsentence orders to improve the content diversity of multi-sentence paragraph.\nWe propose a novel framework PermGen whose objective is to maximize the\nexpected log-likelihood of output paragraph distributions with respect to all\npossible sentence orders. PermGen uses hierarchical positional embedding and\ndesigns new procedures for training, decoding, and candidate ranking in the\nsentence-permuted generation. Experiments on three paragraph generation\nbenchmarks demonstrate PermGen generates more diverse outputs with a higher\nquality than existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07555","description":"<p>QuestEval is a reference-less metric used in text-to-text tasks, that\ncompares the generated summaries directly to the source text, by automatically\nasking and answering questions. Its adaptation to Data-to-Text tasks is not\nstraightforward, as it requires multimodal Question Generation and Answering\nsystems on the considered tasks, which are seldom available. To this purpose,\nwe propose a method to build synthetic multimodal corpora enabling to train\nmultimodal components for a data-QuestEval metric. The resulting metric is\nreference-less and multimodal; it obtains state-of-the-art correlations with\nhuman judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's\ncode and models available for reproducibility purpose, as part of the QuestEval\nproject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1\">Cl&#xe9;ment Rebuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03861","description":"<p>Identifying political perspective in news media has become an important task\ndue to the rapid growth of political commentary and the increasingly polarized\nideologies. Previous approaches only focus on leveraging the semantic\ninformation and leaves out the rich social and political context that helps\nindividuals understand political stances. In this paper, we propose a\nperspective detection method that incorporates external knowledge of real-world\npolitics. Specifically, we construct a contemporary political knowledge graph\nwith 1,071 entities and 10,703 triples. We then build a heterogeneous\ninformation network for each news document that jointly models article\nsemantics and external knowledge in knowledge graphs. Finally, we apply gated\nrelational graph convolutional networks and conduct political perspective\ndetection as graph-level classification. Extensive experiments show that our\nmethod achieves the best performance and outperforms state-of-the-art methods\nby 5.49%. Numerous ablation studies further bear out the necessity of external\nknowledge and the effectiveness of our graph-based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03881","description":"<p>Political stance detection has become an important task due to the\nincreasingly polarized political ideologies. Most existing works focus on\nidentifying perspectives in news articles or social media posts, while social\nentities, such as individuals and organizations, produce these texts and\nactually take stances. In this paper, we propose the novel task of entity\nstance prediction, which aims to predict entities' stances given their social\nand political context. Specifically, we retrieve facts from Wikipedia about\nsocial entities regarding contemporary U.S. politics. We then annotate social\nentities' stances towards political ideologies with the help of domain experts.\nAfter defining the task of entity stance prediction, we propose a graph-based\nsolution, which constructs a heterogeneous information network from collected\nfacts and adopts gated relational graph convolutional networks for\nrepresentation learning. Our model is then trained with a combination of\nsupervised, self-supervised and unsupervised loss functions, which are\nmotivated by multiple social and political phenomenons. We conduct extensive\nexperiments to compare our method with existing text and graph analysis\nbaselines. Our model achieves highest stance detection accuracy and yields\ninspiring insights regarding social entity stances. We further conduct ablation\nstudy and parameter analysis to study the mechanism and effectiveness of our\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peisheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13140","description":"<p>While deep learning models have greatly improved the performance of most\nartificial intelligence tasks, they are often criticized to be untrustworthy\ndue to the black-box problem. Consequently, many works have been proposed to\nstudy the trustworthiness of deep learning. However, as most open datasets are\ndesigned for evaluating the accuracy of model outputs, there is still a lack of\nappropriate datasets for evaluating the inner workings of neural networks. The\nlack of datasets obviously hinders the development of trustworthiness research.\nTherefore, in order to systematically evaluate the factors for building\ntrustworthy systems, we propose a novel and well-annotated sentiment analysis\ndataset to evaluate robustness and interpretability. To evaluate these factors,\nour dataset contains diverse annotations about the challenging distribution of\ninstances, manual adversarial instances and sentiment explanations. Several\nevaluation metrics are further proposed for interpretability and robustness.\nBased on the dataset and metrics, we conduct comprehensive comparisons for the\ntrustworthiness of three typical models, and also study the relations between\naccuracy, robustness and interpretability. We release this trustworthiness\nevaluation dataset at \\url{https://github/xyz} and hope our work can facilitate\nthe progress on building more trustworthy systems for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00122","description":"<p>The sheer volume of financial statements makes it difficult for humans to\naccess and analyze a business's financials. Robust numerical reasoning likewise\nfaces unique challenges in this domain. In this work, we focus on answering\ndeep questions over financial data, aiming to automate the analysis of a large\ncorpus of financial documents. In contrast to existing tasks on general domain,\nthe finance domain includes complex numerical reasoning and understanding of\nheterogeneous representations. To facilitate analytical progress, we propose a\nnew large-scale dataset, FinQA, with Question-Answering pairs over Financial\nreports, written by financial experts. We also annotate the gold reasoning\nprograms to ensure full explainability. We further introduce baselines and\nconduct comprehensive experiments in our dataset. The results demonstrate that\npopular, large, pre-trained models fall far short of expert humans in acquiring\nfinance knowledge and in complex multi-step numerical reasoning on that\nknowledge. Our dataset -- the first of its kind -- should therefore enable\nsignificant, new community research into complex application domains. The\ndataset and code are publicly available\\url{https://github.com/czyssrs/FinQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smiley_C/0/1/0/all/0/1\">Charese Smiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borova_I/0/1/0/all/0/1\">Iana Borova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langdon_D/0/1/0/all/0/1\">Dylan Langdon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussa_R/0/1/0/all/0/1\">Reema Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beane_M/0/1/0/all/0/1\">Matt Beane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Routledge_B/0/1/0/all/0/1\">Bryan Routledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00904","description":"<p>We introduce MULTI-EURLEX, a new multilingual dataset for topic\nclassification of legal documents. The dataset comprises 65k European Union\n(EU) laws, officially translated in 23 languages, annotated with multiple\nlabels from the EUROVOC taxonomy. We highlight the effect of temporal concept\ndrift and the importance of chronological, instead of random splits. We use the\ndataset as a testbed for zero-shot cross-lingual transfer, where we exploit\nannotated training documents in one language (source) to classify documents in\nanother language (target). We find that fine-tuning a multilingually pretrained\nmodel (XLM-ROBERTA, MT5) in a single source language leads to catastrophic\nforgetting of multilingual knowledge and, consequently, poor zero-shot transfer\nto other languages. Adaptation strategies, namely partial fine-tuning,\nadapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new\nend-tasks, help retain multilingual knowledge from pretraining, substantially\nimproving zero-shot cross-lingual transfer, but their impact also depends on\nthe pretrained model used and the size of the label set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction. (arXiv:2109.02403v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02403","description":"<p>Aspect-level sentiment classification (ALSC) aims at identifying the\nsentiment polarity of a specified aspect in a sentence. ALSC is a practical\nsetting in aspect-based sentiment analysis due to no opinion term labeling\nneeded, but it fails to interpret why a sentiment polarity is derived for the\naspect. To address this problem, recent works fine-tune pre-trained Transformer\nencoders for ALSC to extract an aspect-centric dependency tree that can locate\nthe opinion words. However, the induced opinion words only provide an intuitive\ncue far below human-level interpretability. Besides, the pre-trained encoder\ntends to internalize an aspect's intrinsic sentiment, causing sentiment bias\nand thus affecting model performance. In this paper, we propose a span-based\nanti-bias aspect representation learning framework. It first eliminates the\nsentiment bias in the aspect embedding by adversarial learning against aspects'\nprior sentiment. Then, it aligns the distilled opinion candidates with the\naspect by span-based dependency modeling to highlight the interpretable opinion\nterms. Our method achieves new state-of-the-art performance on five benchmarks,\nwith the capability of unsupervised opinion extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}