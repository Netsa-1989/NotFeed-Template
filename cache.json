{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Fast Contextual Adaptation with Neural Associative Memory for On-Device Personalized Speech Recognition. (arXiv:2110.02220v1 [eess.AS])","link":"http://arxiv.org/abs/2110.02220","description":"<p>Fast contextual adaptation has shown to be effective in improving Automatic\nSpeech Recognition (ASR) of rare words and when combined with an on-device\npersonalized training, it can yield an even better recognition result. However,\nthe traditional re-scoring approaches based on an external language model is\nprone to diverge during the personalized training. In this work, we introduce a\nmodel-based end-to-end contextual adaptation approach that is decoder-agnostic\nand amenable to on-device personalization. Our on-device simulation experiments\ndemonstrate that the proposed approach outperforms the traditional re-scoring\ntechnique by 12% relative WER and 15.7% entity mention specific F1-score in a\ncontinues personalization scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Munkhdalai_T/0/1/0/all/0/1\">Tsendsuren Munkhdalai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1\">Angad Chandorkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Fan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chua_M/0/1/0/all/0/1\">Mason Chua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disambiguation-BERT for N-best Rescoring in Low-Resource Conversational ASR. (arXiv:2110.02267v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02267","description":"<p>We study the inclusion of past conversational context through BERT language\nmodels into a CTC-based Automatic Speech Recognition (ASR) system via N-best\nrescoring. We introduce a data-efficient strategy to fine-tune BERT on\ntranscript disambiguation without external data. Our results show word error\nrate recoveries up to 37.2% with context-augmented BERT rescoring. We do this\nin low-resource data domains, both in language (Norwegian), tone (spontaneous,\nconversational), and topics (parliament proceedings and customer service phone\ncalls). We show how the nature of the data greatly affects the performance of\ncontext-augmented N-best rescoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_P/0/1/0/all/0/1\">Pablo Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burud_S/0/1/0/all/0/1\">Simen Burud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-training an Unsupervised Constituency Parser with Weak Supervision. (arXiv:2110.02283v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02283","description":"<p>We introduce a method for unsupervised parsing that relies on bootstrapping\nclassifiers to identify if a node dominates a specific span in a sentence.\nThere are two types of classifiers, an inside classifier that acts on a span,\nand an outside classifier that acts on everything outside of a given span.\nThrough self-training and co-training with the two classifiers, we show that\nthe interplay between them helps improve the accuracy of both, and as a result,\neffectively parse. A seed bootstrapping technique prepares the data to train\nthese classifiers. Our analyses further validate that such an approach in\nconjunction with weak supervision using prior branching knowledge of a known\nlanguage (left/right-branching) and minimal heuristics injects strong inductive\nbias into the parser, achieving 63.1 F$_1$ on the English (PTB) test set. In\naddition, we show the effectiveness of our architecture by evaluating on\ntreebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art\nresults.\\footnote{For code or data, please contact the authors.}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maveli_N/0/1/0/all/0/1\">Nickil Maveli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Shay B. Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 India Dataset: Parsing Detailed COVID-19 Data in Daily Health Bulletins from States in India. (arXiv:2110.02311v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02311","description":"<p>While India remains one of the hotspots of the COVID-19 pandemic, data about\nthe pandemic from the country has proved to be largely inaccessible for use at\nscale. Much of the data exists in an unstructured form on the web, and limited\naspects of such data are available through public APIs maintained manually\nthrough volunteer efforts. This has proved to be difficult both in terms of\nease of access to detailed data as well as with regards to the maintenance of\nmanual data-keeping over time. This paper reports on a recently launched\nproject aimed at automating the extraction of such data from public health\nbulletins with the help of a combination of classical PDF parsers as well as\nstate-of-the-art ML-based documents extraction APIs. In this paper, we will\ndescribe the automated data-extraction technique, the nature of the generated\ndata, and exciting avenues of ongoing work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mayank Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1\">Tathagata Chakraborti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_S/0/1/0/all/0/1\">Sachin Grover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Conditional Text Generation for Aspect-Based Sentiment Analysis. (arXiv:2110.02334v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02334","description":"<p>Aspect-based sentiment analysis (ABSA) is an NLP task that entails processing\nuser-generated reviews to determine (i) the target being evaluated, (ii) the\naspect category to which it belongs, and (iii) the sentiment expressed towards\nthe target and aspect pair. In this article, we propose transforming ABSA into\nan abstract summary-like conditional text generation task that uses targets,\naspects, and polarities to generate auxiliary statements. To demonstrate the\nefficacy of our task formulation and a proposed system, we fine-tune a\npre-trained model for conditional text generation tasks to get new\nstate-of-the-art results on a few restaurant domains and urban neighborhoods\ndomain benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1\">Siva Uday Sampreeth Chebolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EntQA: Entity Linking as Question Answering. (arXiv:2110.02369v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02369","description":"<p>A conventional approach to entity linking is to first find mentions in a\ngiven document and then infer their underlying entities in the knowledge base.\nA well-known limitation of this approach is that it requires finding mentions\nwithout knowing their entities, which is unnatural and difficult. We present a\nnew model that does not suffer from this limitation called EntQA, which stands\nfor Entity linking as Question Answering. EntQA first proposes candidate\nentities with a fast retrieval module, and then scrutinizes the document to\nfind mentions of each candidate with a powerful reader module. Our approach\ncombines progress in entity linking with that in open-domain question answering\nand capitalizes on pretrained models for dense entity retrieval and reading\ncomprehension. Unlike in previous works, we do not rely on a mention-candidates\ndictionary or large-scale weak supervision. EntQA achieves strong results on\nthe GERBIL benchmarking platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenzheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wenyue Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning. (arXiv:2110.02370v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02370","description":"<p>Large natural language models (such as GPT-3 or T5) demonstrate impressive\nabilities across a range of general NLP tasks. Here, we show that the knowledge\nembedded in such models provides a useful inductive bias, not just on\ntraditional NLP tasks, but also in the nontraditional task of training a\nsymbolic reasoning engine. We observe that these engines learn quickly and\ngeneralize in a natural way that reflects human intuition. For example,\ntraining such a system to model block-stacking might naturally generalize to\nstacking other types of objects because of structure in the real world that has\nbeen partially captured by the language describing it. We study several\nabstract textual reasoning tasks, such as object manipulation and navigation,\nand demonstrate multiple types of generalization to novel scenarios and the\nsymbols that comprise them. We also demonstrate the surprising utility of\n\\textit{compositional learning}, where a learner dedicated to mastering a\ncomplicated task gains an advantage by training on relevant simpler tasks\ninstead of jumping straight to the complicated task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rytting_C/0/1/0/all/0/1\">Christopher Michael Rytting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting intermediate convolutional layers in unsupervised acoustic word classification. (arXiv:2110.02375v1 [cs.SD])","link":"http://arxiv.org/abs/2110.02375","description":"<p>Understanding how deep convolutional neural networks classify data has been\nsubject to extensive research. This paper proposes a technique to visualize and\ninterpret intermediate layers of unsupervised deep convolutional neural\nnetworks by averaging over individual feature maps in each convolutional layer\nand inferring underlying distributions of words with non-linear regression\ntechniques. A GAN-based architecture (ciwGAN <a href=\"/abs/2006.02951\">arXiv:2006.02951</a>) that includes\nthree convolutional networks (a Generator, a Discriminator, and a classifier)\nwas trained on unlabeled sliced lexical items from TIMIT. The training results\nin a deep convolutional network that learns to classify words into discrete\nclasses only from the requirement of the Generator to output informative data.\nThe classifier network has no access to the training data -- only to the\ngenerated data -- which means lexical learning needs to emerge in a fully\nunsupervised manner. We propose a technique to visualize individual\nconvolutional layers in the classifier that yields highly informative\ntime-series data for each convolutional layer and apply it to unobserved test\ndata. Using non-linear regression, we infer underlying distributions for each\nword which allows us to analyze both absolute values and shapes of individual\nwords at different convolutional layers as well as perform hypothesis testing\non their acoustic properties. The technique also allows us to tests individual\nphone contrasts and how they are represented at each layer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer Performance. (arXiv:2110.02386v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02386","description":"<p>Multilingual language models achieve impressive zero-shot accuracies in many\nlanguages in complex tasks such as Natural Language Inference (NLI). Examples\nin NLI (and equivalent complex tasks) often pertain to various types of\nsub-tasks, requiring different kinds of reasoning. Certain types of reasoning\nhave proven to be more difficult to learn in a monolingual context, and in the\ncrosslingual context, similar observations may shed light on zero-shot transfer\nefficiency and few-shot sample selection. Hence, to investigate the effects of\ntypes of reasoning on transfer performance, we propose a category-annotated\nmultilingual NLI dataset and discuss the challenges to scale monolingual\nannotations to multiple languages. We statistically observe interesting effects\nthat the confluence of reasoning types and language similarities have on\ntransfer performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karthikeyan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathe_A/0/1/0/all/0/1\">Aalok Sathe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1\">Somak Aditya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Modeling using LMUs: 10x Better Data Efficiency or Improved Scaling Compared to Transformers. (arXiv:2110.02402v1 [cs.LG])","link":"http://arxiv.org/abs/2110.02402","description":"<p>Recent studies have demonstrated that the performance of transformers on the\ntask of language modeling obeys a power-law relationship with model size over\nsix orders of magnitude. While transformers exhibit impressive scaling, their\nperformance hinges on processing large amounts of data, and their computational\nand memory requirements grow quadratically with sequence length. Motivated by\nthese considerations, we construct a Legendre Memory Unit based model that\nintroduces a general prior for sequence processing and exhibits an $O(n)$ and\n$O(n \\ln n)$ (or better) dependency for memory and computation respectively.\nOver three orders of magnitude, we show that our new architecture attains the\nsame accuracy as transformers with 10x fewer tokens. We also show that for the\nsame amount of training our model improves the loss over transformers about as\nmuch as transformers improve over LSTMs. Additionally, we demonstrate that\nadding global self-attention complements our architecture and the augmented\nmodel improves performance even further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chilkuri_N/0/1/0/all/0/1\">Narsimha Chilkuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hunsberger_E/0/1/0/all/0/1\">Eric Hunsberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voelker_A/0/1/0/all/0/1\">Aaron Voelker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_G/0/1/0/all/0/1\">Gurshaant Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eliasmith_C/0/1/0/all/0/1\">Chris Eliasmith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Acquisition in Neural Language Models. (arXiv:2110.02406v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02406","description":"<p>We investigate how neural language models acquire individual words during\ntraining, extracting learning curves and ages of acquisition for over 600 words\non the MacArthur-Bates Communicative Development Inventory (Fenson et al.,\n2007). Drawing on studies of word acquisition in children, we evaluate multiple\npredictors for words' ages of acquisition in LSTMs, BERT, and GPT-2. We find\nthat the effects of concreteness, word length, and lexical class are pointedly\ndifferent in children and language models, reinforcing the importance of\ninteraction and sensorimotor experience in child language acquisition. Language\nmodels rely far more on word frequency than children, but like children, they\nexhibit slower learning of words in longer utterances. Interestingly, models\nfollow consistent patterns during training for both unidirectional and\nbidirectional models, and for both LSTM and Transformer architectures. Models\npredict based on unigram token frequencies early in training, before\ntransitioning loosely to bigram probabilities, eventually converging on more\nnuanced predictions. These results shed light on the role of distributional\nlearning mechanisms in children, while also providing insights for more\nhuman-like language acquisition in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tyler A. Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergen_B/0/1/0/all/0/1\">Benjamin K. Bergen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice Aging with Audio-Visual Style Transfer. (arXiv:2110.02411v1 [cs.SD])","link":"http://arxiv.org/abs/2110.02411","description":"<p>Face aging techniques have used generative adversarial networks (GANs) and\nstyle transfer learning to transform one's appearance to look younger/older.\nIdentity is maintained by conditioning these generative networks on a learned\nvector representation of the source content. In this work, we apply a similar\napproach to age a speaker's voice, referred to as voice aging. We first analyze\nthe classification of a speaker's age by training a convolutional neural\nnetwork (CNN) on the speaker's voice and face data from Common Voice and\nVoxCeleb datasets. We generate aged voices from style transfer to transform an\ninput spectrogram to various ages and demonstrate our method on a mobile app.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1\">Justin Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunyeong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_S/0/1/0/all/0/1\">Seunghye J. Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming C. Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Distillation of Natural Language Understanding with Confident Sinkhorns. (arXiv:2110.02432v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02432","description":"<p>Enhancing the user experience is an essential task for application service\nproviders. For instance, two users living wide apart may have different tastes\nof food. A food recommender mobile application installed on an edge device\nmight want to learn from user feedback (reviews) to satisfy the client's needs\npertaining to distinct domains. Retrieving user data comes at the cost of\nprivacy while asking for model parameters trained on a user device becomes\nspace inefficient at a large scale. In this work, we propose an approach to\nlearn a central (global) model from the federation of (local) models which are\ntrained on user-devices, without disclosing the local data or model parameters\nto the server. We propose a federation mechanism for the problems with natural\nsimilarity metric between the labels which commonly appear in natural language\nunderstanding (NLU) tasks. To learn the global model, the objective is to\nminimize the optimal transport cost of the global model's predictions from the\nconfident sum of soft-targets assigned by local models. The confidence (a model\nweighting scheme) score of a model is defined as the L2 distance of a model's\nprediction from its probability bias. The method improves the global model's\nperformance over the baseline designed on three NLU tasks with intrinsic label\nspace semantics, i.e., fine-grained sentiment analysis, emotion recognition in\nconversation, and natural language inference. We make our codes public at\nhttps://github.com/declare-lab/sinkhorn-loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1\">Rishabh Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_T/0/1/0/all/0/1\">Tushar Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02442","description":"<p>Transformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 96.0% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models. (arXiv:2110.02467v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02467","description":"<p>Pre-trained Natural Language Processing (NLP) models can be easily adapted to\na variety of downstream language tasks. This significantly accelerates the\ndevelopment of language models. However, NLP models have been shown to be\nvulnerable to backdoor attacks, where a pre-defined trigger word in the input\ntext causes model misprediction. Previous NLP backdoor attacks mainly focus on\nsome specific tasks. This makes those attacks less general and applicable to\nother kinds of NLP models and tasks. In this work, we propose \\Name, the first\ntask-agnostic backdoor attack against the pre-trained NLP models. The key\nfeature of our attack is that the adversary does not need prior information\nabout the downstream tasks when implanting the backdoor to the pre-trained\nmodel. When this malicious model is released, any downstream models transferred\nfrom it will also inherit the backdoor, even after the extensive transfer\nlearning process. We further design a simple yet effective strategy to bypass a\nstate-of-the-art defense. Experimental results indicate that our approach can\ncompromise a wide range of downstream NLP tasks in an effective and stealthy\nway.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kangjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABC: Attention with Bounded-memory Control. (arXiv:2110.02488v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02488","description":"<p>Transformer architectures have achieved state-of-the-art results on a variety\nof sequence modeling tasks. However, their attention mechanism comes with a\nquadratic complexity in sequence lengths, making the computational overhead\nprohibitive, especially for long sequences. Attention context can be seen as a\nrandom-access memory with each token taking a slot. Under this perspective, the\nmemory size grows linearly with the sequence length, and so does the overhead\nof reading from it. One way to improve the efficiency is to bound the memory\nsize. We show that disparate approaches can be subsumed into one abstraction,\nattention with bounded-memory control (ABC), and they vary in their\norganization of the memory. ABC reveals new, unexplored possibilities. First,\nit connects several efficient attention variants that would otherwise seem\napart. Second, this abstraction gives new insights--an established approach\n(Wang et al., 2020b) previously thought to be not applicable in causal\nattention, actually is. Last, we present a new instance of ABC, which draws\ninspiration from existing ABC approaches, but replaces their heuristic\nmemory-organizing functions with a learned, contextualized one. Our experiments\non language modeling, machine translation, and masked language model finetuning\nshow that our approach outperforms previous efficient attention models;\ncompared to the strong transformer baselines, it significantly improves the\ninference time and space efficiency with no or negligible accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KNN-BERT: Fine-Tuning Pre-Trained Models with KNN Classifier. (arXiv:2110.02523v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02523","description":"<p>Pre-trained models are widely used in fine-tuning downstream tasks with\nlinear classifiers optimized by the cross-entropy loss, which might face\nrobustness and stability problems. These problems can be improved by learning\nrepresentations that focus on similarities in the same class and contradictions\nin different classes when making predictions. In this paper, we utilize the\nK-Nearest Neighbors Classifier in pre-trained model fine-tuning. For this KNN\nclassifier, we introduce a supervised momentum contrastive learning framework\nto learn the clustered representations of the supervised downstream tasks.\nExtensive experiments on text classification tasks and robustness tests show\nthat by incorporating KNNs with the traditional fine-tuning process, we can\nobtain significant improvements on the clean accuracy in both rich-source and\nfew-shot settings and can improve the robustness against adversarial attacks.\n\\footnote{all codes is available at https://github.com/LinyangLee/KNN-BERT}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Demin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Multi-Modal Embeddings from Structured Data. (arXiv:2110.02577v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02577","description":"<p>Multi-modal word semantics aims to enhance embeddings with perceptual input,\nassuming that human meaning representation is grounded in sensory experience.\nMost research focuses on evaluation involving direct visual input, however,\nvisual grounding can contribute to linguistic applications as well. Another\nmotivation for this paper is the growing need for more interpretable models and\nfor evaluating model efficiency regarding size and performance. This work\nexplores the impact of visual information for semantics when the evaluation\ninvolves no direct visual input, specifically semantic similarity and\nrelatedness. We investigate a new embedding type in-between linguistic and\nvisual modalities, based on the structured annotations of Visual Genome. We\ncompare uni- and multi-modal models including structured, linguistic and image\nbased representations. We measure the efficiency of each model with regard to\ndata and model size, modality / data distribution and information gain. The\nanalysis includes an interpretation of embedding structures. We found that this\nnew embedding conveys complementary information for text based embeddings. It\nachieves comparable performance in an economic way, using orders of magnitude\nless resources than visual models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vero_A/0/1/0/all/0/1\">Anita L. Ver&#x151;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copestake_A/0/1/0/all/0/1\">Ann Copestake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised Text Classification Based on Keyword Graph. (arXiv:2110.02591v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02591","description":"<p>Weakly-supervised text classification has received much attention in recent\nyears for it can alleviate the heavy burden of annotating massive data. Among\nthem, keyword-driven methods are the mainstream where user-provided keywords\nare exploited to generate pseudo-labels for unlabeled texts. However, existing\nmethods treat keywords independently, thus ignore the correlation among them,\nwhich should be useful if properly exploited. In this paper, we propose a novel\nframework called ClassKG to explore keyword-keyword correlation on keyword\ngraph by GNN. Our framework is an iterative process. In each iteration, we\nfirst construct a keyword graph, so the task of assigning pseudo labels is\ntransformed to annotating keyword subgraphs. To improve the annotation quality,\nwe introduce a self-supervised task to pretrain a subgraph annotator, and then\nfinetune it. With the pseudo labels generated by the subgraph annotator, we\nthen train a text classifier to classify the unlabeled texts. Finally, we\nre-extract keywords from the classified texts. Extensive experiments on both\nlong-text and short-text datasets show that our method substantially\noutperforms the existing ones\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiandong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning. (arXiv:2110.02600v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02600","description":"<p>Multilingual models jointly pretrained on multiple languages have achieved\nremarkable performance on various multilingual downstream tasks. Moreover,\nmodels finetuned on a single monolingual downstream task have shown to\ngeneralize to unseen languages. In this paper, we first show that it is crucial\nfor those tasks to align gradients between them in order to maximize knowledge\ntransfer while minimizing negative transfer. Despite its importance, the\nexisting methods for gradient alignment either have a completely different\npurpose, ignore inter-task alignment, or aim to solve continual learning\nproblems in rather inefficient ways. As a result of the misaligned gradients\nbetween tasks, the model suffers from severe negative transfer in the form of\ncatastrophic forgetting of the knowledge acquired from the pretraining. To\novercome the limitations, we propose a simple yet effective method that can\nefficiently align gradients between tasks. Specifically, we perform each\ninner-optimization by sequentially sampling batches from all the tasks,\nfollowed by a Reptile outer update. Thanks to the gradients aligned between\ntasks by our method, the model becomes less vulnerable to negative transfer and\ncatastrophic forgetting. We extensively validate our method on various\nmulti-task learning and zero-shot cross-lingual transfer tasks, where our\nmethod largely outperforms all the relevant baselines we consider.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seanie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hae Beom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of the interactive Leipzig Corpus Miner as a generic research platform for the use in the social sciences. (arXiv:2110.02708v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02708","description":"<p>This article introduces to the interactive Leipzig Corpus Miner (iLCM) - a\nnewly released, open-source software to perform automatic content analysis.\nSince the iLCM is based on the R-programming language, its generic text mining\nprocedures provided via a user-friendly graphical user interface (GUI) can\neasily be extended using the integrated IDE RStudio-Server or numerous other\ninterfaces in the tool. Furthermore, the iLCM offers various possibilities to\nuse quantitative and qualitative research approaches in combination. Some of\nthese possibilities will be presented in more detail in the following.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahmann_C/0/1/0/all/0/1\">Christian Kahmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekler_A/0/1/0/all/0/1\">Andreas Niekler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiedemann_G/0/1/0/all/0/1\">Gregor Wiedemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How BPE Affects Memorization in Transformers. (arXiv:2110.02782v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02782","description":"<p>Training data memorization in NLP can both be beneficial (e.g., closed-book\nQA) and undesirable (personal data extraction). In any case, successful model\ntraining requires a non-trivial amount of memorization to store word spellings,\nvarious linguistic idiosyncrasies and common knowledge. However, little is\nknown about what affects the memorization behavior of NLP models, as the field\ntends to focus on the equally important question of generalization. In this\nwork, we demonstrate that the size of the subword vocabulary learned by\nByte-Pair Encoding (BPE) greatly affects both ability and tendency of standard\nTransformer models to memorize training data, even when we control for the\nnumber of learned parameters. We find that with a large subword vocabulary\nsize, Transformer models fit random mappings more easily and are more\nvulnerable to membership inference attacks. Similarly, given a prompt,\nTransformer-based language models with large subword vocabularies reproduce the\ntraining data more often. We conjecture this effect is caused by reduction in\nthe sequences' length that happens as the BPE vocabulary grows. Our findings\ncan allow a more informed choice of hyper-parameters, that is better tailored\nfor a particular use-case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spell my name: keyword boosted speech recognition. (arXiv:2110.02791v1 [cs.SD])","link":"http://arxiv.org/abs/2110.02791","description":"<p>Recognition of uncommon words such as names and technical terminology is\nimportant to understanding conversations in context. However, the ability to\nrecognise such words remains a challenge in modern automatic speech recognition\n(ASR) systems.\n</p>\n<p>In this paper, we propose a simple but powerful ASR decoding method that can\nbetter recognise these uncommon keywords, which in turn enables better\nreadability of the results. The method boosts the probabilities of given\nkeywords in a beam search based on acoustic model predictions. The method does\nnot require any training in advance.\n</p>\n<p>We demonstrate the effectiveness of our method on the LibriSpeeech test sets\nand also internal data of real-world conversations. Our method significantly\nboosts keyword accuracy on the test sets, while maintaining the accuracy of the\nother words, and as well as providing significant qualitative improvements.\nThis method is applicable to other tasks such as machine translation, or\nwherever unseen and difficult keywords need to be recognised in beam search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_N/0/1/0/all/0/1\">Namkyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geonmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-conditioning pre-trained language models. (arXiv:2110.02802v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02802","description":"<p>We study the presence of expert units in pre-trained Transformer-based\nLanguage Models (TLMs), and how they can be used to condition text generation\nto contain specific concepts. We define expert units to be neurons that are\nable to detect a concept in the input with a given average precision. A concept\nis represented with a set of sentences that either do or do not contain the\nconcept. Leveraging the OneSec dataset, we compile a dataset of 1344 concepts\nthat allows diverse expert units in TLMs to be discovered. Our experiments\ndemonstrate that off-the-shelf pre-trained TLMs can be conditioned on their own\nknowledge (self-conditioning) to generate text that contains a given concept.\nTo this end, we intervene on the top expert units by fixing their output during\ninference, and we show experimentally that this is an effective method to\ncondition TLMs. Our method does not require fine-tuning the model or using\nadditional parameters, which allows conditioning large TLM with minimal compute\nresources. Furthermore, by intervening on a small number of experts in GPT2, we\ncan achieve parity with respect to two concepts at generation time. The\nspecific case of gender bias is explored, and we show that, for given contexts,\ngender parity is achieved while maintaining the model's perplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1\">Xavier Suau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1\">Luca Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Prediction as an Auxiliary Training Objective for Improving Multi-Relational Graph Representations. (arXiv:2110.02834v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02834","description":"<p>Learning good representations on multi-relational graphs is essential to\nknowledge base completion (KBC). In this paper, we propose a new\nself-supervised training objective for multi-relational graph representation\nlearning, via simply incorporating relation prediction into the commonly used\n1vsAll objective. The new training objective contains not only terms for\npredicting the subject and object of a given triple, but also a term for\npredicting the relation type. We analyse how this new objective impacts\nmulti-relational learning in KBC: experiments on a variety of datasets and\nmodels show that relation prediction can significantly improve entity ranking,\nthe most widely used evaluation task for KBC, yielding a 6.1% increase in MRR\nand 9.9% increase in Hits@1 on FB15k-237 as well as a 3.1% increase in MRR and\n3.4% in Hits@1 on Aristo-v4. Moreover, we observe that the proposed objective\nis especially effective on highly multi-relational datasets, i.e. datasets with\na large number of predicates, and generates better representations when larger\nembedding sizes are used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Composition of Weighted Finite-State Transducers. (arXiv:2110.02848v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02848","description":"<p>Finite-state transducers (FSTs) are frequently used in speech recognition.\nTransducer composition is an essential operation for combining different\nsources of information at different granularities. However, composition is also\none of the more computationally expensive operations. Due to the heterogeneous\nstructure of FSTs, parallel algorithms for composition are suboptimal in\nefficiency, generality, or both. We propose an algorithm for parallel\ncomposition and implement it on graphics processing units. We benchmark our\nparallel algorithm on the composition of random graphs and the composition of\ngraphs commonly used in speech recognition. The parallel composition scales\nbetter with the size of the input graphs and for large graphs can be as much as\n10 to 30 times faster than a sequential CPU algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Shubho Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1\">Awni Hannun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSG HASOC-Dravidian CodeMixFIRE2021: Pretrained Transformers for Offensive Language Identification in Tanglish. (arXiv:2110.02852v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02852","description":"<p>This paper describes the system submitted to Dravidian-Codemix-HASOC2021:\nHate Speech and Offensive Language Identification in Dravidian Languages\n(Tamil-English and Malayalam-English). This task aims to identify offensive\ncontent in code-mixed comments/posts in Dravidian Languages collected from\nsocial media. Our approach utilizes pooling the last layers of pretrained\ntransformer multilingual BERT for this task which helped us achieve rank nine\non the leaderboard with a weighted average score of 0.61 for the Tamil-English\ndataset in subtask B. After the task deadline, we sampled the dataset uniformly\nand used the MuRIL pretrained model, which helped us achieve a weighted average\nscore of 0.67, the top score in the leaderboard. Furthermore, our approach to\nutilizing the pretrained models helps reuse our models for the same task with a\ndifferent dataset. Our code and models are available in GitHub 1\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Lexical Normalization with Multilingual Transformers. (arXiv:2110.02869v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02869","description":"<p>Current benchmark tasks for natural language processing contain text that is\nqualitatively different from the text used in informal day to day digital\ncommunication. This discrepancy has led to severe performance degradation of\nstate-of-the-art NLP models when fine-tuned on real-world data. One way to\nresolve this issue is through lexical normalization, which is the process of\ntransforming non-standard text, usually from social media, into a more\nstandardized form. In this work, we propose a sentence-level\nsequence-to-sequence model based on mBART, which frames the problem as a\nmachine translation problem. As the noisy text is a pervasive problem across\nlanguages, not just English, we leverage the multi-lingual pre-training of\nmBART to fine-tune it to our data. While current approaches mainly operate at\nthe word or subword level, we argue that this approach is straightforward from\na technical standpoint and builds upon existing pre-trained transformer\nnetworks. Our results show that while word-level, intrinsic, performance\nevaluation is behind other methods, our model improves performance on\nextrinsic, downstream tasks through normalization compared to models operating\non raw, unprocessed, social media text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Structural Locality in Non-parametric Language Models. (arXiv:2110.02870v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02870","description":"<p>Structural locality is a ubiquitous feature of real-world datasets, wherein\ndata points are organized into local hierarchies. Some examples include topical\nclusters in text or project hierarchies in source code repositories. In this\npaper, we explore utilizing this structural locality within non-parametric\nlanguage models, which generate sequences that reference retrieved examples\nfrom an external source. We propose a simple yet effective approach for adding\nlocality information into such models by adding learned parameters that improve\nthe likelihood of retrieving examples from local neighborhoods. Experiments on\ntwo different domains, Java source code and Wikipedia text, demonstrate that\nlocality features improve model efficacy over models without access to these\nfeatures, with interesting differences. We also perform an analysis of how and\nwhere locality features contribute to improved performance and why the\ntraditionally used contextual similarity metrics alone are not enough to grasp\nthe locality structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-in-the-Loop Refinement of Word Embeddings. (arXiv:2110.02884v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02884","description":"<p>Word embeddings are a fixed, distributional representation of the context of\nwords in a corpus learned from word co-occurrences. Despite their proven\nutility in machine learning tasks, word embedding models may capture uneven\nsemantic and syntactic representations, and can inadvertently reflect various\nkinds of bias present within corpora upon which they were trained. It has been\ndemonstrated that post-processing of word embeddings to apply information found\nin lexical dictionaries can improve the semantic associations, thus improving\ntheir quality. Building on this idea, we propose a system that incorporates an\nadaptation of word embedding post-processing, which we call \"interactive\nrefitting\", to address some of the most daunting qualitative problems found in\nword embeddings. Our approach allows a human to identify and address potential\nquality issues with word embeddings interactively. This has the advantage of\nnegating the question of who decides what constitutes bias or what other\nquality issues may affect downstream tasks. It allows each organization or\nentity to address concerns they may have at a fine grained level and to do so\nin an iterative and interactive fashion. It also allows for better insight into\nwhat effect word embeddings, and refinements to word embeddings, have on\nmachine learning pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Powell_J/0/1/0/all/0/1\">James Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sentz_K/0/1/0/all/0/1\">Kari Sentz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_M/0/1/0/all/0/1\">Martin Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Optimal Transport as Alignment Objective for fine-tuning Multilingual Contextualized Embeddings. (arXiv:2110.02887v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02887","description":"<p>Recent studies have proposed different methods to improve multilingual word\nrepresentations in contextualized settings including techniques that align\nbetween source and target embedding spaces. For contextualized embeddings,\nalignment becomes more complex as we additionally take context into\nconsideration. In this work, we propose using Optimal Transport (OT) as an\nalignment objective during fine-tuning to further improve multilingual\ncontextualized representations for downstream cross-lingual transfer. This\napproach does not require word-alignment pairs prior to fine-tuning that may\nlead to sub-optimal matching and instead learns the word alignments within\ncontext in an unsupervised manner. It also allows different types of mappings\ndue to soft matching between source and target sentences. We benchmark our\nproposed method on two tasks (XNLI and XQuAD) and achieve improvements over\nbaselines as well as competitive results compared to similar recent works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alqahtani_S/0/1/0/all/0/1\">Sawsan Alqahtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalwani_G/0/1/0/all/0/1\">Garima Lalwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Salvatore Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Knowledge Assimilation for Expert-Layman Text Style Transfer. (arXiv:2110.02950v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02950","description":"<p>Expert-layman text style transfer technologies have the potential to improve\ncommunication between members of scientific communities and the general public.\nHigh-quality information produced by experts is often filled with difficult\njargon laypeople struggle to understand. This is a particularly notable issue\nin the medical domain, where layman are often confused by medical text online.\nAt present, two bottlenecks interfere with the goal of building high-quality\nmedical expert-layman style transfer systems: a dearth of pretrained\nmedical-domain language models spanning both expert and layman terminologies\nand a lack of parallel corpora for training the transfer task itself. To\nmitigate the first issue, we propose a novel language model (LM) pretraining\ntask, Knowledge Base Assimilation, to synthesize pretraining data from the\nedges of a graph of expert- and layman-style medical terminology terms into an\nLM during self-supervised learning. To mitigate the second issue, we build a\nlarge-scale parallel corpus in the medical expert-layman domain using a\nmargin-based criterion. Our experiments show that transformer-based models\npretrained on knowledge base assimilation and other well-established\npretraining tasks fine-tuning on our new parallel corpus leads to considerable\nimprovement against expert-layman transfer benchmarks, gaining an average\nrelative improvement of our human evaluation, the Overall Success Rate (OSR),\nby 106%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_M/0/1/0/all/0/1\">Misha Sra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical prosody modeling and control in non-autoregressive parallel neural TTS. (arXiv:2110.02952v1 [eess.AS])","link":"http://arxiv.org/abs/2110.02952","description":"<p>Neural text-to-speech (TTS) synthesis can generate speech that is\nindistinguishable from natural speech. However, the synthetic speech often\nrepresents the average prosodic style of the database instead of having more\nversatile prosodic variation. Moreover, many models lack the ability to control\nthe output prosody, which does not allow for different styles for the same text\ninput. In this work, we train a non-autoregressive parallel neural TTS model\nhierarchically conditioned on both coarse and fine-grained acoustic speech\nfeatures to learn a latent prosody space with intuitive and meaningful\ndimensions. Experiments show that a non-autoregressive TTS model hierarchically\nconditioned on utterance-wise pitch, pitch range, duration, energy, and\nspectral tilt can effectively control each prosodic dimension, generate a wide\nvariety of speaking styles, and provide word-wise emphasis control, while\nmaintaining equal or better quality to the baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Raitio_T/0/1/0/all/0/1\">Tuomo Raitio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangchuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seshadri_S/0/1/0/all/0/1\">Shreyas Seshadri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On learning an interpreted language with recurrent models. (arXiv:1809.04128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1809.04128","description":"<p>Can recurrent neural nets, inspired by human sequential data processing,\nlearn to understand language? We construct simplified datasets reflecting core\nproperties of natural language as modeled in formal syntax and semantics:\nrecursive syntactic structure and compositionality. We find LSTM and GRU\nnetworks to generalise to compositional interpretation well, but only in the\nmost favorable learning settings, with a well-paced curriculum, extensive\ntraining data, and left-to-right (but not right-to-left) composition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning. (arXiv:2003.06658v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2003.06658","description":"<p>Humans can systematically generalize to novel compositions of existing\nconcepts. There have been extensive conjectures into the extent to which neural\nnetworks can do the same. Recent arguments supported by evidence on the SCAN\ndataset claim that neural networks are inherently ineffective in such cognitive\ncapacity. In this paper, we revisit systematic generalization from the\nperspective of meaningful learning, an exceptional capability of humans to\nlearn new concepts by connecting them with other previously known knowledge. We\npropose to augment a training dataset in either an inductive or deductive\nmanner to build semantic links between new and old concepts. Our observations\non SCAN suggest that, following the meaningful learning principle, modern\nsequence-to-sequence models, including RNNs, CNNs, and Transformers, can\nsuccessfully generalize to compositions of new concepts. We further validate\nour findings on two real-world datasets on semantic parsing and consistent\ncompositional generalization is also observed. Moreover, our experiments\ndemonstrate that both prior knowledge and semantic linking play a key role to\nachieve systematic generalization. Meanwhile, inductive learning generally\nworks better than deductive learning in our experiments. Finally, we provide an\nexplanation for data augmentation techniques by concluding them into either\ninductive-based or deductive-based meaningful learning. We hope our findings\nwill encourage excavating existing neural networks' potential in systematic\ngeneralization through more advanced learning schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HittER: Hierarchical Transformers for Knowledge Graph Embeddings. (arXiv:2008.12813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.12813","description":"<p>This paper examines the challenging problem of learning representations of\nentities and relations in a complex multi-relational knowledge graph. We\npropose HittER, a Hierarchical Transformer model to jointly learn\nEntity-relation composition and Relational contextualization based on a source\nentity's neighborhood. Our proposed model consists of two different Transformer\nblocks: the bottom block extracts features of each entity-relation pair in the\nlocal neighborhood of the source entity and the top block aggregates the\nrelational information from outputs of the bottom block. We further design a\nmasked entity prediction task to balance information from the relational\ncontext and the source entity itself. Experimental results show that HittER\nachieves new state-of-the-art results on multiple link prediction datasets. We\nadditionally propose a simple approach to integrate HittER into BERT and\ndemonstrate its effectiveness on two Freebase factoid question answering\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterDrop: On the Efficiency of Adapters in Transformers. (arXiv:2010.11918v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.11918","description":"<p>Massively pre-trained transformer models are computationally expensive to\nfine-tune, slow for inference, and have large storage requirements. Recent\napproaches tackle these shortcomings by training smaller models, dynamically\nreducing the model size, and by training light-weight adapters. In this paper,\nwe propose AdapterDrop, removing adapters from lower transformer layers during\ntraining and inference, which incorporates concepts from all three directions.\nWe show that AdapterDrop can dynamically reduce the computational overhead when\nperforming inference over multiple tasks simultaneously, with minimal decrease\nin task performances. We further prune adapters from AdapterFusion, which\nimproves the inference efficiency while maintaining the task performances\nentirely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1\">Gregor Geigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glockner_M/0/1/0/all/0/1\">Max Glockner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPG: A Multi-ingredient Pizza Image Generator with Conditional StyleGANs. (arXiv:2012.02821v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02821","description":"<p>Multilabel conditional image generation is a challenging problem in computer\nvision. In this work we propose Multi-ingredient Pizza Generator (MPG), a\nconditional Generative Neural Network (GAN) framework for synthesizing\nmultilabel images. We design MPG based on a state-of-the-art GAN structure\ncalled StyleGAN2, in which we develop a new conditioning technique by enforcing\nintermediate feature maps to learn scalewise label information. Because of the\ncomplex nature of the multilabel image generation problem, we also regularize\nsynthetic image by predicting the corresponding ingredients as well as\nencourage the discriminator to distinguish between matched image and mismatched\nimage. To verify the efficacy of MPG, we test it on Pizza10, which is a\ncarefully annotated multi-ingredient pizza image dataset. MPG can successfully\ngenerate photo-realist pizza images with desired ingredients. The framework can\nbe easily extend to other multilabel image generation scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fangda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guoyao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_R/0/1/0/all/0/1\">Ricardo Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast WordPiece Tokenization. (arXiv:2012.15524v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15524","description":"<p>Tokenization is a fundamental preprocessing step for almost all NLP tasks. In\nthis paper, we propose efficient algorithms for the WordPiece tokenization used\nin BERT, from single-word tokenization to general text (e.g., sentence)\ntokenization. When tokenizing a single word, WordPiece uses a\nlongest-match-first strategy, known as maximum matching. The best known\nalgorithms so far are O(n^2) (where n is the input length) or O(nm) (where m is\nthe maximum vocabulary token length). We propose a novel algorithm whose\ntokenization complexity is strictly O(n). Our method is inspired by the\nAho-Corasick algorithm. We introduce additional linkages on top of the trie\nbuilt from the vocabulary, allowing smart transitions when the trie matching\ncannot continue. For general text, we further propose an algorithm that\ncombines pre-tokenization (splitting the text into words) and our linear-time\nWordPiece method into a single pass. Experimental results show that our method\nis 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text\non average for general text tokenization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salcianu_A/0/1/0/all/0/1\">Alex Salcianu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dopson_D/0/1/0/all/0/1\">Dave Dopson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records. (arXiv:2102.02340v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.02340","description":"<p>One important challenge of applying deep learning to electronic health\nrecords (EHR) is the complexity of their multimodal structure. EHR usually\ncontains a mixture of structured (codes) and unstructured (free-text) data with\nsparse and irregular longitudinal features -- all of which doctors utilize when\nmaking decisions. In the deep learning regime, determining how different\nmodality representations should be fused together is a difficult problem, which\nis often addressed by handcrafted modeling and intuition. In this work, we\nextend state-of-the-art neural architecture search (NAS) methods and propose\nMUltimodal Fusion Architecture SeArch (MUFASA) to simultaneously search across\nmultimodal fusion strategies and modality-specific architectures for the first\ntime. We demonstrate empirically that our MUFASA method outperforms established\nunimodal NAS on public EHR data with comparable computation costs. In addition,\nMUFASA produces architectures that outperform Transformer and Evolved\nTransformer. Compared with these baselines on CCS diagnosis code prediction,\nour discovered models improve top-5 recall from 0.88 to 0.91 and demonstrate\nthe ability to generalize to other EHR tasks. Studying our top architecture in\ndepth, we provide empirical evidence that MUFASA's improvements are derived\nfrom its ability to both customize modeling for each data modality and find\neffective fusion strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Attention with Linear Units. (arXiv:2104.07012v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07012","description":"<p>Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques. (arXiv:2104.13225v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2104.13225","description":"<p>This survey provides an overview of the evolution of visually grounded models\nof spoken language over the last 20 years. Such models are inspired by the\nobservation that when children pick up a language, they rely on a wide range of\nindirect and noisy clues, crucially including signals from the visual modality\nco-occurring with spoken utterances. Several fields have made important\ncontributions to this approach to modeling or mimicking the process of learning\nlanguage: Machine Learning, Natural Language and Speech Processing, Computer\nVision and Cognitive Science. The current paper brings together these\ncontributions in order to provide a useful introduction and overview for\npractitioners in all these areas. We discuss the central research questions\naddressed, the timeline of developments, and the datasets which enabled much of\nthis work. We then summarize the main modeling architectures and offer an\nexhaustive overview of the evaluation metrics and analysis techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrupala_G/0/1/0/all/0/1\">Grzegorz Chrupa&#x142;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Ackermannian lower bound for the Petri nets reachability problem. (arXiv:2105.08551v3 [cs.FL] UPDATED)","link":"http://arxiv.org/abs/2105.08551","description":"<p>Petri nets, equivalently presentable as vector addition systems with states,\nare an established model of concurrency with widespread applications. The\nreachability problem, where we ask whether from a given initial configuration\nthere exists a sequence of valid execution steps reaching a given final\nconfiguration, is the central algorithmic problem for this model. The\ncomplexity of the problem has remained, until recently, one of the hardest open\nquestions in verification of concurrent systems. A first upper bound has been\nprovided only in 2015 by Leroux and Schmitz, then refined by the same authors\nto non-primitive recursive Ackermannian upper bound in 2019. The exponential\nspace lower bound, shown by Lipton already in 1976, remained the only known for\nover 40 years until a breakthrough non-elementary lower bound by\nCzerwi{\\'n}ski, Lasota, Lazic, Leroux and Mazowiecki in 2019. Finally, a\nmatching Ackermannian lower bound announced this year by Czerwi{\\'n}ski and\nOrlikowski, and independently by Leroux, established the complexity of the\nproblem.\n</p>\n<p>Our contribution is an improvement of the former construction, making it\nconceptually simpler and more direct. On the way we improve the lower bound for\nvector addition systems with states in fixed dimension (or, equivalently, Petri\nnets with fixed number of places): while Czerwi{\\'n}ski and Orlikowski prove\n$F_k$-hardness (hardness for $k$th level in Grzegorczyk Hierarchy) in dimension\n$6k$, and Leroux in dimension $4k+5$, our simplified construction yields\n$F_k$-hardness already in dimension $3k+2$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lasota_S/0/1/0/all/0/1\">S&#x142;awomir Lasota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Itihasa: A large-scale corpus for Sanskrit to English translation. (arXiv:2106.03269v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.03269","description":"<p>This work introduces Itihasa, a large-scale translation dataset containing\n93,000 pairs of Sanskrit shlokas and their English translations. The shlokas\nare extracted from two Indian epics viz., The Ramayana and The Mahabharata. We\nfirst describe the motivation behind the curation of such a dataset and follow\nup with empirical analysis to bring out its nuances. We then benchmark the\nperformance of standard translation models on this corpus and show that even\nstate-of-the-art transformer architectures perform poorly, emphasizing the\ncomplexity of the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Generation with Efficient (Soft) Q-Learning. (arXiv:2106.07704v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07704","description":"<p>Maximum likelihood estimation (MLE) is the predominant algorithm for training\ntext generation models. This paradigm relies on direct supervision examples,\nwhich is not applicable to many emerging applications, such as generating\nadversarial attacks or generating prompts to control language models.\nReinforcement learning (RL) on the other hand offers a more flexible solution\nby allowing users to plug in arbitrary task metrics as reward. Yet previous RL\nalgorithms for text generation, such as policy gradient (on-policy RL) and\nQ-learning (off-policy RL), are often notoriously inefficient or unstable to\ntrain due to the large sequence space and the sparse reward received only at\nthe end of sequences. In this paper, we introduce a new RL formulation for text\ngeneration from the soft Q-learning (SQL) perspective. It enables us to draw\nfrom the latest RL advances, such as path consistency learning, to combine the\nbest of on-/off-policy updates, and learn effectively from sparse reward. We\napply the approach to a wide range of text generation tasks, including learning\nfrom noisy/negative examples, adversarial attacks, and prompt generation.\nExperiments show our approach consistently outperforms both task-specialized\nalgorithms and the previous RL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15078","description":"<p>Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence is not perfect,\ne.g., when the target sequence is corrupted with noises, or when only weak\nsequence supervision is available. To address this challenge, we propose a\nnovel Edit-Invariant Sequence Loss (EISL), which computes the matching loss of\na target n-gram with all n-grams in the generated sequence. Drawing\ninspirations from the classical convolutional networks (ConvNets) which capture\nshift-invariance in image modeling, EISL is designed to be robust to the shift\nof n-grams to tolerate various noises and edits in the target sequences.\nMoreover, the EISL computation is essentially a convolution operation with\ntarget n-grams as kernels, which is easy to implement and efficient to compute\nwith existing libraries. To demonstrate the effectiveness of EISL, we conduct\nexperiments on a wide range of tasks, including machine translation with noisy\ntarget sequences, unsupervised text style transfer with only weak training\nsignals, and non-autoregressive generation with non-predefined generation\norder. Experimental results show our method significantly outperforms the\ncommon cross-entropy loss and other strong baselines on all the tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14638","description":"<p>It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). A corpus of 7,873 scientific articles dealing with natural products\n(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine\nautomatically extracts categorical and numerical parameters such as (i) the\nplant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it can potentially make obsolete the current article\nreviewing process in some areas, especially those in which machine learning\nmodels capture texts structure, text semantics, and latent knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Amauri J Paula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution. (arXiv:2108.12777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12777","description":"<p>Recent studies have shown that deep neural networks are vulnerable to\nintentionally crafted adversarial examples, and various methods have been\nproposed to defend against adversarial word-substitution attacks for neural NLP\nmodels. However, there is a lack of systematic study on comparing different\ndefense approaches under the same attacking setting. In this paper, we seek to\nfill the gap of systematic studies through comprehensive researches on\nunderstanding the behavior of neural text classifiers trained by various\ndefense methods under representative adversarial attacks. In addition, we\npropose an effective method to further improve the robustness of neural text\nclassifiers against such attacks and achieved the highest accuracy on both\nclean and adversarial examples on AGNEWS and IMDB datasets by a significant\nmargin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08722","description":"<p>Prerequisite chain learning helps people acquire new knowledge efficiently.\nWhile people may quickly determine learning paths over concepts in a domain,\nfinding such paths in other domains can be challenging. We introduce\nDomain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this\ncross-domain prerequisite chain learning task efficiently. Our novel model\nconsists of a variational graph autoencoder (VGAE) and a domain discriminator.\nThe VGAE is trained to predict concept relations through link prediction, while\nthe domain discriminator takes both source and target domain data as input and\nis trained to predict domain labels. Most importantly, this method only needs\nsimple homogeneous graphs as input, compared with the current state-of-the-art\nmodel. We evaluate our model on the LectureBankCD dataset, and results show\nthat our model outperforms recent graph-based benchmarks while using only 1/10\nof graph scale and 1/3 computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tipping the Scales: A Corpus-Based Reconstruction of Adjective Scales in the McGill Pain Questionnaire. (arXiv:2109.14788v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14788","description":"<p>Modern medical diagnosis relies on precise pain assessment tools in\ntranslating clinical information from patient to physician. The McGill Pain\nQuestionnaire (MPQ) is a clinical pain assessment technique that utilizes 78\nadjectives of different intensities in 20 different categories to quantity a\npatient's pain. The questionnaire's efficacy depends on a predictable pattern\nof adjective use by patients experiencing pain. In this study, I recreate the\nMPQ's adjective intensity orderings using data gathered from patient forums and\nmodern NLP techniques. I extract adjective intensity relationships by searching\nfor key linguistic contexts, and then combine the relationship information to\nform robust adjective scales. Of 17 adjective relationships predicted by this\nresearch, only 4 diverge from the MPQ's orderings, which is statistically\nsignificant at the 0.1 alpha level. The results suggest predictable patterns of\nadjective use by people experiencing pain, but call into question the MPQ's\ncategories for grouping adjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stern_M/0/1/0/all/0/1\">Miriam Stern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01500","description":"<p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)\nsystems have achieved great success due to their simplicity and promising\nperformance. Neural Transducer based models are increasingly popular in\nstreaming E2E based ASR systems and have been reported to outperform the\ntraditional hybrid system in some scenarios. However, the joint optimization of\nacoustic model, lexicon and language model in neural Transducer also brings\nabout challenges to utilize pure text for language model adaptation. This\ndrawback might prevent their potential applications in practice. In order to\naddress this issue, in this paper, we propose a novel model, factorized neural\nTransducer, by factorizing the blank and vocabulary prediction, and adopting a\nstandalone language model for the vocabulary prediction. It is expected that\nthis factorization can transfer the improvement of the standalone language\nmodel to the Transducer for speech recognition, which allows various language\nmodel adaptation techniques to be applied. We demonstrate that the proposed\nfactorized neural Transducer yields 15% to 20% WER improvements when\nout-of-domain text data is used for language model adaptation, at the cost of a\nminor degradation in WER on a general test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Sarangarajan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01661","description":"<p>Iterating with new and improved OCR solutions enforces decisions to be taken\nwhen it comes to targeting the right reprocessing candidates. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those exact decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. As an extension of this technique, another contribution\ncomes in the form of a regression model that takes the enhancement potential of\na new OCR engine into account. They both mark promising approaches, especially\nfor cultural institutions dealing with historic data of lower quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01900","description":"<p>Self-supervised speech representation learning methods like wav2vec 2.0 and\nHidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and\noffer good representations for numerous speech processing tasks. Despite the\nsuccess of these methods, they require large memory and high pre-training\ncosts, making them inaccessible for researchers in academia and small\ncompanies. Therefore, this paper introduces DistilHuBERT, a novel multi-task\nlearning framework to distill hidden representations from a HuBERT model\ndirectly. This method reduces HuBERT's size by 75% and 73% faster while\nretaining most performance in ten different tasks. Moreover, DistilHuBERT\nrequired little training time and data, opening the possibilities of\npre-training personal and on-device SSL models for speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactively Generating Explanations for Transformer Language Models. (arXiv:2110.02058v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02058","description":"<p>Transformer language models are state-of-the-art in a multitude of NLP tasks.\nDespite these successes, their opaqueness remains problematic. Recent methods\naiming to provide interpretability and explainability to black-box models\nprimarily focus on post-hoc explanations of (sometimes spurious) input-output\ncorrelations. Instead, we emphasize using prototype networks directly\nincorporated into the model architecture and hence explain the reasoning\nprocess behind the network's decisions. Moreover, while our architecture\nperforms on par with several language models, it enables one to learn from user\ninteractions. This not only offers a better understanding of language models\nbut uses human capabilities to incorporate knowledge outside of the rigid range\nof purely data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tauchmann_C/0/1/0/all/0/1\">Christopher Tauchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sense-Specific Static Embeddings using Contextualised Word Embeddings as a Proxy. (arXiv:2110.02204v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02204","description":"<p>Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}