{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"When a Computer Cracks a Joke: Automated Generation of Humorous Headlines. (arXiv:2109.08702v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08702","description":"<p>Automated news generation has become a major interest for new agencies in the\npast. Oftentimes headlines for such automatically generated news articles are\nunimaginative as they have been generated with ready-made templates. We present\na computationally creative approach for headline generation that can generate\nhumorous versions of existing headlines. We evaluate our system with human\njudges and compare the results to human authored humorous titles. The headlines\nproduced by the system are considered funny 36\\% of the time by human\nevaluators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relating Neural Text Degeneration to Exposure Bias. (arXiv:2109.08705v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08705","description":"<p>This work focuses on relating two mysteries in neural-based text generation:\nexposure bias, and text degeneration. Despite the long time since exposure bias\nwas mentioned and the numerous studies for its remedy, to our knowledge, its\nimpact on text generation has not yet been verified. Text degeneration is a\nproblem that the widely-used pre-trained language model GPT-2 was recently\nfound to suffer from (Holtzman et al., 2020). Motivated by the unknown\ncausation of the text degeneration, in this paper we attempt to relate these\ntwo mysteries. Specifically, we first qualitatively quantitatively identify\nmistakes made before text degeneration occurs. Then we investigate the\nsignificance of the mistakes by inspecting the hidden states in GPT-2. Our\nresults show that text degeneration is likely to be partly caused by exposure\nbias. We also study the self-reinforcing mechanism of text degeneration,\nexplaining why the mistakes amplify. In sum, our study provides a more concrete\nfoundation for further investigation on exposure bias and text degeneration\nproblems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-device neural speech synthesis. (arXiv:2109.08710v1 [eess.AS])","link":"http://arxiv.org/abs/2109.08710","description":"<p>Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and\nWaveRNN, have made it possible to construct a fully neural network based TTS\nsystem, by coupling the two components together. Such a system is conceptually\nsimple as it only takes grapheme or phoneme input, uses Mel-spectrogram as an\nintermediate feature, and directly generates speech samples. The system\nachieves quality equal or close to natural speech. However, the high\ncomputational cost of the system and issues with robustness have limited their\nusage in real-world speech synthesis applications and products. In this paper,\nwe present key modeling improvements and optimization strategies that enable\ndeploying these models, not only on GPU servers, but also on mobile devices.\nThe proposed system can generate high-quality 24 kHz speech at 5x faster than\nreal time on server and 3x faster than real time on mobile devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Achanta_S/0/1/0/all/0/1\">Sivanand Achanta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antony_A/0/1/0/all/0/1\">Albert Antony</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golipour_L/0/1/0/all/0/1\">Ladan Golipour</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangchuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raitio_T/0/1/0/all/0/1\">Tuomo Raitio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rasipuram_R/0/1/0/all/0/1\">Ramya Rasipuram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rossi_F/0/1/0/all/0/1\">Francesco Rossi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jennifer Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_J/0/1/0/all/0/1\">Jaimin Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Winarsky_D/0/1/0/all/0/1\">David Winarsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hepeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back-translation for Large-Scale Multilingual Machine Translation. (arXiv:2109.08712v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08712","description":"<p>This paper illustrates our approach to the shared task on large-scale\nmultilingual machine translation in the sixth conference on machine translation\n(WMT-21). This work aims to build a single multilingual translation system with\na hypothesis that a universal cross-language representation leads to better\nmultilingual translation performance. We extend the exploration of different\nback-translation methods from bilingual translation to multilingual\ntranslation. Better performance is obtained by the constrained sampling method,\nwhich is different from the finding of the bilingual translation. Besides, we\nalso explore the effect of vocabularies and the amount of synthetic data.\nSurprisingly, the smaller size of vocabularies perform better, and the\nextensive monolingual English data offers a modest improvement. We submitted to\nboth the small tasks and achieved the second place.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1\">Baohao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khadivi_S/0/1/0/all/0/1\">Shahram Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewavitharana_S/0/1/0/all/0/1\">Sanjika Hewavitharana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08722","description":"<p>Prerequisite chain learning helps people acquire new knowledge efficiently.\nWhile people may quickly determine learning paths over concepts in a domain,\nfinding such paths in other domains can be challenging. We introduce\nDomain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this\ncross-domain prerequisite chain learning task efficiently. Our novel model\nconsists of a variational graph autoencoder (VGAE) and a domain discriminator.\nThe VGAE is trained to predict concept relations through link prediction, while\nthe domain discriminator takes both source and target domain data as input and\nis trained to predict domain labels. Most importantly, this method only needs\nsimple homogeneous graphs as input, compared with the current state-of-the-art\nmodel. We evaluate our model on the LectureBankCD dataset, and results show\nthat our model outperforms recent graph-based benchmarks while using only 1/10\nof graph scale and 1/3 computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The JHU-Microsoft Submission for WMT21 Quality Estimation Shared Task. (arXiv:2109.08724v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08724","description":"<p>This paper presents the JHU-Microsoft joint submission for WMT 2021 quality\nestimation shared task. We only participate in Task 2 (post-editing effort\nestimation) of the shared task, focusing on the target-side word-level quality\nestimation. The techniques we experimented with include Levenshtein Transformer\ntraining and data augmentation with a combination of forward, backward,\nround-trip translation, and pseudo post-editing of the MT output. We\ndemonstrate the competitiveness of our system compared to the widely adopted\nOpenKiwi-XLM baseline. Our system is also the top-ranking system on the MT MCC\nmetric for the English-German language pair.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Few-Shot Intent Classification and Slot Filling. (arXiv:2109.08754v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08754","description":"<p>Intent classification (IC) and slot filling (SF) are two fundamental tasks in\nmodern Natural Language Understanding (NLU) systems. Collecting and annotating\nlarge amounts of data to train deep learning models for such systems is not\nscalable. This problem can be addressed by learning from few examples using\nfast supervised meta-learning techniques such as prototypical networks. In this\nwork, we systematically investigate how contrastive learning and unsupervised\ndata augmentation methods can benefit these existing supervised meta-learning\npipelines for jointly modelled IC/SF tasks. Through extensive experiments\nacross standard IC/SF benchmarks (SNIPS and ATIS), we show that our proposed\nsemi-supervised approaches outperform standard supervised meta-learning\nmethods: contrastive losses in conjunction with prototypical networks\nconsistently outperform the existing state-of-the-art for both IC and SF tasks,\nwhile data augmentation strategies primarily improve few-shot IC by a\nsignificant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1\">Samyadeep Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_K/0/1/0/all/0/1\">Karine lp Kiun Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1\">Amr Sharaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1\">Alex Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohra_V/0/1/0/all/0/1\">Vishal Rohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amoake_M/0/1/0/all/0/1\">Michael Amoake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Hammamy_H/0/1/0/all/0/1\">Hazem El-Hammamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nosakhare_E/0/1/0/all/0/1\">Ehi Nosakhare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramani_V/0/1/0/all/0/1\">Vijay Ramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Benjamin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solar cell patent classification method based on keyword extraction and deep neural network. (arXiv:2109.08796v1 [cs.IR])","link":"http://arxiv.org/abs/2109.08796","description":"<p>With the growing impact of ESG on businesses, research related to renewable\nenergy is receiving great attention. Solar cells are one of them, and\naccordingly, it can be said that the research value of solar cell patent\nanalysis is very high. Patent documents have high research value. Being able to\naccurately analyze and classify patent documents can reveal several important\ntechnical relationships. It can also describe the business trends in that\ntechnology. And when it comes to investment, new industrial solutions will also\nbe inspired and proposed to make important decisions. Therefore, we must\ncarefully analyze patent documents and utilize the value of patents. To solve\nthe solar cell patent classification problem, we propose a keyword extraction\nmethod and a deep neural network-based solar cell patent classification method.\nFirst, solar cell patents are analyzed for pretreatment. It then uses the\nKeyBERT algorithm to extract keywords and key phrases from the patent abstract\nto construct a lexical dictionary. We then build a solar cell patent\nclassification model according to the deep neural network. Finally, we use a\ndeep neural network-based solar cell patent classification model to classify\npower patents, and the training accuracy is greater than 95%. Also, the\nvalidation accuracy is about 87.5%. It can be seen that the deep neural network\nmethod can not only realize the classification of complex and difficult solar\ncell patents, but also have a good classification effect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Yongmin Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1\">Dongjin Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_T/0/1/0/all/0/1\">Tak-Sung Heo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-Beta: A Proactive Probabilistic Approach to Text Moderation. (arXiv:2109.08805v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08805","description":"<p>Text moderation for user generated content, which helps to promote healthy\ninteraction among users, has been widely studied and many machine learning\nmodels have been proposed. In this work, we explore an alternative perspective\nby augmenting reactive reviews with proactive forecasting. Specifically, we\npropose a new concept {\\it text toxicity propensity} to characterize the extent\nto which a text tends to attract toxic comments. Beta regression is then\nintroduced to do the probabilistic modeling, which is demonstrated to function\nwell in comprehensive experiments. We also propose an explanation method to\ncommunicate the model decision clearly. Both propensity scoring and\ninterpretation benefit text moderation in a novel manner. Finally, the proposed\nscaling mechanism for the linear model offers useful insights beyond this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Changwei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Pattern Pruning Using Regularization. (arXiv:2109.08814v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08814","description":"<p>Iterative Magnitude Pruning (IMP) is a network pruning method that repeats\nthe process of removing weights with the least magnitudes and retraining the\nmodel. When visualizing the weight matrices of language models pruned by IMP,\nprevious research has shown that a structured pattern emerges, wherein the\nresulting surviving weights tend to prominently cluster in a select few rows\nand columns of the matrix. Though the need for further research in utilizing\nthese structured patterns for potential performance gains has previously been\nindicated, it has yet to be thoroughly studied. We propose SPUR (Structured\nPattern pruning Using Regularization), a novel pruning mechanism that\npreemptively induces structured patterns in compression by adding a\nregularization term to the objective function in the IMP. Our results show that\nSPUR can significantly preserve model performance under high sparsity settings\nregardless of the language or the task. Our contributions are as follows: (i)\nWe propose SPUR, a network pruning mechanism that improves upon IMP regardless\nof the language or the task. (ii) We are the first to empirically verify the\nefficacy of \"structured patterns\" observed previously in pruning research.\n(iii) SPUR is a resource-efficient mechanism in that it does not require\nsignificant additional computations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dongjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Geung-Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyLex: Incoporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08818","description":"<p>Incorporating lexical knowledge into deep learning models has been proved to\nbe very effective for sequence labeling tasks. However, previous works commonly\nhave difficulty dealing with large-scale dynamic lexicons which often cause\nexcessive matching noise and problems of frequent updates. In this paper, we\npropose DyLex, a plug-in lexicon incorporation approach for BERT based sequence\nlabeling tasks. Instead of leveraging embeddings of words in the lexicon as in\nconventional methods, we adopt word-agnostic tag embeddings to avoid\nre-training the representation while updating the lexicon. Moreover, we employ\nan effective supervised lexical knowledge denoising method to smooth out\nmatching noise. Finally, we introduce a col-wise attention based knowledge\nfusion mechanism to guarantee the pluggability of the proposed framework.\nExperiments on ten datasets of three tasks show that the proposed framework\nachieves new SOTA, even with very large scale lexicons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guang-Yuan Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero and Few-shot Knowledge-seeking Turn Detection in Task-orientated Dialogue Systems. (arXiv:2109.08820v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08820","description":"<p>Most prior work on task-oriented dialogue systems is restricted to supporting\ndomain APIs. However, users may have requests that are out of the scope of\nthese APIs. This work focuses on identifying such user requests. Existing\nmethods for this task mainly rely on fine-tuning pre-trained models on large\nannotated data. We propose a novel method, REDE, based on adaptive\nrepresentation learning and density estimation. REDE can be applied to\nzero-shot cases, and quickly learns a high-performing detector with only a few\nshots by updating less than 3K parameters. We demonstrate REDE's competitive\nperformance on DSTC9 data and our newly collected test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08828","description":"<p>Empathy is a complex cognitive ability based on the reasoning of others'\naffective states. In order to better understand others and express stronger\nempathy in dialogues, we argue that two issues must be tackled at the same\ntime: (i) identifying which word is the cause for the other's emotion from his\nor her utterance and (ii) reflecting those specific words in the response\ngeneration. However, previous approaches for recognizing emotion cause words in\ntext require sub-utterance level annotations, which can be demanding. Taking\ninspiration from social cognition, we leverage a generative estimator to infer\nemotion cause words from utterances with no word-level label. Also, we\nintroduce a novel method based on pragmatics to make dialogue models focus on\ntargeted words in the input during generation. Our method is applicable to any\ndialogue models with no additional training on the fly. We show our approach\nimproves multiple best-performing dialogue agents on generating more focused\nempathetic responses in terms of both automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeongchang Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-Deacon: Multimodal molecular domain embedding analysis via contrastive learning. (arXiv:2109.08830v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08830","description":"<p>Molecular representation learning plays an essential role in cheminformatics.\nRecently, language model-based approaches have been popular as an alternative\nto traditional expert-designed features to encode molecules. However, these\napproaches only utilize a single modality for representing molecules. Driven by\nthe fact that a given molecule can be described through different modalities\nsuch as Simplified Molecular Line Entry System (SMILES), The International\nUnion of Pure and Applied Chemistry (IUPAC), and The IUPAC International\nChemical Identifier (InChI), we propose a multimodal molecular embedding\ngeneration approach called MM-Deacon (multimodal molecular domain embedding\nanalysis via contrastive learning). MM-Deacon is trained using SMILES and IUPAC\nmolecule representations as two different modalities. First, SMILES and IUPAC\nstrings are encoded by using two different transformer-based language models\nindependently, then the contrastive loss is utilized to bring these encoded\nrepresentations from different modalities closer to each other if they belong\nto the same molecule, and to push embeddings farther from each other if they\nbelong to different molecules. We evaluate the robustness of our molecule\nembeddings on molecule clustering, cross-modal molecule search, drug similarity\nassessment and drug-drug interaction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhihui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pramod Kumar Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_R/0/1/0/all/0/1\">Robin Abraham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TVRecap: A Dataset for Generating Stories with Character Descriptions. (arXiv:2109.08833v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08833","description":"<p>We introduce TVRecap, a story generation dataset that requires generating\ndetailed TV show episode recaps from a brief summary and a set of documents\ndescribing the characters involved. Unlike other story generation datasets,\nTVRecap contains stories that are authored by professional screenwriters and\nthat feature complex interactions among multiple characters. Generating stories\nin TVRecap requires drawing relevant information from the lengthy provided\ndocuments about characters based on the brief summary. In addition, by swapping\nthe input and output, TVRecap can serve as a challenging testbed for\nabstractive summarization. We create TVRecap from fan-contributed websites,\nwhich allows us to collect 26k episode recaps with 1868.7 tokens on average.\nEmpirically, we take a hierarchical story generation approach and find that the\nneural model that uses oracle content selectors for character descriptions\ndemonstrates the best performance on automatic metrics, showing the potential\nof our dataset to inspire future research on story generation with constraints.\nQualitative analysis shows that the best-performing model sometimes generates\ncontent that is unfaithful to the short summaries, suggesting promising\ndirections for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification. (arXiv:2109.08839v1 [cs.SD])","link":"http://arxiv.org/abs/2109.08839","description":"<p>Recently, x-vector has been a successful and popular approach for speaker\nverification, which employs a time delay neural network (TDNN) and statistics\npooling to extract speaker characterizing embedding from variable-length\nutterances. Improvement upon the x-vector has been an active research area, and\nenormous neural networks have been elaborately designed based on the x-vector,\neg, extended TDNN (E-TDNN), factorized TDNN (F-TDNN), and densely connected\nTDNN (D-TDNN). In this work, we try to identify the optimal architectures from\na TDNN based search space employing neural architecture search (NAS), named\nSpeechNAS. Leveraging the recent advances in the speaker recognition, such as\nhigh-order statistics pooling, multi-branch mechanism, D-TDNN and angular\nadditive margin softmax (AAM) loss with a minimum hyper-spherical energy (MHE),\nSpeechNAS automatically discovers five network architectures, from SpeechNAS-1\nto SpeechNAS-5, of various numbers of parameters and GFLOPs on the large-scale\ntext-independent speaker recognition dataset VoxCeleb1. Our derived best neural\nnetwork achieves an equal error rate (EER) of 1.02% on the standard test set of\nVoxCeleb1, which surpasses previous TDNN based state-of-the-art approaches by a\nlarge margin. Code and trained weights are in\nhttps://github.com/wentaozhu/speechnas.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tianlong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jixiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Feng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Engineering for US State Legislative Hearings: Stance, Affiliation, Engagement and Absentees. (arXiv:2109.08855v1 [cs.IR])","link":"http://arxiv.org/abs/2109.08855","description":"<p>In US State government legislatures, most of the activity occurs in\ncommittees made up of lawmakers discussing bills. When analyzing, classifying\nor summarizing these committee proceedings, some important features become\nbroadly interesting. In this paper, we engineer four useful features, two\napplying to lawmakers (engagement and absence), and two to non-lawmakers\n(stance and affiliation). We propose a system to automatically track the\naffiliation of organizations in public comments and whether the organizational\nrepresentative supports or opposes the bill. The model tracking affiliation\nachieves an F1 of 0.872 while the support determination has an F1 of 0.979.\nAdditionally, a metric to compute legislator engagement and absenteeism is also\nproposed and as proof-of-concept, a list of the most and least engaged\nlegislators over one full California legislative session is presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grace_J/0/1/0/all/0/1\">Josh Grace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosmood_F/0/1/0/all/0/1\">Foaad Khosmood</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emily: Developing An Emotion-affective Open-Domain Chatbot with Knowledge Graph-based Persona. (arXiv:2109.08875v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08875","description":"<p>In this paper, we describe approaches for developing Emily, an\nemotion-affective open-domain chatbot. Emily can perceive a user's negative\nemotion state and offer supports by positively converting the user's emotion\nstates. This is done by finetuning a pretrained dialogue model upon data\ncapturing dialogue contexts and desirable emotion states transition across\nturns. Emily can differentiate a general open-domain dialogue utterance with\nquestions relating to personal information. By leveraging a question-answering\napproach based on knowledge graphs to handle personal information, Emily\nmaintains personality consistency. We evaluate Emily against a few\nstate-of-the-art open-domain chatbots and show the effects of the proposed\napproaches in emotion affecting and addressing personality inconsistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weixuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xiaoling Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chong Hsuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haonan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Ximing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuRecDial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation. (arXiv:2109.08877v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08877","description":"<p>In this paper, we provide a bilingual parallel human-to-human recommendation\ndialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging\ntask of multilingual and cross-lingual conversational recommendation. The\ndifference between DuRecDial 2.0 and existing conversational recommendation\ndatasets is that the data item (Profile, Goal, Knowledge, Context, Response) in\nDuRecDial 2.0 is annotated in two languages, both English and Chinese, while\nother datasets are built with the setting of a single language. We collect 8.2k\ndialogs aligned across English and Chinese languages (16.5k dialogs and 255k\nutterances in total) that are annotated by crowdsourced workers with strict\nquality control procedure. We then build monolingual, multilingual, and\ncross-lingual conversational recommendation baselines on DuRecDial 2.0.\nExperiment results show that the use of additional English data can bring\nperformance improvement for Chinese conversational recommendation, indicating\nthe benefits of DuRecDial 2.0. Finally, this dataset provides a challenging\ntestbed for future studies of monolingual, multilingual, and cross-lingual\nconversational recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zheng-Yu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Joint Intent Detection and Slot Filling via Higher-order Attention. (arXiv:2109.08890v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08890","description":"<p>Intent detection (ID) and Slot filling (SF) are two major tasks in spoken\nlanguage understanding (SLU). Recently, attention mechanism has been shown to\nbe effective in jointly optimizing these two tasks in an interactive manner.\nHowever, latest attention-based works concentrated only on the first-order\nattention design, while ignoring the exploration of higher-order attention\nmechanisms. In this paper, we propose a BiLinear attention block, which\nleverages bilinear pooling to simultaneously exploit both the contextual and\nchannel-wise bilinear attention distributions to capture the second-order\ninteractions between the input intent or slot features. Higher and even\ninfinity order interactions are built by stacking numerous blocks and assigning\nExponential Linear Unit (ELU) to blocks. Before the decoding stage, we\nintroduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot\ninformation in a more effective way. Technically, instead of simply\nconcatenating intent and slot features, we first compute two correlation\nmatrices to weight on two features. Furthermore, we present Higher-order\nAttention Network for the SLU tasks. Experiments on two benchmark datasets show\nthat our approach yields improvements compared with the state-of-the-art\napproach. We also provide discussion to demonstrate the effectiveness of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongsheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency distance minimization predicts compression. (arXiv:2109.08900v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08900","description":"<p>Dependency distance minimization (DDm) is a well-established principle of\nword order. It has been predicted theoretically that DDm implies compression,\nnamely the minimization of word lengths. This is a second order prediction\nbecause it links a principle with another principle, rather than a principle\nand a manifestation as in a first order prediction. Here we test that second\norder prediction with a parallel collection of treebanks controlling for\nannotation style with Universal Dependencies and Surface-Syntactic Universal\nDependencies. To test it, we use a recently introduced score that has many\nmathematical and statistical advantages with respect to the widely used sum of\ndependency distances. We find that the prediction is confirmed by the new score\nwhen word lengths are measured in phonemes, independently of the annotation\nstyle, but not when word lengths are measured in syllables. In contrast, one of\nthe most widely used scores, i.e. the sum of dependency distances, fails to\nconfirm that prediction, showing the weakness of raw dependency distances for\nresearch on word order. Finally, our findings expand the theory of natural\ncommunication by linking two distinct levels of organization, namely syntax\n(word order) and word internal structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Detoxification using Large Pre-trained Neural Models. (arXiv:2109.08914v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08914","description":"<p>We present two novel unsupervised methods for eliminating toxicity in text.\nOur first method combines two recent ideas: (1) guidance of the generation\nprocess with small style-conditional language models and (2) use of\nparaphrasing models to perform style transfer. We use a well-performing\nparaphraser guided by style-trained language models to keep the text content\nand remove toxicity. Our second method uses BERT to replace toxic words with\ntheir non-offensive synonyms. We make the method more flexible by enabling BERT\nto replace mask tokens with a variable number of words. Finally, we present the\nfirst large-scale comparative study of style transfer models on the task of\ntoxicity removal. We compare our models with a number of methods for style\ntransfer. The models are evaluated in a reference-free way using a combination\nof unsupervised style transfer metrics. Both methods we suggest yield new SOTA\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1\">David Dale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voronov_A/0/1/0/all/0/1\">Anton Voronov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dementieva_D/0/1/0/all/0/1\">Daryna Dementieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Logacheva_V/0/1/0/all/0/1\">Varvara Logacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozlova_O/0/1/0/all/0/1\">Olga Kozlova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semenov_N/0/1/0/all/0/1\">Nikita Semenov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs. (arXiv:2109.08925v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08925","description":"<p>Complex Query Answering (CQA) is an important reasoning task on knowledge\ngraphs. Current CQA learning models have been shown to be able to generalize\nfrom atomic operators to more complex formulas, which can be regarded as the\ncombinatorial generalizability. In this paper, we present EFO-1-QA, a new\ndataset to benchmark the combinatorial generalizability of CQA models by\nincluding 301 different queries types, which is 20 times larger than existing\ndatasets. Besides, our work, for the first time, provides a benchmark to\nevaluate and analyze the impact of different operators and normal forms by\nusing (a) 7 choices of the operator systems and (b) 9 forms of complex queries.\nSpecifically, we provide the detailed study of the combinatorial\ngeneralizability of two commonly used operators, i.e., projection and\nintersection, and justify the impact of the forms of queries given the\ncanonical choice of operators. Our code and data can provide an effective\npipeline to benchmark CQA models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic. (arXiv:2109.08927v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08927","description":"<p>Natural language inference (NLI) aims to determine the logical relationship\nbetween two sentences among the target labels Entailment, Contradiction, and\nNeutral. In recent years, deep learning models have become a prevailing\napproach to NLI, but they lack interpretability and explainability. In this\nwork, we address the explainability for NLI by weakly supervised logical\nreasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our\nmodel first detects phrases as the semantic unit and aligns corresponding\nphrases. Then, the model predicts the NLI label for the aligned phrases, and\ninduces the sentence label by fuzzy logic formulas. Our EPR is almost\neverywhere differentiable and thus the system can be trained end-to-end in a\nweakly supervised manner. We annotated a corpus and developed a set of metrics\nto evaluate phrasal reasoning. Results show that our EPR yields much more\nmeaningful explanations in terms of F scores than previous studies. To the best\nof our knowledge, we are the first to develop a weakly supervised phrasal\nreasoning model for the NLI task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Atharva Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zi Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Temporal Question Answering on Knowledge Graphs. (arXiv:2109.08935v1 [cs.IR])","link":"http://arxiv.org/abs/2109.08935","description":"<p>Question answering over knowledge graphs (KG-QA) is a vital topic in IR.\nQuestions with temporal intent are a special class of practical importance, but\nhave not received much attention in research. This work presents EXAQT, the\nfirst end-to-end system for answering complex temporal questions that have\nmultiple entities and predicates, and associated temporal conditions. EXAQT\nanswers natural language questions over KGs in two stages, one geared towards\nhigh recall, the other towards precision at top ranks. The first step computes\nquestion-relevant compact subgraphs within the KG, and judiciously enhances\nthem with pertinent temporal facts, using Group Steiner Trees and fine-tuned\nBERT models. The second step constructs relational graph convolutional networks\n(R-GCNs) from the first step's output, and enhances the R-GCNs with time-aware\nentity embeddings and attention over temporal relations. We evaluate EXAQT on\nTimeQuestions, a large dataset of 16k temporal questions we compiled from a\nvariety of general purpose KG-QA benchmarks. Results show that EXAQT\noutperforms three state-of-the-art systems for answering complex questions over\nKGs, thereby justifying specialized treatment of temporal QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReaSCAN: Compositional Reasoning in Language Grounding. (arXiv:2109.08994v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08994","description":"<p>The ability to compositionally map language to referents, relations, and\nactions is an essential component of language understanding. The recent gSCAN\ndataset (Ruis et al. 2020, NeurIPS) is an inspiring attempt to assess the\ncapacity of models to learn this kind of grounding in scenarios involving\nnavigational instructions. However, we show that gSCAN's highly constrained\ndesign means that it does not require compositional interpretation and that\nmany details of its instructions and scenarios are not required for task\nsuccess. To address these limitations, we propose ReaSCAN, a benchmark dataset\nthat builds off gSCAN but requires compositional language interpretation and\nreasoning about entities and relations. We assess two models on ReaSCAN: a\nmulti-modal baseline and a state-of-the-art graph convolutional neural model.\nThese experiments show that ReaSCAN is substantially harder than gSCAN for both\nneural architectures. This suggests that ReaSCAN can serve as a valuable\nbenchmark for advancing our understanding of models' compositional\ngeneralization and reasoning capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1\">Elisa Kreiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting semantic lexicons using word embeddings and transfer learning. (arXiv:2109.09010v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09010","description":"<p>Sentiment-aware intelligent systems are essential to a wide array of\napplications including marketing, political campaigns, recommender systems,\nbehavioral economics, social psychology, and national security. These\nsentiment-aware intelligent systems are driven by language models which broadly\nfall into two paradigms: 1. Lexicon-based and 2. Contextual. Although recent\ncontextual models are increasingly dominant, we still see demand for\nlexicon-based models because of their interpretability and ease of use. For\nexample, lexicon-based models allow researchers to readily determine which\nwords and phrases contribute most to a change in measured sentiment. A\nchallenge for any lexicon-based approach is that the lexicon needs to be\nroutinely expanded with new words and expressions. Crowdsourcing annotations\nfor semantic dictionaries may be an expensive and time-consuming task. Here, we\npropose two models for predicting sentiment scores to augment semantic lexicons\nat a relatively low cost using word embeddings and transfer learning. Our first\nmodel establishes a baseline employing a simple and shallow neural network\ninitialized with pre-trained word embeddings using a non-contextual approach.\nOur second model improves upon our baseline, featuring a deep Transformer-based\nnetwork that brings to bear word definitions to estimate their lexical\npolarity. Our evaluation shows that both models are able to score new words\nwith a similar accuracy to reviewers from Amazon Mechanical Turk, but at a\nfraction of the cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oort_C/0/1/0/all/0/1\">Colin Van Oort</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">Mikaela Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine Learning Pipeline to Examine Political Bias with Congressional Speeches. (arXiv:2109.09014v1 [cs.CY])","link":"http://arxiv.org/abs/2109.09014","description":"<p>Computational methods to model political bias in social media involve several\nchallenges due to heterogeneity, high-dimensional, multiple modalities, and the\nscale of the data. Political bias in social media has been studied in multiple\nviewpoints like media bias, political ideology, echo chambers, and\ncontroversies using machine learning pipelines. Most of the current methods\nrely heavily on the manually-labeled ground-truth data for the underlying\npolitical bias prediction tasks. Limitations of such methods include\nhuman-intensive labeling, labels related to only a specific problem, and the\ninability to determine the near future bias state of a social media\nconversation. In this work, we address such problems and give machine learning\napproaches to study political bias in two ideologically diverse social media\nforums: Gab and Twitter without the availability of human-annotated data. Our\nproposed methods exploit the use of transcripts collected from political\nspeeches in US congress to label the data and achieve the highest accuracy of\n70.5% and 65.1% in Twitter and Gab data respectively to predict political bias.\nWe also present a machine learning approach that combines features from\ncascades and text to forecast cascade's political bias with an accuracy of\nabout 85%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+hajare_P/0/1/0/all/0/1\">Prasad hajare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_S/0/1/0/all/0/1\">Sadia Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1\">Siddharth Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagavathi_A/0/1/0/all/0/1\">Arunkumar Bagavathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Relation-Guided Type-Sentence Alignment for Long-Tail Relation Extraction with Distant Supervision. (arXiv:2109.09036v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09036","description":"<p>Distant supervision uses triple facts in knowledge graphs to label a corpus\nfor relation extraction, leading to wrong labeling and long-tail problems. Some\nworks use the hierarchy of relations for knowledge transfer to long-tail\nrelations. However, a coarse-grained relation often implies only an attribute\n(e.g., domain or topic) of the distant fact, making it hard to discriminate\nrelations based solely on sentence semantics. One solution is resorting to\nentity types, but open questions remain about how to fully leverage the\ninformation of entity types and how to align multi-granular entity types with\nsentences. In this work, we propose a novel model to enrich\ndistantly-supervised sentences with entity types. It consists of (1) a pairwise\ntype-enriched sentence encoding module injecting both context-free and -related\nbackgrounds to alleviate sentence-level wrong labeling, and (2) a hierarchical\ntype-sentence alignment module enriching a sentence with the triple fact's\nbasic attributes to support long-tail relations. Our model achieves new\nstate-of-the-art results in overall and long-tail performance on benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Enhanced Evidence Retrieval for Counterargument Generation. (arXiv:2109.09057v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09057","description":"<p>Finding counterevidence to statements is key to many tasks, including\ncounterargument generation. We build a system that, given a statement,\nretrieves counterevidence from diverse sources on the Web. At the core of this\nsystem is a natural language inference (NLI) model that determines whether a\ncandidate sentence is valid counterevidence or not. Most NLI models to date,\nhowever, lack proper reasoning abilities necessary to find counterevidence that\ninvolves complex inference. Thus, we present a knowledge-enhanced NLI model\nthat aims to handle causality- and example-based inference by incorporating\nknowledge graphs. Our NLI model outperforms baselines for NLI tasks, especially\nfor instances that require the targeted inference. In addition, this NLI model\nfurther improves the counterevidence retrieval system, notably finding complex\ncounterevidence better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yohan Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1\">Haneul Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_J/0/1/0/all/0/1\">JinYeong Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Chris Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Training with Contrastive Learning in NLP. (arXiv:2109.09075v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09075","description":"<p>For years, adversarial training has been extensively studied in natural\nlanguage processing (NLP) settings. The main goal is to make models robust so\nthat similar inputs derive in semantically similar outcomes, which is not a\ntrivial problem since there is no objective measure of semantic similarity in\nlanguage. Previous works use an external pre-trained NLP model to tackle this\nchallenge, introducing an extra training stage with huge memory consumption\nduring training. However, the recent popular approach of contrastive learning\nin language processing hints a convenient way of obtaining such similarity\nrestrictions. The main advantage of the contrastive learning approach is that\nit aims for similar data points to be mapped close to each other and further\nfrom different ones in the representation space. In this work, we propose\nadversarial training with contrastive learning (ATCL) to adversarially train a\nlanguage processing task using the benefits of contrastive learning. The core\nidea is to make linear perturbations in the embedding space of the input via\nfast gradient methods (FGM) and train the model to keep the original and\nperturbed representations close via contrastive learning. In NLP experiments,\nwe applied ATCL to language modeling and neural machine translation tasks. The\nresults show not only an improvement in the quantitative (perplexity and BLEU)\nscores when compared to the baselines, but ATCL also achieves good qualitative\nresults in the semantic level for both tasks without using a pre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rim_D/0/1/0/all/0/1\">Daniela N. Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1\">DongNyeong Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09105","description":"<p>Language Models (LMs) have been ubiquitously leveraged in various tasks\nincluding spoken language understanding (SLU). Spoken language requires careful\nunderstanding of speaker interactions, dialog states and speech induced\nmultimodal behaviors to generate a meaningful representation of the\nconversation.In this work, we propose to dissect SLU into three representative\nproperties:conversational(disfluency, pause, overtalk), channel(speaker-type,\nturn-tasks) andASR(insertion, deletion,substitution). We probe BERT based\nlanguage models (BERT, RoBERTa) trained on spoken transcripts to investigate\nits ability to understand multifarious properties in absence of any speech\ncues. Empirical results indicate that LM is surprisingly good at capturing\nconversational properties such as pause prediction and overtalk detection from\nlexical tokens. On the downsides, the LM scores low on turn-tasks and ASR\nerrors predictions. Additionally, pre-training the LM on spoken transcripts\nrestrain its linguistic understanding. Finally,we establish the efficacy and\ntransferability of the mentioned properties on two benchmark datasets:\nSwitchboard Dialog Act and Disfluency datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_M/0/1/0/all/0/1\">Mukuntha Narayanan Sundararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vepa_J/0/1/0/all/0/1\">Jithendra Vepa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Long-Range Language Models Actually Use Long-Range Context?. (arXiv:2109.09115v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09115","description":"<p>Language models are generally trained on short, truncated input sequences,\nwhich limits their ability to use discourse-level information present in\nlong-range context to improve their predictions. Recent efforts to improve the\nefficiency of self-attention have led to a proliferation of long-range\nTransformer language models, which can process much longer sequences than\nmodels of the past. However, the ways in which such models take advantage of\nthe long-range context remain unclear. In this paper, we perform a fine-grained\nanalysis of two long-range Transformer language models (including the\n\\emph{Routing Transformer}, which achieves state-of-the-art perplexity on the\nPG-19 long-sequence LM benchmark dataset) that accept input sequences of up to\n8K tokens. Our results reveal that providing long-range context (i.e., beyond\nthe previous 2K tokens) to these models only improves their predictions on a\nsmall set of tokens (e.g., those that can be copied from the distant context)\nand does not help at all for sentence-level prediction tasks. Finally, we\ndiscover that PG-19 contains a variety of different document types and domains,\nand that long-range context helps most for literary novels (as opposed to\ntextbooks or magazines).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattarella_Micke_A/0/1/0/all/0/1\">Andrew Mattarella-Micke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preventing Author Profiling through Zero-Shot Multilingual Back-Translation. (arXiv:2109.09133v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09133","description":"<p>Documents as short as a single sentence may inadvertently reveal sensitive\ninformation about their authors, including e.g. their gender or ethnicity.\nStyle transfer is an effective way of transforming texts in order to remove any\ninformation that enables author profiling. However, for a number of current\nstate-of-the-art approaches the improved privacy is accompanied by an\nundesirable drop in the down-stream utility of the transformed data. In this\npaper, we propose a simple, zero-shot way to effectively lower the risk of\nauthor profiling through multilingual back-translation using off-the-shelf\ntranslation models. We compare our models with five representative text style\ntransfer models on three datasets across different domains. Results from both\nan automatic and a human evaluation show that our approach achieves the best\noverall performance while requiring no training data. We are able to lower the\nadversarial prediction of gender and race by up to $22\\%$ while retaining\n$95\\%$ of the original utility on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davody_A/0/1/0/all/0/1\">Ali Davody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1\">Thomas Kleinbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition. (arXiv:2109.09161v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09161","description":"<p>Unifying acoustic and linguistic representation learning has become\nincreasingly crucial to transfer the knowledge learned on the abundance of\nhigh-resource language data for low-resource speech recognition. Existing\napproaches simply cascade pre-trained acoustic and language models to learn the\ntransfer from speech to text. However, how to solve the representation\ndiscrepancy of speech and text is unexplored, which hinders the utilization of\nacoustic and linguistic information. Moreover, previous works simply replace\nthe embedding layer of the pre-trained language model with the acoustic\nfeatures, which may cause the catastrophic forgetting problem. In this work, we\nintroduce Wav-BERT, a cooperative acoustic and linguistic representation\nlearning method to fuse and utilize the contextual information of speech and\ntext. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a\nlanguage model (BERT) into an end-to-end trainable framework. A Representation\nAggregation Module is designed to aggregate acoustic and linguistic\nrepresentation, and an Embedding Attention Module is introduced to incorporate\nacoustic information into BERT, which can effectively facilitate the\ncooperation of two pre-trained models and thus boost the representation\nlearning. Extensive experiments show that our Wav-BERT significantly\noutperforms the existing approaches and achieves state-of-the-art performance\non low-resource speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guolin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yubei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1\">Ke Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FST Morphological Analyser and Generator for Mapud\\\"ungun. (arXiv:2109.09176v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09176","description":"<p>Following the Mapuche grammar by Smeets, this article describes the main\nmorphophonological aspects of Mapud\\\"ungun, explaining what triggers them and\nthe contexts where they arise. We present a computational approach producing a\nfinite state morphological analyser (and generator) capable of classifying and\nappropriately tagging all the components (roots and suffixes) that interact in\na Mapuche word form. The bulk of the article focuses on presenting details\nabout the morphology of Mapud\\\"ungun verb and its formalisation using FOMA. A\nsystem evaluation process and its results are also present in this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandia_A/0/1/0/all/0/1\">Andr&#xe9;s Chand&#xed;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Dynamic based data filtering may not work for NLP datasets. (arXiv:2109.09191v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09191","description":"<p>The recent increase in dataset size has brought about significant advances in\nnatural language understanding. These large datasets are usually collected\nthrough automation (search engines or web crawlers) or crowdsourcing which\ninherently introduces incorrectly labeled data. Training on these datasets\nleads to memorization and poor generalization. Thus, it is pertinent to develop\ntechniques that help in the identification and isolation of mislabelled data.\nIn this paper, we study the applicability of the Area Under the Margin (AUM)\nmetric to identify and remove/rectify mislabelled examples in NLP datasets. We\nfind that mislabelled samples can be filtered using the AUM metric in NLP\ndatasets but it also removes a significant number of correctly labeled points\nand leads to the loss of a large amount of relevant language information. We\nshow that models rely on the distributional information instead of relying on\nsyntactic and semantic representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_A/0/1/0/all/0/1\">Arka Talukdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagar_M/0/1/0/all/0/1\">Monika Dagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prachi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_V/0/1/0/all/0/1\">Varun Menon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-Label Language Learning. (arXiv:2109.09193v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09193","description":"<p>This paper explores zero-label learning in Natural Language Processing (NLP),\nwhereby no human-annotated data is used anywhere during training and models are\ntrained purely on synthetic data. At the core of our framework is a novel\napproach for better leveraging the powerful pretrained language models.\nSpecifically, inspired by the recent success of few-shot inference on GPT-3, we\npresent a training data creation procedure named Unsupervised Data Generation\n(UDG), which leverages few-shot prompts to synthesize high-quality training\ndata without real human annotations. Our method enables zero-label learning as\nwe train task-specific models solely on the synthetic data, yet we achieve\nbetter or comparable results from strong baseline models trained on\nhuman-labeled data. Furthermore, when mixed with labeled data, our approach\nserves as a highly effective data augmentation procedure, achieving new\nstate-of-the-art results on the SuperGLUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Crowdsourcing Protocols for Evaluatingthe Factual Consistency of Summaries. (arXiv:2109.09195v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09195","description":"<p>Current pre-trained models applied to summarization are prone to factual\ninconsistencies which either misrepresent the source text or introduce\nextraneous information. Thus, comparing the factual consistency of summaries is\nnecessary as we develop improved models. However, the optimal human evaluation\nsetup for factual consistency has not been standardized. To address this issue,\nwe crowdsourced evaluations for factual consistency using the rating-based\nLikert scale and ranking-based Best-Worst Scaling protocols, on 100 articles\nfrom each of the CNN-Daily Mail and XSum datasets over four state-of-the-art\nmodels, to determine the most reliable evaluation framework. We find that\nranking-based protocols offer a more reliable measure of summary quality across\ndatasets, while the reliability of Likert ratings depends on the target dataset\nand the evaluation design. Our crowdsourcing templates and summary evaluations\nwill be publicly available to facilitate future research on factual consistency\nin summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_G/0/1/0/all/0/1\">Griffin Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Borui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization. (arXiv:2109.09209v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09209","description":"<p>We study generating abstractive summaries that are faithful and factually\nconsistent with the given articles. A novel contrastive learning formulation is\npresented, which leverages both reference summaries, as positive training data,\nand automatically generated erroneous summaries, as negative training data, to\ntrain summarization systems that are better at distinguishing between them. We\nfurther design four types of strategies for creating negative samples, to\nresemble errors made commonly by two state-of-the-art models, BART and PEGASUS,\nfound in our new human annotations of summary errors. Experiments on XSum and\nCNN/Daily Mail show that our contrastive learning framework is robust across\ndatasets and models. It consistently produces more factual summaries than\nstrong comparisons with post error correction, entailment-based reranking, and\nunlikelihood training, according to QA-based factuality evaluation. Human\njudges echo the observation and find that our model summaries correct more\nerrors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UPV at CheckThat! 2021: Mitigating Cultural Differences for Identifying Multilingual Check-worthy Claims. (arXiv:2109.09232v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09232","description":"<p>Identifying check-worthy claims is often the first step of automated\nfact-checking systems. Tackling this task in a multilingual setting has been\nunderstudied. Encoding inputs with multilingual text representations could be\none approach to solve the multilingual check-worthiness detection. However,\nthis approach could suffer if cultural bias exists within the communities on\ndetermining what is check-worthy.In this paper, we propose a language\nidentification task as an auxiliary task to mitigate unintended bias.With this\npurpose, we experiment joint training by using the datasets from CLEF-2021\nCheckThat!, that contain tweets in English, Arabic, Bulgarian, Spanish and\nTurkish. Our results show that joint training of language identification and\ncheck-worthy claim detection tasks can provide performance gains for some of\nthe selected languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek Baris Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Angel Felipe Magnoss&#xe3;o de Paula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified and Multilingual Author Profiling for Detecting Haters. (arXiv:2109.09233v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09233","description":"<p>This paper presents a unified user profiling framework to identify hate\nspeech spreaders by processing their tweets regardless of the language. The\nframework encodes the tweets with sentence transformers and applies an\nattention mechanism to select important tweets for learning user profiles.\nFurthermore, the attention layer helps to explain why a user is a hate speech\nspreader by producing attention weights at both token and post level. Our\nproposed model outperformed the state-of-the-art multilingual transformer\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek Baris Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Angel Felipe Magnoss&#xe3;o de Paula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional probing: measuring usable information beyond a baseline. (arXiv:2109.09234v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09234","description":"<p>Probing experiments investigate the extent to which neural representations\nmake properties -- like part-of-speech -- predictable. One suggests that a\nrepresentation encodes a property if probing that representation produces\nhigher accuracy than probing a baseline representation like non-contextual word\nembeddings. Instead of using baselines as a point of comparison, we're\ninterested in measuring information that is contained in the representation but\nnot in the baseline. For example, current methods can detect when a\nrepresentation is more useful than the word identity (a baseline) for\npredicting part-of-speech; however, they cannot detect when the representation\nis predictive of just the aspects of part-of-speech not explainable by the word\nidentity. In this work, we extend a theory of usable information called\n$\\mathcal{V}$-information and propose conditional probing, which explicitly\nconditions on the information in the baseline. In a case study, we find that\nafter conditioning on non-contextual word embeddings, properties like\npart-of-speech are accessible at deeper layers of a network than previously\nthought.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_J/0/1/0/all/0/1\">John Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ethayarajh_K/0/1/0/all/0/1\">Kawin Ethayarajh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models. (arXiv:2109.09237v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09237","description":"<p>Recent work indicated that pretrained language models (PLMs) such as BERT and\nRoBERTa can be transformed into effective sentence and word encoders even via\nsimple self-supervised techniques. Inspired by this line of work, in this paper\nwe propose a fully unsupervised approach to improving word-in-context (WiC)\nrepresentations in PLMs, achieved via a simple and efficient WiC-targeted\nfine-tuning procedure: MirrorWiC. The proposed method leverages only raw texts\nsampled from Wikipedia, assuming no sense-annotated data, and learns\ncontext-aware word representations within a standard contrastive learning\nsetup. We experiment with a series of standard and comprehensive WiC benchmarks\nacross multiple languages. Our proposed fully unsupervised MirrorWiC models\nobtain substantial gains over off-the-shelf PLMs across all monolingual,\nmultilingual and cross-lingual setups. Moreover, on some standard WiC\nbenchmarks, MirrorWiC is even on-par with supervised models fine-tuned with\nin-task data and sense labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring the Volatility of the Political agenda in Public Opinion and News Media. (arXiv:1808.09037v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/1808.09037","description":"<p>Recent election surprises, regime changes, and political shocks indicate that\npolitical agendas have become more fast-moving and volatile. The ability to\nmeasure the complex dynamics of agenda change and capture the nature and extent\nof volatility in political systems is therefore more crucial than ever before.\nThis study proposes a definition and operationalization of volatility that\ncombines insights from political science, communications, information theory,\nand computational techniques. The proposed measures of fractionalization and\nagenda change encompass the shifting salience of issues in the agenda as a\nwhole and allow the study of agendas across different domains. We evaluate\nthese metrics and compare them to other measures such as issue-level survival\nrates and the Pedersen Index, which uses public-opinion poll data to measure\npublic agendas, as well as traditional media content to measure media agendas\nin the UK and Germany. We show how these measures complement existing\napproaches and could be employed in future agenda-setting research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camargo_C/0/1/0/all/0/1\">Chico Q. Camargo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+John_P/0/1/0/all/0/1\">Peter John</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margetts_H/0/1/0/all/0/1\">Helen Z. Margetts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NatCat: Weakly Supervised Text Classification with Naturally Annotated Resources. (arXiv:2009.14335v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.14335","description":"<p>We describe NatCat, a large-scale resource for text classification\nconstructed from three data sources: Wikipedia, Stack Exchange, and Reddit.\nNatCat consists of document-category pairs derived from manual curation that\noccurs naturally within online communities. To demonstrate its usefulness, we\nbuild general purpose text classifiers by training on NatCat and evaluate them\non a suite of 11 text classification tasks (CatEval), reporting large\nimprovements compared to prior work. We benchmark different modeling choices\nand resource combinations and show how tasks benefit from particular NatCat\ndata sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zewei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stratos_K/0/1/0/all/0/1\">Karl Stratos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exemplar-Controllable Paraphrasing and Translation using Bitext. (arXiv:2010.05856v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.05856","description":"<p>Most prior work on exemplar-based syntactically controlled paraphrase\ngeneration relies on automatically-constructed large-scale paraphrase datasets,\nwhich are costly to create. We sidestep this prerequisite by adapting models\nfrom prior work to be able to learn solely from bilingual text (bitext).\nDespite only using bitext for training, and in near zero-shot conditions, our\nsingle proposed model can perform four tasks: controlled paraphrase generation\nin both languages and controlled machine translation in both language\ndirections. To evaluate these tasks quantitatively, we create three novel\nevaluation datasets. Our experimental results show that our models achieve\ncompetitive results on controlled paraphrase generation and strong performance\non controlled machine translation. Analysis shows that our models learn to\ndisentangle semantics and syntax in their latent representations, but still\nsuffer from semantic drift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can images help recognize entities? A study of the role of images for Multimodal NER. (arXiv:2010.12712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12712","description":"<p>Multimodal named entity recognition (MNER) requires to bridge the gap between\nlanguage understanding and visual context. While many multimodal neural\ntechniques have been proposed to incorporate images into the MNER task, the\nmodel's ability to leverage multimodal interactions remains poorly understood.\nIn this work, we conduct in-depth analyses of existing multimodal fusion\ntechniques from different perspectives and describe the scenarios where adding\ninformation from the image does not always boost performance. We also study the\nuse of captions as a way to enrich the context for MNER. Experiments on three\ndatasets from popular social platforms expose the bottleneck of existing\nmultimodal models and the situations where using captions is beneficial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.08091","description":"<p>Sentiment quantification is the task of training, by means of supervised\nlearning, estimators of the relative frequency (also called ``prevalence'') of\nsentiment-related classes (such as \\textsf{Positive}, \\textsf{Neutral},\n\\textsf{Negative}) in a sample of unlabelled texts. This task is especially\nimportant when these texts are tweets, since the final goal of most sentiment\nclassification efforts carried out on Twitter data is actually quantification\n(and not the classification of individual tweets). It is well-known that\nsolving quantification by means of ``classify and count'' (i.e., by classifying\nall unlabelled items by means of a standard classifier and counting the items\nthat have been assigned to a given class) is less than optimal in terms of\naccuracy, and that more accurate quantification methods exist. Gao and\nSebastiani (2016) carried out a systematic comparison of quantification methods\non the task of tweet sentiment quantification. In hindsight, we observe that\nthe experimental protocol followed in that work was weak, and that the\nreliability of the conclusions that were drawn from the results is thus\nquestionable. We now re-evaluate those quantification methods (plus a few more\nmodern ones) on exactly the same same datasets, this time following a now\nconsolidated and much more robust experimental protocol (which also involves\nsimulating the presence, in the test data, of class prevalence values very\ndifferent from those of the training set). This experimental protocol (even\nwithout counting the newly added methods) involves a number of experiments\n5,775 times larger than that of the original study. The results of our\nexperiments are dramatically different from those obtained by Gao and\nSebastiani, and they provide a different, much more solid understanding of the\nrelative strengths and weaknesses of different sentiment quantification\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons Learned. (arXiv:2101.00133v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00133","description":"<p>We review the EfficientQA competition from NeurIPS 2020. The competition\nfocused on open-domain question answering (QA), where systems take natural\nlanguage questions as input and return natural language answers. The aim of the\ncompetition was to build systems that can predict correct answers while also\nsatisfying strict on-disk memory budgets. These memory budgets were designed to\nencourage contestants to explore the trade-off between storing retrieval\ncorpora or the parameters of learned models. In this report, we describe the\nmotivation and organization of the competition, review the best submissions,\nand analyze system predictions to inform a discussion of evaluation for\nopen-domain QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberti_C/0/1/0/all/0/1\">Chris Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palomaki_J/0/1/0/all/0/1\">Jennimaria Palomaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowski_T/0/1/0/all/0/1\">Tom Kwiatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuttler_H/0/1/0/all/0/1\">Heinrich K&#xfc;ttler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1\">Pasquale Minervini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1\">Gautier Izacard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_L/0/1/0/all/0/1\">Lucas Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1\">Edouard Grave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimaoka_S/0/1/0/all/0/1\">Sonse Shimaoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masatoshi Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyawaki_S/0/1/0/all/0/1\">Shumpei Miyawaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_S/0/1/0/all/0/1\">Shun Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_R/0/1/0/all/0/1\">Ryo Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docekal_M/0/1/0/all/0/1\">Martin Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas Oguz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Size Does Not Fit All: Finding the Optimal Subword Sizes for FastText Models across Languages. (arXiv:2102.02585v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.02585","description":"<p>Unsupervised representation learning of words from large multilingual corpora\nis useful for downstream tasks such as word sense disambiguation, semantic text\nsimilarity, and information retrieval. The representation precision of\nlog-bilinear fastText models is mostly due to their use of subword information.\nIn previous work, the optimization of fastText's subword sizes has not been\nfully explored, and non-English fastText models were trained using subword\nsizes optimized for English and German word analogy tasks. In our work, we find\nthe optimal subword sizes on the English, German, Czech, Italian, Spanish,\nFrench, Hindi, Turkish, and Russian word analogy tasks. We then propose a\nsimple n-gram coverage model and we show that it predicts better-than-default\nsubword sizes on the Spanish, French, Hindi, Turkish, and Russian word analogy\ntasks. We show that the optimization of fastText's subword sizes matters and\nresults in a 14% improvement on the Czech word analogy task. We also show that\nexpensive parameter optimization can be replaced by a simple n-gram coverage\nmodel that consistently improves the accuracy of fastText models on the word\nanalogy tasks by up to 3% compared to the default subword sizes, and that it is\nwithin 1% accuracy of the optimal subword sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ayetiran_E/0/1/0/all/0/1\">Eniafe Festus Ayetiran</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bacovsky_D/0/1/0/all/0/1\">Dalibor Ba&#x10d;ovsk&#xfd;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1\">D&#xe1;vid Lupt&#xe1;k</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a> (1) ((1) Faculty of Informatics Masaryk University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Post-Processing Technique for Improving Readability Assessment of Texts using Word Mover's Distance. (arXiv:2103.07277v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07277","description":"<p>Assessing the proper difficulty levels of reading materials or texts in\ngeneral is the first step towards effective comprehension and learning. In this\nstudy, we improve the conventional methodology of automatic readability\nassessment by incorporating the Word Mover's Distance (WMD) of ranked texts as\nan additional post-processing technique to further ground the difficulty level\ngiven by a model. Results of our experiments on three multilingual datasets in\nFilipino, German, and English show that the post-processing technique\noutperforms previous vanilla and ranking-based models using SVM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Ethel Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning Pretrained Transformers into RNNs. (arXiv:2103.13076v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.13076","description":"<p>Transformers have outperformed recurrent neural networks (RNNs) in natural\nlanguage generation. But this comes with a significant computational cost, as\nthe attention mechanism's complexity scales quadratically with sequence length.\nEfficient transformer variants have received increasing interest in recent\nworks. Among them, a linear-complexity recurrent variant has proven well suited\nfor autoregressive generation. It approximates the softmax attention with\nrandomized or heuristic feature maps, but can be difficult to train and may\nyield suboptimal accuracy. This work aims to convert a pretrained transformer\ninto its efficient recurrent counterpart, improving efficiency while\nmaintaining accuracy. Specifically, we propose a swap-then-finetune procedure:\nin an off-the-shelf pretrained transformer, we replace the softmax attention\nwith its linear-complexity recurrent alternative and then finetune. With a\nlearned feature map, our approach provides an improved tradeoff between\nefficiency and accuracy over the standard transformer and other recurrent\nvariants. We also show that the finetuning process has lower training cost\nrelative to training these recurrent variants from scratch. As many models for\nnatural language tasks are increasingly dependent on large-scale pretrained\ntransformers, this work presents a viable approach to improving inference\nefficiency without repeating the expensive pretraining process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No Keyword is an Island: In search of covert associations. (arXiv:2103.17114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.17114","description":"<p>This paper describes how corpus-assisted discourse analysis based on keyword\n(KW) identification and interpretation can benefit from employing Market basket\nanalysis (MBA) after KW extraction. MBA is a data mining technique used\noriginally in marketing that can reveal consistent associations between items\nin a shopping cart, but also between keywords in a corpus of many texts. By\nidentifying recurring associations between KWs we can compensate for the lack\nof wider context which is a major issue impeding the interpretation of isolated\nKWs (esp. when analyzing large data). To showcase the advantages of MBA in\n\"re-contextualizing\" keywords within the discourse, a pilot study on the topic\nof migration was conducted contrasting anti-system and center-right Czech\ninternet media. was conducted. The results show that MBA is useful in\nidentifying the dominant strategy of anti-system news portals: to weave in a\nconfounding ideological undercurrent and connect the concept of migrants to a\nmultitude of other topics (i.e., flooding the discourse).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cvrcek_V/0/1/0/all/0/1\">V&#xe1;clav Cvr&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_M/0/1/0/all/0/1\">Masako Ueda Fidler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SYSML: StYlometry with Structure and Multitask Learning: Implications for Darknet Forum Migrant Analysis. (arXiv:2104.00764v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.00764","description":"<p>Darknet market forums are frequently used to exchange illegal goods and\nservices between parties who use encryption to conceal their identities. The\nTor network is used to host these markets, which guarantees additional\nanonymization from IP and location tracking, making it challenging to link\nacross malicious users using multiple accounts (sybils). Additionally, users\nmigrate to new forums when one is closed, making it difficult to link users\nacross multiple forums. We develop a novel stylometry-based multitask learning\napproach for natural language and interaction modeling using graph embeddings\nto construct low-dimensional representations of short episodes of user activity\nfor authorship attribution. We provide a comprehensive evaluation of our\nmethods across four different darknet forums demonstrating its efficacy over\nthe state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X\non Recall@10.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maneriker_P/0/1/0/all/0/1\">Pranav Maneriker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuntian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGA : An Intent-Guided Authoring Assistant. (arXiv:2104.07000v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07000","description":"<p>While large-scale pretrained language models have significantly improved\nwriting assistance functionalities such as autocomplete, more complex and\ncontrollable writing assistants have yet to be explored. We leverage advances\nin language modeling to build an interactive writing assistant that generates\nand rephrases text according to fine-grained author specifications. Users\nprovide input to our Intent-Guided Assistant (IGA) in the form of text\ninterspersed with tags that correspond to specific rhetorical directives (e.g.,\nadding description or contrast, or rephrasing a particular sentence). We\nfine-tune a language model on a dataset heuristically-labeled with author\nintent, which allows IGA to fill in these tags with generated text that users\ncan subsequently edit to their liking. A series of automatic and crowdsourced\nevaluations confirm the quality of IGA's generated outputs, while a small-scale\nuser study demonstrates author preference for IGA over baseline methods in a\ncreative writing task. We release our dataset, code, and demo to spur further\nresearch into AI-assisted writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenlong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunatha_V/0/1/0/all/0/1\">Varun Manjunatha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rajiv Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morariu_V/0/1/0/all/0/1\">Vlad Morariu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_B/0/1/0/all/0/1\">Balaji Vasan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Alignment-Agnostic Model for Chinese Text Error Correction. (arXiv:2104.07190v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07190","description":"<p>This paper investigates how to correct Chinese text errors with types of\nmistaken, missing and redundant characters, which is common for Chinese native\nspeakers. Most existing models based on detect-correct framework can correct\nmistaken characters errors, but they cannot deal with missing or redundant\ncharacters. The reason is that lengths of sentences before and after correction\nare not the same, leading to the inconsistence between model inputs and\noutputs. Although the Seq2Seq-based or sequence tagging methods provide\nsolutions to the problem and achieved relatively good results on English\ncontext, but they do not perform well in Chinese context according to our\nexperimental results. In our work, we propose a novel detect-correct framework\nwhich is alignment-agnostic, meaning that it can handle both text aligned and\nnon-aligned occasions, and it can also serve as a cold start model when there\nare no annotated data provided. Experimental results on three datasets\ndemonstrate that our method is effective and achieves the best performance\namong existing published models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liying Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yue Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Weishun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multivalent Entailment Graphs for Question Answering. (arXiv:2104.07846v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07846","description":"<p>Drawing inferences between open-domain natural language predicates is a\nnecessity for true language understanding. There has been much progress in\nunsupervised learning of entailment graphs for this purpose. We make three\ncontributions: (1) we reinterpret the Distributional Inclusion Hypothesis to\nmodel entailment between predicates of different valencies, like DEFEAT(Biden,\nTrump) entails WIN(Biden); (2) we actualize this theory by learning\nunsupervised Multivalent Entailment Graphs of open-domain predicates; and (3)\nwe demonstrate the capabilities of these graphs on a novel question answering\ntask. We show that directional entailment is more helpful for inference than\nbidirectional similarity on questions of fine-grained semantics. We also show\nthat drawing on evidence across valencies answers more questions than by using\nonly the same valency evidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKenna_N/0/1/0/all/0/1\">Nick McKenna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1\">Mohammad Javad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vroe_S/0/1/0/all/0/1\">Sander Bijl de Vroe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Across Time: What Does RoBERTa Know and When?. (arXiv:2104.07885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07885","description":"<p>Models of language trained on very large corpora have been demonstrated\nuseful for NLP. As fixed artifacts, they have become the object of intense\nstudy, with many researchers \"probing\" the extent to which linguistic\nabstractions, factual and commonsense knowledge, and reasoning abilities they\nacquire and readily demonstrate. Building on this line of work, we consider a\nnew question: for types of knowledge a language model learns, when during\n(pre)training are they acquired? We plot probing performance across iterations,\nusing RoBERTa as a case study. Among our findings: linguistic knowledge is\nacquired fast, stably, and robustly across domains. Facts and commonsense are\nslower and more domain-sensitive. Reasoning abilities are, in general, not\nstably acquired. As new datasets, pretraining protocols, and probes emerge, we\nbelieve that probing-across-time analyses can help researchers understand the\ncomplex, intermingled learning that these models undergo and guide us toward\nmore efficient approaches that accomplish necessary learning faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leo Z. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models are Few-Shot Butlers. (arXiv:2104.07972v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07972","description":"<p>Pretrained language models demonstrate strong performance in most NLP tasks\nwhen fine-tuned on small task-specific datasets. Hence, these autoregressive\nmodels constitute ideal agents to operate in text-based environments where\nlanguage understanding and generative capabilities are essential. Nonetheless,\ncollecting expert demonstrations in such environments is a time-consuming\nendeavour. We introduce a two-stage procedure to learn from a small set of\ndemonstrations and further improve by interacting with an environment. We show\nthat language models fine-tuned with only 1.2% of the expert demonstrations and\na simple reinforcement learning algorithm achieve a 51% absolute improvement in\nsuccess rate over existing methods in the ALFWorld environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Micheli_V/0/1/0/all/0/1\">Vincent Micheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Fran&#xe7;ois Fleuret</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Passage Ranking for Diverse Multi-Answer Retrieval. (arXiv:2104.08445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08445","description":"<p>We study multi-answer retrieval, an under-explored problem that requires\nretrieving passages to cover multiple distinct answers for a given question.\nThis task requires joint modeling of retrieved passages, as models should not\nrepeatedly retrieve passages containing the same answer at the cost of missing\na different valid answer. In this paper, we introduce JPR, the first joint\npassage retrieval model for multi-answer retrieval. JPR makes use of an\nautoregressive reranker that selects a sequence of passages, each conditioned\non previously selected passages. JPR is trained to select passages that cover\nnew answers at each timestep and uses a tree-decoding algorithm to enable\nflexibility in the degree of diversity. Compared to prior approaches, JPR\nachieves significantly better answer coverage on three multi-answer datasets.\nWhen combined with downstream question answering, the improved retrieval\nenables larger answer generation models since they need to consider fewer\npassages, establishing a new state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Sewon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Interaction Network for Question Matching. (arXiv:2104.08451v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08451","description":"<p>Impressive milestones have been achieved in text matching by adopting a\ncross-attention mechanism to capture pertinent semantic connections between two\nsentence representations. However, regular cross-attention focuses on\nword-level links between the two input sequences, neglecting the importance of\ncontextual information. We propose a context-aware interaction network (COIN)\nto properly align two sequences and infer their semantic relationship.\nSpecifically, each interaction block includes (1) a context-aware\ncross-attention mechanism to effectively integrate contextual information when\naligning two sequences, and (2) a gate fusion layer to flexibly interpolate\naligned representations. We apply multiple stacked interaction blocks to\nproduce alignments at different levels and gradually refine the attention\nresults. Experiments on two question matching datasets and detailed analyses\ndemonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zuohui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages with Adversarial Examples. (arXiv:2104.08639v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08639","description":"<p>Capturing word meaning in context and distinguishing between correspondences\nand variations across languages is key to building successful multilingual and\ncross-lingual text representation models. However, existing multilingual\nevaluation datasets that evaluate lexical semantics \"in-context\" have various\nlimitations. In particular, 1) their language coverage is restricted to\nhigh-resource languages and skewed in favor of only a few language families and\nareas, 2) a design that makes the task solvable via superficial cues, which\nresults in artificially inflated (and sometimes super-human) performances of\npretrained encoders, on many target languages, which limits their usefulness\nfor model probing and diagnostics, and 3) little support for cross-lingual\nevaluation. In order to address these gaps, we present AM2iCo (Adversarial and\nMultilingual Meaning in Context), a wide-coverage cross-lingual and\nmultilingual evaluation set; it aims to faithfully assess the ability of\nstate-of-the-art (SotA) representation models to understand the identity of\nword meaning in cross-lingual contexts for 14 language pairs. We conduct a\nseries of experiments in a wide range of setups and demonstrate the challenging\nnature of AM2iCo. The results reveal that current SotA pretrained encoders\nsubstantially lag behind human performance, and the largest gaps are observed\nfor low-resource languages and languages dissimilar to English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianchu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_D/0/1/0/all/0/1\">Diana McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Answers with Entailment Trees. (arXiv:2104.08661v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08661","description":"<p>Our goal, in the context of open-domain textual question-answering (QA), is\nto explain answers by showing the line of reasoning from what is known to the\nanswer, rather than simply showing a fragment of textual evidence (a\n\"rationale'\"). If this could be done, new opportunities for understanding and\ndebugging the system's reasoning become possible. Our approach is to generate\nexplanations in the form of entailment trees, namely a tree of multipremise\nentailment steps from facts that are known, through intermediate conclusions,\nto the hypothesis of interest (namely the question + answer). To train a model\nwith this skill, we created ENTAILMENTBANK, the first dataset to contain\nmultistep entailment trees. Given a hypothesis (question + answer), we define\nthree increasingly difficult explanation tasks: generate a valid entailment\ntree given (a) all relevant sentences (b) all relevant and some irrelevant\nsentences, or (c) a corpus. We show that a strong language model can partially\nsolve these tasks, in particular when the relevant sentences are included in\nthe input (e.g., 35% of trees for (a) are perfect), and with indications of\ngeneralization to other domains. This work is significant as it provides a new\ntype of dataset (multistep entailments) and baselines, offering a new avenue\nfor the community to generate richer, more systematic explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_B/0/1/0/all/0/1\">Bhavana Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhengnan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_H/0/1/0/all/0/1\">Hannah Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pipatanangkura_L/0/1/0/all/0/1\">Leighanna Pipatanangkura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When FastText Pays Attention: Efficient Estimation of Word Representations using Constrained Positional Weighting. (arXiv:2104.09691v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09691","description":"<p>Since the seminal work of Mikolov et al. (2013a) and Bojanowski et al.\n(2017), word representations of shallow log-bilinear language models have found\ntheir way into many NLP applications. Mikolov et al. (2018) introduced a\npositional log-bilinear language model, which has characteristics of an\nattention-based language model and which has reached state-of-the-art\nperformance on the intrinsic word analogy task. However, the positional model\nhas never been evaluated on qualitative criteria or extrinsic tasks and its\nspeed is impractical.\n</p>\n<p>We outline the similarities between the attention mechanism and the\npositional model, and we propose a constrained positional model, which adapts\nthe sparse attention mechanism of Dai et al. (2018). We evaluate the positional\nand constrained positional models on three novel qualitative criteria and on\nthe extrinsic language modeling task of Botha and Blunsom (2014).\n</p>\n<p>We show that the positional and constrained positional models contain\ninterpretable information about word order and outperform the subword model of\nBojanowski et al. (2017) on language modeling. We also show that the\nconstrained positional model outperforms the positional model on language\nmodeling and is twice as fast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1\">V&#xed;t Novotn&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1\">Michal &#x160;tef&#xe1;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayetiran_E/0/1/0/all/0/1\">Eniafe Festus Ayetiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRILL: Dynamic Representations for Imbalanced Lifelong Learning. (arXiv:2105.08445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08445","description":"<p>Continual or lifelong learning has been a long-standing challenge in machine\nlearning to date, especially in natural language processing (NLP). Although\nstate-of-the-art language models such as BERT have ushered in a new era in this\nfield due to their outstanding performance in multitask learning scenarios,\nthey suffer from forgetting when being exposed to a continuous stream of data\nwith shifting data distributions. In this paper, we introduce DRILL, a novel\ncontinual learning architecture for open-domain text classification. DRILL\nleverages a biologically inspired self-organizing neural architecture to\nselectively gate latent language representations from BERT in a\ntask-incremental manner. We demonstrate in our experiments that DRILL\noutperforms current methods in a realistic scenario of imbalanced,\nnon-stationary data without prior knowledge about task boundaries. To the best\nof our knowledge, DRILL is the first of its kind to use a self-organizing\nneural architecture for open-domain lifelong learning in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahrens_K/0/1/0/all/0/1\">Kyra Ahrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abawi_F/0/1/0/all/0/1\">Fares Abawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What if This Modified That? Syntactic Interventions via Counterfactual Embeddings. (arXiv:2105.14002v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14002","description":"<p>Neural language models exhibit impressive performance on a variety of tasks,\nbut their internal reasoning may be difficult to understand. Prior art aims to\nuncover meaningful properties within model representations via probes, but it\nis unclear how faithfully such probes portray information that the models\nactually use. To overcome such limitations, we propose a technique, inspired by\ncausal analysis, for generating counterfactual embeddings within models. In\nexperiments testing our technique, we produce evidence that suggests some\nBERT-based models use a tree-distance-like representation of syntax in\ndownstream prediction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1\">Mycal Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"You made me feel this way\": Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions using Speech Data. (arXiv:2106.01526v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01526","description":"<p>How romantic partners interact with each other during a conflict influences\nhow they feel at the end of the interaction and is predictive of whether the\npartners stay together in the long term. Hence understanding the emotions of\neach partner is important. Yet current approaches that are used include\nself-reports which are burdensome and hence limit the frequency of this data\ncollection. Automatic emotion prediction could address this challenge. Insights\nfrom psychology research indicate that partners' behaviors influence each\nother's emotions in conflict interaction and hence, the behavior of both\npartners could be considered to better predict each partner's emotion. However,\nit is yet to be investigated how doing so compares to only using each partner's\nown behavior in terms of emotion prediction performance. In this work, we used\nBERT to extract linguistic features (i.e., what partners said) and openSMILE to\nextract paralinguistic features (i.e., how they said it) from a data set of 368\nGerman-speaking Swiss couples (N = 736 individuals) who were videotaped during\nan 8-minutes conflict interaction in the laboratory. Based on those features,\nwe trained machine learning models to predict if partners feel positive or\nnegative after the conflict interaction. Our results show that including the\nbehavior of the other partner improves the prediction performance. Furthermore,\nfor men, considering how their female partners spoke is most important and for\nwomen considering what their male partner said is most important in getting\nbetter prediction performance. This work is a step towards automatically\nrecognizing each partners' emotion based on the behavior of both, which would\nenable a better understanding of couples in research, therapy, and the real\nworld.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1\">George Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilpert_P/0/1/0/all/0/1\">Peter Hilpert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenmann_G/0/1/0/all/0/1\">Guy Bodenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neysari_M/0/1/0/all/0/1\">Mona Neysari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowatsch_T/0/1/0/all/0/1\">Tobias Kowatsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions. (arXiv:2106.01536v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01536","description":"<p>Many processes in psychology are complex, such as dyadic interactions between\ntwo interacting partners (e.g. patient-therapist, intimate relationship\npartners). Nevertheless, many basic questions about interactions are difficult\nto investigate because dyadic processes can be within a person and between\npartners, they are based on multimodal aspects of behavior and unfold rapidly.\nCurrent analyses are mainly based on the behavioral coding method, whereby\nhuman coders annotate behavior based on a coding schema. But coding is\nlabor-intensive, expensive, slow, focuses on few modalities. Current approaches\nin psychology use LIWC for analyzing couples' interactions. However, advances\nin natural language processing such as BERT could enable the development of\nsystems to potentially automate behavioral coding, which in turn could\nsubstantially improve psychological research. In this work, we train machine\nlearning models to automatically predict positive and negative communication\nbehavioral codes of 368 German-speaking Swiss couples during an 8-minute\nconflict interaction on a fine-grained scale (10-seconds sequences) using\nlinguistic features and paralinguistic features derived with openSMILE. Our\nresults show that both simpler TF-IDF features as well as more complex BERT\nfeatures performed better than LIWC, and that adding paralinguistic features\ndid not improve the performance. These results suggest it might be time to\nconsider modern alternatives to LIWC, the de facto linguistic features in\npsychology, for prediction tasks in couples research. This work is a further\nstep towards the automated coding of couples' behavior which could enhance\ncouple research and therapy, and be utilized for other dyadic interactions as\nwell.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biggiogera_J/0/1/0/all/0/1\">Jacopo Biggiogera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1\">George Boateng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilpert_P/0/1/0/all/0/1\">Peter Hilpert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vowels_M/0/1/0/all/0/1\">Matthew Vowels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenmann_G/0/1/0/all/0/1\">Guy Bodenmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neysari_M/0/1/0/all/0/1\">Mona Neysari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussbeck_F/0/1/0/all/0/1\">Fridtjof Nussbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowatsch_T/0/1/0/all/0/1\">Tobias Kowatsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Programming Puzzles. (arXiv:2106.05784v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05784","description":"<p>We introduce a new type of programming challenge called programming puzzles,\nas an objective and comprehensive evaluation of program synthesis, and release\nan open-source dataset of Python Programming Puzzles (P3). Each puzzle is\ndefined by a short Python program $f$, and the goal is to find an input $x$\nwhich makes $f$ output \"True\". The puzzles are objective in that each one is\nspecified entirely by the source code of its verifier $f$, so evaluating $f(x)$\nis all that is needed to test a candidate solution $x$. They do not require an\nanswer key or input/output examples, nor do they depend on natural language\nunderstanding. The dataset is comprehensive in that it spans problems of a\nrange of difficulties and domains, ranging from trivial string manipulation\nproblems that are immediately obvious to human programmers (but not necessarily\nto AI), to classic programming puzzles (e.g., Towers of Hanoi), to\ninterview/competitive-programming problems (e.g., dynamic programming), to\nlongstanding open problems in algorithms and mathematics (e.g., factoring). The\nobjective nature of P3 readily supports self-supervised bootstrapping. We\ndevelop baseline enumerative program synthesis and GPT-3 solvers that are\ncapable of solving easy puzzles -- even without access to any reference\nsolutions -- by learning from their own past solutions. Based on a small user\nstudy, we find puzzle difficulty to correlate between human programmers and the\nbaseline AI solvers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1\">Oleksandr Polozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1\">Adam Tauman Kalai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised and Unsupervised Sense Annotation via Translations. (arXiv:2106.06462v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06462","description":"<p>Acquisition of multilingual training data continues to be a challenge in word\nsense disambiguation (WSD). To address this problem, unsupervised approaches\nhave been proposed to automatically generate sense annotations for training\nsupervised WSD systems. We present three new methods for creating\nsense-annotated corpora which leverage translations, parallel bitexts, lexical\nresources, as well as contextual and synset embeddings. Our semi-supervised\nmethod applies machine translation to transfer existing sense annotations to\nother languages. Our two unsupervised methods refine sense annotations produced\nby a knowledge-based WSD system via lexical translations in a parallel corpus.\nWe obtain state-of-the-art results on standard WSD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hauer_B/0/1/0/all/0/1\">Bradley Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondrak_G/0/1/0/all/0/1\">Grzegorz Kondrak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1\">Yixing Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_A/0/1/0/all/0/1\">Arnob Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAS: Self-Augmented Strategy for Language Model Pre-training. (arXiv:2106.07176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07176","description":"<p>The core of a self-supervised learning method for pre-training language\nmodels includes the design of appropriate data augmentation and corresponding\npre-training task(s). Most data augmentations in language model pre-training\nare context-independent. The seminal contextualized augmentation recently\nproposed by the ELECTRA requires a separate generator, which leads to extra\ncomputation cost as well as the challenge in adjusting the capability of its\ngenerator relative to that of the other model component(s). We propose a\nself-augmented strategy (SAS) that uses a single forward pass through the model\nto augment the input data for model training in the next epoch. Essentially our\nstrategy eliminates a separate generator network and uses only one network to\ngenerate the data augmentation and undertake two pre-training tasks (the MLM\ntask and the RTD task) jointly, which naturally avoids the challenge in\nadjusting the generator's capability as well as reduces the computation cost.\nAdditionally, our SAS is a general strategy such that it can seamlessly\nincorporate many new techniques emerging recently or in the future, such as the\ndisentangled attention mechanism recently proposed by the DeBERTa model. Our\nexperiments show that our SAS is able to outperform the ELECTRA and other\nstate-of-the-art models in the GLUE tasks with the same or less computation\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ru He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Liangzhu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral Personalized Dialogue Generation with Dynamic Persona-Aware Fusion. (arXiv:2106.07857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07857","description":"<p>Generating personalized responses is one of the major challenges in natural\nhuman-robot interaction. Current researches in this field mainly focus on\ngenerating responses consistent with the robot's pre-assigned persona, while\nignoring the user's persona. Such responses may be inappropriate or even\noffensive, which may lead to the bad user experience. Therefore, we propose a\nbilateral personalized dialogue generation (BPDG) method with dynamic\npersona-aware fusion via multi-task transfer learning to generate responses\nconsistent with both personas. The proposed method aims to accomplish three\nlearning tasks: 1) an encoder is trained with dialogue utterances added with\ncorresponded personalized attributes and relative position (language model\ntask), 2) a dynamic persona-aware fusion module predicts the persona presence\nto adaptively fuse the contextual and bilateral personas encodings (persona\nprediction task) and 3) a decoder generates natural, fluent and personalized\nresponses (dialogue generation task). To make the generated responses more\npersonalized and bilateral persona-consistent, the Conditional Mutual\nInformation Maximum (CMIM) criterion is adopted to select the final response\nfrom the generated candidates. The experimental results show that the proposed\nmethod outperforms several state-of-the-art methods in terms of both automatic\nand manual evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a> (Member, IEEE), <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a> (Fellow, IEEE)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Modular and Joint Approaches for Speaker-Attributed ASR on Monaural Long-Form Audio. (arXiv:2107.02852v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.02852","description":"<p>Speaker-attributed automatic speech recognition (SA-ASR) is a task to\nrecognize \"who spoke what\" from multi-talker recordings. An SA-ASR system\nusually consists of multiple modules such as speech separation, speaker\ndiarization and ASR. On the other hand, considering the joint optimization, an\nend-to-end (E2E) SA-ASR model has recently been proposed with promising results\non simulation data. In this paper, we present our recent study on the\ncomparison of such modular and joint approaches towards SA-ASR on real monaural\nrecordings. We develop state-of-the-art SA-ASR systems for both modular and\njoint approaches by leveraging large-scale training data, including 75 thousand\nhours of ASR training data and the VoxCeleb corpus for speaker representation\nlearning. We also propose a new pipeline that performs the E2E SA-ASR model\nafter speaker clustering. Our evaluation on the AMI meeting corpus reveals that\nafter fine-tuning with a small real data, the joint system performs 8.9--29.9%\nbetter in accuracy compared to the best modular system while the modular system\nperforms better before such fine-tuning. We also conduct various error analyses\nto show the remaining issues for the monaural SA-ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translatotron 2: Robust direct speech-to-speech translation. (arXiv:2107.08661v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.08661","description":"<p>We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a phoneme decoder, a mel-spectrogram synthesizer, and an attention\nmodule that connects all the previous three components. Experimental results\nsuggest that Translatotron 2 outperforms the original Translatotron by a large\nmargin in terms of translation quality and predicted speech naturalness, and\ndrastically improves the robustness of the predicted speech by mitigating\nover-generation, such as babbling or long pause. We also propose a new method\nfor retaining the source speaker's voice in the translated speech. The trained\nmodel is restricted to retain the source speaker's voice, but unlike the\noriginal Translatotron, it is not able to generate speech in a different\nspeaker's voice, making the model more robust for production deployment, by\nmitigating potential misuse for creating spoofing audio artifacts. When the new\nmethod is used together with a simple concatenation-based data augmentation,\nthe trained Translatotron 2 model is able to retain each speaker's voice for\ninput with speaker turns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural News Recommendation with Collaborative News Encoding and Structural User Encoding. (arXiv:2109.00750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00750","description":"<p>Automatic news recommendation has gained much attention from the academic\ncommunity and industry. Recent studies reveal that the key to this task lies\nwithin the effective representation learning of both news and users. Existing\nworks typically encode news title and content separately while neglecting their\nsemantic interaction, which is inadequate for news text comprehension. Besides,\nprevious models encode user browsing history without leveraging the structural\ncorrelation of user browsed news to reflect user interests explicitly. In this\nwork, we propose a news recommendation framework consisting of collaborative\nnews encoding (CNE) and structural user encoding (SUE) to enhance news and user\nrepresentation learning. CNE equipped with bidirectional LSTMs encodes news\ntitle and content collaboratively with cross-selection and cross-attention\nmodules to learn semantic-interactive news representations. SUE utilizes graph\nconvolutional networks to extract cluster-structural features of user history,\nfollowed by intra-cluster and inter-cluster attention modules to learn\nhierarchical user interest representations. Experiment results on the MIND\ndataset validate the effectiveness of our model to improve the performance of\nnews recommendation. Our code is released at\nhttps://github.com/Veason-silverbullet/NNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers. (arXiv:2109.00799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00799","description":"<p>Developing automatic Math Word Problem (MWP) solvers has been an interest of\nNLP researchers since the 1960s. Over the last few years, there are a growing\nnumber of datasets and deep learning-based methods proposed for effectively\nsolving MWPs. However, most existing methods are benchmarked soly on one or two\ndatasets, varying in different configurations, which leads to a lack of\nunified, standardized, fair, and comprehensive comparison between methods. This\npaper presents MWPToolkit, the first open-source framework for solving MWPs. In\nMWPToolkit, we decompose the procedure of existing MWP solvers into multiple\ncore components and decouple their models into highly reusable modules. We also\nprovide a hyper-parameter search function to boost the performance. In total,\nwe implement and compare 17 MWP solvers on 4 widely-used single equation\ngeneration benchmarks and 2 multiple equations generation benchmarks. These\nfeatures enable our MWPToolkit to be suitable for researchers to reproduce\nadvanced baseline models and develop new MWP solvers quickly. Code and\ndocuments are available at https://github.com/LYH-YF/MWPToolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yihuai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bing Tian Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2109.02051","description":"<p>Many endeavors have sought to develop countermeasure techniques as\nenhancements on Automatic Speaker Verification (ASV) systems, in order to make\nthem more robust against spoof attacks. As evidenced by the latest ASVspoof\n2019 countermeasure challenge, models currently deployed for the task of ASV\nare, at their best, devoid of suitable degrees of generalization to unseen\nattacks. Upon further investigation of the proposed methods, it appears that a\nbroader three-tiered view of the proposed systems. comprised of the classifier,\nfeature extraction phase, and model loss function, may to some extent lessen\nthe problem. Accordingly, the present study proposes the Efficient Attention\nBranch Network (EABN) modular architecture with a combined loss function to\naddress the generalization problem...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_A/0/1/0/all/0/1\">Amir Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayounpour_M/0/1/0/all/0/1\">Mohammad Mehdi Homayounpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Language Models with Plug-and-Play Large-Scale Commonsense. (arXiv:2109.02572v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02572","description":"<p>We study how to enhance language models (LMs) with textual commonsense\nknowledge. Previous work (e.g., KnowBERT) has focused on the integrating entity\nknowledge from knowledge graphs. In order to introduce the external entity\nembeddings, they learn to jointly represent the original sentences and external\nknowledge by pre-training on a large scale corpus. However, when switching to\ntextual commonsense, unlike the light entity embeddings, the encoding of\ncommonsense descriptions is heavy. Therefore, the pre-training for learning to\njointly represent the target sentence and external commonsense descriptions is\nunaffordable. On the other hand, since pre-trained LMs for representing the\ntarget sentences alone are readily available, is it feasible to introduce\ncommonsense knowledge in downstream tasks by fine-tuning them only? In this\npaper, we propose a plug-and-play method for large-scale commonsense\nintegration without pre-training. Our method is inspired by the observation\nthat in the regular fine-tuning for downstream tasks where no external\nknowledge was introduced, the variation in the parameters of the language model\nwas minor. Our method starts from a pre-trained LM that represents the target\nsentences only (e.g., BERT). We think that the pre-training for joint\nrepresentation learning can be avoided, if the joint representation reduces the\nimpact of parameters on the starting LM. Previous methods such as KnowBERT\nproposed complex modifications to the vanilla LM to introduce external\nknowledge. Our model (Cook-Transformer, COmmOnsense Knowledge-enhanced\nTransformer), on the other hand, hardly changes the vanilla LM except adding a\nknowledge token in each Transformer layer. In a variety of experiments,\nCOOK-Transformer-based BERT/RoBERTa improve their effect without any\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1\">Wanyun Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingran Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Select One Among All? An Extensive Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding. (arXiv:2109.05696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05696","description":"<p>Knowledge Distillation (KD) is a model compression algorithm that helps\ntransfer the knowledge of a large neural network into a smaller one. Even\nthough KD has shown promise on a wide range of Natural Language Processing\n(NLP) applications, little is understood about how one KD algorithm compares to\nanother and whether these approaches can be complimentary to each other. In\nthis work, we evaluate various KD algorithms on in-domain, out-of-domain and\nadversarial testing. We propose a framework to assess the adversarial\nrobustness of multiple KD algorithms. Moreover, we introduce a new KD\nalgorithm, Combined-KD, which takes advantage of two promising approaches\n(better training scheme and more efficient data augmentation). Our extensive\nexperimental results show that Combined-KD achieves state-of-the-art results on\nthe GLUE benchmark, out-of-domain generalization, and adversarial robustness\ncompared to competitive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Aref Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pranav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation. (arXiv:2109.05778v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05778","description":"<p>Open-domain dialogue generation in natural language processing (NLP) is by\ndefault a pure-language task, which aims to satisfy human need for daily\ncommunication on open-ended topics by producing related and informative\nresponses. In this paper, we point out that hidden images, named as visual\nimpressions (VIs), can be explored from the text-only data to enhance dialogue\nunderstanding and help generate better responses. Besides, the semantic\ndependency between an dialogue post and its response is complicated, e.g., few\nword alignments and some topic transitions. Therefore, the visual impressions\nof them are not shared, and it is more reasonable to integrate the response\nvisual impressions (RVIs) into the decoder, rather than the post visual\nimpressions (PVIs). However, both the response and its RVIs are not given\ndirectly in the test process. To handle the above issues, we propose a\nframework to explicitly construct VIs based on pure-language dialogue datasets\nand utilize them for better dialogue understanding and generation.\nSpecifically, we obtain a group of images (PVIs) for each post based on a\npre-trained word-image mapping model. These PVIs are used in a co-attention\nencoder to get a post representation with both visual and textual information.\nSince the RVIs are not provided directly during testing, we design a cascade\ndecoder that consists of two sub-decoders. The first sub-decoder predicts the\ncontent words in response, and applies the word-image mapping model to get\nthose RVIs. Then, the second sub-decoder generates the response based on the\npost and RVIs. Experimental results on two open-domain dialogue datasets show\nthat our proposed approach achieves superior performance over competitive\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. (arXiv:2109.07102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07102","description":"<p>There have been many efforts to try to understand what gram-matical knowledge\n(e.g., ability to understand the part of speech of a token) is encoded in large\npre-trained language models (LM). This is done through 'Edge Probing' (EP)\ntests: simple ML models that predict the grammatical properties ofa span\n(whether it has a particular part of speech) using only the LM's token\nrepresentations. However, most NLP applications use fine-tuned LMs. Here, we\nask: if a LM is fine-tuned, does the encoding of linguistic information in it\nchange, as measured by EP tests? Conducting experiments on multiple\nquestion-answering (QA) datasets, we answer that question negatively: the EP\ntest results do not change significantly when the fine-tuned QA model performs\nwell or in adversarial situations where the model is forced to learn wrong\ncorrelations. However, a critical analysis of the EP task datasets reveals that\nEP models may rely on spurious correlations to make predictions. This indicates\neven if fine-tuning changes the encoding of such knowledge, the EP tests might\nfail to measure it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Sagnik Ray Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1\">Nikita Bhutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Modeling Aspect and Polarity for Aspect-based Sentiment Analysis in Persian Reviews. (arXiv:2109.07680v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07680","description":"<p>Identification of user's opinions from natural language text has become an\nexciting field of research due to its growing applications in the real world.\nThe research field is known as sentiment analysis and classification, where\naspect category detection (ACD) and aspect category polarity (ACP) are two\nimportant sub-tasks of aspect-based sentiment analysis. The goal in ACD is to\nspecify which aspect of the entity comes up in opinion while ACP aims to\nspecify the polarity of each aspect category from the ACD task. The previous\nworks mostly propose separate solutions for these two sub-tasks. This paper\nfocuses on the ACD and ACP sub-tasks to solve both problems simultaneously. The\nproposed method carries out multi-label classification where four different\ndeep models were employed and comparatively evaluated to examine their\nperformance. A dataset of Persian reviews was collected from CinemaTicket\nwebsite including 2200 samples from 14 categories. The developed models were\nevaluated using the collected dataset in terms of example-based and label-based\nmetrics. The results indicate the high applicability and preference of the CNN\nand GRU models in comparison to LSTM and Bi-LSTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vazan_M/0/1/0/all/0/1\">Milad Vazan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razmara_J/0/1/0/all/0/1\">Jafar Razmara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Emotion Consensus and Utilizing Unpaired Data for Empathetic Dialogue Generation. (arXiv:2109.07779v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07779","description":"<p>Researches on dialogue empathy aim to endow an agent with the capacity of\naccurate understanding and proper responding for emotions. Existing models for\nempathetic dialogue generation focus on the emotion flow in one direction, that\nis, from the context to response. We argue that conducting an empathetic\nconversation is a bidirectional process, where empathy occurs when the emotions\nof two interlocutors could converge on the same point, i.e., reaching an\nemotion consensus. Besides, we also find that the empathetic dialogue corpus is\nextremely limited, which further restricts the model performance. To address\nthe above issues, we propose a dual-generative model, Dual-Emp, to\nsimultaneously construct the emotion consensus and utilize some external\nunpaired data. Specifically, our model integrates a forward dialogue model, a\nbackward dialogue model, and a discrete latent variable representing the\nemotion consensus into a unified architecture. Then, to alleviate the\nconstraint of paired data, we extract unpaired emotional data from open-domain\nconversations and employ Dual-Emp to produce pseudo paired empathetic samples,\nwhich is more efficient and low-cost than the human annotation. Automatic and\nhuman evaluations demonstrate that our method outperforms competitive baselines\nin producing coherent and empathetic responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiao Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.08270","description":"<p>Language models (LMs) are sentence-completion engines trained on massive\ncorpora. LMs have emerged as a significant breakthrough in natural-language\nprocessing, providing capabilities that go far beyond sentence completion\nincluding question answering, summarization, and natural-language inference.\nWhile many of these capabilities have potential application to cognitive\nsystems, exploiting language models as a source of task knowledge, especially\nfor task learning, offers significant, near-term benefits. We introduce\nlanguage models and the various tasks to which they have been applied and then\nreview methods of knowledge extraction from language models. The resulting\nanalysis outlines both the challenges and opportunities for using language\nmodels as a new knowledge source for cognitive systems. It also identifies\npossible ways to improve knowledge extraction from language models using the\ncapabilities provided by cognitive systems. Central to success will be the\nability of a cognitive agent to itself learn an abstract model of the knowledge\nimplicit in the LM as well as methods to extract high-quality knowledge\neffectively and efficiently. To illustrate, we introduce a hypothetical robot\nagent and describe how language models could extend its task knowledge and\nimprove its performance and the kinds of knowledge and methods the agent can\nuse to exploit the knowledge within a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wray_R/0/1/0/all/0/1\">Robert E. Wray, III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_J/0/1/0/all/0/1\">James R. Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_J/0/1/0/all/0/1\">John E. Laird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08406","description":"<p>Despite the success of fine-tuning pretrained language encoders like BERT for\ndownstream natural language understanding (NLU) tasks, it is still poorly\nunderstood how neural networks change after fine-tuning. In this work, we use\ncentered kernel alignment (CKA), a method for comparing learned\nrepresentations, to measure the similarity of representations in task-tuned\nmodels across layers. In experiments across twelve NLU tasks, we discover a\nconsistent block diagonal structure in the similarity of representations within\nfine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of\nearlier and later layers, but not between them. The similarity of later layer\nrepresentations implies that later layers only marginally contribute to task\nperformance, and we verify in experiments that the top few layers of fine-tuned\nTransformers can be discarded without hurting performance, even with no further\ntuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haokun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}