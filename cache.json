{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Higher-Order Concurrency for Microcontrollers. (arXiv:2108.07805v1 [cs.PL])","link":"http://arxiv.org/abs/2108.07805","description":"<p>Programming microcontrollers involves low-level interfacing with hardware and\nperipherals that are concurrent and reactive. Such programs are typically\nwritten in a mixture of C and assembly using concurrent language extensions\n(like $\\texttt{FreeRTOS tasks}$ and $\\texttt{semaphores}$), resulting in\nunsafe, callback-driven, error-prone and difficult-to-maintain code.\n</p>\n<p>We address this challenge by introducing $\\texttt{SenseVM}$ - a\nbytecode-interpreted virtual machine that provides a message-passing based\n$\\textit{higher-order concurrency}$ model, originally introduced by Reppy, for\nmicrocontroller programming. This model treats synchronous operations as\nfirst-class values (called $\\texttt{Events}$) akin to the treatment of\nfirst-class functions in functional languages. This primarily allows the\nprogrammer to compose and tailor their own concurrency abstractions and,\nadditionally, abstracts away unsafe memory operations, common in shared-memory\nconcurrency models, thereby making microcontroller programs safer, composable\nand easier-to-maintain.\n</p>\n<p>Our VM is made portable via a low-level $\\textit{bridge}$ interface, built\natop the embedded OS - Zephyr. The bridge is implemented by all drivers and\ndesigned such that programming in response to a software message or a hardware\ninterrupt remains uniform and indistinguishable. In this paper we demonstrate\nthe features of our VM through an example, written in a Caml-like functional\nlanguage, running on the $\\texttt{nRF52840}$ and $\\texttt{STM32F4}$\nmicrocontrollers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Abhiroop Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krook_R/0/1/0/all/0/1\">Robert Krook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_B/0/1/0/all/0/1\">Bo Joel Svensson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheeran_M/0/1/0/all/0/1\">Mary Sheeran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021): Workshop and Shared Task Report. (arXiv:2108.07865v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07865","description":"<p>This workshop is the fourth issue of a series of workshops on automatic\nextraction of socio-political events from news, organized by the Emerging\nMarket Welfare Project, with the support of the Joint Research Centre of the\nEuropean Commission and with contributions from many other prominent scholars\nin this field. The purpose of this series of workshops is to foster research\nand development of reliable, valid, robust, and practical solutions for\nautomatically detecting descriptions of socio-political events, such as\nprotests, riots, wars and armed conflicts, in text streams. This year workshop\ncontributors make use of the state-of-the-art NLP technologies, such as Deep\nLearning, Word Embeddings and Transformers and cover a wide range of topics\nfrom text classification to news bias detection. Around 40 teams have\nregistered and 15 teams contributed to three tasks that are i) multilingual\nprotest news detection, ii) fine-grained classification of socio-political\nevents, and iii) discovering Black Lives Matter protest events. The workshop\nalso highlights two keynote and four invited talks about various aspects of\ncreating event data sets and multi- and cross-lingual machine learning in few-\nand zero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hurriyetoglu_A/0/1/0/all/0/1\">Ali H&#xfc;rriyeto&#x11f;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanev_H/0/1/0/all/0/1\">Hristo Tanev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavarella_V/0/1/0/all/0/1\">Vanni Zavarella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piskorski_J/0/1/0/all/0/1\">Jakub Piskorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeniterzi_R/0/1/0/all/0/1\">Reyyan Yeniterzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoruk_E/0/1/0/all/0/1\">Erdem Y&#xf6;r&#xfc;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualizing Variation in Text Style Transfer Datasets. (arXiv:2108.07871v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07871","description":"<p>Text style transfer involves rewriting the content of a source sentence in a\ntarget style. Despite there being a number of style tasks with available data,\nthere has been limited systematic discussion of how text style datasets relate\nto each other. This understanding, however, is likely to have implications for\nselecting multiple data sources for model training. While it is prudent to\nconsider inherent stylistic properties when determining these relationships, we\nalso must consider how a style is realized in a particular dataset. In this\npaper, we conduct several empirical analyses of existing text style datasets.\nBased on our results, we propose a categorization of stylistic and dataset\nproperties to consider when utilizing or comparing text style datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schoch_S/0/1/0/all/0/1\">Stephanie Schoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modulating Language Models with Emotions. (arXiv:2108.07886v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07886","description":"<p>Generating context-aware language that embodies diverse emotions is an\nimportant step towards building empathetic NLP systems. In this paper, we\npropose a formulation of modulated layer normalization -- a technique inspired\nby computer vision -- that allows us to use large-scale language models for\nemotional response generation. In automatic and human evaluation on the\nMojiTalk dataset, our proposed modulated layer normalization method outperforms\nprior baseline methods while maintaining diversity, fluency, and coherence. Our\nmethod also obtains competitive performance even when using only 10% of the\navailable training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1\">Soroush Vosoughi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparative study of universal quantum computing models: towards a physical unification. (arXiv:2108.07909v1 [quant-ph])","link":"http://arxiv.org/abs/2108.07909","description":"<p>Quantum computing has been a fascinating research field in quantum physics.\nRecent progresses motivate us to study in depth the universal quantum computing\nmodels (UQCM), which lie at the foundation of quantum computing and have tight\nconnections with fundamental physics. Although being developed decades ago, a\nphysically concise principle or picture to formalize and understand UQCM is\nstill lacking. This is challenging given the diversity of still-emerging\nmodels, but important to understand the difference between classical and\nquantum computing. In this work, we carried out a primary attempt to unify UQCM\nby classifying a few of them as two categories, hence making a table of models.\nWith such a table, some known models or schemes appear as hybridization or\ncombination of models, and more importantly, it leads to new schemes that have\nnot been explored yet. Our study of UQCM also leads to some insights into\nquantum algorithms. This work reveals the importance and feasibility of\nsystematic study of computing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_D/0/1/0/all/0/1\">D.-S. Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Implicit User Profiles for Personalized Retrieval-Based Chatbot. (arXiv:2108.07935v1 [cs.IR])","link":"http://arxiv.org/abs/2108.07935","description":"<p>In this paper, we explore the problem of developing personalized chatbots. A\npersonalized chatbot is designed as a digital chatting assistant for a user.\nThe key characteristic of a personalized chatbot is that it should have a\nconsistent personality with the corresponding user. It can talk the same way as\nthe user when it is delegated to respond to others' messages. We present a\nretrieval-based personalized chatbot model, namely IMPChat, to learn an\nimplicit user profile from the user's dialogue history. We argue that the\nimplicit user profile is superior to the explicit user profile regarding\naccessibility and flexibility. IMPChat aims to learn an implicit user profile\nthrough modeling user's personalized language style and personalized\npreferences separately. To learn a user's personalized language style, we\nelaborately build language models from shallow to deep using the user's\nhistorical responses; To model a user's personalized preferences, we explore\nthe conditional relations underneath each post-response pair of the user. The\npersonalized preferences are dynamic and context-aware: we assign higher\nweights to those historical pairs that are topically related to the current\nquery when aggregating the personalized preferences. We match each response\ncandidate with the personalized language style and personalized preference,\nrespectively, and fuse the two matching signals to determine the final ranking\nscore. Comprehensive experiments on two large datasets show that our method\noutperforms all baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1\">Hongjin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yueyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07971","description":"<p>In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EviDR: Evidence-Emphasized Discrete Reasoning for Reasoning Machine Reading Comprehension. (arXiv:2108.07994v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07994","description":"<p>Reasoning machine reading comprehension (R-MRC) aims to answer complex\nquestions that require discrete reasoning based on text. To support discrete\nreasoning, evidence, typically the concise textual fragments that describe\nquestion-related facts, including topic entities and attribute values, are\ncrucial clues from question to answer. However, previous end-to-end methods\nthat achieve state-of-the-art performance rarely solve the problem by paying\nenough emphasis on the modeling of evidence, missing the opportunity to further\nimprove the model's reasoning ability for R-MRC. To alleviate the above issue,\nin this paper, we propose an evidence-emphasized discrete reasoning approach\n(EviDR), in which sentence and clause level evidence is first detected based on\ndistant supervision, and then used to drive a reasoning module implemented with\na relational heterogeneous graph convolutional network to derive answers.\nExtensive experiments are conducted on DROP (discrete reasoning over\nparagraphs) dataset, and the results demonstrate the effectiveness of our\nproposed approach. In addition, qualitative analysis verifies the capability of\nthe proposed evidence-emphasized discrete reasoning for R-MRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haipeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GGP: A Graph-based Grouping Planner for Explicit Control of Long Text Generation. (arXiv:2108.07998v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07998","description":"<p>Existing data-driven methods can well handle short text generation. However,\nwhen applied to the long-text generation scenarios such as story generation or\nadvertising text generation in the commercial scenario, these methods may\ngenerate illogical and uncontrollable texts. To address these aforementioned\nissues, we propose a graph-based grouping planner(GGP) following the idea of\nfirst-plan-then-generate. Specifically, given a collection of key phrases, GGP\nfirstly encodes these phrases into an instance-level sequential representation\nand a corpus-level graph-based representation separately. With these two\nsynergic representations, we then regroup these phrases into a fine-grained\nplan, based on which we generate the final long text. We conduct our\nexperiments on three long text generation datasets and the experimental results\nreveal that GGP significantly outperforms baselines, which proves that GGP can\ncontrol the long text generation by knowing how to say and in what order.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUSTOM: Aspect-Oriented Product Summarization for E-Commerce. (arXiv:2108.08010v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08010","description":"<p>Product summarization aims to automatically generate product descriptions,\nwhich is of great commercial potential. Considering the customer preferences on\ndifferent product aspects, it would benefit from generating aspect-oriented\ncustomized summaries. However, conventional systems typically focus on\nproviding general product summaries, which may miss the opportunity to match\nproducts with customer interests. To address the problem, we propose CUSTOM,\naspect-oriented product summarization for e-commerce, which generates diverse\nand controllable summaries towards different product aspects. To support the\nstudy of CUSTOM and further this line of research, we construct two Chinese\ndatasets, i.e., SMARTPHONE and COMPUTER, including 76,279 / 49,280 short\nsummaries for 12,118 / 11,497 real-world commercial products, respectively.\nFurthermore, we introduce EXT, an extraction-enhanced generation framework for\nCUSTOM, where two famous sequence-to-sequence models are implemented in this\npaper. We conduct extensive experiments on the two proposed datasets for CUSTOM\nand show results of two famous baseline models and EXT, which indicates that\nEXT can generate diverse, high-quality, and consistent summaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahui Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Multiple Intent Detection and Slot Filling via Self-distillation. (arXiv:2108.08042v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08042","description":"<p>Intent detection and slot filling are two main tasks in natural language\nunderstanding (NLU) for identifying users' needs from their utterances. These\ntwo tasks are highly related and often trained jointly. However, most previous\nworks assume that each utterance only corresponds to one intent, ignoring the\nfact that a user utterance in many cases could include multiple intents. In\nthis paper, we propose a novel Self-Distillation Joint NLU model (SDJN) for\nmulti-intent NLU. First, we formulate multiple intent detection as a weakly\nsupervised problem and approach with multiple instance learning (MIL). Then, we\ndesign an auxiliary loop via self-distillation with three orderly arranged\ndecoders: Initial Slot Decoder, MIL Intent Decoder, and Final Slot Decoder. The\noutput of each decoder will serve as auxiliary information for the next\ndecoder. With the auxiliary knowledge provided by the MIL Intent Decoder, we\nset Final Slot Decoder as the teacher model that imparts knowledge back to\nInitial Slot Decoder to complete the loop. The auxiliary loop enables intents\nand slots to guide mutually in-depth and further boost the overall NLU\nperformance. Experimental results on two public multi-intent datasets indicate\nthat our model achieves strong performance compared to others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lisong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeDiaQA: A Question Answering Dataset on Medical Dialogues. (arXiv:2108.08074v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08074","description":"<p>In this paper, we introduce MeDiaQA, a novel question answering(QA) dataset,\nwhich constructed on real online Medical Dialogues. It contains 22k\nmultiple-choice questions annotated by human for over 11k dialogues with 120k\nutterances between patients and doctors, covering 150 specialties of diseases,\nwhich are collected from haodf.com and dxy.com. MeDiaQA is the first QA dataset\nwhere reasoning over medical dialogues, especially their quantitative contents.\nThe dataset has the potential to test the computing, reasoning and\nunderstanding ability of models across multi-turn dialogues, which is\nchallenging compared with the existing datasets. To address the challenges, we\ndesign MeDia-BERT, and it achieves 64.3% accuracy, while human performance of\n93% accuracy, which indicates that there still remains a large room for\nimprovement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suri_H/0/1/0/all/0/1\">Huqun Suri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_W/0/1/0/all/0/1\">Wenhua Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Chunsheng Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Decoding for Empathetic Response Generation. (arXiv:2108.08102v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08102","description":"<p>Understanding speaker's feelings and producing appropriate responses with\nemotion connection is a key communicative skill for empathetic dialogue\nsystems. In this paper, we propose a simple technique called Affective Decoding\nfor empathetic response generation. Our method can effectively incorporate\nemotion signals during each decoding step, and can additionally be augmented\nwith an auxiliary dual emotion encoder, which learns separate embeddings for\nthe speaker and listener given the emotion base of the dialogue. Extensive\nempirical studies show that our models are perceived to be more empathetic by\nhuman evaluations, in comparison to several strong mainstream methods for\nempathetic responding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chengkun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters. (arXiv:2108.08103v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08103","description":"<p>The open-access dissemination of pretrained language models through online\nrepositories has led to a democratization of state-of-the-art natural language\nprocessing (NLP) research. This also allows people outside of NLP to use such\nmodels and adapt them to specific use-cases. However, a certain amount of\ntechnical proficiency is still required which is an entry barrier for users who\nwant to apply these models to a certain task but lack the necessary knowledge\nor resources. In this work, we aim to overcome this gap by providing a tool\nwhich allows researchers to leverage pretrained models without writing a single\nline of code. Built upon the parameter-efficient adapter modules for transfer\nlearning, our AdapterHub Playground provides an intuitive interface, allowing\nthe usage of adapters for prediction, training and analysis of textual data for\na variety of NLP tasks. We present the tool's architecture and demonstrate its\nadvantages with prototypical use-cases, where we show that predictive\nperformance can easily be increased in a few-shot learning scenario. Finally,\nwe evaluate its usability in a user study. We provide the code and a live\ninterface at https://adapter-hub.github.io/playground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohlender_B/0/1/0/all/0/1\">Bela Bohlender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1\">Christina Viehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hane_V/0/1/0/all/0/1\">Vincent Hane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamson_Y/0/1/0/all/0/1\">Yanik Adamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khuri_J/0/1/0/all/0/1\">Jaber Khuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossmann_J/0/1/0/all/0/1\">Jonas Brossmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table Caption Generation in Scholarly Documents Leveraging Pre-trained Language Models. (arXiv:2108.08111v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08111","description":"<p>This paper addresses the problem of generating table captions for scholarly\ndocuments, which often require additional information outside the table. To\nthis end, we propose a method of retrieving relevant sentences from the paper\nbody, and feeding the table content as well as the retrieved sentences into\npre-trained language models (e.g. T5 and GPT-2) for generating table captions.\nThe contributions of this paper are: (1) discussion on the challenges in table\ncaptioning for scholarly documents; (2) development of a dataset DocBank-TB,\nwhich is publicly available; and (3) comparison of caption generation methods\nfor scholarly documents with different strategies to retrieve relevant\nsentences from the paper body. Our experimental results showed that T5 is the\nbetter generation model for this task, as it outperformed GPT-2 in BLEU and\nMETEOR implying that the generated text are clearer and more precise. Moreover,\ninputting relevant sentences matching the row header or whole table is\neffective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie H. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinden_K/0/1/0/all/0/1\">Kohei Shinden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Makoto P. Kato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RTE: A Tool for Annotating Relation Triplets from Text. (arXiv:2108.08184v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08184","description":"<p>In this work, we present a Web-based annotation tool `Relation Triplets\nExtractor' \\footnote{https://abera87.github.io/annotate/} (RTE) for annotating\nrelation triplets from the text. Relation extraction is an important task for\nextracting structured information about real-world entities from the\nunstructured text available on the Web. In relation extraction, we focus on\nbinary relation that refers to relations between two entities. Recently, many\nsupervised models are proposed to solve this task, but they mostly use noisy\ntraining data obtained using the distant supervision method. In many cases,\nevaluation of the models is also done based on a noisy test dataset. The lack\nof annotated clean dataset is a key challenge in this area of research. In this\nwork, we built a web-based tool where researchers can annotate datasets for\nrelation extraction on their own very easily. We use a server-less architecture\nfor this tool, and the entire annotation operation is processed using\nclient-side code. Thus it does not suffer from any network latency, and the\nprivacy of the user's data is also maintained. We hope that this tool will be\nbeneficial for the researchers to advance the field of relation extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1\">Ankan Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Animesh Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHAQ: Single Headed Attention with Quasi-Recurrence. (arXiv:2108.08207v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08207","description":"<p>Natural Language Processing research has recently been dominated by large\nscale transformer models. Although they achieve state of the art on many\nimportant language tasks, transformers often require expensive compute\nresources, and days spanning to weeks to train. This is feasible for\nresearchers at big tech companies and leading research universities, but not\nfor scrappy start-up founders, students, and independent researchers. Stephen\nMerity's SHA-RNN, a compact, hybrid attention-RNN model, is designed for\nconsumer-grade modeling as it requires significantly fewer parameters and less\ntraining time to reach near state of the art results. We analyze Merity's model\nhere through an exploratory model analysis over several units of the\narchitecture considering both training time and overall quality in our\nassessment. Ultimately, we combine these findings into a new architecture which\nwe call SHAQ: Single Headed Attention Quasi-recurrent Neural Network. With our\nnew architecture we achieved similar accuracy results as the SHA-RNN while\naccomplishing a 4x speed boost in training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bharwani_N/0/1/0/all/0/1\">Nashwin Bharwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushner_W/0/1/0/all/0/1\">Warren Kushner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandona_S/0/1/0/all/0/1\">Sangeet Dandona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schreiber_B/0/1/0/all/0/1\">Ben Schreiber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics. (arXiv:2108.08217v1 [cs.CV])","link":"http://arxiv.org/abs/2108.08217","description":"<p>With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yehao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yingwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSI: an Ad Text Strength Indicator using Text-to-CTR and Semantic-Ad-Similarity. (arXiv:2108.08226v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08226","description":"<p>Coming up with effective ad text is a time consuming process, and\nparticularly challenging for small businesses with limited advertising\nexperience. When an inexperienced advertiser onboards with a poorly written ad\ntext, the ad platform has the opportunity to detect low performing ad text, and\nprovide improvement suggestions. To realize this opportunity, we propose an ad\ntext strength indicator (TSI) which: (i) predicts the click-through-rate (CTR)\nfor an input ad text, (ii) fetches similar existing ads to create a\nneighborhood around the input ad, (iii) and compares the predicted CTRs in the\nneighborhood to declare whether the input ad is strong or weak. In addition, as\nsuggestions for ad text improvement, TSI shows anonymized versions of superior\nads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT\nbased text-to-CTR model trained on impressions and clicks associated with an ad\ntext. For (ii), we propose a sentence-BERT based semantic-ad-similarity model\ntrained using weak labels from ad campaign setup data. Offline experiments\ndemonstrate that our BERT based text-to-CTR model achieves a significant lift\nin CTR prediction AUC for cold start (new) advertisers compared to bag-of-words\nbased baselines. In addition, our semantic-textual-similarity model for similar\nads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same\nproduct category); this is significantly higher compared to unsupervised\nTF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising\nonline results from advertisers in the Yahoo (Verizon Media) ad platform where\na variant of TSI was implemented with sub-second end-to-end latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shaunak Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Changwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Manisha Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1\">Maxim Sviridenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Natural Language Processing for LinkedIn Search Systems. (arXiv:2108.08252v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08252","description":"<p>Many search systems work with large amounts of natural language data, e.g.,\nsearch queries, user profiles and documents, where deep learning based natural\nlanguage processing techniques (deep NLP) can be of great help. In this paper,\nwe introduce a comprehensive study of applying deep NLP techniques to five\nrepresentative tasks in search engines. Through the model design and\nexperiments of the five tasks, readers can find answers to three important\nquestions: (1) When is deep NLP helpful/not helpful in search systems? (2) How\nto address latency challenges? (3) How to ensure model robustness? This work\nbuilds on existing efforts of LinkedIn search, and is tested at scale on a\ncommercial search engine. We believe our experiences can provide useful\ninsights for the industry and research communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weiwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaowei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazi_M/0/1/0/all/0/1\">Michaeel Kazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhoutong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Huiji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake News and Phishing Detection Using a Machine Learning Trained Expert System. (arXiv:2108.08264v1 [cs.CR])","link":"http://arxiv.org/abs/2108.08264","description":"<p>Expert systems have been used to enable computers to make recommendations and\ndecisions. This paper presents the use of a machine learning trained expert\nsystem (MLES) for phishing site detection and fake news detection. Both topics\nshare a similar goal: to design a rule-fact network that allows a computer to\nmake explainable decisions like domain experts in each respective area. The\nphishing website detection study uses a MLES to detect potential phishing\nwebsites by analyzing site properties (like URL length and expiration time).\nThe fake news detection study uses a MLES rule-fact network to gauge news story\ntruthfulness based on factors such as emotion, the speaker's political\naffiliation status, and job. The two studies use different MLES network\nimplementations, which are presented and compared herein. The fake news study\nutilized a more linear design while the phishing project utilized a more\ncomplex connection structure. Both networks' inputs are based on commonly\navailable data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fitzpatrick_B/0/1/0/all/0/1\">Benjamin Fitzpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinyu &quot;Sherwin&quot; Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straub_J/0/1/0/all/0/1\">Jeremy Straub</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Do Your Biomedical Named Entity Models Generalize to Novel Entities?. (arXiv:2101.00160v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00160","description":"<p>The number of biomedical literature on new biomedical concepts is rapidly\nincreasing, which necessitates a reliable biomedical named entity recognition\n(BioNER) model for identifying new and unseen entity mentions. However, it is\nquestionable whether existing BioNER models can effectively handle them. In\nthis work, we systematically analyze the three types of recognition abilities\nof BioNER models: memorization, synonym generalization, and concept\ngeneralization. We find that although BioNER models achieve state-of-the-art\nperformance on BioNER benchmarks based on overall performance, they have\nlimitations in identifying synonyms and new biomedical concepts such as\nCOVID-19. From this observation, we conclude that existing BioNER models are\noverestimated in terms of their generalization abilities. Also, we identify\nseveral difficulties in recognizing unseen mentions in BioNER and make the\nfollowing conclusions: (1) BioNER models tend to exploit dataset biases, which\nhinders the models' abilities to generalize, and (2) several biomedical names\nhave novel morphological patterns with little name regularity such as COVID-19,\nand models fail to recognize them. We apply a current statistics-based\ndebiasing method to our problem as a simple remedy and show the improvement in\ngeneralization to unseen mentions. We hope that our analyses and findings would\nbe able to facilitate further research into the generalization capabilities of\nNER models in a domain where their reliability is of utmost importance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewoo Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniT: Multimodal Multitask Learning with a Unified Transformer. (arXiv:2102.10772v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10772","description":"<p>We propose UniT, a Unified Transformer model to simultaneously learn the most\nprominent tasks across different domains, ranging from object detection to\nnatural language understanding and multimodal reasoning. Based on the\ntransformer encoder-decoder architecture, our UniT model encodes each input\nmodality with an encoder and makes predictions on each task with a shared\ndecoder over the encoded input representations, followed by task-specific\noutput heads. The entire model is jointly trained end-to-end with losses from\neach task. Compared to previous efforts on multi-task learning with\ntransformers, we share the same model parameters across all tasks instead of\nseparately fine-tuning task-specific models and handle a much higher variety of\ntasks across different domains. In our experiments, we learn 7 tasks jointly\nover 8 datasets, achieving strong performance on each task with significantly\nfewer parameters. Our code is available in MMF at https://mmf.sh.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ronghang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amanpreet Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks. (arXiv:2105.03761v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03761","description":"<p>Recently, there has been an increasing number of efforts to introduce models\ncapable of generating natural language explanations (NLEs) for their\npredictions on vision-language (VL) tasks. Such models are appealing, because\nthey can provide human-friendly and comprehensive explanations. However, there\nis a lack of comparison between existing methods, which is due to a lack of\nre-usable evaluation frameworks and a scarcity of datasets. In this work, we\nintroduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable\nvision-language tasks that establishes a unified evaluation framework and\nprovides the first comprehensive comparison of existing approaches that\ngenerate NLEs for VL tasks. It spans four models and three datasets and both\nautomatic metrics and human evaluation are used to assess model-generated\nexplanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs\n(over 430k instances). We also propose a new model that combines UNITER, which\nlearns joint embeddings of images and text, and GPT-2, a pre-trained language\nmodel that is well-suited for text generation. It surpasses the previous state\nof the art by a large margin across all datasets. Code and data are available\nhere: https://github.com/maximek3/e-ViL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kayser_M/0/1/0/all/0/1\">Maxime Kayser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1\">Leonard Salewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emde_C/0/1/0/all/0/1\">Cornelius Emde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Virginie Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforced Generative Adversarial Network for Abstractive Text Summarization. (arXiv:2105.15176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.15176","description":"<p>Sequence-to-sequence models provide a viable new approach to generative\nsummarization, allowing models that are no longer limited to simply selecting\nand recombining sentences from the original text. However, these models have\nthree drawbacks: their grasp of the details of the original text is often\ninaccurate, and the text generated by such models often has repetitions, while\nit is difficult to handle words that are beyond the word list. In this paper,\nwe propose a new architecture that combines reinforcement learning and\nadversarial generative networks to enhance the sequence-to-sequence attention\nmodel. First, we use a hybrid pointer-generator network that copies words\ndirectly from the source text, contributing to accurate reproduction of\ninformation without sacrificing the ability of generators to generate new\nwords. Second, we use both intra-temporal and intra-decoder attention to\npenalize summarized content and thus discourage repetition. We apply our model\nto our own proposed COVID-19 paper title summarization task and achieve close\napproximations to the current model on ROUEG, while bringing better\nreadability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders. (arXiv:2106.13736v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.13736","description":"<p>While pretrained encoders have achieved success in various natural language\nunderstanding (NLU) tasks, there is a gap between these pretrained encoders and\nnatural language generation (NLG). NLG tasks are often based on the\nencoder-decoder framework, where the pretrained encoders can only benefit part\nof it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual\nencoder-decoder model that regards the decoder as the task layer of\noff-the-shelf pretrained encoders. Specifically, we augment the pretrained\nmultilingual encoder with a decoder and pre-train it in a self-supervised way.\nTo take advantage of both the large-scale monolingual data and bilingual data,\nwe adopt the span corruption and translation span corruption as the\npre-training tasks. Experiments show that DeltaLM outperforms various strong\nbaselines on both natural language generation and translation tasks, including\nmachine translation, abstractive text summarization, data-to-text, and question\ngeneration. The code and pretrained models are available at\n\\url{https://aka.ms/deltalm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05067","description":"<p>Medical imaging technologies, including computed tomography (CT) or chest\nX-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.\nSince manual report writing is usually too time-consuming, a more intelligent\nauxiliary medical system that could generate medical reports automatically and\nimmediately is urgently needed. In this article, we propose to use the medical\nvisual language BERT (Medical-VLBERT) model to identify the abnormality on the\nCOVID-19 scans and generate the medical report automatically based on the\ndetected lesion regions. To produce more accurate medical reports and minimize\nthe visual-and-linguistic differences, this model adopts an alternate learning\nstrategy with two procedures that are knowledge pretraining and transferring.\nTo be more precise, the knowledge pretraining procedure is to memorize the\nknowledge from medical texts, while the transferring procedure is to utilize\nthe acquired knowledge for professional medical sentences generations through\nobservations of medical images. In practice, for automatic medical report\ngeneration on the COVID-19 cases, we constructed a dataset of 368 medical\nfindings in Chinese and 1104 chest CT scans from The First Affiliated Hospital\nof Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun\nYat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of\nthe COVID-19 training samples, our model was first trained on the large-scale\nChinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for\nfurther fine-tuning. The experimental results showed that Medical-VLBERT\nachieved state-of-the-art performances on terminology prediction and report\ngeneration with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The\nChinese COVID-19 CT dataset is available at https://covid19ct.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06314","description":"<p>Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and align them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes two novel challenges: 1) the model needs to understand both explicit and\nimplicit mention of time information in the long document, 2) the model needs\nto perform temporal reasoning like comparison, addition, subtraction. We\nevaluate different SoTA long-document QA systems like BigBird and FiD on our\ndataset. The best-performing model FiD can only achieve 46\\% accuracy, still\nfar behind the human performance of 87\\%. We demonstrate that these models are\nstill lacking the ability to perform robust temporal understanding and\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\nempower future studies in temporal reasoning. The dataset and code are released\nin~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Findings of the LoResMT 2021 Shared Task on COVID and Sign Language for Low-resource Languages. (arXiv:2108.06598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06598","description":"<p>We present the findings of the LoResMT 2021 shared task which focuses on\nmachine translation (MT) of COVID-19 data for both low-resource spoken and sign\nlanguages. The organization of this task was conducted as part of the fourth\nworkshop on technologies for machine translation of low resource languages\n(LoResMT). Parallel corpora is presented and publicly available which includes\nthe following directions: English$\\leftrightarrow$Irish,\nEnglish$\\leftrightarrow$Marathi, and Taiwanese Sign\nlanguage$\\leftrightarrow$Traditional Chinese. Training data consists of 8112,\n20933 and 128608 segments, respectively. There are additional monolingual data\nsets for Marathi and English that consist of 21901 segments. The results\npresented here are based on entries from a total of eight teams. Three teams\nsubmitted systems for English$\\leftrightarrow$Irish while five teams submitted\nsystems for English$\\leftrightarrow$Marathi. Unfortunately, there were no\nsystems submissions for the Taiwanese Sign language$\\leftrightarrow$Traditional\nChinese task. Maximum system performance was computed using BLEU and follow as\n36.0 for English--Irish, 34.6 for Irish--English, 24.2 for English--Marathi,\nand 31.3 for Marathi--English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Atul Kr. Ojha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_J/0/1/0/all/0/1\">John Ortega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shatam_S/0/1/0/all/0/1\">Sheetal Shatam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fransen_T/0/1/0/all/0/1\">Theodorus Fransen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPMoE: Generate Multiple Pattern-Aware Outputs with Sparse Pattern Mixture of Experts. (arXiv:2108.07535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07535","description":"<p>Many generation tasks follow a one-to-many mapping relationship: each input\ncould be associated with multiple outputs. Existing methods like Conditional\nVariational AutoEncoder(CVAE) employ a latent variable to model this\none-to-many relationship. However, this high-dimensional and dense latent\nvariable lacks explainability and usually leads to poor and uncontrollable\ngenerations. In this paper, we innovatively introduce the linguistic concept of\npattern to decompose the one-to-many mapping into multiple one-to-one mappings\nand further propose a model named Sparse Pattern Mixture of Experts(SPMoE).\nEach one-to-one mapping is associated with a conditional generation pattern and\nis modeled with an expert in SPMoE. To ensure each language pattern can be\nexclusively handled with an expert model for better explainability and\ndiversity, a sparse mechanism is employed to coordinate all the expert models\nin SPMoE. We assess the performance of our SPMoE on the paraphrase generation\ntask and the experiment results prove that SPMoE can achieve a good balance in\nterms of quality, pattern-level diversity, and corpus-level diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xintong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}