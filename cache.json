{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-16T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05921","description":"<p>Detecting online hate is a complex task, and low-performing detection models\nhave harmful consequences when used for sensitive applications such as content\nmoderation. Emoji-based hate is a key emerging challenge for online hate\ndetection. We present HatemojiCheck, a test suite of 3,930 short-form\nstatements that allows us to evaluate how detection models perform on hateful\nlanguage expressed with emoji. Using the test suite, we expose weaknesses in\nexisting hate detection models. To address these weaknesses, we create the\nHatemojiTrain dataset using an innovative human-and-model-in-the-loop approach.\nModels trained on these 5,912 adversarial examples perform substantially better\nat detecting emoji-based hate, while retaining strong performance on text-only\nhate. Both HatemojiCheck and HatemojiTrain are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertram Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the HASOC track at FIRE 2020: Hate Speech and Offensive Content Identification in Indo-European Languages. (arXiv:2108.05927v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05927","description":"<p>With the growth of social media, the spread of hate speech is also increasing\nrapidly. Social media are widely used in many countries. Also Hate Speech is\nspreading in these countries. This brings a need for multilingual Hate Speech\ndetection algorithms. Much research in this area is dedicated to English at the\nmoment. The HASOC track intends to provide a platform to develop and optimize\nHate Speech detection algorithms for Hindi, German and English. The dataset is\ncollected from a Twitter archive and pre-classified by a machine learning\nsystem. HASOC has two sub-task for all three languages: task A is a binary\nclassification problem (Hate and Not Offensive) while task B is a fine-grained\nclassification problem for three classes (HATE) Hate speech, OFFENSIVE and\nPROFANITY. Overall, 252 runs were submitted by 40 teams. The performance of the\nbest classification algorithms for task A are F1 measures of 0.51, 0.53 and\n0.52 for English, Hindi, and German, respectively. For task B, the best\nclassification algorithms achieved F1 measures of 0.26, 0.33 and 0.29 for\nEnglish, Hindi, and German, respectively. This article presents the tasks and\nthe data development as well as the results. The best performing algorithms\nwere mainly variants of the transformer architecture BERT. However, also other\nsystems were applied with good success\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandla_T/0/1/0/all/0/1\">Thomas Mandla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modha_S/0/1/0/all/0/1\">Sandip Modha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Amit Kumar Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandini_D/0/1/0/all/0/1\">Durgesh Nandini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Daksh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schafer_J/0/1/0/all/0/1\">Johannes Sch&#xe4;fer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GQE-PRF: Generative Query Expansion with Pseudo-Relevance Feedback. (arXiv:2108.06010v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06010","description":"<p>Query expansion with pseudo-relevance feedback (PRF) is a powerful approach\nto enhance the effectiveness in information retrieval. Recently, with the rapid\nadvance of deep learning techniques, neural text generation has achieved\npromising success in many natural language tasks. To leverage the strength of\ntext generation for information retrieval, in this article, we propose a novel\napproach which effectively integrates text generation models into PRF-based\nquery expansion. In particular, our approach generates augmented query terms\nvia neural text generation models conditioned on both the initial query and\npseudo-relevance feedback. Moreover, in order to train the generative model, we\nadopt the conditional generative adversarial nets (CGANs) and propose the\nPRF-CGAN method in which both the generator and the discriminator are\nconditioned on the pseudo-relevance feedback. We evaluate the performance of\nour approach on information retrieval tasks using two benchmark datasets. The\nexperimental results show that our approach achieves comparable performance or\noutperforms traditional query expansion methods on both the retrieval and\nreranking tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meizhen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPRM: A Topic-based Personalized Ranking Model for Web Search. (arXiv:2108.06014v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06014","description":"<p>Ranking models have achieved promising results, but it remains challenging to\ndesign personalized ranking systems to leverage user profiles and semantic\nrepresentations between queries and documents. In this paper, we propose a\ntopic-based personalized ranking model (TPRM) that integrates user topical\nprofile with pretrained contextualized term representations to tailor the\ngeneral document ranking list. Experiments on the real-world dataset\ndemonstrate that TPRM outperforms state-of-the-art ad-hoc ranking models and\npersonalized ranking models significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minghui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval. (arXiv:2108.06027v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06027","description":"<p>Recently, dense passage retrieval has become a mainstream approach to finding\nrelevant information in various natural language processing tasks. A number of\nstudies have been devoted to improving the widely adopted dual-encoder\narchitecture. However, most of the previous studies only consider query-centric\nsimilarity relation when learning the dual-encoder retriever. In order to\ncapture more comprehensive similarity relations, we propose a novel approach\nthat leverages both query-centric and PAssage-centric sImilarity Relations\n(called PAIR) for dense passage retrieval. To implement our approach, we make\nthree major technical contributions by introducing formal formulations of the\ntwo kinds of similarity relations, generating high-quality pseudo labeled data\nvia knowledge distillation, and designing an effective two-stage training\nprocedure that incorporates passage-centric similarity relation constraint.\nExtensive experiments show that our approach significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shangwen Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">QiaoQiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Reasoning with Relational Directed Graph. (arXiv:2108.06040v1 [cs.AI])","link":"http://arxiv.org/abs/2108.06040","description":"<p>Reasoning on the knowledge graph (KG) aims to infer new facts from existing\nones. Methods based on the relational path in the literature have shown strong,\ninterpretable, and inductive reasoning ability. However, the paths are\nnaturally limited in capturing complex topology in KG. In this paper, we\nintroduce a novel relational structure, i.e., relational directed graph\n(r-digraph), which is composed of overlapped relational paths, to capture the\nKG's structural information. Since the digraph exhibits more complex structure\nthan paths, constructing and learning on the r-digraph are challenging. Here,\nwe propose a variant of graph neural network, i.e., RED-GNN, to address the\nabove challenges by learning the RElational Digraph with a variant of GNN.\nSpecifically, RED-GNN recursively encodes multiple r-digraphs with shared edges\nand selects the strongly correlated edges through query-dependent attention\nweights. We demonstrate the significant gains on reasoning both KG with unseen\nentities and incompletion KG benchmarks by the r-digraph, the efficiency of\nRED-GNN, and the interpretable dependencies learned on the r-digraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Task Transfer for Invoice Extraction via Class-aware QA Ensemble. (arXiv:2108.06069v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06069","description":"<p>We present VESPA, an intentionally simple yet novel zero-shot system for\nlayout, locale, and domain agnostic document extraction. In spite of the\navailability of large corpora of documents, the lack of labeled and validated\ndatasets makes it a challenge to discriminatively train document extraction\nmodels for enterprises. We show that this problem can be addressed by simply\ntransferring the information extraction (IE) task to a natural language\nQuestion-Answering (QA) task without engineering task-specific architectures.\nWe demonstrate the effectiveness of our system by evaluating on a closed corpus\nof real-world retail and tax invoices with multiple complex layouts, domains,\nand geographies. The empirical evaluation shows that our system outperforms 4\nprominent commercial invoice solutions that use discriminatively trained models\nwith architectures specifically crafted for invoice extraction. We extracted 6\nfields with zero upfront human annotation or training with an Avg. F1 of 87.50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_P/0/1/0/all/0/1\">Prithiviraj Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhkaran Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achankuju_J/0/1/0/all/0/1\">Josemon Achankuju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect Sentiment Triplet Extraction Using Reinforcement Learning. (arXiv:2108.06107v1 [cs.CL])","link":"http://arxiv.org/abs/2108.06107","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting triplets\nof aspect terms, their associated sentiments, and the opinion terms that\nprovide evidence for the expressed sentiments. Previous approaches to ASTE\nusually simultaneously extract all three components or first identify the\naspect and opinion terms, then pair them up to predict their sentiment\npolarities. In this work, we present a novel paradigm, ASTE-RL, by regarding\nthe aspect and opinion terms as arguments of the expressed sentiment in a\nhierarchical reinforcement learning (RL) framework. We first focus on\nsentiments expressed in a sentence, then identify the target aspect and opinion\nterms for that sentiment. This takes into account the mutual interactions among\nthe triplet's components while improving exploration and sample efficiency.\nFurthermore, this hierarchical RLsetup enables us to deal with multiple and\noverlapping triplets. In our experiments, we evaluate our model on existing\ndatasets from laptop and restaurant domains and show that it achieves\nstate-of-the-art performance. The implementation of this work is publicly\navailable at https://github.com/declare-lab/ASTE-RL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jian_S/0/1/0/all/0/1\">Samson Yu Bai Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1\">Tapas Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v1 [cs.CL])","link":"http://arxiv.org/abs/2108.06130","description":"<p>The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutsch_J/0/1/0/all/0/1\">Julian Gutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietsch_M/0/1/0/all/0/1\">Malte Pietsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparison of Latent Semantic Analysis and Correspondence Analysis for Text Mining. (arXiv:2108.06197v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06197","description":"<p>Both latent semantic analysis (LSA) and correspondence analysis (CA) use a\nsingular value decomposition (SVD) for dimensionality reduction. In this\narticle, LSA and CA are compared from a theoretical point of view and applied\nin both a toy example and an authorship attribution example. In text mining\ninterest goes out to the relationships among documents and terms: for example,\nwhat terms are more often used in what documents. However, the LSA solution\ndisplays a mix of marginal effects and these relationships. It appears that CA\nhas more attractive properties than LSA. One such property is that, in CA, the\neffect of the margins is effectively eliminated, so that the CA solution is\noptimally suited to focus on the relationships among documents and terms. Three\nmechanisms are distinguished to weight documents and terms, and a unifying\nframework is proposed that includes these three mechanisms and includes both CA\nand LSA as special cases. In the authorship attribution example, the national\nanthem of the Netherlands, the application of the discussed methods is\nillustrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qianqian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessen_D/0/1/0/all/0/1\">David J. Hessen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heijden_P/0/1/0/all/0/1\">Peter G. M. van der Heijden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Hate in Online Memes. (arXiv:2108.06207v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06207","description":"<p>Hateful and offensive content detection has been extensively explored in a\nsingle modality such as text. However, such toxic information could also be\ncommunicated via multimodal content such as online memes. Therefore, detecting\nmultimodal hateful content has recently garnered much attention in academic and\nindustry research communities. This paper aims to contribute to this emerging\nresearch topic by proposing DisMultiHate, which is a novel framework that\nperformed the classification of multimodal hateful content. Specifically,\nDisMultiHate is designed to disentangle target entities in multimodal memes to\nimprove hateful content classification and explainability. We conduct extensive\nexperiments on two publicly available hateful and offensive memes datasets. Our\nexperiment results show that DisMultiHate is able to outperform\nstate-of-the-art unimodal and multimodal baselines in the hateful meme\nclassification task. Empirical case studies were also conducted to demonstrate\nDisMultiHate's ability to disentangle target entities in memes and ultimately\nshowcase DisMultiHate's explainability of the multimodal hateful content\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Ziqing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_R/0/1/0/all/0/1\">Roy Ka-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1\">Wen-Haw Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis of the COVID-related r/Depression Posts. (arXiv:2108.06215v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06215","description":"<p>Reddit.com is a popular social media platform among young people. Reddit\nusers share their stories to seek support from other users, especially during\nthe Covid-19 pandemic. Messages posted on Reddit and their content have\nprovided researchers with opportunity to analyze public concerns. In this\nstudy, we analyzed sentiments of COVID-related messages posted on r/Depression.\nOur study poses the following questions: a) What are the common topics that the\nReddit users discuss? b) Can we use these topics to classify sentiments of the\nposts? c) What matters concern people more during the pandemic?\n</p>\n<p>Key Words: Sentiment Classification, Depression, COVID-19, Reddit, LDA, BERT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1\">Marina Sokolova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAIR: Framework for mining relationships between research articles, strategies, and regulations in the field of explainable artificial intelligence. (arXiv:2108.06216v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06216","description":"<p>The growing number of AI applications, also for high-stake decisions,\nincreases the interest in Explainable and Interpretable Machine Learning\n(XI-ML). This trend can be seen both in the increasing number of regulations\nand strategies for developing trustworthy AI and the growing number of\nscientific papers dedicated to this topic. To ensure the sustainable\ndevelopment of AI, it is essential to understand the dynamics of the impact of\nregulation on research papers as well as the impact of scientific discourse on\nAI-related policies. This paper introduces a novel framework for joint analysis\nof AI-related policy documents and eXplainable Artificial Intelligence (XAI)\nresearch papers. The collected documents are enriched with metadata and\ninterconnections, using various NLP methods combined with a methodology\ninspired by Institutional Grammar. Based on the information extracted from\ncollected documents, we showcase a series of analyses that help understand\ninteractions, similarities, and differences between documents at different\nstages of institutionalization. To the best of our knowledge, this is the first\nwork to use automatic language analysis tools to understand the dynamics\nbetween XI-ML methods and regulations. We believe that such a system\ncontributes to better cooperation between XAI researchers and AI policymakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gizinski_S/0/1/0/all/0/1\">Stanis&#x142;aw Gizinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuzba_M/0/1/0/all/0/1\">Micha&#x142; Kuzba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pielinski_B/0/1/0/all/0/1\">Bartosz Pielinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sienkiewicz_J/0/1/0/all/0/1\">Julian Sienkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laniewski_S/0/1/0/all/0/1\">Stanis&#x142;aw &#x141;aniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biecek_P/0/1/0/all/0/1\">Przemys&#x142;aw Biecek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Structured Dynamic Sparse Pre-Training of BERT. (arXiv:2108.06277v1 [cs.CL])","link":"http://arxiv.org/abs/2108.06277","description":"<p>Identifying algorithms for computational efficient unsupervised training of\nlarge language models is an important and active area of research. In this\nwork, we develop and study a straightforward, dynamic always-sparse\npre-training approach for BERT language modeling task, which leverages periodic\ncompression steps based on magnitude pruning followed by random parameter\nre-allocation. This approach enables us to achieve Pareto improvements in terms\nof the number of floating-point operations (FLOPs) over statically sparse and\ndense models across a broad spectrum of network sizes. Furthermore, we\ndemonstrate that training remains FLOP-efficient when using coarse-grained\nblock sparsity, making it particularly promising for efficient execution on\nmodern hardware accelerators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dietrich_A/0/1/0/all/0/1\">Anastasia Dietrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gressmann_F/0/1/0/all/0/1\">Frithjof Gressmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_D/0/1/0/all/0/1\">Douglas Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chelombiev_I/0/1/0/all/0/1\">Ivan Chelombiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Justus_D/0/1/0/all/0/1\">Daniel Justus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luschi_C/0/1/0/all/0/1\">Carlo Luschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Single and Multiple Representations in Dense Passage Retrieval. (arXiv:2108.06279v1 [cs.IR])","link":"http://arxiv.org/abs/2108.06279","description":"<p>The advent of contextualised language models has brought gains in search\neffectiveness, not just when applied for re-ranking the output of classical\nweighting models such as BM25, but also when used directly for passage indexing\nand retrieval, a technique which is called dense retrieval. In the existing\nliterature in neural ranking, two dense retrieval families have become\napparent: single representation, where entire passages are represented by a\nsingle embedding (usually BERT's [CLS] token, as exemplified by the recent ANCE\napproach), or multiple representations, where each token in a passage is\nrepresented by its own embedding (as exemplified by the recent ColBERT\napproach). These two families have not been directly compared. However, because\nof the likely importance of dense retrieval moving forward, a clear\nunderstanding of their advantages and disadvantages is paramount. To this end,\nthis paper contributes a direct study on their comparative effectiveness,\nnoting situations where each method under/over performs w.r.t. each other, and\nw.r.t. a BM25 baseline. We observe that, while ANCE is more efficient than\nColBERT in terms of response time and memory usage, multiple representations\nare statistically more effective than the single representations for MAP and\nMRR@10. We also show that multiple representations obtain better improvements\nthan single representations for queries that are the hardest for BM25, as well\nas for definitional queries, and those with complex information needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diachronic Analysis of German Parliamentary Proceedings: Ideological Shifts through the Lens of Political Biases. (arXiv:2108.06295v1 [cs.CL])","link":"http://arxiv.org/abs/2108.06295","description":"<p>We analyze bias in historical corpora as encoded in diachronic distributional\nsemantic models by focusing on two specific forms of bias, namely a political\n(i.e., anti-communism) and racist (i.e., antisemitism) one. For this, we use a\nnew corpus of German parliamentary proceedings, DeuPARL, spanning the period\n1867--2020. We complement this analysis of historical biases in diachronic word\nembeddings with a novel measure of bias on the basis of term co-occurrences and\ngraph-based label propagation. The results of our bias measurements align with\ncommonly perceived historical trends of antisemitic and anti-communist biases\nin German politics in different time periods, thus indicating the viability of\nanalyzing historical bias trends using semantic spaces induced from historical\ncorpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walter_T/0/1/0/all/0/1\">Tobias Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirschner_C/0/1/0/all/0/1\">Celina Kirschner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeetSum: Transforming Meeting Transcript Summarization using Transformers!. (arXiv:2108.06310v1 [cs.CL])","link":"http://arxiv.org/abs/2108.06310","description":"<p>Creating abstractive summaries from meeting transcripts has proven to be\nchallenging due to the limited amount of labeled data available for training\nneural network models. Moreover, Transformer-based architectures have proven to\nbeat state-of-the-art models in summarizing news data. In this paper, we\nutilize a Transformer-based Pointer Generator Network to generate abstract\nsummaries for meeting transcripts. This model uses 2 LSTMs as an encoder and a\ndecoder, a Pointer network which copies words from the inputted text, and a\nGenerator network to produce out-of-vocabulary words (hence making the summary\nabstractive). Moreover, a coverage mechanism is used to avoid repetition of\nwords in the generated summary. First, we show that training the model on a\nnews summary dataset and using zero-shot learning to test it on the meeting\ndataset proves to produce better results than training it on the AMI meeting\ndataset. Second, we show that training this model first on out-of-domain data,\nsuch as the CNN-Dailymail dataset, followed by a fine-tuning stage on the AMI\nmeeting dataset is able to improve the performance of the model significantly.\nWe test our model on a testing set from the AMI dataset and report the ROUGE-2\nscore of the generated summary to compare with previous literature. We also\nreport the Factual score of our summaries since it is a better benchmark for\nabstractive summaries since the ROUGE-2 score is limited to measuring\nword-overlaps. We show that our improved model is able to improve on previous\nmodels by at least 5 ROUGE-2 scores, which is a substantial improvement. Also,\na qualitative analysis of the summaries generated by our model shows that these\nsummaries and human-readable and indeed capture most of the important\ninformation from the transcripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadri_N/0/1/0/all/0/1\">Nima Sadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bihan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v1 [cs.CL])","link":"http://arxiv.org/abs/2108.06314","description":"<p>Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and align them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes two novel challenges: 1) the model needs to understand both explicit and\nimplicit mention of time information in the long document, 2) the model needs\nto perform temporal reasoning like comparison, addition, subtraction. We\nevaluate different SoTA long-document QA systems like BigBird and FiD on our\ndataset. The best-performing model FiD can only achieve 46\\% accuracy, still\nfar behind the human performance of 87\\%. We demonstrate that these models are\nstill lacking the ability to perform robust temporal understanding and\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\nempower future studies in temporal reasoning. The dataset and code are released\nin~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Adaptation of Open-Domain Generative Chatbots. (arXiv:2108.06329v1 [cs.CL])","link":"http://arxiv.org/abs/2108.06329","description":"<p>Recent work building open-domain chatbots has demonstrated that increasing\nmodel size improves performance. On the other hand, latency and connectivity\nconsiderations dictate the move of digital assistants on the device. Giving a\ndigital assistant like Siri, Alexa, or Google Assistant the ability to discuss\njust about anything leads to the need for reducing the chatbot model size such\nthat it fits on the user's device. We demonstrate that low parameter models can\nsimultaneously retain their general knowledge conversational abilities while\nimproving in a specific domain. Additionally, we propose a generic framework\nthat accounts for variety in question types, tracks reference throughout\nmulti-turn conversations, and removes inconsistent and potentially toxic\nresponses. Our framework seamlessly transitions between chatting and performing\ntransactional tasks, which will ultimately make interactions with digital\nassistants more human-like. We evaluate our framework on 1 internal and 4\npublic benchmark datasets using both automatic (Perplexity) and human (SSA -\nSensibleness and Specificity Average) evaluation metrics and establish\ncomparable performance while reducing model parameters by 90%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gerhard_Young_G/0/1/0/all/0/1\">Greyson Gerhard-Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantha_R/0/1/0/all/0/1\">Raviteja Anantha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chappidi_S/0/1/0/all/0/1\">Srinivas Chappidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmeister_B/0/1/0/all/0/1\">Bj&#xf6;rn Hoffmeister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning. (arXiv:2108.06332v1 [cs.CL])","link":"http://arxiv.org/abs/2108.06332","description":"<p>Most previous methods for text data augmentation are limited to simple tasks\nand weak baselines. We explore data augmentation on hard tasks (i.e., few-shot\nnatural language understanding) and strong baselines (i.e., pretrained models\nwith over one billion parameters). Under this setting, we reproduced a large\nnumber of previous augmentation methods and found that these methods bring\nmarginal gains at best and sometimes degrade the performance much. To address\nthis challenge, we propose a novel data augmentation method FlipDA that jointly\nuses a generative model and a classifier to generate label-flipped data.\nCentral to the idea of FlipDA is the discovery that generating label-flipped\ndata is more crucial to the performance than generating label-preserved data.\nExperiments show that FlipDA achieves a good tradeoff between effectiveness and\nrobustness---it substantially improves many tasks while not negatively\naffecting the others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Relationships Between Scientific Documents. (arXiv:2002.00317v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2002.00317","description":"<p>We address the task of explaining relationships between two scientific\ndocuments using natural language text. This task requires modeling the complex\ncontent of long technical documents, deducing a relationship between these\ndocuments, and expressing the details of that relationship in text. In addition\nto the theoretical interest of this task, successful solutions can help improve\nresearcher efficiency in search and review. In this paper we establish a\ndataset of 622K examples from 154K documents. We pretrain a large language\nmodel to serve as the foundation for autoregressive approaches to the task. We\nexplore the impact of taking different views on the two documents, including\nthe use of dense representations extracted with scientific IE systems. We\nprovide extensive automatic and human evaluations which show the promise of\nsuch models, but make clear challenges for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Kelvin Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cachola_I/0/1/0/all/0/1\">Isabel Cachola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplifying Paragraph-level Question Generation via Transformer Language Models. (arXiv:2005.01107v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.01107","description":"<p>Question generation (QG) is a natural language generation task where a model\nis trained to ask questions corresponding to some input text. Most recent\napproaches frame QG as a sequence-to-sequence problem and rely on additional\nfeatures and mechanisms to increase performance; however, these often increase\nmodel complexity, and can rely on auxiliary data unavailable in practical use.\nA single Transformer-based unidirectional language model leveraging transfer\nlearning can be used to produce high quality questions while disposing of\nadditional task-specific complexity. Our QG model, finetuned from GPT-2 Small,\noutperforms several paragraph-level QG baselines on the SQuAD dataset by 0.95\nMETEOR points. Human evaluators rated questions as easy to answer, relevant to\ntheir context paragraph, and corresponding well to natural human speech. Also\nintroduced is a new set of baseline scores on the RACE dataset, which has not\npreviously been used for QG tasks. Further experimentation with varying model\ncapacities and datasets with non-identification type questions is recommended\nin order to further verify the robustness of pretrained Transformer-based LMs\nas question generators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopez_L/0/1/0/all/0/1\">Luis Enrico Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_D/0/1/0/all/0/1\">Diane Kathryn Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting News Article Structure for Automatic Corpus Generation of Entailment Datasets. (arXiv:2010.11574v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11574","description":"<p>Transformers represent the state-of-the-art in Natural Language Processing\n(NLP) in recent years, proving effective even in tasks done in low-resource\nlanguages. While pretrained transformers for these languages can be made, it is\nchallenging to measure their true performance and capacity due to the lack of\nhard benchmark datasets, as well as the difficulty and cost of producing them.\nIn this paper, we present three contributions: First, we propose a methodology\nfor automatically producing Natural Language Inference (NLI) benchmark datasets\nfor low-resource languages using published news articles. Through this, we\ncreate and release NewsPH-NLI, the first sentence entailment benchmark dataset\nin the low-resource Filipino language. Second, we produce new pretrained\ntransformers based on the ELECTRA technique to further alleviate the resource\nscarcity in Filipino, benchmarking them on our dataset against other\ncommonly-used transfer learning techniques. Lastly, we perform analyses on\ntransfer learning techniques to shed light on their true performance when\noperating in low-data domains through the use of degradation tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1\">Jan Christian Blaise Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resabal_J/0/1/0/all/0/1\">Jose Kristian Resabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">James Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velasco_D/0/1/0/all/0/1\">Dan John Velasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Charibeth Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Smart Chatbot Prototype for Patient Monitoring. (arXiv:2103.06816v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.06816","description":"<p>Many COVID-19 patients developed prolonged symptoms after the infection,\nincluding fatigue, delirium, and headache. The long-term health impact of these\nconditions is still not clear. It is necessary to develop a way to follow up\nwith these patients for monitoring their health status to support timely\nintervention and treatment. In the lack of sufficient human resources to follow\nup with patients, we propose a novel smart chatbot solution backed with machine\nlearning to collect information (i.e., generating digital diary) in a\npersonalized manner. In this article, we describe the design framework and\ncomponents of our prototype.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Hannah Lei</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiqi Lu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ji_A/0/1/0/all/0/1\">Alan Ji</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Bertram_E/0/1/0/all/0/1\">Emmett Bertram</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Paul Gao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Barman_A/0/1/0/all/0/1\">Arko Barman</a> (1) ((1) Rice University, Houston, United States, (2) The University of Texas Health Science Center at Houston, United States)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. (arXiv:2106.00245v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00245","description":"<p>Benefiting from large-scale pre-training, we have witnessed significant\nperformance boost on the popular Visual Question Answering (VQA) task. Despite\nrapid progress, it remains unclear whether these state-of-the-art (SOTA) models\nare robust when encountering examples in the wild. To study this, we introduce\nAdversarial VQA, a new large-scale VQA benchmark, collected iteratively via an\nadversarial human-and-model-in-the-loop procedure. Through this new benchmark,\nwe discover several interesting findings. (i) Surprisingly, we find that during\ndataset collection, non-expert annotators can easily attack SOTA VQA models\nsuccessfully. (ii) Both large-scale pre-trained models and adversarial training\nmethods achieve far worse performance on the new benchmark than over standard\nVQA v2 dataset, revealing the fragility of these models while demonstrating the\neffectiveness of our adversarial dataset. (iii) When used for data\naugmentation, our dataset can effectively boost model performance on other\nrobust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light\non robustness study in the community and serve as a valuable benchmark for\nfuture work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spanish Language Models. (arXiv:2107.07253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07253","description":"<p>This paper presents the Spanish RoBERTa-base and RoBERTa-large models, as\nwell as the corresponding performance evaluations. Both models were pre-trained\nusing the largest Spanish corpus known to date, with a total of 570GB of clean\nand deduplicated text processed for this work, compiled from the web crawlings\nperformed by the National Library of Spain from 2009 to 2019. We extended the\ncurrent evaluation datasets with an extractive Question Answering dataset and\nour models outperform the existing Spanish models across tasks and settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silveira_Ocampo_J/0/1/0/all/0/1\">Joaqu&#xed;n Silveira-Ocampo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armentano_Oller_C/0/1/0/all/0/1\">Carme Armentano-Oller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Penagos_C/0/1/0/all/0/1\">Carlos Rodriguez-Penagos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04840","description":"<p>Natural Language Processing (NLP) models have become increasingly more\ncomplex and widespread. With recent developments in neural networks, a growing\nconcern is whether it is responsible to use these models. Concerns such as\nsafety and ethics can be partially addressed by providing explanations.\nFurthermore, when models do fail, providing explanations is paramount for\naccountability purposes. To this end, interpretability serves to provide these\nexplanations in terms that are understandable to humans. Central to what is\nunderstandable is how explanations are communicated. Therefore, this survey\nprovides a categorization of how recent interpretability methods communicate\nexplanations and discusses the methods in depth. Furthermore, the survey\nfocuses on post-hoc methods, which provide explanations after a model is\nlearned and generally model-agnostic. A common concern for this class of\nmethods is whether they accurately reflect the model. Hence, how these post-hoc\nmethods are evaluated is discussed throughout the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Diverse Descriptions from Semantic Graphs. (arXiv:2108.05659v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05659","description":"<p>Text generation from semantic graphs is traditionally performed with\ndeterministic methods, which generate a unique description given an input\ngraph. However, the generation problem admits a range of acceptable textual\noutputs, exhibiting lexical, syntactic and semantic variation. To address this\ndisconnect, we present two main contributions. First, we propose a stochastic\ngraph-to-text model, incorporating a latent variable in an encoder-decoder\nmodel, and its use in an ensemble. Second, to assess the diversity of the\ngenerated sentences, we propose a new automatic evaluation metric which jointly\nevaluates output diversity and quality in a multi-reference setting. We\nevaluate the models on WebNLG datasets in English and Russian, and show an\nensemble of stochastic models produces diverse sets of generated sentences,\nwhile retaining similar quality to state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiuzhou Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_D/0/1/0/all/0/1\">Daniel Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-15T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}