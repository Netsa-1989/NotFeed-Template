{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"End-to-End Open Vocabulary Keyword Search. (arXiv:2108.10357v1 [eess.AS])","link":"http://arxiv.org/abs/2108.10357","description":"<p>Recently, neural approaches to spoken content retrieval have become popular.\nHowever, they tend to be restricted in their vocabulary or in their ability to\ndeal with imbalanced test settings. These restrictions limit their\napplicability in keyword search, where the set of queries is not known\nbeforehand, and where the system should return not just whether an utterance\ncontains a query but the exact location of any such occurrences. In this work,\nwe propose a model directly optimized for keyword search. The model takes a\nquery and an utterance as input and returns a sequence of probabilities for\neach frame of the utterance of the query having occurred in that frame.\nExperiments show that the proposed model not only outperforms similar\nend-to-end models on a task where the ratio of positive and negative trials is\nartificially balanced, but it is also able to deal with the far more\nchallenging task of keyword search with its inherent imbalance. Furthermore,\nusing our system to rescore the outputs an LVCSR-based keyword search system\nleads to significant improvements on the latter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yusuf_B/0/1/0/all/0/1\">Bolaji Yusuf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gok_A/0/1/0/all/0/1\">Alican Gok</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gundogdu_B/0/1/0/all/0/1\">Batuhan Gundogdu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saraclar_M/0/1/0/all/0/1\">Murat Saraclar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Covert Gender Bias: A Case Study in Turkish and English Machine Translation Models. (arXiv:2108.10379v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10379","description":"<p>As Machine Translation (MT) has become increasingly more powerful,\naccessible, and widespread, the potential for the perpetuation of bias has\ngrown alongside its advances. While overt indicators of bias have been studied\nin machine translation, we argue that covert biases expose a problem that is\nfurther entrenched. Through the use of the gender-neutral language Turkish and\nthe gendered language English, we examine cases of both overt and covert gender\nbias in MT models. Specifically, we introduce a method to investigate\nasymmetrical gender markings. We also assess bias in the attribution of\npersonhood and examine occupational and personality stereotypes through overt\nbias indicators in MT models. Our work explores a deeper layer of bias in MT\nmodels and demonstrates the continued need for language-specific,\ninterdisciplinary methodology in MT model development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ciora_C/0/1/0/all/0/1\">Chloe Ciora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iren_N/0/1/0/all/0/1\">Nur Iren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent multiple shared layers in Depth for Neural Machine Translation. (arXiv:2108.10417v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10417","description":"<p>Learning deeper models is usually a simple and effective approach to improve\nmodel performance, but deeper models have larger model parameters and are more\ndifficult to train. To get a deeper model, simply stacking more layers of the\nmodel seems to work well, but previous works have claimed that it cannot\nbenefit the model. We propose to train a deeper model with recurrent mechanism,\nwhich loops the encoder and decoder blocks of Transformer in the depth\ndirection. To address the increasing of model parameters, we choose to share\nparameters in different recursive moments. We conduct our experiments on WMT16\nEnglish-to-German and WMT14 English-to-France translation tasks, our model\noutperforms the shallow Transformer-Base/Big baseline by 0.35, 1.45 BLEU\npoints, which is 27.23% of Transformer-Big model parameters. Compared to the\ndeep Transformer(20-layer encoder, 6-layer decoder), our model has similar\nmodel performance and infer speed, but our model parameters are 54.72% of the\nformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">GuoLiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One TTS Alignment To Rule Them All. (arXiv:2108.10447v1 [cs.SD])","link":"http://arxiv.org/abs/2108.10447","description":"<p>Speech-to-text alignment is a critical component of neural textto-speech\n(TTS) models. Autoregressive TTS models typically use an attention mechanism to\nlearn these alignments on-line. However, these alignments tend to be brittle\nand often fail to generalize to long utterances and out-of-domain text, leading\nto missing or repeating words. Most non-autoregressive endto-end TTS models\nrely on durations extracted from external sources. In this paper we leverage\nthe alignment mechanism proposed in RAD-TTS as a generic alignment learning\nframework, easily applicable to a variety of neural TTS models. The framework\ncombines forward-sum algorithm, the Viterbi algorithm, and a simple and\nefficient static prior. In our experiments, the alignment learning framework\nimproves all tested TTS architectures, both autoregressive (Flowtron, Tacotron\n2) and non-autoregressive (FastPitch, FastSpeech 2, RAD-TTS). Specifically, it\nimproves alignment convergence speed of existing attention-based mechanisms,\nsimplifies the training pipeline, and makes the models more robust to errors on\nlong utterances. Most importantly, the framework improves the perceived speech\nsynthesis quality, as judged by human evaluators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Badlani_R/0/1/0/all/0/1\">Rohan Badlani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancucki_A/0/1/0/all/0/1\">Adrian &#x141;ancucki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_K/0/1/0/all/0/1\">Kevin J. Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_R/0/1/0/all/0/1\">Rafael Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming the Beast: Learning to Control Neural Conversational Models. (arXiv:2108.10561v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10561","description":"<p>This thesis investigates the controllability of deep learning-based,\nend-to-end, generative dialogue systems in both task-oriented and chit-chat\nscenarios. In particular, we study the different aspects of controlling\ngenerative dialogue systems, including controlling styles and topics and\ncontinuously adding and combining dialogue skills. In the three decades since\nthe first dialogue system was commercialized, the basic architecture of such\nsystems has remained substantially unchanged, consisting of four pipelined\nbasic components, namely, natural language understanding (NLU), dialogue state\ntracking (DST), a dialogue manager (DM) and natural language generation (NLG).\nThe dialogue manager, which is the critical component of the modularized\nsystem, controls the response content and style. This module is usually\nprogrammed by rules and is designed to be highly controllable and easily\nextendable. With the emergence of powerful \"deep learning\" architectures,\nend-to-end generative dialogue systems have been proposed to optimize overall\nsystem performance and simplify training. However, these systems cannot be\neasily controlled and extended as the modularized dialogue manager can. This is\nbecause a single neural system is used, which is usually a large pre-trained\nlanguage model (e.g., GPT-2), and thus it is hard to surgically change\ndesirable attributes (e.g., style, topics, etc.). More importantly,\nuncontrollable dialogue systems can generate offensive and even toxic\nresponses. Therefore, in this thesis, we study controllable methods for\nend-to-end generative dialogue systems in task-oriented and chit-chat\nscenarios. Throughout the chapters, we describe 1) how to control the style and\ntopics of chit-chat models, 2) how to continuously control and extend\ntask-oriented dialogue systems, and 3) how to compose and control multi-skill\ndialogue models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Criminal Texts for the Polish State Border Guard. (arXiv:2108.10580v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10580","description":"<p>This paper describes research on the detection of Polish criminal texts\nappearing on the Internet. We carried out experiments to find the best\navailable setup for the efficient classification of unbalanced and noisy data.\nThe best performance was achieved when our model was fine-tuned on a\npre-trained Polish-based transformer language model. For the detection task, a\nlarge corpus of annotated Internet snippets was collected as training data. We\nshare this dataset and create a new task for the detection of criminal texts\nusing the Gonito platform as the benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nowakowski_A/0/1/0/all/0/1\">Artur Nowakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jassem_K/0/1/0/all/0/1\">Krzysztof Jassem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Learning for Fine-Grained Entity Typing. (arXiv:2108.10604v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10604","description":"<p>As an effective approach to tune pre-trained language models (PLMs) for\nspecific tasks, prompt-learning has recently attracted much attention from\nresearchers. By using \\textit{cloze}-style language prompts to stimulate the\nversatile knowledge of PLMs, prompt-learning can achieve promising results on a\nseries of NLP tasks, such as natural language inference, sentiment\nclassification, and knowledge probing. In this work, we investigate the\napplication of prompt-learning on fine-grained entity typing in fully\nsupervised, few-shot and zero-shot scenarios. We first develop a simple and\neffective prompt-learning pipeline by constructing entity-oriented verbalizers\nand templates and conducting masked language modeling. Further, to tackle the\nzero-shot regime, we propose a self-supervised strategy that carries out\ndistribution-level optimization in prompt-learning to automatically summarize\nthe information of entity types. Extensive experiments on three fine-grained\nentity typing benchmarks (with up to 86 classes) under fully supervised,\nfew-shot and zero-shot settings show that prompt-learning methods significantly\noutperform fine-tuning baselines, especially when the training data is\ninsufficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hong-Gee Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Cross-platform Teenager Detection with Adversarial BERT. (arXiv:2108.10619v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10619","description":"<p>Teenager detection is an important case of the age detection task in social\nmedia, which aims to detect teenage users to protect them from negative\ninfluences. The teenager detection task suffers from the scarcity of labelled\ndata, which exacerbates the ability to perform well across social media\nplatforms. To further research in teenager detection in settings where no\nlabelled data is available for a platform, we propose a novel cross-platform\nframework based on Adversarial BERT. Our framework can operate with a limited\namount of labelled instances from the source platform and with no labelled data\nfrom the target platform, transferring knowledge from the source to the target\nsocial media. We experiment on four publicly available datasets, obtaining\nresults demonstrating that our framework can significantly improve over\ncompetitive baseline models on the cross-platform teenager detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_P/0/1/0/all/0/1\">Peiling Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are the Multilingual Models Better? Improving Czech Sentiment with Transformers. (arXiv:2108.10640v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10640","description":"<p>In this paper, we aim at improving Czech sentiment with transformer-based\nmodels and their multilingual versions. More concretely, we study the task of\npolarity detection for the Czech language on three sentiment polarity datasets.\nWe fine-tune and perform experiments with five multilingual and three\nmonolingual models. We compare the monolingual and multilingual models'\nperformance, including comparison with the older approach based on recurrent\nneural networks. Furthermore, we test the multilingual models and their ability\nto transfer knowledge from English to Czech (and vice versa) with zero-shot\ncross-lingual classification. Our experiments show that the huge multilingual\nmodels can overcome the performance of the monolingual models. They are also\nable to detect polarity in another language without any training data, with\nperformance not worse than 4.4 % compared to state-of-the-art monolingual\ntrained models. Moreover, we achieved new state-of-the-art results on all three\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priban_P/0/1/0/all/0/1\">Pavel P&#x159;ib&#xe1;&#x148;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinberger_J/0/1/0/all/0/1\">Josef Steinberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morality-based Assertion and Homophily on Social Media: A Cultural Comparison between English and Japanese Languages. (arXiv:2108.10643v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10643","description":"<p>Moral psychology is a domain that deals with moral identity, appraisals and\nemotions. Previous work has greatly focused on moral development and the\nassociated role of culture. Knowing that language is an inherent element of a\nculture, we used the social media platform Twitter for comparing the moral\nbehaviors of Japanese users with English users. The five basic moral\nfoundations i.e., Care, Fairness, Ingroup, Authority and Purity, along with the\nassociated emotional valence are compared for English and Japanese tweets. The\ntweets from Japanese users depicted relatively higher Fairness, Ingroup and\nPurity. As far as emotions related to morality are concerned, the English\ntweets expressed more positive emotions for all moral dimensions. Considering\nthe role of moral similarities in connecting users on social media, we\nquantified homophily concerning different moral dimensions using our proposed\nmethod. The moral dimensions Care, Authority and Purity for English and Ingroup\nfor Japanese depicted homophily on Twitter. Overall, our study uncovers the\nunderlying cultural differences with respect to moral behavior in English and\nJapanese speaking users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rishemjit Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_A/0/1/0/all/0/1\">Akiko Matsuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyengar_S/0/1/0/all/0/1\">S.R.S. Iyengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasahara_K/0/1/0/all/0/1\">Kazutoshi Sasahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Density-Based Dynamic Curriculum Learning for Intent Detection. (arXiv:2108.10674v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10674","description":"<p>Pre-trained language models have achieved noticeable performance on the\nintent detection task. However, due to assigning an identical weight to each\nsample, they suffer from the overfitting of simple samples and the failure to\nlearn complex samples well. To handle this problem, we propose a density-based\ndynamic curriculum learning model. Our model defines the sample's difficulty\nlevel according to their eigenvectors' density. In this way, we exploit the\noverall distribution of all samples' eigenvectors simultaneously. Then we apply\na dynamic curriculum learning strategy, which pays distinct attention to\nsamples of various difficulty levels and alters the proportion of samples\nduring the training process. Through the above operation, simple samples are\nwell-trained, and complex samples are enhanced. Experiments on three open\ndatasets verify that the proposed density-based algorithm can distinguish\nsimple and complex samples significantly. Besides, our model obtains obvious\nimprovement over the strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yantao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Cao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiazhen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xunliang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Guanglu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiansong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_R/0/1/0/all/0/1\">Ruiyao Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Houfeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid deep learning methods for phenotype prediction from clinical notes. (arXiv:2108.10682v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10682","description":"<p>Identifying patient cohorts from clinical notes in secondary electronic\nhealth records is a fundamental task in clinical information management. The\npatient cohort identification needs to identify the patient phenotypes.\nHowever, with the growing number of clinical notes, it becomes challenging to\nanalyze the data manually. Therefore, automatic extraction of clinical concepts\nwould be an essential task to identify the patient phenotypes correctly. This\npaper proposes a novel hybrid model for automatically extracting patient\nphenotypes using natural language processing and deep learning models to\ndetermine the patient phenotypes without dictionaries and human intervention.\n</p>\n<p>The proposed hybrid model is based on a neural bidirectional sequence model\n(BiLSTM or BiGRU) and a Convolutional Neural Network (CNN) for identifying\npatient's phenotypes in discharge reports. Furthermore, to extract more\nfeatures related to each phenotype, an extra CNN layer is run parallel to the\nhybrid proposed model. We used pre-trained embeddings such as FastText and\nWord2vec separately as the input layers to evaluate other embedding's\nperformance in identifying patient phenotypes. We also measured the effect of\napplying additional data cleaning steps on discharge reports to identify\npatient phenotypes by deep learning models. We used discharge reports in the\nMedical Information Mart for Intensive Care III (MIMIC III) database.\nExperimental results in internal comparison demonstrate significant performance\nimprovement over existing models. The enhanced model with an extra CNN layer\nobtained a relatively higher F1-score than the original hybrid model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalafi_S/0/1/0/all/0/1\">Sahar Khalafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_N/0/1/0/all/0/1\">Nasser Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression. (arXiv:2108.10684v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10684","description":"<p>Organizing complex peer production projects and advancing scientific\nknowledge of open collaboration each depend on the ability to measure quality.\nArticle quality ratings on English language Wikipedia have been widely used by\nboth Wikipedia community members and academic researchers for purposes like\ntracking knowledge gaps and studying how political polarization shapes\ncollaboration. Even so, measuring quality presents many methodological\nchallenges. The most widely used systems use labels on discrete ordinal scales\nwhen assessing quality, but such labels can be inconvenient for statistics and\nmachine learning. Prior work handles this by assuming that different levels of\nquality are \"evenly spaced\" from one another. This assumption runs counter to\nintuitions about the relative degrees of effort needed to raise Wikipedia\nencyclopedia articles to different quality levels. Furthermore, models from\nprior work are fit to datasets that oversample high-quality articles. This\nlimits their accuracy for representative samples of articles or revisions. I\ndescribe a technique extending the Wikimedia Foundations' ORES article quality\nmodel to address these limitations. My method uses weighted ordinal regression\nmodels to construct one-dimensional continuous measures of quality. While\nscores from my technique and from prior approaches are correlated, my approach\nimproves accuracy for research datasets and provides evidence that the \"evenly\nspaced\" assumption is unfounded in practice on English Wikipedia. I conclude\nwith recommendations for using quality scores in future research and include\nthe full code, data, and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+TeBlunthuis_N/0/1/0/all/0/1\">Nathan TeBlunthuis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Active Learning for Text Classification with Diverse Interpretations. (arXiv:2108.10687v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10687","description":"<p>Recently, Deep Neural Networks (DNNs) have made remarkable progress for text\nclassification, which, however, still require a large number of labeled data.\nTo train high-performing models with the minimal annotation cost, active\nlearning is proposed to select and label the most informative samples, yet it\nis still challenging to measure informativeness of samples used in DNNs. In\nthis paper, inspired by piece-wise linear interpretability of DNNs, we propose\na novel Active Learning with DivErse iNterpretations (ALDEN) approach. With\nlocal interpretations in DNNs, ALDEN identifies linearly separable regions of\nsamples. Then, it selects samples according to their diversity of local\ninterpretations and queries their labels. To tackle the text classification\nproblem, we choose the word with the most diverse interpretations to represent\nthe whole sentence. Extensive experiments demonstrate that ALDEN consistently\noutperforms several state-of-the-art deep active learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanqiao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaocheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Optimal Linear Orders. (arXiv:2108.10692v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10692","description":"<p>The sequential structure of language, and the order of words in a sentence\nspecifically, plays a central role in human language processing. Consequently,\nin designing computational models of language, the de facto approach is to\npresent sentences to machines with the words ordered in the same order as in\nthe original human-authored sentence. The very essence of this work is to\nquestion the implicit assumption that this is desirable and inject theoretical\nsoundness into the consideration of word order in natural language processing.\nIn this thesis, we begin by uniting the disparate treatments of word order in\ncognitive science, psycholinguistics, computational linguistics, and natural\nlanguage processing under a flexible algorithmic framework. We proceed to use\nthis heterogeneous theoretical foundation as the basis for exploring new word\norders with an undercurrent of psycholinguistic optimality. In particular, we\nfocus on notions of dependency length minimization given the difficulties in\nhuman and computational language processing in handling long-distance\ndependencies. We then discuss algorithms for finding optimal word orders\nefficiently in spite of the combinatorial space of possibilities. We conclude\nby addressing the implications of these word orders on human language and their\ndownstream impacts when integrated in computational models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1\">Rishi Bommasani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficacy of BERT embeddings on predicting disaster from Twitter data. (arXiv:2108.10698v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10698","description":"<p>Social media like Twitter provide a common platform to share and communicate\npersonal experiences with other people. People often post their life\nexperiences, local news, and events on social media to inform others. Many\nrescue agencies monitor this type of data regularly to identify disasters and\nreduce the risk of lives. However, it is impossible for humans to manually\ncheck the mass amount of data and identify disasters in real-time. For this\npurpose, many research works have been proposed to present words in\nmachine-understandable representations and apply machine learning methods on\nthe word representations to identify the sentiment of a text. The previous\nresearch methods provide a single representation or embedding of a word from a\ngiven document. However, the recent advanced contextual embedding method (BERT)\nconstructs different vectors for the same word in different contexts. BERT\nembeddings have been successfully used in different natural language processing\n(NLP) tasks, yet there is no concrete analysis of how these representations are\nhelpful in disaster-type tweet analysis. In this research work, we explore the\nefficacy of BERT embeddings on predicting disaster from Twitter data and\ncompare these to traditional context-free word embedding methods (GloVe,\nSkip-gram, and FastText). We use both traditional machine learning methods and\ndeep learning methods for this purpose. We provide both quantitative and\nqualitative results for this study. The results show that the BERT embeddings\nhave the best results in disaster prediction task than the traditional word\nembeddings. Our codes are made freely accessible to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chanda_A/0/1/0/all/0/1\">Ashis Kumar Chanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Hateful are Movies? A Study and Prediction on Movie Subtitles. (arXiv:2108.10724v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10724","description":"<p>In this research, we investigate techniques to detect hate speech in movies.\nWe introduce a new dataset collected from the subtitles of six movies, where\neach utterance is annotated either as hate, offensive or normal. We apply\ntransfer learning techniques of domain adaptation and fine-tuning on existing\nsocial media datasets, namely from Twitter and Fox News. We evaluate different\nrepresentations, i.e., Bag of Words (BoW), Bi-directional Long short-term\nmemory (Bi-LSTM), and Bidirectional Encoder Representations from Transformers\n(BERT) on 11k movie subtitles. The BERT model obtained the best macro-averaged\nF1-score of 77%. Hence, we show that transfer learning from the social media\ndomain is efficacious in classifying hate and offensive speech in movies\nthrough subtitles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boguszewski_N/0/1/0/all/0/1\">Niklas von Boguszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moin_S/0/1/0/all/0/1\">Sana Moin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_A/0/1/0/all/0/1\">Anirban Bhowmick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misleading the Covid-19 vaccination discourse on Twitter: An exploratory study of infodemic around the pandemic. (arXiv:2108.10735v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10735","description":"<p>In this work, we collect a moderate-sized representative corpus of tweets\n(200,000 approx.) pertaining Covid-19 vaccination spanning over a period of\nseven months (September 2020 - March 2021). Following a Transfer Learning\napproach, we utilize the pre-trained Transformer-based XLNet model to classify\ntweets as Misleading or Non-Misleading and validate against a random subset of\nresults manually. We build on this to study and contrast the characteristics of\ntweets in the corpus that are misleading in nature against non-misleading ones.\nThis exploratory analysis enables us to design features (such as sentiments,\nhashtags, nouns, pronouns, etc) that can, in turn, be exploited for classifying\ntweets as (Non-)Misleading using various ML models in an explainable manner.\nSpecifically, several ML models are employed for prediction, with up to 90%\naccuracy, and the importance of each feature is explained using SHAP\nExplainable AI (XAI) tool. While the thrust of this work is principally\nexploratory analysis in order to obtain insights on the online discourse on\nCovid-19 vaccination, we conclude the paper by outlining how these insights\nprovide the foundations for a more actionable approach to mitigate\nmisinformation. The curated dataset and code is made available (Github\nrepository) so that the research community at large can reproduce, compare\nagainst, or build upon this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Anwitaman Datta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10750","description":"<p>Relation Extraction (RE) from tables is the task of identifying relations\nbetween pairs of columns. Generally, RE models for this task require labelled\ntables for training. Luckily, labelled tables can also be generated\nartificially from a Knowledge Graph (KG), which makes the cost to acquire them\nmuch lower in comparison to manual annotations. However, these tables have one\ndrawback compared to real tables, which is that they lack associated metadata,\nsuch as column-headers, captions, etc. This is because synthetic tables are\ncreated out of KGs that do not store such metadata. Unfortunately, metadata can\nprovide strong signals for RE from tables. To address this issue, we propose\nmethods to artificially create some of this metadata for synthetic tables. We\nthen experiment with a RE model that uses artificial metadata as input. Our\nempirical results show that this leads to an improvement of 9\\%-45\\% in F1\nscore, in absolute terms, over 2 tabular datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+singh_G/0/1/0/all/0/1\">Gaurav singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Joshua Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Than Words: Collocation Tokenization for Latent Dirichlet Allocation Models. (arXiv:2108.10755v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10755","description":"<p>Traditionally, Latent Dirichlet Allocation (LDA) ingests words in a\ncollection of documents to discover their latent topics using word-document\nco-occurrences. However, it is unclear how to achieve the best results for\nlanguages without marked word boundaries such as Chinese and Thai. Here, we\nexplore the use of Pearson's chi-squared test, t-statistics, and Word Pair\nEncoding (WPE) to produce tokens as input to the LDA model. The Chi-squared, t,\nand WPE tokenizers are trained on Wikipedia text to look for words that should\nbe grouped together, such as compound nouns, proper nouns, and complex event\nverbs. We propose a new metric for measuring the clustering quality in settings\nwhere the vocabularies of the models differ. Based on this metric and other\nestablished metrics, we show that topics trained with merged tokens result in\ntopic keys that are clearer, more coherent, and more effective at\ndistinguishing topics than those unmerged models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheevaprawatdomrong_J/0/1/0/all/0/1\">Jin Cheevaprawatdomrong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schofield_A/0/1/0/all/0/1\">Alexandra Schofield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_A/0/1/0/all/0/1\">Attapol T. Rutherford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ComSum: Commit Messages Summarization and Meaning Preservation. (arXiv:2108.10763v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10763","description":"<p>We present ComSum, a data set of 7 million commit messages for text\nsummarization. When documenting commits, software code changes, both a message\nand its summary are posted. We gather and filter those to curate developers'\nwork summarization data set. Along with its growing size, practicality and\nchallenging language domain, the data set benefits from the living field of\nempirical software engineering. As commits follow a typology, we propose to not\nonly evaluate outputs by Rouge, but by their meaning preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amit_I/0/1/0/all/0/1\">Idan Amit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularizing Transformers With Deep Probabilistic Layers. (arXiv:2108.10764v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10764","description":"<p>Language models (LM) have grown with non-stop in the last decade, from\nsequence-to-sequence architectures to the state-of-the-art and utter\nattention-based Transformers. In this work, we demonstrate how the inclusion of\ndeep generative models within BERT can bring more versatile models, able to\nimpute missing/noisy words with richer text or even improve BLEU score. More\nprecisely, we use a Gaussian Mixture Variational Autoencoder (GMVAE) as a\nregularizer layer and prove its effectiveness not only in Transformers but also\nin the most relevant encoder-decoder based LM, seq2seq with and without\nattention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aguilera_A/0/1/0/all/0/1\">Aurora Cobo Aguilera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olmos_P/0/1/0/all/0/1\">Pablo Mart&#xed;nez Olmos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artes_Rodriguez_A/0/1/0/all/0/1\">Antonio Art&#xe9;s-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Cruz_F/0/1/0/all/0/1\">Fernando P&#xe9;rez-Cruz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensuring the Inclusive Use of Natural Language Processing in the Global Response to COVID-19. (arXiv:2108.10791v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10791","description":"<p>Natural language processing (NLP) plays a significant role in tools for the\nCOVID-19 pandemic response, from detecting misinformation on social media to\nhelping to provide accurate clinical information or summarizing scientific\nresearch. However, the approaches developed thus far have not benefited all\npopulations, regions or languages equally. We discuss ways in which current and\nfuture NLP approaches can be made more inclusive by covering low-resource\nlanguages, including alternative modalities, leveraging out-of-the-box tools\nand forming meaningful partnerships. We suggest several future directions for\nresearchers interested in maximizing the positive societal impacts of NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1\">Alexandra Sasha Luccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_K/0/1/0/all/0/1\">Katherine Hoffmann Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_C/0/1/0/all/0/1\">Cynthia Sin Nga Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aylett_Bullock_J/0/1/0/all/0/1\">Joseph Aylett-Bullock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luengo_Oroz_M/0/1/0/all/0/1\">Miguel Luengo-Oroz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Exposure Bias in Training Recurrent Neural Network Transducers. (arXiv:2108.10803v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10803","description":"<p>When recurrent neural network transducers (RNNTs) are trained using the\ntypical maximum likelihood criterion, the prediction network is trained only on\nground truth label sequences. This leads to a mismatch during inference, known\nas exposure bias, when the model must deal with label sequences containing\nerrors. In this paper we investigate approaches to reducing exposure bias in\ntraining to improve the generalization of RNNT models for automatic speech\nrecognition (ASR). A label-preserving input perturbation to the prediction\nnetwork is introduced. The input token sequences are perturbed using SwitchOut\nand scheduled sampling based on an additional token language model. Experiments\nconducted on the 300-hour Switchboard dataset demonstrate their effectiveness.\nBy reducing the exposure bias, we show that we can further improve the accuracy\nof a high-performance RNNT ASR model and obtain state-of-the-art results on the\n300-hour Switchboard dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaodong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haws_D/0/1/0/all/0/1\">David Haws</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuske_Z/0/1/0/all/0/1\">Zoltan Tuske</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-level Neural Machine Translation with Associated Memory Network. (arXiv:1910.14528v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1910.14528","description":"<p>Standard neural machine translation (NMT) is on the assumption that the\ndocument-level context is independent. Most existing document-level NMT\napproaches are satisfied with a smattering sense of global document-level\ninformation, while this work focuses on exploiting detailed document-level\ncontext in terms of a memory network. The capacity of the memory network that\ndetecting the most relevant part of the current sentence from memory renders a\nnatural solution to model the rich document-level context. In this work, the\nproposed document-aware memory network is implemented to enhance the\nTransformer NMT baseline. Experiments on several tasks show that the proposed\nmethod significantly improves the NMT performance over strong Transformer\nbaselines and other related studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kehai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bao-liang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. (arXiv:1911.04942v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.04942","description":"<p>When translating natural language questions into SQL queries to answer\nquestions from a database, contemporary semantic parsing models struggle to\ngeneralize to unseen database schemas. The generalization challenge lies in (a)\nencoding the database relations in an accessible way for the semantic parser,\nand (b) modeling alignment between database columns and their mentions in a\ngiven query. We present a unified framework, based on the relation-aware\nself-attention mechanism, to address schema encoding, schema linking, and\nfeature representation within a text-to-SQL encoder. On the challenging Spider\ndataset this framework boosts the exact match accuracy to 57.2%, surpassing its\nbest counterparts by 8.7% absolute improvement. Further augmented with BERT, it\nachieves the new state-of-the-art performance of 65.6% on the Spider\nleaderboard. In addition, we observe qualitative improvements in the model's\nunderstanding of schema linking and alignment. Our implementation will be\nopen-sourced at https://github.com/Microsoft/rat-sql.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1\">Richard Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polozov_O/0/1/0/all/0/1\">Oleksandr Polozov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_M/0/1/0/all/0/1\">Matthew Richardson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review on Fact Extraction and Verification. (arXiv:2010.03001v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.03001","description":"<p>We study the fact checking problem, which aims to identify the veracity of a\ngiven claim. Specifically, we focus on the task of Fact Extraction and\nVERification (FEVER) and its accompanied dataset. The task consists of the\nsubtasks of retrieving the relevant documents (and sentences) from Wikipedia\nand validating whether the information in the documents supports or refutes a\ngiven claim. This task is essential and can be the building block of\napplications such as fake news detection and medical claim verification. In\nthis paper, we aim at a better understanding of the challenges of the task by\npresenting the literature in a structured and comprehensive way. We describe\nthe proposed methods by analyzing the technical perspectives of the different\napproaches and discussing the performance results on the FEVER dataset, which\nis the most well-studied and formally structured dataset on the fact extraction\nand verification task. We also conduct the largest experimental study to date\non identifying beneficial loss functions for the sentence retrieval component.\nOur analysis indicates that sampling negative sentences is important for\nimproving the performance and decreasing the computational complexity. Finally,\nwe describe open issues and future challenges, and we motivate future research\nin the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bekoulis_G/0/1/0/all/0/1\">Giannis Bekoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papagiannopoulou_C/0/1/0/all/0/1\">Christina Papagiannopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Visual Reasoning via Induced Symbolic Space. (arXiv:2011.11603v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11603","description":"<p>We study the problem of concept induction in visual reasoning, i.e.,\nidentifying concepts and their hierarchical relationships from question-answer\npairs associated with images; and achieve an interpretable model via working on\nthe induced symbolic concept space. To this end, we first design a new\nframework named object-centric compositional attention model (OCCAM) to perform\nthe visual reasoning task with object-level visual features. Then, we come up\nwith a method to induce concepts of objects and relations using clues from the\nattention patterns between objects' visual features and question words.\nFinally, we achieve a higher level of interpretability by imposing OCCAM on the\nobjects represented in the induced symbolic concept space. Our model design\nmakes this an easy adaption via first predicting the concepts of objects and\nrelations and then projecting the predicted concepts back to the visual feature\nspace so the compositional reasoning module can process normally. Experiments\non the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of\nthe art without human-annotated functional programs; 2) our induced concepts\nare both accurate and sufficient as OCCAM achieves an on-par performance on\nobjects represented either in visual features or in the induced symbolic\nconcept space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhonghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. (arXiv:2104.04473v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04473","description":"<p>Large language models have led to state-of-the-art accuracies across a range\nof tasks. However, training these models efficiently is challenging for two\nreasons: a) GPU memory capacity is limited, making it impossible to fit large\nmodels on even a multi-GPU server, and b) the number of compute operations\nrequired to train these models can result in unrealistically long training\ntimes. Consequently, new methods of model parallelism such as tensor and\npipeline parallelism have been proposed. Unfortunately, naive usage of these\nmethods leads to fundamental scaling issues at thousands of GPUs, e.g., due to\nexpensive cross-node communication or devices spending significant time waiting\non other devices to make progress.\n</p>\n<p>In this paper, we show how different types of parallelism methods (tensor,\npipeline, and data parallelism) can be composed to scale to thousands of GPUs\nand models with trillions of parameters. We survey techniques for pipeline\nparallelism and propose a novel interleaved pipeline parallelism schedule that\ncan improve throughput by 10+% with memory footprint comparable to existing\napproaches. We quantitatively study the trade-offs between tensor, pipeline,\nand data parallelism, and provide intuition as to how to configure distributed\ntraining of a large model. Our approach allows us to perform training\niterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs\nwith achieved per-GPU throughput of 52% of theoretical peak. Our code is open\nsourced at https://github.com/nvidia/megatron-lm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Deepak Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1\">Patrick LeGresley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Anand Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainbrand_D/0/1/0/all/0/1\">Dmitri Vainbrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashinkunti_P/0/1/0/all/0/1\">Prethvi Kashinkunti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julie Bernauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phanishayee_A/0/1/0/all/0/1\">Amar Phanishayee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fibrational Initial Algebra-Final Coalgebra Coincidence over Initial Algebras: Turning Verification Witnesses Upside Down. (arXiv:2105.04817v3 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2105.04817","description":"<p>The coincidence between initial algebras (IAs) and final coalgebras (FCs) is\na phenomenon that underpins various important results in theoretical computer\nscience. In this paper, we identify a general fibrational condition for the\nIA-FC coincidence, namely in the fiber over an initial algebra in the base\ncategory. Identifying (co)algebras in a fiber as (co)inductive predicates, our\nfibrational IA-FC coincidence allows one to use coinductive witnesses (such as\ninvariants) for verifying inductive properties (such as liveness). Our general\nfibrational theory features the technical condition of stability of chain\ncolimits; we extend the framework to the presence of a monadic effect, too,\nrestricting to fibrations of complete lattice-valued predicates. Practical\nbenefits of our categorical theory are exemplified by new \"upside-down\" witness\nnotions for three verification problems: probabilistic liveness, and acceptance\nand model-checking with respect to bottom-up tree automata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kori_M/0/1/0/all/0/1\">Mayuko Kori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasuo_I/0/1/0/all/0/1\">Ichiro Hasuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsumata_S/0/1/0/all/0/1\">Shin-ya Katsumata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark. (arXiv:2106.08087v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08087","description":"<p>Artificial Intelligence (AI), along with the recent progress in biomedical\nlanguage understanding, is gradually changing medical practice. With the\ndevelopment of biomedical language understanding benchmarks, AI applications\nare widely used in the medical field. However, most benchmarks are limited to\nEnglish, which makes it challenging to replicate many of the successes in\nEnglish for other languages. To facilitate research in this direction, we\ncollect real-world biomedical data and present the first Chinese Biomedical\nLanguage Understanding Evaluation (CBLUE) benchmark: a collection of natural\nlanguage understanding tasks including named entity recognition, information\nextraction, clinical diagnosis normalization, single-sentence/sentence-pair\nclassification, and an associated online platform for model evaluation,\ncomparison, and analysis. To establish evaluation on these tasks, we report\nempirical results with the current 11 pre-trained Chinese models, and\nexperimental results show that state-of-the-art neural models perform by far\nworse than the human ceiling. Our benchmark is released at\n\\url{https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&amp;lang=en-us}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaozhuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_X/0/1/0/all/0/1\">Xin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kangping Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yuan Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_H/0/1/0/all/0/1\">Hui Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zan_H/0/1/0/all/0/1\">Hongying Zan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer-assisted construct classification of organizational performance concerning different stakeholder groups. (arXiv:2107.05133v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05133","description":"<p>The number of research articles in business and management has dramatically\nincreased along with terminology, constructs, and measures. Proper\nclassification of organizational performance constructs from research articles\nplays an important role in categorizing the literature and understanding to\nwhom its research implications may be relevant. In this work, we classify\nconstructs (i.e., concepts and terminology used to capture different aspects of\norganizational performance) in research articles into a three-level\ncategorization: (a) performance and non-performance categories (Level 0); (b)\nfor performance constructs, stakeholder group-level of performance concerning\ninvestors, customers, employees, and the society (community and natural\nenvironment) (Level 1); and (c) for each stakeholder group-level, subcategories\nof different ways of measurement (Level 2). We observed that increasing\ncontextual information with features extracted from surrounding sentences and\nexternal references improves classification of disaggregate-level labels, given\nlimited training data. Our research has implications for computer-assisted\nconstruct identification and classification - an essential step for research\nsynthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_S/0/1/0/all/0/1\">Seethalakshmi Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Victor Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_Powell_G/0/1/0/all/0/1\">Gus Hahn-Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tirunagar_B/0/1/0/all/0/1\">Bharadwaj Tirunagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04539","description":"<p>Understanding documents from their visual snapshots is an emerging problem\nthat requires both advanced computer vision and NLP methods. The recent advance\nin OCR enables the accurate recognition of text blocks, yet it is still\nchallenging to extract key information from documents due to the diversity of\ntheir layouts. Although recent studies on pre-trained language models show the\nimportance of incorporating layout information on this task, the conjugation of\ntexts and their layouts still follows the style of BERT optimized for\nunderstanding the 1D text. This implies there is room for further improvement\nconsidering the 2D nature of text layouts. This paper introduces a pre-trained\nlanguage model, BERT Relying On Spatiality (BROS), which effectively utilizes\nthe information included in individual text blocks and their layouts.\nSpecifically, BROS encodes spatial information by utilizing relative positions\nand learns spatial dependencies between OCR blocks with a novel area-masking\nstrategy. These two novel approaches lead to an efficient encoding of spatial\nlayout information highlighted by the robust performance of BROS under\nlow-resource environments. We also introduce a general-purpose parser that can\nbe combined with BROS to extract key information even when there is no order\ninformation between text blocks. BROS shows its superiority on four public\nbenchmarks -- FUNSD, SROIE*, CORD, and SciTSR -- and its robustness in\npractical cases where order information of text blocks is not available.\nFurther experiments with a varying number of training examples demonstrate the\nhigh training efficiency of our approach. Our code will be open to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}