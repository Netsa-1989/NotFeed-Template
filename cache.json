{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Contextual Sentence Classification: Detecting Sustainability Initiatives in Company Reports. (arXiv:2110.03727v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03727","description":"<p>We introduce the novel task of detecting sustainability initiatives in\ncompany reports. Given a full report, the aim is to automatically identify\nmentions of practical activities that a company has performed in order to\ntackle specific societal issues. As a single initiative can often be described\nover multiples sentences, new methods for identifying continuous sentence spans\nneeds to be developed. We release a new dataset of company reports in which the\ntext has been manually annotated with sustainability initiatives. We also\nevaluate different models for initiative detection, introducing a novel\naggregation and evaluation methodology. Our proposed architecture uses\nsequences of five consecutive sentences to account for contextual information\nwhen making classification decisions at the individual sentence level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirlea_D/0/1/0/all/0/1\">Dan Hirlea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryant_C/0/1/0/all/0/1\">Christopher Bryant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UoB at SemEval-2021 Task 5: Extending Pre-Trained Language Models to Include Task and Domain-Specific Information for Toxic Span Prediction. (arXiv:2110.03730v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03730","description":"<p>Toxicity is pervasive in social media and poses a major threat to the health\nof online communities. The recent introduction of pre-trained language models,\nwhich have achieved state-of-the-art results in many NLP tasks, has transformed\nthe way in which we approach natural language processing. However, the inherent\nnature of pre-training means that they are unlikely to capture task-specific\nstatistical information or learn domain-specific knowledge. Additionally, most\nimplementations of these models typically do not employ conditional random\nfields, a method for simultaneous token classification. We show that these\nmodifications can improve model performance on the Toxic Spans Detection task\nat SemEval-2021 to achieve a score within 4 percentage points of the top\nperforming team.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Erik Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference. (arXiv:2110.03742v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03742","description":"<p>Sparse Mixture-of-Experts (MoE) has been a successful approach for scaling\nmultilingual translation models to billions of parameters without a\nproportional increase in training computation. However, MoE models are\nprohibitively large and practitioners often resort to methods such as\ndistillation for serving. In this work, we investigate routing strategies at\ndifferent granularity (token, sentence, task) in MoE models to bypass\ndistillation. Experiments on WMT and a web-scale dataset suggest that\ntask-level routing (task-MoE) enables us to extract smaller, ready-to-deploy\nsub-networks from large sparse models. On WMT, our task-MoE with 32 experts\n(533M parameters) outperforms the best performing token-level MoE model\n(token-MoE) by +1.0 BLEU on average across 30 language pairs. The peak\ninference throughput is also improved by a factor of 1.9x when we route by\ntasks instead of tokens. While distilling a token-MoE to a smaller dense model\npreserves only 32% of the BLEU gains, our sub-network task-MoE, by design,\npreserves all the gains with the same inference cost as the distilled student\nmodel. Finally, when scaling up to 200 language pairs, our 128-expert task-MoE\n(13B parameters) performs competitively with a token-level counterpart, while\nimproving the peak inference throughput by a factor of 2.6x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krikun_M/0/1/0/all/0/1\">Maxim Krikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepikhin_D/0/1/0/all/0/1\">Dmitry Lepikhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sonorant spectra and coarticulation distinguish speakers with different dialects. (arXiv:2110.03756v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03756","description":"<p>The aim of this study is to determine the effect of language varieties on the\nspectral distribution of stressed and unstressed sonorants (nasals /m, n/,\nlateral approximants /l/, and rhotics /r/) and on their coarticulatory effects\non adjacent sounds. To quantify the shape of the spectral distribution, we\ncalculated the spectral moments from the sonorant spectra of nasals /m, n/,\nlateral approximants /l/, and rhotics /r/ produced by Athenian Greek and\nCypriot Greek speakers. To estimate the co-articulatory effects of sonorants on\nthe adjacent vowels' F1 - F4 formant frequencies, we developed polynomial\nmodels of the adjacent vowel's formant contours. We found significant effects\nof language variety (sociolinguistic information) on the spectral moments of\neach sonorant /m/, /n/, /l/, /r/ (except between /m/ and /n/) and on the\nformant contours of the adjacent vowel. All sonorants (including /m/ and /n/)\nhad distinct effects on adjacent vowel's formant contours, especially for F3\nand F4. The study highlights that the combination of spectral moments and\ncoarticulatory effects of sonorants determines linguistic (stress and phonemic\ncategory) and sociolinguistic (language variety) characteristics of sonorants.\nIt also provides the first comparative acoustic analysis of Athenian Greek and\nCypriot Greek sonorants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Themistocleous_C/0/1/0/all/0/1\">Charalambos Themistocleous</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyndanis_V/0/1/0/all/0/1\">Valantis Fyndanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsapkini_K/0/1/0/all/0/1\">Kyrana Tsapkini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Input Length Matters: An Empirical Study Of RNN-T And MWER Training For Long-form Telephony Speech Recognition. (arXiv:2110.03841v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03841","description":"<p>End-to-end models have achieved state-of-the-art results on several automatic\nspeech recognition tasks. However, they perform poorly when evaluated on\nlong-form data, e.g., minutes long conversational telephony audio. One reason\nthe model fails on long-form speech is that it has only seen short utterances\nduring training. This paper presents an empirical study on the effect of\ntraining utterance length on the word error rate (WER) for RNN-transducer\n(RNN-T) model. We compare two widely used training objectives, log loss (or\nRNN-T loss) and minimum word error rate (MWER) loss. We conduct experiments on\ntelephony datasets in four languages. Our experiments show that for both\nlosses, the WER on long-form speech reduces substantially as the training\nutterance length increases. The average relative WER gain is 15.7% for log loss\nand 8.8% for MWER loss. When training on short utterances, MWER loss leads to a\nlower WER than the log loss. Such difference between the two losses diminishes\nwhen the input length increases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyun Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yanwei Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doutre_T/0/1/0/all/0/1\">Thibault Doutre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation Verbosity Control for Automatic Dubbing. (arXiv:2110.03847v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03847","description":"<p>Automatic dubbing aims at seamlessly replacing the speech in a video document\nwith synthetic speech in a different language. The task implies many\nchallenges, one of which is generating translations that not only convey the\noriginal content, but also match the duration of the corresponding utterances.\nIn this paper, we focus on the problem of controlling the verbosity of machine\ntranslation output, so that subsequent steps of our automatic dubbing pipeline\ncan generate dubs of better quality. We propose new methods to control the\nverbosity of MT output and compare them against the state of the art with both\nintrinsic and extrinsic evaluations. For our experiments we use a public data\nset to dub English speeches into French, Italian, German and Spanish. Finally,\nwe report extensive subjective tests that measure the impact of MT verbosity\ncontrol on the final quality of dubbed video clips.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakew_S/0/1/0/all/0/1\">Surafel M. Lakew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_C/0/1/0/all/0/1\">Cuong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1\">Yogesh Virkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barra_Chicote_R/0/1/0/all/0/1\">Roberto Barra-Chicote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Enyedi_R/0/1/0/all/0/1\">Robert Enyedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speeding up Deep Model Training by Sharing Weights and Then Unsharing. (arXiv:2110.03848v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03848","description":"<p>We propose a simple and efficient approach for training the BERT model. Our\napproach exploits the special structure of BERT that contains a stack of\nrepeated modules (i.e., transformer encoders). Our proposed approach first\ntrains BERT with the weights shared across all the repeated modules till some\npoint. This is for learning the commonly shared component of weights across all\nrepeated layers. We then stop weight sharing and continue training until\nconvergence. We present theoretic insights for training by sharing weights then\nunsharing with analysis for simplified models. Empirical experiments on the\nBERT model show that our method yields better performance of trained models,\nand significantly reduces the number of training iterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Le Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaodan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A study on the efficacy of model pre-training in developing neural text-to-speech system. (arXiv:2110.03857v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03857","description":"<p>In the development of neural text-to-speech systems, model pre-training with\na large amount of non-target speakers' data is a common approach. However, in\nterms of ultimately achieved system performance for target speaker(s), the\nactual benefits of model pre-training are uncertain and unstable, depending\nvery much on the quantity and text content of training data. This study aims to\nunderstand better why and how model pre-training can positively contribute to\nTTS system performance. It is postulated that the pre-training process plays a\ncritical role in learning text-related variation in speech, while further\ntraining with the target speaker's data aims to capture the speaker-related\nvariation. Different test sets are created with varying degrees of similarity\nto target speaker data in terms of text content. Experiments show that\nleveraging a speaker-independent TTS trained on speech data with diverse text\ncontent can improve the target speaker TTS on domain-mismatched text. We also\nattempt to reduce the amount of pre-training data for a new text domain and\nimprove the data and computational efficiency. It is found that the TTS system\ncould achieve comparable performance when the pre-training data is reduced to\n1/8 of its original size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guangyan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_D/0/1/0/all/0/1\">Daxin Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_Y/0/1/0/all/0/1\">Ying Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1\">Kaitao Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1\">Tan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks. (arXiv:2110.03861v1 [quant-ph])","link":"http://arxiv.org/abs/2110.03861","description":"<p>The advent of noisy intermediate-scale quantum (NISQ) computers raises a\ncrucial challenge to design quantum neural networks for fully quantum learning\ntasks. To bridge the gap, this work proposes an end-to-end learning framework\nnamed QTN-VQC, by introducing a trainable quantum tensor network (QTN) for\nquantum embedding on a variational quantum circuit (VQC). The architecture of\nQTN is composed of a parametric tensor-train network for feature extraction and\na tensor product encoding for quantum encoding. We highlight the QTN for\nquantum embedding in terms of two perspectives: (1) we theoretically\ncharacterize QTN by analyzing its representation power of input features; (2)\nQTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the\ngeneration of quantum embedding to the output measurement. Our experiments on\nthe MNIST dataset demonstrate the advantages of QTN for quantum embedding over\nother quantum embedding approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Qi_J/0/1/0/all/0/1\">Jun Qi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cross-Lingual Transfer of Structured Predictors without Source Data. (arXiv:2110.03866v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03866","description":"<p>Providing technologies to communities or domains where training data is\nscarce or protected e.g., for privacy reasons, is becoming increasingly\nimportant. To that end, we generalise methods for unsupervised transfer from\nmultiple input models for structured prediction. We show that the means of\naggregating over the input models is critical, and that multiplying marginal\nprobabilities of substructures to obtain high-probability structures for\ndistant supervision is substantially better than taking the union of such\nstructures over the input models, as done in prior work. Testing on 18\nlanguages, we demonstrate that the method works in a cross-lingual setting,\nconsidering both dependency parsing and part-of-speech structured prediction\nproblems. Our analyses show that the proposed method produces less noisy labels\nfor the distant supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurniawan_K/0/1/0/all/0/1\">Kemal Kurniawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_P/0/1/0/all/0/1\">Philip Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation of professions in entertainment media: Insights into frequency and sentiment trends through computational text analysis. (arXiv:2110.03873v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03873","description":"<p>Societal ideas and trends dictate media narratives and cinematic depictions\nwhich in turn influences people's beliefs and perceptions of the real world.\nMedia portrayal of culture, education, government, religion, and family affect\ntheir function and evolution over time as people interpret and perceive these\nrepresentations and incorporate them into their beliefs and actions. It is\nimportant to study media depictions of these social structures so that they do\nnot propagate or reinforce negative stereotypes, or discriminate against any\ndemographic section. In this work, we examine media representation of\nprofessions and provide computational insights into their incidence, and\nsentiment expressed, in entertainment media content. We create a searchable\ntaxonomy of professional groups and titles to facilitate their retrieval from\nspeaker-agnostic text passages like movie and television (TV) show subtitles.\nWe leverage this taxonomy and relevant natural language processing (NLP) models\nto create a corpus of professional mentions in media content, spanning more\nthan 136,000 IMDb titles over seven decades (1950-2017). We analyze the\nfrequency and sentiment trends of different occupations, study the effect of\nmedia attributes like genre, country of production, and title type on these\ntrends, and investigate if the incidence of professions in media subtitles\ncorrelate with their real-world employment statistics. We observe increased\nmedia mentions of STEM, arts, sports, and entertainment occupations in the\nanalyzed subtitles, and a decreased frequency of manual labor jobs and military\noccupations. The sentiment expressed toward lawyers, police, and doctors is\nbecoming negative over time, whereas astronauts, musicians, singers, and\nengineers are mentioned favorably. Professions that employ more people have\nincreased media frequency, supporting our hypothesis that media acts as a\nmirror to society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baruah_S/0/1/0/all/0/1\">Sabyasachee Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phone-to-audio alignment without text: A Semi-supervised Approach. (arXiv:2110.03876v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03876","description":"<p>The task of phone-to-audio alignment has many applications in speech\nresearch. Here we introduce two Wav2Vec2-based models for both text-dependent\nand text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a\nsemi-supervised model, directly learns phone-to-audio alignment through\ncontrastive learning and a forward sum loss, and can be coupled with a\npretrained phone recognizer to achieve text-independent alignment. The other\nmodel, Wav2Vec2-FC, is a frame classification model trained on forced aligned\nlabels that can both perform forced alignment and text-independent\nsegmentation. Evaluation results suggest that both proposed methods, even when\ntranscriptions are not available, generate highly close results to existing\nforced alignment tools. Our work presents a neural pipeline of fully automated\nphone-to-audio alignment. Code and pretrained models are available at\nhttps://github.com/lingjzhu/charsiu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining the Attention Mechanism of End-to-End Speech Recognition Using Decision Trees. (arXiv:2110.03879v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03879","description":"<p>The attention mechanism has largely improved the performance of end-to-end\nspeech recognition systems. However, the underlying behaviours of attention is\nnot yet clearer. In this study, we use decision trees to explain how the\nattention mechanism impact itself in speech recognition. The results indicate\nthat attention levels are largely impacted by their previous states rather than\nthe encoder and decoder patterns. Additionally, the default attention mechanism\nseems to put more weights on closer states, but behaves poorly on modelling\nlong-term dependencies of attention states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuanchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenji Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chenghao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanyan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. (arXiv:2110.03888v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03888","description":"<p>Recent expeditious developments in deep learning algorithms, distributed\ntraining, and even hardware design for large models have enabled training\nextreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of\nbillions or even trillions of parameters. However, under limited resources,\nextreme-scale model training that requires enormous amounts of computes and\nmemory footprint suffers from frustratingly low efficiency in model\nconvergence. In this paper, we propose a simple training strategy called\n\"Pseudo-to-Real\" for high-memory-footprint-required large models.\nPseudo-to-Real is compatible with large models with architecture of sequential\nlayers. We demonstrate a practice of pretraining unprecedented\n10-trillion-parameter model, an order of magnitude larger than the\nstate-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the\napplication of Pseudo-to-Real, we also provide a technique, Granular CPU\noffloading, to manage CPU memory for training large model and maintain high GPU\nutilities. Fast training of extreme-scale models on a decent amount of\nresources can bring much smaller carbon footprint and contribute to greener AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinze Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer Assessments. (arXiv:2110.03895v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03895","description":"<p>Peer assessment has been widely applied across diverse academic fields over\nthe last few decades and has demonstrated its effectiveness. However, the\nadvantages of peer assessment can only be achieved with high-quality peer\nreviews. Previous studies have found that high-quality review comments usually\ncomprise several features (e.g., contain suggestions, mention problems, use a\npositive tone). Thus, researchers have attempted to evaluate peer-review\ncomments by detecting different features using various machine learning and\ndeep learning models. However, there is no single study that investigates using\na multi-task learning (MTL) model to detect multiple features simultaneously.\nThis paper presents two MTL models for evaluating peer-review comments by\nleveraging the state-of-the-art pre-trained language representation models BERT\nand DistilBERT. Our results demonstrate that BERT-based models significantly\noutperform previous GloVe-based methods by around 6% in F1-score on tasks of\ndetecting a single feature, and MTL further improves performance while reducing\nmodel size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qinjin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jialin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yunkai Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_P/0/1/0/all/0/1\">Parvez Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehringer_E/0/1/0/all/0/1\">Edward F. Gehringer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CheerBots: Chatbots toward Empathy and Emotionusing Reinforcement Learning. (arXiv:2110.03949v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03949","description":"<p>Apart from the coherence and fluency of responses, an empathetic chatbot\nemphasizes more on people's feelings. By considering altruistic behaviors\nbetween human interaction, empathetic chatbots enable people to get a better\ninteractive and supportive experience. This study presents a framework whereby\nseveral empathetic chatbots are based on understanding users' implied feelings\nand replying empathetically for multiple dialogue turns. We call these chatbots\nCheerBots. CheerBots can be retrieval-based or generative-based and were\nfinetuned by deep reinforcement learning. To respond in an empathetic way, we\ndevelop a simulating agent, a Conceptual Human Model, as aids for CheerBots in\ntraining with considerations on changes in user's emotional states in the\nfuture to arouse sympathy. Finally, automatic metrics and human rating results\ndemonstrate that CheerBots outperform other baseline chatbots and achieves\nreciprocal altruism. The code and the pre-trained models will be made\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jhan_J/0/1/0/all/0/1\">Jiun-Hao Jhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao-Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeng_S/0/1/0/all/0/1\">Shyh-Kang Jeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceived and Intended Sarcasm Detection with Graph Attention Networks. (arXiv:2110.04001v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04001","description":"<p>Existing sarcasm detection systems focus on exploiting linguistic markers,\ncontext, or user-level priors. However, social studies suggest that the\nrelationship between the author and the audience can be equally relevant for\nthe sarcasm usage and interpretation. In this work, we propose a framework\njointly leveraging (1) a user context from their historical tweets together\nwith (2) the social information from a user's conversational neighborhood in an\ninteraction graph, to contextualize the interpretation of the post. We use\ngraph attention networks (GAT) over users and tweets in a conversation thread,\ncombined with dense user history representations. Apart from achieving\nstate-of-the-art results on the recently published dataset of 19k Twitter users\nwith 30K labeled tweets, adding 10M unlabeled tweets as context, our results\nindicate that the model contributes to interpreting the sarcastic intentions of\nan author more than to predicting the sarcasm perception by others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plepi_J/0/1/0/all/0/1\">Joan Plepi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Math-Aware Automated Classification and Similarity Search of Scientific Publications: Methods of Mathematical Content Representations. (arXiv:2110.04040v1 [cs.IR])","link":"http://arxiv.org/abs/2110.04040","description":"<p>In this paper, we investigate mathematical content representations suitable\nfor the automated classification of and the similarity search in STEM documents\nusing standard machine learning algorithms: the Latent Dirichlet Allocation\n(LDA) and the Latent Semantic Indexing (LSI). The methods are evaluated on a\nsubset of arXiv.org papers with the Mathematics Subject Classification (MSC) as\na reference classification and using the standard precision/recall/F1-measure\nmetrics. The results give insight into how different math representations may\ninfluence the performance of the classification and similarity search tasks in\nSTEM repositories. Non-surprisingly, machine learning methods are able to grab\ndistributional semantics from textual tokens. A proper selection of weighted\ntokens representing math may improve the quality of the results slightly. A\nstructured math representation that imitates successful text-processing\ntechniques with math is shown to yield better results than flat TeX tokens.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R%5Cr%7Bu%7Dzicka_M/0/1/0/all/0/1\">Michal R&#x16f;&#x17e;i&#x10d;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1\">Petr Sojka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Do Things without Words: Modeling Semantic Drift of Emoji. (arXiv:2110.04093v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04093","description":"<p>Emoji have become a significant part of our informal textual communication.\nPrevious work addressing the societal and linguistic functions of emoji\noverlook the evolving meaning of the symbol. This evolution could be addressed\nthrough the framework of semantic drifts. In this paper we model and analyze\nthe semantic drift of emoji and discuss the features that may be contributing\nto the drift, some are unique to emoji and some are more general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arviv_E/0/1/0/all/0/1\">Eyal Arviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsur_O/0/1/0/all/0/1\">Oren Tsur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Conditional End-to-End ASR with CTC and Multi-Granular Subword Units. (arXiv:2110.04109v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04109","description":"<p>In end-to-end automatic speech recognition (ASR), a model is expected to\nimplicitly learn representations suitable for recognizing a word-level\nsequence. However, the huge abstraction gap between input acoustic signals and\noutput linguistic tokens makes it challenging for a model to learn the\nrepresentations. In this work, to promote the word-level representation\nlearning in end-to-end ASR, we propose a hierarchical conditional model that is\nbased on connectionist temporal classification (CTC). Our model is trained by\nauxiliary CTC losses applied to intermediate layers, where the vocabulary size\nof each target subword sequence is gradually increased as the layer becomes\nclose to the word-level output. Here, we make each level of sequence prediction\nexplicitly conditioned on the previous sequences predicted at lower levels.\nWith the proposed approach, we expect the proposed model to learn the\nword-level representations effectively by exploiting a hierarchy of linguistic\nstructures. Experimental results on LibriSpeech-{100h, 960h} and TEDLIUM2\ndemonstrate that the proposed model improves over a standard CTC-based model\nand other competitive models from prior work. We further analyze the results to\nconfirm the effectiveness of the intended representation learning with our\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karube_K/0/1/0/all/0/1\">Keita Karube</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogawa_T/0/1/0/all/0/1\">Tetsuji Ogawa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_T/0/1/0/all/0/1\">Tetsunori Kobayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I Do Not Understand What I Cannot Define: Automatic Question Generation With Pedagogically-Driven Content Selection. (arXiv:2110.04123v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04123","description":"<p>Most learners fail to develop deep text comprehension when reading textbooks\npassively. Posing questions about what learners have read is a well-established\nway of fostering their text comprehension. However, many textbooks lack\nself-assessment questions because authoring them is timeconsuming and\nexpensive. Automatic question generators may alleviate this scarcity by\ngenerating sound pedagogical questions. However, generating questions\nautomatically poses linguistic and pedagogical challenges. What should we ask?\nAnd, how do we phrase the question automatically? We address those challenges\nwith an automatic question generator grounded in learning theory. The paper\nintroduces a novel pedagogically meaningful content selection mechanism to find\nquestion-worthy sentences and answers in arbitrary textbook contents. We\nconducted an empirical evaluation study with educational experts, annotating\n150 generated questions in six different domains. Results indicate a high\nlinguistic quality of the generated questions. Furthermore, the evaluation\nresults imply that the majority of the generated questions inquire central\ninformation related to the given text and may foster text comprehension in\nspecific learning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steuer_T/0/1/0/all/0/1\">Tim Steuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filighera_A/0/1/0/all/0/1\">Anna Filighera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1\">Tobias Meuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rensing_C/0/1/0/all/0/1\">Christoph Rensing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text analysis and deep learning: A network approach. (arXiv:2110.04151v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04151","description":"<p>Much information available to applied researchers is contained within written\nlanguage or spoken text. Deep language models such as BERT have achieved\nunprecedented success in many applications of computational linguistics.\nHowever, much less is known about how these models can be used to analyze\nexisting text. We propose a novel method that combines transformer models with\nnetwork analysis to form a self-referential representation of language use\nwithin a corpus of interest. Our approach produces linguistic relations\nstrongly consistent with the underlying model as well as mathematically\nwell-defined operations on them, while reducing the amount of discretionary\nchoices of representation and distance measures. It represents, to the best of\nour knowledge, the first unsupervised method to extract semantic networks\ndirectly from deep language models. We illustrate our approach in a semantic\nanalysis of the term \"founder\". Using the entire corpus of Harvard Business\nReview from 1980 to 2020, we find that ties in our network track the semantics\nof discourse over time, and across contexts, identifying and relating clusters\nof semantic and syntactic relations. Finally, we discuss how this method can\nalso complement and inform analyses of the behavior of deep learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marquart_I/0/1/0/all/0/1\">Ingo Marquart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Decoding for Compositional Generalization in Transformers. (arXiv:2110.04169v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04169","description":"<p>Deep learning models do well at generalizing to in-distribution data but\nstruggle to generalize compositionally, i.e., to combine a set of learned\nprimitives to solve more complex tasks. In particular, in sequence-to-sequence\n(seq2seq) learning, transformers are often unable to predict correct outputs\nfor even marginally longer examples than those seen during training. This paper\nintroduces iterative decoding, an alternative to seq2seq learning that (i)\nimproves transformer compositional generalization and (ii) evidences that, in\ngeneral, seq2seq transformers do not learn iterations that are not unrolled.\nInspired by the idea of compositionality -- that complex tasks can be solved by\ncomposing basic primitives -- training examples are broken down into a sequence\nof intermediate steps that the transformer then learns iteratively. At\ninference time, the intermediate outputs are fed back to the transformer as\nintermediate inputs until an end-of-iteration token is predicted. Through\nnumerical experiments, we show that transfomers trained via iterative decoding\noutperform their seq2seq counterparts on the PCFG dataset, and solve the\nproblem of calculating Cartesian products between vectors longer than those\nseen during training with 100% accuracy, a task at which seq2seq models have\nbeen shown to fail. We also illustrate a limitation of iterative decoding,\nspecifically, that it can make sorting harder to learn on the CFQ dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_L/0/1/0/all/0/1\">Luana Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of an Extractive Title Generation System Using Titles of Papers of Top Conferences for Intermediate English Students. (arXiv:2110.04204v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04204","description":"<p>The formulation of good academic paper titles in English is challenging for\nintermediate English authors (particularly students). This is because such\nauthors are not aware of the type of titles that are generally in use. We aim\nto realize a support system for formulating more effective English titles for\nintermediate English and beginner authors. This study develops an extractive\ntitle generation system that formulates titles from keywords extracted from an\nabstract. Moreover, we realize a title evaluation model that can evaluate the\nappropriateness of paper titles. We train the model with titles of\ntop-conference papers by using BERT. This paper describes the training data,\nimplementation, and experimental results. The results show that our evaluation\nmodel can identify top-conference titles more effectively than intermediate\nEnglish and beginner students.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaku_K/0/1/0/all/0/1\">Kento Kaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_M/0/1/0/all/0/1\">Masato Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozono_T/0/1/0/all/0/1\">Tadachika Ozono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shintani_T/0/1/0/all/0/1\">Toramatsu Shintani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive String Representation Learning using Synthetic Data. (arXiv:2110.04217v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04217","description":"<p>String representation Learning (SRL) is an important task in the field of\nNatural Language Processing, but it remains under-explored. The goal of SRL is\nto learn dense and low-dimensional vectors (or embeddings) for encoding\ncharacter sequences. The learned representation from this task can be used in\nmany downstream application tasks such as string similarity matching or lexical\nnormalization. In this paper, we propose a new method for to train a SRL model\nby only using synthetic data. Our approach makes use of Contrastive Learning in\norder to maximize similarity between related strings while minimizing it for\nunrelated strings. We demonstrate the effectiveness of our approach by\nevaluating the learned representation on the task of string similarity\nmatching. Codes, data and pretrained models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1\">Urchade Zaratiana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"lambeq: An Efficient High-Level Python Library for Quantum NLP. (arXiv:2110.04236v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04236","description":"<p>We present lambeq, the first high-level Python library for Quantum Natural\nLanguage Processing (QNLP). The open-source toolkit offers a detailed hierarchy\nof modules and classes implementing all stages of a pipeline for converting\nsentences to string diagrams, tensor networks, and quantum circuits ready to be\nused on a quantum computer. lambeq supports syntactic parsing, rewriting and\nsimplification of string diagrams, ansatz creation and manipulation, as well as\na number of compositional models for preparing quantum-friendly representations\nof sentences, employing various degrees of syntax sensitivity. We present the\ngeneric architecture and describe the most important modules in detail,\ndemonstrating the usage with illustrative examples. Further, we test the\ntoolkit in practice by using it to perform a number of experiments on simple\nNLP tasks, implementing both classical and quantum pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kartsaklis_D/0/1/0/all/0/1\">Dimitri Kartsaklis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_I/0/1/0/all/0/1\">Ian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_R/0/1/0/all/0/1\">Richie Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearson_A/0/1/0/all/0/1\">Anna Pearson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorenz_R/0/1/0/all/0/1\">Robin Lorenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1\">Stephen Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VieSum: How Robust Are Transformer-based Models on Vietnamese Summarization?. (arXiv:2110.04257v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04257","description":"<p>Text summarization is a challenging task within natural language processing\nthat involves text generation from lengthy input sequences. While this task has\nbeen widely studied in English, there is very limited research on summarization\nfor Vietnamese text. In this paper, we investigate the robustness of\ntransformer-based encoder-decoder architectures for Vietnamese abstractive\nsummarization. Leveraging transfer learning and self-supervised learning, we\nvalidate the performance of the methods on two Vietnamese datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hieu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anibal_J/0/1/0/all/0/1\">James Anibal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peltekian_A/0/1/0/all/0/1\">Alec Peltekian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04260","description":"<p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\neasily scale to have outrageously large amounts of parameters without\nsignificant increase in computational cost. However, SAMs are reported to be\nparameter inefficient such that larger models do not always lead to better\nperformance. While most on-going research focuses on improving SAMs models by\nexploring methods of routing inputs to experts, our analysis reveals that such\nresearch might not lead to the solution we expect, i.e., the commonly-used\nrouting methods based on gating mechanisms do not work better than randomly\nrouting inputs to experts. In this paper, we propose a new expert-based model,\nTHOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,\nsuch as the Switch Transformer, experts in THOR are randomly activated for each\ninput during training and inference. THOR models are trained using a\nconsistency regularized loss, where experts learn not only from training data\nbut also from other experts as teachers, such that all the experts make\nconsistent predictions. We validate the effectiveness of THOR on machine\ntranslation tasks. Results show that THOR models are more parameter efficient\nin that they significantly outperform the Transformer and MoE models across\nvarious settings. For example, in multilingual translation, THOR outperforms\nthe Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as\nthat of a state-of-the-art MoE model that is 18 times larger. Our code is\npublicly available at: github.com/microsoft/Stochastic-Mixture-of-Experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Heterogeneous Characteristics of Layers in ASR Models for More Efficient Training. (arXiv:2110.04267v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04267","description":"<p>Transformer-based architectures have been the subject of research aimed at\nunderstanding their overparameterization and the non-uniform importance of\ntheir layers. Applying these approaches to Automatic Speech Recognition, we\ndemonstrate that the state-of-the-art Conformer models generally have multiple\nambient layers. We study the stability of these layers across runs and model\nsizes, propose that group normalization may be used without disrupting their\nformation, and examine their correlation with model weight updates in each\nlayer. Finally, we apply these findings to Federated Learning in order to\nimprove the training procedure, by targeting Federated Dropout to layers by\nimportance. This allows us to reduce the model size optimized by clients\nwithout quality degradation, and shows potential for future exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lillian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guliani_D/0/1/0/all/0/1\">Dhruv Guliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabel_A/0/1/0/all/0/1\">Andreas Kabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motta_G/0/1/0/all/0/1\">Giovanni Motta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local and Global Context-Based Pairwise Models for Sentence Ordering. (arXiv:2110.04291v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04291","description":"<p>Sentence Ordering refers to the task of rearranging a set of sentences into\nthe appropriate coherent order. For this task, most previous approaches have\nexplored global context-based end-to-end methods using Sequence Generation\ntechniques. In this paper, we put forward a set of robust local and global\ncontext-based pairwise ordering strategies, leveraging which our prediction\nstrategies outperform all previous works in this domain. Our proposed encoding\nmethod utilizes the paragraph's rich global contextual information to predict\nthe pairwise order using novel transformer architectures. Analysis of the two\nproposed decoding strategies helps better explain error propagation in pairwise\nmodels. This approach is the most accurate pure pairwise model and our encoding\nstrategy also significantly improves the performance of other recent approaches\nthat use pairwise models, including the previous state-of-the-art,\ndemonstrating the research novelty and generalizability of this work.\nAdditionally, we show how the pre-training task for ALBERT helps it to\nsignificantly outperform BERT, despite having considerably lesser parameters.\nThe extensive experimental results, architectural analysis and ablation studies\ndemonstrate the effectiveness and superiority of the proposed models compared\nto the previous state-of-the-art, besides providing a much better understanding\nof the functioning of pairwise models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manku_R/0/1/0/all/0/1\">Ruskin Raj Manku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Aditya Jyoti Paul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From SCAN to Real Data: Systematic Generalization via Meaningful Learning. (arXiv:2003.06658v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2003.06658","description":"<p>Humans can systematically generalize to novel compositions of existing\nconcepts. There have been extensive conjectures into the extent to which neural\nnetworks can do the same. Recent arguments supported by evidence on the SCAN\ndataset claim that neural networks are inherently ineffective in such cognitive\ncapacity. In this paper, we revisit systematic generalization from the\nperspective of meaningful learning, an exceptional capability of humans to\nlearn new concepts by connecting them with other previously known knowledge. We\npropose to augment a training dataset in either an inductive or deductive\nmanner to build semantic links between new and old concepts. Our observations\non SCAN suggest that, following the meaningful learning principle, modern\nsequence-to-sequence models, including RNNs, CNNs, and Transformers, can\nsuccessfully generalize to compositions of new concepts. We further validate\nour findings on two real-world datasets on semantic parsing and consistent\ncompositional generalization is also observed. Moreover, our experiments\ndemonstrate that both prior knowledge and semantic linking play a key role to\nachieve systematic generalization. Meanwhile, inductive learning generally\nworks better than deductive learning in our experiments. Finally, we provide an\nexplanation for data augmentation techniques by concluding them into either\ninductive-based or deductive-based meaningful learning. We hope our findings\nwill encourage excavating existing neural networks' potential in systematic\ngeneralization through more advanced learning schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinbing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"English Machine Reading Comprehension Datasets: A Survey. (arXiv:2101.10421v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.10421","description":"<p>This paper surveys 60 English Machine Reading Comprehension datasets, with a\nview to providing a convenient resource for other researchers interested in\nthis problem. We categorize the datasets according to their question and answer\nform and compare them across various dimensions including size, vocabulary,\ndata source, method of creation, human performance level, and first question\nword. Our analysis reveals that Wikipedia is by far the most common data source\nand that there is a relative lack of why, when, and where questions across\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dzendzik_D/0/1/0/all/0/1\">Daria Dzendzik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_C/0/1/0/all/0/1\">Carl Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jennifer Foster</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention. (arXiv:2103.15722v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2103.15722","description":"<p>Self-attention (SA), which encodes vector sequences according to their\npairwise similarity, is widely used in speech recognition due to its strong\ncontext modeling ability. However, when applied to long sequence data, its\naccuracy is reduced. This is caused by the fact that its weighted average\noperator may lead to the dispersion of the attention distribution, which\nresults in the relationship between adjacent signals ignored. To address this\nissue, in this paper, we introduce relative-position-awareness self-attention\n(RPSA). It not only maintains the global-range dependency modeling ability of\nself-attention, but also improves the localness modeling ability. Because the\nlocal window length of the original RPSA is fixed and sensitive to different\ntest data, here we propose Gaussian-based self-attention (GSA) whose window\nlength is learnable and adaptive to the test data automatically. We further\ngeneralize GSA to a new residual Gaussian self-attention (resGSA) for the\nperformance improvement. We apply RPSA, GSA, and resGSA to Transformer-based\nspeech recognition respectively. Experimental results on the AISHELL-1 Mandarin\nspeech recognition corpus demonstrate the effectiveness of the proposed\nmethods. For example, the resGSA-Transformer achieves a character error rate\n(CER) of 5.86% on the test set, which is relative 7.8% lower than that of the\nSA-Transformer. Although the performance of the proposed resGSA-Transformer is\nonly slightly better than that of the RPSA-Transformer, it does not have to\ntune the window length manually.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chengdong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relaxing the Conditional Independence Assumption of CTC-based ASR by Conditioning on Intermediate Predictions. (arXiv:2104.02724v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2104.02724","description":"<p>This paper proposes a method to relax the conditional independence assumption\nof connectionist temporal classification (CTC)-based automatic speech\nrecognition (ASR) models. We train a CTC-based ASR model with auxiliary CTC\nlosses in intermediate layers in addition to the original CTC loss in the last\nlayer. During both training and inference, each generated prediction in the\nintermediate layers is summed to the input of the next layer to condition the\nprediction of the last layer on those intermediate predictions. Our method is\neasy to implement and retains the merits of CTC-based ASR: a simple model\narchitecture and fast decoding speed. We conduct experiments on three different\nASR corpora. Our proposed method improves a standard CTC model significantly\n(e.g., more than 20 % relative word error rate reduction on the WSJ corpus)\nwith a little computational overhead. Moreover, for the TEDLIUM2 corpus and the\nAISHELL-1 corpus, it achieves a comparable performance to a strong\nautoregressive model with beam search, but the decoding speed is at least 30\ntimes faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nozaki_J/0/1/0/all/0/1\">Jumon Nozaki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Komatsu_T/0/1/0/all/0/1\">Tatsuya Komatsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly Supervised Dataset of Fine-Grained Emotions in Portuguese. (arXiv:2108.07638v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07638","description":"<p>Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task inNLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weakly supervised corpus for fine-grained emotion in Portuguese.\nWe evaluated our dataset by fine-tuning a transformer-based language model\n(BERT) and validating it on a Gold Standard annotated validation set. Our\nresults (F1-score=.64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resourced environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cortiz_D/0/1/0/all/0/1\">Diogo Cortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jefferson O. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calegari_N/0/1/0/all/0/1\">Newton Calegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Ana Lu&#xed;sa Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Ana Ang&#xe9;lica Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botelho_C/0/1/0/all/0/1\">Carolina Botelho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_G/0/1/0/all/0/1\">Gabriel Gaudencio R&#xea;go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampaio_W/0/1/0/all/0/1\">Waldir Sampaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggio_P/0/1/0/all/0/1\">Paulo Sergio Boggio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04620","description":"<p>Current approaches to incorporating terminology constraints in machine\ntranslation (MT) typically assume that the constraint terms are provided in\ntheir correct morphological forms. This limits their application to real-world\nscenarios where constraint terms are provided as lemmas. In this paper, we\nintroduce a modular framework for incorporating lemma constraints in neural MT\n(NMT) in which linguistic knowledge and diverse types of NMT models can be\nflexibly applied. It is based on a novel cross-lingual inflection module that\ninflects the target lemma constraints based on the source context. We explore\nlinguistically motivated rule-based and data-driven neural-based inflection\nmodules and design English-German health and English-Lithuanian news test\nsuites to evaluate them in domain adaptation and low-resource MT settings.\nResults show that our rule-based inflection module helps NMT models incorporate\nlemma constraints more accurately than a neural module and outperforms the\nexisting end-to-end approach with lower training costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. (arXiv:2109.05729v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05729","description":"<p>In this paper, we take the advantage of previous pre-trained models (PTMs)\nand propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different\nfrom previous Chinese PTMs, CPT is designed for both natural language\nunderstanding (NLU) and natural language generation (NLG) tasks. CPT consists\nof three parts: a shared encoder, an understanding decoder, and a generation\ndecoder. Two specific decoders with a shared encoder are pre-trained with\nmasked language modeling (MLM) and denoising auto-encoding (DAE) tasks,\nrespectively. With the partially shared architecture and multi-task\npre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks\nwith two decoders and (2) be fine-tuned flexibly that fully exploits the\npotential of the model. Moreover, the unbalanced Transformer saves the\ncomputational and storage cost, which makes CPT competitive and greatly\naccelerates the inference of text generation. Experimental results on a wide\nrange of Chinese NLU and NLG tasks show the effectiveness of CPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_L/0/1/0/all/0/1\">Li Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Critic: Language Models for Unsupervised Grammatical Error Correction. (arXiv:2109.06822v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06822","description":"<p>Training a model for grammatical error correction (GEC) requires a set of\nlabeled ungrammatical / grammatical sentence pairs, but manually annotating\nsuch pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has\ndemonstrated strong results on learning to repair a broken program without any\nlabeled examples, but this relies on a perfect critic (e.g., a compiler) that\nreturns whether an example is valid or not, which does not exist for the GEC\ntask. In this work, we show how to leverage a pretrained language model (LM) in\ndefining an LM-Critic, which judges a sentence to be grammatical if the LM\nassigns it a higher probability than its local perturbations. We apply this\nLM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap\nrealistic ungrammatical / grammatical pairs for training a corrector. We\nevaluate our approach on GEC datasets across multiple domains (CoNLL-2014,\nBEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing\nmethods in both the unsupervised setting (+7.7 F0.5) and the supervised setting\n(+0.5 F0.5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models. (arXiv:2109.11797v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11797","description":"<p>Pre-Trained Vision-Language Models (VL-PTMs) have shown promising\ncapabilities in grounding natural language in image data, facilitating a broad\nvariety of cross-modal tasks. However, we note that there exists a significant\ngap between the objective forms of model pre-training and fine-tuning,\nresulting in a need for large amounts of labeled data to stimulate the visual\ngrounding capability of VL-PTMs for downstream tasks. To address the challenge,\nwe present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt\nTuning), a novel paradigm for tuning VL-PTMs, which reformulates visual\ngrounding into a fill-in-the-blank problem with color-based co-referential\nmarkers in image and text, maximally mitigating the gap. In this way, CPT\nenables strong few-shot and even zero-shot visual grounding capabilities of\nVL-PTMs. Comprehensive experimental results show that the prompt-tuned VL-PTMs\noutperform their fine-tuned counterparts by a large margin (e.g., 17.3%\nabsolute accuracy improvement, and 73.8% relative standard deviation reduction\non average with one shot in RefCOCO evaluation). All the data and codes will be\navailable to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoodChem: A food-chemical relation extraction model. (arXiv:2110.02019v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02019","description":"<p>In this paper, we present FoodChem, a new Relation Extraction (RE) model for\nidentifying chemicals present in the composition of food entities, based on\ntextual information provided in biomedical peer-reviewed scientific literature.\nThe RE task is treated as a binary classification problem, aimed at identifying\nwhether the contains relation exists between a food-chemical entity pair. This\nis accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models.\nFor evaluation purposes, a novel dataset with annotated contains relations in\nfood-chemical entity pairs is generated, in a golden and silver version. The\nmodels are integrated into a voting scheme in order to produce the silver\nversion of the dataset which we use for augmenting the individual models, while\nthe manually annotated golden version is used for their evaluation. Out of the\nthree evaluated models, the BioBERT model achieves the best results, with a\nmacro averaged F1 score of 0.902 in the unbalanced augmentation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cenikj_G/0/1/0/all/0/1\">Gjorgjina Cenikj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seljak_B/0/1/0/all/0/1\">Barbara Korou&#x161;i&#x107; Seljak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eftimov_T/0/1/0/all/0/1\">Tome Eftimov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Conditional Text Generation for Aspect-Based Sentiment Analysis. (arXiv:2110.02334v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02334","description":"<p>Aspect-based sentiment analysis (ABSA) is an NLP task that entails processing\nuser-generated reviews to determine (i) the target being evaluated, (ii) the\naspect category to which it belongs, and (iii) the sentiment expressed towards\nthe target and aspect pair. In this article, we propose transforming ABSA into\nan abstract summary-like conditional text generation task that uses targets,\naspects, and polarities to generate auxiliary statements. To demonstrate the\nefficacy of our task formulation and a proposed system, we fine-tune a\npre-trained model for conditional text generation tasks to get new\nstate-of-the-art results on a few restaurant domains and urban neighborhoods\ndomain benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1\">Siva Uday Sampreeth Chebolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cut the CARP: Fishing for zero-shot story evaluation. (arXiv:2110.03111v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03111","description":"<p>Recent advances in large-scale language models (Raffel et al., 2019; Brown et\nal., 2020) have brought significant qualitative and quantitative improvements\nin machine-driven text generation. Despite this, generation and evaluation of\nmachine-generated narrative text remains a challenging problem. Objective\nevaluation of computationally-generated stories may be prohibitively expensive,\nrequire meticulously annotated datasets, or may not adequately measure the\nlogical coherence of a generated story's narratological structure.\n</p>\n<p>Informed by recent advances in contrastive learning (Radford et al., 2021),\nwe present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,\nefficient method for performing qualitatively superior, zero-shot evaluation of\nstories. We show a strong correlation between human evaluation of stories and\nthose of CARP. Model outputs more significantly correlate with corresponding\nhuman input than those language-model based methods which utilize finetuning or\nprompt engineering approaches. We also present and analyze the Story-Critique\nDataset, a new corpora composed of 1.3 million aligned story-critique pairs\nderived from over 80,000 stories. We expect this corpus to be of interest to\nNLP researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1\">Shahbuland Matiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">JR Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1\">Ryan Teehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_S/0/1/0/all/0/1\">Spencer Frazier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual Knowledge Learning of Language Models. (arXiv:2110.03215v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03215","description":"<p>Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Janghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeonghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Stanley Jungkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Logic-Based Framework for Natural Language Inference in Dutch. (arXiv:2110.03323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03323","description":"<p>We present a framework for deriving inference relations between Dutch\nsentence pairs. The proposed framework relies on logic-based reasoning to\nproduce inspectable proofs leading up to inference labels; its judgements are\ntherefore transparent and formally verifiable. At its core, the system is\npowered by two ${\\lambda}$-calculi, used as syntactic and semantic theories,\nrespectively. Sentences are first converted to syntactic proofs and terms of\nthe linear ${\\lambda}$-calculus using a choice of two parsers: an Alpino-based\npipeline, and Neural Proof Nets. The syntactic terms are then converted to\nsemantic terms of the simply typed ${\\lambda}$-calculus, via a set of hand\ndesigned type- and term-level transformations. Pairs of semantic terms are then\nfed to an automated theorem prover for natural logic which reasons with them\nwhile using lexical relations found in the Open Dutch WordNet. We evaluate the\nreasoning pipeline on the recently created Dutch natural language inference\ndataset, and achieve promising results, remaining only within a $1.1-3.2{\\%}$\nperformance margin to strong neural baselines. To the best of our knowledge,\nthe reasoning pipeline is the first logic-based system for Dutch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abzianidze_L/0/1/0/all/0/1\">Lasha Abzianidze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03611","description":"<p>Current dense text retrieval models face two typical challenges. First, it\nadopts a siamese dual-encoder architecture to encode query and document\nindependently for fast indexing and searching, whereas neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, it highly relies on a negative sampling technique to build\nup the negative documents in its contrastive loss. To address these challenges,\nwe present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder\nretriever plus a cross-encoder ranker. The two models are jointly optimized\naccording to a minimax adversarial objective: the retriever learns to retrieve\nnegative documents to cheat the ranker, while the ranker learns to rank a\ncollection of candidates including both the ground-truth and the retrieved\nones, as well as providing progressive direct feedback to the dual-encoder\nretriever. Through this adversarial game, the retriever gradually produces\nharder negative documents to train a better ranker, whereas the cross-encoder\nranker provides progressive feedback to improve retriever. We evaluate AR2 on\nthree benchmarks. Experimental results show that AR2 consistently and\nsignificantly outperforms existing dense retriever methods and achieves new\nstate-of-the-art results on all of them. This includes the improvements on\nNatural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and\nMS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}