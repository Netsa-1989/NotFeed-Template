{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Generative Relation Linking for Question Answering over Knowledge Bases. (arXiv:2108.07337v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07337","description":"<p>Relation linking is essential to enable question answering over knowledge\nbases. Although there are various efforts to improve relation linking\nperformance, the current state-of-the-art methods do not achieve optimal\nresults, therefore, negatively impacting the overall end-to-end question\nanswering performance. In this work, we propose a novel approach for relation\nlinking framing it as a generative problem facilitating the use of pre-trained\nsequence-to-sequence models. We extend such sequence-to-sequence models with\nthe idea of infusing structured data from the target knowledge base, primarily\nto enable these models to handle the nuances of the knowledge base. Moreover,\nwe train the model with the aim to generate a structured output consisting of a\nlist of argument-relation pairs, enabling a knowledge validation step. We\ncompared our method against the existing relation linking systems on four\ndifferent datasets derived from DBpedia and Wikidata. Our method reports large\nimprovements over the state-of-the-art while using a much simpler model that\ncan be easily adapted to different knowledge bases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bornea_M/0/1/0/all/0/1\">Mihaela Bornea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IsoScore: Measuring the Uniformity of Vector Space Utilization. (arXiv:2108.07344v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07344","description":"<p>The recent success of distributed word representations has led to an\nincreased interest in analyzing the properties of their spatial distribution.\nCurrent metrics suggest that contextualized word embedding models do not\nuniformly utilize all dimensions when embedding tokens in vector space. Here we\nargue that existing metrics are fragile and tend to obfuscate the true spatial\ndistribution of point clouds. To ameliorate this issue, we propose IsoScore: a\nnovel metric which quantifies the degree to which a point cloud uniformly\nutilizes the ambient vector space. We demonstrate that IsoScore has several\ndesirable properties such as mean invariance and direct correspondence to the\nnumber of dimensions used, which are properties that existing scores do not\npossess. Furthermore, IsoScore is conceptually intuitive and computationally\nefficient, making it well suited for analyzing the distribution of point clouds\nin arbitrary vector spaces, not necessarily limited to those of word embeddings\nalone. Additionally, we use IsoScore to demonstrate that a number of recent\nconclusions in the NLP literature that have been derived using brittle metrics\nof spatial distribution, such as average cosine similarity, may be incomplete\nor altogether inaccurate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rudman_W/0/1/0/all/0/1\">William Rudman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillman_N/0/1/0/all/0/1\">Nate Gillman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayne_T/0/1/0/all/0/1\">Taylor Rayne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An NLP approach to quantify dynamic salience of predefined topics in a text corpus. (arXiv:2108.07345v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07345","description":"<p>The proliferation of news media available online simultaneously presents a\nvaluable resource and significant challenge to analysts aiming to profile and\nunderstand social and cultural trends in a geographic location of interest.\nWhile an abundance of news reports documenting significant events, trends, and\nresponses provides a more democratized picture of the social characteristics of\na location, making sense of an entire corpus to extract significant trends is a\nsteep challenge for any one analyst or team. Here, we present an approach using\nnatural language processing techniques that seeks to quantify how a set of\npre-defined topics of interest change over time across a large corpus of text.\nWe found that, given a predefined topic, we can identify and rank sets of\nterms, or n-grams, that map to those topics and have usage patterns that\ndeviate from a normal baseline. Emergence, disappearance, or significant\nvariations in n-gram usage present a ground-up picture of a topic's dynamic\nsalience within a corpus of interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bock_A/0/1/0/all/0/1\">A. Bock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palladino_A/0/1/0/all/0/1\">A. Palladino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_Heisters_S/0/1/0/all/0/1\">S. Smith-Heisters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boardman_I/0/1/0/all/0/1\">I. Boardman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellegrini_E/0/1/0/all/0/1\">E. Pellegrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bienenstock_E/0/1/0/all/0/1\">E.J. Bienenstock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenti_A/0/1/0/all/0/1\">A. Valenti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards. (arXiv:2108.07374v1 [cs.DB])","link":"http://arxiv.org/abs/2108.07374","description":"<p>Developing documentation guidelines and easy-to-use templates for datasets\nand models is a challenging task, especially given the variety of backgrounds,\nskills, and incentives of the people involved in the building of natural\nlanguage processing (NLP) tools. Nevertheless, the adoption of standard\ndocumentation practices across the field of NLP promotes more accessible and\ndetailed descriptions of NLP datasets and models, while supporting researchers\nand developers in reflecting on their work. To help with the standardization of\ndocumentation, we present two case studies of efforts that aim to develop\nreusable documentation templates -- the HuggingFace data card, a general\npurpose card for datasets in NLP, and the GEM benchmark data and model cards\nwith a focus on natural language generation. We describe our process for\ndeveloping these templates, including the identification of relevant\nstakeholder groups, the definition of a set of guiding principles, the use of\nexisting templates as our foundation, and iterative revisions based on\nfeedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osei_S/0/1/0/all/0/1\">Salomey Osei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan Diego Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanamanchi_P/0/1/0/all/0/1\">Pawan Sasanka Ammanamanchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Protein Using Large-scale Pretrain Language Model. (arXiv:2108.07435v1 [cs.LG])","link":"http://arxiv.org/abs/2108.07435","description":"<p>Protein is linked to almost every life process. Therefore, analyzing the\nbiological structure and property of protein sequences is critical to the\nexploration of life, as well as disease detection and drug discovery.\nTraditional protein analysis methods tend to be labor-intensive and\ntime-consuming. The emergence of deep learning models makes modeling data\npatterns in large quantities of data possible. Interdisciplinary researchers\nhave begun to leverage deep learning methods to model large biological\ndatasets, e.g. using long short-term memory and convolutional neural network\nfor protein sequence classification. After millions of years of evolution,\nevolutionary information is encoded in protein sequences. Inspired by the\nsimilarity between natural language and protein sequences, we use large-scale\nlanguage models to model evolutionary-scale protein sequences, encoding protein\nbiology information in representation. Significant improvements are observed in\nboth token-level and sequence-level tasks, demonstrating that our large-scale\nmodel can accurately capture evolution information from pretraining on\nevolutionary-scale individual sequences. Our code and model are available at\nhttps://github.com/THUDM/ProteinLM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yijia Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chang-Yu Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Light-weight contextual spelling correction model for customizing transducer-based speech recognition systems. (arXiv:2108.07493v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07493","description":"<p>It's challenging to customize transducer-based automatic speech recognition\n(ASR) system with context information which is dynamic and unavailable during\nmodel training. In this work, we introduce a light-weight contextual spelling\ncorrection model to correct context-related recognition errors in\ntransducer-based ASR systems. We incorporate the context information into the\nspelling correction model with a shared context encoder and use a filtering\nalgorithm to handle large-size context lists. Experiments show that the model\nimproves baseline ASR model performance with about 50% relative word error rate\nreduction, which also significantly outperforms the baseline method such as\ncontextual LM biasing. The model also shows excellent performance for\nout-of-vocabulary terms not seen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Guidelines for the Turku Paraphrase Corpus. (arXiv:2108.07499v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07499","description":"<p>This document describes the annotation guidelines used to construct the Turku\nParaphrase Corpus. These guidelines were developed together with the corpus\nannotation, revising and extending the guidelines regularly during the\nannotation work. Our paraphrase annotation scheme uses the base scale 1-4,\nwhere labels 1 and 2 are used for negative candidates (not paraphrases), while\nlabels 3 and 4 are paraphrases at least in the given context if not everywhere.\nIn addition to base labeling, the scheme is enriched with additional\nsubcategories (flags) for categorizing different types of paraphrases inside\nthe two positive labels, making the annotation scheme suitable for more\nfine-grained paraphrase categorization. The annotation scheme is used to\nannotate over 100,000 Finnish paraphrase pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanerva_J/0/1/0/all/0/1\">Jenna Kanerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Li-Hsin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastas_I/0/1/0/all/0/1\">Iiro Rastas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skantsi_V/0/1/0/all/0/1\">Valtteri Skantsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilpelainen_J/0/1/0/all/0/1\">Jemina Kilpel&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kupari_H/0/1/0/all/0/1\">Hanna-Mari Kupari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piirto_A/0/1/0/all/0/1\">Aurora Piirto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saarni_J/0/1/0/all/0/1\">Jenna Saarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevon_M/0/1/0/all/0/1\">Maija Sev&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarkka_O/0/1/0/all/0/1\">Otto Tarkka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPMoE: Generate Multiple Pattern-Aware Outputs with Sparse Pattern Mixture of Expert. (arXiv:2108.07535v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07535","description":"<p>Many generation tasks follow a one-to-many mapping relationship: each input\ncould be associated with multiple outputs. Existing methods like Conditional\nVariational AutoEncoder(CVAE) employ a latent variable to model this\none-to-many relationship. However, this high-dimensional and dense latent\nvariable lacks explainability and usually leads to poor and uncontrollable\ngenerations. In this paper, we innovatively introduce the linguistic concept of\npattern to decompose the one-to-many mapping into multiple one-to-one mappings\nand further propose a model named Sparse Pattern Mixture of Experts(SPMoE).\nEach one-to-one mapping is associated with a conditional generation pattern and\nis modeled with an expert in SPMoE. To ensure each language pattern can be\nexclusively handled with an expert model for better explainability and\ndiversity, a sparse mechanism is employed to coordinate all the expert models\nin SPMoE. We assess the performance of our SPMoE on the paraphrase generation\ntask and the experiment results prove that SPMoE can achieve a good balance in\nterms of quality, pattern-level diversity, and corpus-level diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xintong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Capsule Aggregation for Unaligned Multimodal Sequences. (arXiv:2108.07543v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07543","description":"<p>Humans express their opinions and emotions through multiple modalities which\nmainly consist of textual, acoustic and visual modalities. Prior works on\nmultimodal sentiment analysis mostly apply Recurrent Neural Network (RNN) to\nmodel aligned multimodal sequences. However, it is unpractical to align\nmultimodal sequences due to different sample rates for different modalities.\nMoreover, RNN is prone to the issues of gradient vanishing or exploding and it\nhas limited capacity of learning long-range dependency which is the major\nobstacle to model unaligned multimodal sequences. In this paper, we introduce\nGraph Capsule Aggregation (GraphCAGE) to model unaligned multimodal sequences\nwith graph-based neural model and Capsule Network. By converting sequence data\ninto graph, the previously mentioned problems of RNN are avoided. In addition,\nthe aggregation capability of Capsule Network and the graph-based structure\nenable our model to be interpretable and better solve the problem of long-range\ndependency. Experimental results suggest that GraphCAGE achieves\nstate-of-the-art performance on two benchmark datasets with representations\nrefined by Capsule Network and interpretation provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_S/0/1/0/all/0/1\">Sijie Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haifeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Linearizations Are Equally Data-Hungry in Sequence Labeling Parsing. (arXiv:2108.07556v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07556","description":"<p>Different linearizations have been proposed to cast dependency parsing as\nsequence labeling and solve the task as: (i) a head selection problem, (ii)\nfinding a representation of the token arcs as bracket strings, or (iii)\nassociating partial transition sequences of a transition-based parser to words.\nYet, there is little understanding about how these linearizations behave in\nlow-resource setups. Here, we first study their data efficiency, simulating\ndata-restricted setups from a diverse set of rich-resource treebanks. Second,\nwe test whether such differences manifest in truly low-resource setups. The\nresults show that head selection encodings are more data-efficient and perform\nbetter in an ideal (gold) framework, but that such advantage greatly vanishes\nin favour of bracketing formats when the running setup resembles a real-world\nlow-resource configuration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Munoz_Ortiz_A/0/1/0/all/0/1\">Alberto Mu&#xf1;oz-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strzyz_M/0/1/0/all/0/1\">Michalina Strzyz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1\">David Vilares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. (arXiv:2108.07571v1 [cs.IR])","link":"http://arxiv.org/abs/2108.07571","description":"<p>Citation recommendation is intended to assist researchers in the process of\nsearching for relevant papers to cite by recommending appropriate citations for\na given input text. Existing test collections for this task are noisy and\nunreliable since they are built automatically from parsed PDF papers. In this\npaper, we present our ongoing effort at creating a publicly available, manually\nannotated test collection for citation recommendation. We also conduct a series\nof experiments to evaluate the effectiveness of content-based baseline models\non the test collection, providing results for future work to improve upon. Our\ntest collection and code to replicate experiments are available at\nhttps://github.com/boudinfl/acm-cr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boudin_F/0/1/0/all/0/1\">Florian Boudin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MigrationsKB: A Knowledge Base of Public Attitudes towards Migrations and their Driving Factors. (arXiv:2108.07593v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07593","description":"<p>With the increasing trend in the topic of migration in Europe, the public is\nnow more engaged in expressing their opinions through various platforms such as\nTwitter. Understanding the online discourses is therefore essential to capture\nthe public opinion. The goal of this study is the analysis of social media\nplatform to quantify public attitudes towards migrations and the identification\nof different factors causing these attitudes. The tweets spanning from 2013 to\nJul-2021 in the European countries which are hosts to immigrants are collected,\npre-processed, and filtered using advanced topic modeling technique. BERT-based\nentity linking and sentiment analysis, and attention-based hate speech\ndetection are performed to annotate the curated tweets. Moreover, the external\ndatabases are used to identify the potential social and economic factors\ncausing negative attitudes of the people about migration. To further promote\nresearch in the interdisciplinary fields of social science and computer\nscience, the outcomes are integrated into a Knowledge Base (KB), i.e.,\nMigrationsKB which significantly extends the existing models to take into\naccount the public attitudes towards migrations and the economic indicators.\nThis KB is made public using FAIR principles, which can be queried through\nSPARQL endpoint. Data dumps are made available on Zenodo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sack_H/0/1/0/all/0/1\">Harald Sack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehwish Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study. (arXiv:2108.07627v1 [cs.CY])","link":"http://arxiv.org/abs/2108.07627","description":"<p>An independent ethical assessment of an artificial intelligence system is an\nimpartial examination of the system's development, deployment, and use in\nalignment with ethical values. System-level qualitative frameworks that\ndescribe high-level requirements and component-level quantitative metrics that\nmeasure individual ethical dimensions have been developed over the past few\nyears. However, there exists a gap between the two, which hinders the execution\nof independent ethical assessments in practice. This study bridges this gap and\ndesigns a holistic independent ethical assessment process for a text\nclassification model with a special focus on the task of hate speech detection.\nThe assessment is further augmented with protected attributes mining and\ncounterfactual-based analysis to enhance bias assessment. It covers assessments\nof technical performance, data bias, embedding bias, classification bias, and\ninterpretability. The proposed process is demonstrated through an assessment of\na deep hate speech detection model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amitoj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasekh_A/0/1/0/all/0/1\">Amin Rasekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golbin_I/0/1/0/all/0/1\">Ilana Golbin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anand Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weak Supervised Dataset of Fine-Grained Emotions in Portuguese. (arXiv:2108.07638v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07638","description":"<p>Affective Computing is the study of how computers can recognize, interpret\nand simulate human affects. Sentiment Analysis is a common task in NLP related\nto this topic, but it focuses only on emotion valence (positive, negative,\nneutral). An emerging approach in NLP is Emotion Recognition, which relies on\nfined-grained classification. This research describes an approach to create a\nlexical-based weak supervised corpus for fine-grained emotion in Portuguese. We\nevaluate our dataset by fine-tuning a transformer-based language model (BERT)\nand validating it on a Golden Standard annotated validation set. Our results\n(F1-score= .64) suggest lexical-based weak supervision as an appropriate\nstrategy for initial work in low resources environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cortiz_D/0/1/0/all/0/1\">Diogo Cortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jefferson O. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calegari_N/0/1/0/all/0/1\">Newton Calegari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Ana Lu&#xed;sa Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_A/0/1/0/all/0/1\">Ana Ang&#xe9;lica Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botelho_C/0/1/0/all/0/1\">Carolina Botelho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_G/0/1/0/all/0/1\">Gabriel Gaudencio R&#xea;go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampaio_W/0/1/0/all/0/1\">Waldir Sampaio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggio_P/0/1/0/all/0/1\">Paulo Sergio Boggio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Incorrectness Logic and Kleene Algebra With Top and Tests. (arXiv:2108.07707v1 [cs.PL])","link":"http://arxiv.org/abs/2108.07707","description":"<p>Kleene algebra with tests (KAT) is a foundational equational framework for\nreasoning about programs, which has found applications in program\ntransformations, networking and compiler optimizations, among many other areas.\nIn his seminal work, Kozen proved that KAT subsumes propositional Hoare logic,\nshowing that one can reason about the (partial) correctness of while programs\nby means of the equational theory of KAT.\n</p>\n<p>In this work, we investigate the support that KAT provides for reasoning\nabout \\emph{incorrectness}, instead, as embodied by Ohearn's recently proposed\nincorrectness logic. We show that KAT cannot directly express incorrectness\nlogic. The main reason for this limitation can be traced to the fact that KAT\ncannot express explicitly the notion of codomain, which is essential to express\nincorrectness triples. To address this issue, we study Kleene algebra with Top\nand Tests (TopKAT), an extension of KAT with a top element. We show that TopKAT\nis powerful enough to express a codomain operation, to express incorrectness\ntriples, and to prove all the rules of incorrectness logic sound. This shows\nthat one can reason about the incorrectness of while-like programs by means of\nthe equational theory of TopKAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amorim_A/0/1/0/all/0/1\">Arthur Azevedo de Amorim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaboardi_M/0/1/0/all/0/1\">Marco Gaboardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Game Interface to Study Semantic Grounding in Text-Based Models. (arXiv:2108.07708v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07708","description":"<p>Can language models learn grounded representations from text distribution\nalone? This question is both central and recurrent in natural language\nprocessing; authors generally agree that grounding requires more than textual\ndistribution. We propose to experimentally test this claim: if any two words\nhave different meanings and yet cannot be distinguished from distribution\nalone, then grounding is out of the reach of text-based models. To that end, we\npresent early work on an online game for the collection of human judgments on\nthe distributional similarity of word pairs in five languages. We further\nreport early results of our data collection campaign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mickus_T/0/1/0/all/0/1\">Timothee Mickus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_M/0/1/0/all/0/1\">Mathieu Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paperno_D/0/1/0/all/0/1\">Denis Paperno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining speakers of multiple languages to improve quality of neural voices. (arXiv:2108.07737v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07737","description":"<p>In this work, we explore multiple architectures and training procedures for\ndeveloping a multi-speaker and multi-lingual neural TTS system with the goals\nof a) improving the quality when the available data in the target language is\nlimited and b) enabling cross-lingual synthesis. We report results from a large\nexperiment using 30 speakers in 8 different languages across 15 different\nlocales. The system is trained on the same amount of data per speaker. Compared\nto a single-speaker model, when the suggested system is fine tuned to a\nspeaker, it produces significantly better quality in most of the cases while it\nonly uses less than $40\\%$ of the speaker's data used to build the\nsingle-speaker model. In cross-lingual synthesis, on average, the generated\nquality is within $80\\%$ of native single-speaker models, in terms of Mean\nOpinion Score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Latorre_J/0/1/0/all/0/1\">Javier Latorre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailleul_C/0/1/0/all/0/1\">Charlotte Bailleul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrill_T/0/1/0/all/0/1\">Tuuli Morrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conkie_A/0/1/0/all/0/1\">Alistair Conkie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stylianou_Y/0/1/0/all/0/1\">Yannis Stylianou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Multi-scale Convolution for Dialect Identification. (arXiv:2108.07787v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07787","description":"<p>Time Delay Neural Networks (TDNN)-based methods are widely used in dialect\nidentification. However, in previous work with TDNN application, subtle variant\nis being neglected in different feature scales. To address this issue, we\npropose a new architecture, named dynamic multi-scale convolution, which\nconsists of dynamic kernel convolution, local multi-scale learning, and global\nmulti-scale pooling. Dynamic kernel convolution captures features between\nshort-term and long-term context adaptively. Local multi-scale learning, which\nrepresents multi-scale features at a granular level, is able to increase the\nrange of receptive fields for convolution operation. Besides, global\nmulti-scale pooling is applied to aggregate features from different bottleneck\nlayers in order to collect information from multiple aspects. The proposed\narchitecture significantly outperforms state-of-the-art system on the\nAP20-OLR-dialect-task of oriental language recognition (OLR) challenge 2020,\nwith the best average cost performance (Cavg) of 0.067 and the best equal error\nrate (EER) of 6.52%. Compared with the known best results, our method achieves\n9% of Cavg and 45% of EER relative improvement, respectively. Furthermore, the\nparameters of proposed model are 91% fewer than the best known model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tianlong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shouyi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_W/0/1/0/all/0/1\">Wang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dandan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jinwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Huiyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaorui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition. (arXiv:2108.07789v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07789","description":"<p>Language models (LMs) pre-trained on massive amounts of text, in particular\nbidirectional encoder representations from Transformers (BERT), generative\npre-training (GPT), and GPT-2, have become a key technology for many natural\nlanguage processing tasks. In this paper, we present results using fine-tuned\nGPT, GPT-2, and their combination for automatic speech recognition (ASR).\nUnlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct\nproduct of the output probabilities is no longer a valid language prior\nprobability. A conversion method is proposed to compute the correct language\nprior probability based on bidirectional LM outputs in a mathematically exact\nway. Experimental results on the widely used AMI and Switchboard ASR tasks\nshowed that the combination of the fine-tuned GPT and GPT-2 outperformed the\ncombination of three neural LMs with different architectures trained from\nscratch on the in-domain text by up to a 12% relative word error rate reduction\n(WERR). Furthermore, the proposed conversion for language prior probabilities\nenables BERT to receive an extra 3% relative WERR, and the combination of BERT,\nGPT and GPT-2 results in further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xianrui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v1 [cs.CL])","link":"http://arxiv.org/abs/2108.07790","description":"<p>Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Resources for Bhojpuri, Magahi and Maithili: Statistics about them, their Similarity Estimates, and Baselines for Three Applications. (arXiv:2004.13945v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13945","description":"<p>Corpus preparation for low-resource languages and for development of human\nlanguage technology to analyze or computationally process them is a laborious\ntask, primarily due to the unavailability of expert linguists who are native\nspeakers of these languages and also due to the time and resources required.\nBhojpuri, Magahi, and Maithili, languages of the Purvanchal region of India (in\nthe north-eastern parts), are low-resource languages belonging to the\nIndo-Aryan (or Indic) family. They are closely related to Hindi, which is a\nrelatively high-resource language, which is why we compare with Hindi. We\ncollected corpora for these three languages from various sources and cleaned\nthem to the extent possible, without changing the data in them. The text\nbelongs to different domains and genres. We calculated some basic statistical\nmeasures for these corpora at character, word, syllable, and morpheme levels.\nThese corpora were also annotated with parts-of-speech (POS) and chunk tags.\nThe basic statistical measures were both absolute and relative and were\nexptected to indicate of linguistic properties such as morphological, lexical,\nphonological, and syntactic complexities (or richness). The results were\ncompared with a standard Hindi corpus. For most of the measures, we tried to\nthe corpus size the same across the languages to avoid the effect of corpus\nsize, but in some cases it turned out that using the full corpus was better,\neven if sizes were very different. Although the results are not very clear, we\ntry to draw some conclusions about the languages and the corpora. For POS\ntagging and chunking, the BIS tagset was used to manually annotate the data.\nThe POS tagged data sizes are 16067, 14669 and 12310 sentences, respectively,\nfor Bhojpuri, Magahi and Maithili. The sizes for chunking are 9695 and 1954\nsentences for Bhojpuri and Maithili, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mundotiya_R/0/1/0/all/0/1\">Rajesh Kumar Mundotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Manish Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapur_R/0/1/0/all/0/1\">Rahul Kapur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swasti Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anil Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering. (arXiv:2007.15207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.15207","description":"<p>Progress in cross-lingual modeling depends on challenging, realistic, and\ndiverse evaluation sets. We introduce Multilingual Knowledge Questions and\nAnswers (MKQA), an open-domain question answering evaluation set comprising 10k\nquestion-answer pairs aligned across 26 typologically diverse languages (260k\nquestion-answer pairs in total). Answers are based on a heavily curated,\nlanguage-independent data representation, making results comparable across\nlanguages and independent of language-specific passages. With 26 languages,\nthis dataset supplies the widest range of languages to-date for evaluating\nquestion answering. We benchmark a variety of state-of-the-art methods and\nbaselines for generative and extractive question answering, trained on Natural\nQuestions, in zero shot and translation settings. Results indicate this dataset\nis challenging even in English, but especially in low-resource languages\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daiber_J/0/1/0/all/0/1\">Joachim Daiber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction. (arXiv:2008.01377v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.01377","description":"<p>Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heid_S/0/1/0/all/0/1\">Stefan Heid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Chemistry 101: Learning to Reason about Social and Moral Norms. (arXiv:2011.00620v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.00620","description":"<p>Social norms -- the unspoken commonsense rules about acceptable social\nbehavior -- are crucial in understanding the underlying causes and intents of\npeople's actions in narratives. For example, underlying an action such as\n\"wanting to call cops on my neighbors\" are social norms that inform our\nconduct, such as \"It is expected that you report crimes.\"\n</p>\n<p>We present Social Chemistry, a new conceptual formalism to study people's\neveryday social norms and moral judgments over a rich spectrum of real life\nsituations described in natural language. We introduce Social-Chem-101, a\nlarge-scale corpus that catalogs 292k rules-of-thumb such as \"it is rude to run\na blender at 5am\" as the basic conceptual units. Each rule-of-thumb is further\nbroken down with 12 different dimensions of people's judgments, including\nsocial judgments of good and bad, moral foundations, expected cultural\npressure, and assumed legality, which together amount to over 4.5 million\nannotations of categorical labels and free-text descriptions.\n</p>\n<p>Comprehensive empirical results based on state-of-the-art neural models\ndemonstrate that computational modeling of social norms is a promising research\ndirection. Our model framework, Neural Norm Transformer, learns and generalizes\nSocial-Chem-101 to successfully reason about previously unseen situations,\ngenerating relevant (and potentially novel) attribute-aware social\nrules-of-thumb.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Match-Ignition: Plugging PageRank into Transformer for Long-form Text Matching. (arXiv:2101.06423v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.06423","description":"<p>Neural text matching models have been widely used in community question\nanswering, information retrieval, and dialogue. However, these models designed\nfor short texts cannot well address the long-form text matching problem,\nbecause there are many contexts in long-form texts can not be directly aligned\nwith each other, and it is difficult for existing models to capture the key\nmatching signals from such noisy data. Besides, these models are\ncomputationally expensive for simply use all textual data indiscriminately. To\ntackle the effectiveness and efficiency problem, we propose a novel\nhierarchical noise filtering model, namely Match-Ignition. The main idea is to\nplug the well-known PageRank algorithm into the Transformer, to identify and\nfilter both sentence and word level noisy information in the matching process.\nNoisy sentences are usually easy to detect because previous work has shown that\ntheir similarity can be explicitly evaluated by the word overlapping, so we\ndirectly use PageRank to filter such information based on a sentence similarity\ngraph. Unlike sentences, words rely on their contexts to express concrete\nmeanings, so we propose to jointly learn the filtering and matching process, to\nwell capture the critical word-level matching signals. Specifically, a word\ngraph is first built based on the attention scores in each self-attention block\nof Transformer, and key words are then selected by applying PageRank on this\ngraph. In this way, noisy words will be filtered out layer by layer in the\nmatching process. Experimental results show that Match-Ignition outperforms\nboth SOTA short text matching models and recent long-form text matching models.\nWe also conduct detailed analysis to show that Match-Ignition efficiently\ncaptures important sentences and words, to facilitate the long-form text\nmatching process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In Defense of Scene Graphs for Image Captioning. (arXiv:2102.04990v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.04990","description":"<p>The mainstream image captioning models rely on Convolutional Neural Network\n(CNN) image features to generate captions via recurrent models. Recently, image\nscene graphs have been used to augment captioning models so as to leverage\ntheir structural semantics, such as object entities, relationships and\nattributes. Several studies have noted that the naive use of scene graphs from\na black-box scene graph generator harms image captioning performance and that\nscene graph-based captioning models have to incur the overhead of explicit use\nof image features to generate decent captions. Addressing these challenges, we\npropose \\textbf{SG2Caps}, a framework that utilizes only the scene graph labels\nfor competitive image captioning performance. The basic idea is to close the\nsemantic gap between the two scene graphs - one derived from the input image\nand the other from its caption. In order to achieve this, we leverage the\nspatial location of objects and the Human-Object-Interaction (HOI) labels as an\nadditional HOI graph. SG2Caps outperforms existing scene graph-only captioning\nmodels by a large margin, indicating scene graphs as a promising representation\nfor image captioning. Direct utilization of scene graph labels avoids expensive\ngraph convolutions over high-dimensional CNN features resulting in 49% fewer\ntrainable parameters. Our code is available at:\nhttps://github.com/Kien085/SG2Caps\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong Q. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferability of Neural Network Clinical De-identification Systems. (arXiv:2102.08517v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.08517","description":"<p>Objective: Neural network de-identification studies have focused on\nindividual datasets. These studies assume the availability of a sufficient\namount of human-annotated data to train models that can generalize to\ncorresponding test data. In real-world situations, however, researchers often\nhave limited or no in-house training data. Existing systems and external data\ncan help jump-start de-identification on in-house data; however, the most\nefficient way of utilizing existing systems and external data is unclear. This\narticle investigates the transferability of a state-of-the-art neural clinical\nde-identification system, NeuroNER, across a variety of datasets, when it is\nmodified architecturally for domain generalization and when it is trained\nstrategically for domain transfer. Methods and Materials: We conducted a\ncomparative study of the transferability of NeuroNER using four clinical note\ncorpora with multiple note types from two institutions. We modified NeuroNER\narchitecturally to integrate two types of domain generalization approaches. We\nevaluated each architecture using three training strategies. We measured:\ntransferability from external sources; transferability across note types; the\ncontribution of external source data when in-domain training data are\navailable; and transferability across institutions. Results and Conclusions:\nTransferability from a single external source gave inconsistent results. Using\nadditional external sources consistently yielded an F1-score of approximately\n80%. Fine-tuning emerged as a dominant transfer strategy, with or without\ndomain generalization. We also found that external sources were useful even in\ncases where in-domain training data were available. Transferability across\ninstitutions differed by note type and annotation label but resulted in\nimproved performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kahyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobbins_N/0/1/0/all/0/1\">Nicholas J. Dobbins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McInnes_B/0/1/0/all/0/1\">Bridget McInnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">Ozlem Uzuner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06132","description":"<p>Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1\">Yash Kumar Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1\">Nathanael Chambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond Mooney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02605","description":"<p>This report presents the results of the EENLP project, done as a part of EEML\n2021 summer school.\n</p>\n<p>It presents a broad index of NLP resources for Eastern European languages,\nwhich, we hope, could be helpful for the NLP community; several new\nhand-crafted cross-lingual datasets focused on Eastern European languages, and\na sketch evaluation of cross-lingual transfer learning abilities of several\nmodern multilingual Transformer-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1\">Alex Malkhasov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1\">Andrey Manoshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1\">George Dima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1\">R&#xe9;ka Cserh&#xe1;ti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">Md.Sadek Hossain Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1\">Matt S&#xe1;rdi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Who's Waldo? Linking People Across Text and Images. (arXiv:2108.07253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07253","description":"<p>We present a task and benchmark dataset for person-centric visual grounding,\nthe problem of linking between people named in a caption and people pictured in\nan image. In contrast to prior work in visual grounding, which is predominantly\nobject-based, our new task masks out the names of people in captions in order\nto encourage methods trained on such image-caption pairs to focus on contextual\ncues (such as rich interactions between multiple people), rather than learning\nassociations between names and appearances. To facilitate this task, we\nintroduce a new dataset, Who's Waldo, mined automatically from image-caption\ndata on Wikimedia Commons. We propose a Transformer-based method that\noutperforms several strong baselines on this task, and are releasing our data\nto the research community to spur work on contextual models that consider both\nvision and language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Claire Yuqing Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1\">Apoorv Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1\">Hadar Averbuch-Elor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}