{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Variance of Twitter Embeddings and Temporal Trends of COVID-19 cases. (arXiv:2110.00031v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00031","description":"<p>The severity of the coronavirus pandemic necessitates the need of effective\nadministrative decisions. Over 4 lakh people in India succumbed to COVID-19,\nwith over 3 crore confirmed cases, and still counting. The threat of a\nplausible third wave continues to haunt millions. In this ever changing dynamic\nof the virus, predictive modeling methods can serve as an integral tool. The\npandemic has further triggered an unprecedented usage of social media. This\npaper aims to propose a method for harnessing social media, specifically\nTwitter, to predict the upcoming scenarios related to COVID-19 cases. In this\nstudy, we seek to understand how the surges in COVID-19 related tweets can\nindicate rise in the cases. This prospective analysis can be utilised to aid\nadministrators about timely resource allocation to lessen the severity of the\ndamage. Using word embeddings to capture the semantic meaning of tweets, we\nidentify Significant Dimensions (SDs).Our methodology predicts the rise in\ncases with a lead time of 15 days and 30 days with R2 scores of 0.80 and 0.62\nrespectively. Finally, we explain the thematic utility of the SDs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pahwa_K/0/1/0/all/0/1\">Khushbu Pahwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadhu_A/0/1/0/all/0/1\">Ambika Sadhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_M/0/1/0/all/0/1\">Mayank Sethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagpal_S/0/1/0/all/0/1\">Sargun Nagpal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sethi_T/0/1/0/all/0/1\">Tavpritesh Sethi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#ContextMatters: Advantages and Limitations of Using Machine Learning to Support Women in Politics. (arXiv:2110.00116v1 [cs.SI])","link":"http://arxiv.org/abs/2110.00116","description":"<p>The United Nations identified gender equality as a Sustainable Development\nGoal in 2015, recognizing the underrepresentation of women in politics as a\nspecific barrier to achieving gender equality. Political systems around the\nworld experience gender inequality across all levels of elected government as\nfewer women run for office than men. This is due in part to online abuse,\nparticularly on social media platforms like Twitter, where women seeking or in\npower tend to be targeted with more toxic maltreatment than their male\ncounterparts. In this paper, we present reflections on ParityBOT - the first\nnatural language processing-based intervention designed to affect online\ndiscourse for women in politics for the better, at scale. Deployed across\nelections in Canada, the United States and New Zealand, ParityBOT was used to\nanalyse and classify more than 12 million tweets directed at women candidates\nand counter toxic tweets with supportive ones. From these elections we present\nthree case studies highlighting the current limitations of, and future research\nand application opportunities for, using a natural language processing-based\nsystem to detect online toxicity, specifically with regards to contextually\nimportant microaggressions. We examine the rate of false negatives, where\nParityBOT failed to pick up on insults directed at specific high profile women,\nwhich would be obvious to human users. We examine the unaddressed harms of\nmicroaggressions and the potential of yet unseen damage they cause for women in\nthese communities, and for progress towards gender equality overall, in light\nof these technological blindspots. This work concludes with a discussion on the\nbenefits of partnerships between nonprofit social groups and technology experts\nto develop responsible, socially impactful approaches to addressing online\nhate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Comer_J/0/1/0/all/0/1\">Jacqueline Comer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Work_S/0/1/0/all/0/1\">Sam Work</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1\">Kory W Mathewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuthbertson_L/0/1/0/all/0/1\">Lana Cuthbertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machin_K/0/1/0/all/0/1\">Kasey Machin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-Constrained Graph Neural Networks For Argument Mining. (arXiv:2110.00124v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00124","description":"<p>We propose a novel architecture for Graph Neural Networks that is inspired by\nthe idea behind Tree Kernels of measuring similarity between trees by taking\ninto account their common substructures, named fragments. By imposing a series\nof regularization constraints to the learning problem, we exploit a pooling\nmechanism that incorporates such notion of fragments within the node soft\nassignment function that produces the embeddings. We present an extensive\nexperimental evaluation on a collection of sentence classification tasks\nconducted on several argument mining corpora, showing that the proposed\napproach performs well with respect to state-of-the-art techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruggeri_F/0/1/0/all/0/1\">Federico Ruggeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MemBERT: Injecting Unstructured Knowledge into BERT. (arXiv:2110.00125v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00125","description":"<p>Transformers changed modern NLP in many ways. However, they can hardly\nexploit domain knowledge, and like other blackbox models, they lack\ninterpretability. Unfortunately, structured knowledge injection, in the long\nrun, risks to suffer from a knowledge acquisition bottleneck. We thus propose a\nmemory enhancement of transformer models that makes use of unstructured domain\nknowledge expressed in plain natural language. An experimental evaluation\nconducted on two challenging NLP tasks demonstrates that our approach yields\nbetter performance and model interpretability than baseline transformer-based\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruggeri_F/0/1/0/all/0/1\">Federico Ruggeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UserIdentifier: Implicit User Representations for Simple and Effective Personalized Sentiment Analysis. (arXiv:2110.00135v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00135","description":"<p>Global models are trained to be as generalizable as possible, with user\ninvariance considered desirable since the models are shared across multitudes\nof users. As such, these models are often unable to produce personalized\nresponses for individual users, based on their data. Contrary to widely-used\npersonalization techniques based on few-shot learning, we propose\nUserIdentifier, a novel scheme for training a single shared model for all\nusers. Our approach produces personalized responses by adding fixed,\nnon-trainable user identifiers to the input data. We empirically demonstrate\nthat this proposed method outperforms the prefix-tuning based state-of-the-art\napproach by up to 13%, on a suite of sentiment analysis datasets. We also show\nthat, unlike prior work, this method needs neither any additional model\nparameters nor any extra rounds of few-shot fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1\">Vaishnavi Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1\">Milad Shokouhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1\">Robert Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitriadis_D/0/1/0/all/0/1\">Dimitrios Dimitriadis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Labeling Approach for Vietnamese and Chinese Word Segmentation. (arXiv:2110.00156v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00156","description":"<p>In this paper, we propose a span labeling approach to model n-gram\ninformation for Vietnamese word segmentation, namely SPAN SEG. We compare the\nspan labeling approach with the conditional random field by using encoders with\nthe same architecture. Since Vietnamese and Chinese have similar linguistic\nphenomena, we evaluated the proposed method on the Vietnamese treebank\nbenchmark dataset and five Chinese benchmark datasets. Through our experimental\nresults, the proposed approach SpanSeg achieves higher performance than the\nsequence tagging approach with the state-of-the-art F-score of 98.31% on the\nVietnamese treebank benchmark, when they both apply the contextual pre-trained\nlanguage model XLM-RoBERTa and the predicted word boundary information.\nBesides, we do fine-tuning experiments for the span labeling approach on BERT\nand ZEN pre-trained language model for Chinese with fewer parameters, faster\ninference time, and competitive or higher F-scores than the previous\nstate-of-the-art approach, word segmentation with word-hood memory networks, on\nfive Chinese benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_L/0/1/0/all/0/1\">Linh-Bao Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thin_D/0/1/0/all/0/1\">Dang Van Thin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Under the Microscope: Interpreting Readability Assessment Models for Filipino. (arXiv:2110.00157v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00157","description":"<p>Readability assessment is the process of identifying the level of ease or\ndifficulty of a certain piece of text for its intended audience. Approaches\nhave evolved from the use of arithmetic formulas to more complex\npattern-recognizing models trained using machine learning algorithms. While\nusing these approaches provide competitive results, limited work is done on\nanalyzing how linguistic variables affect model inference quantitatively. In\nthis work, we dissect machine learning-based readability assessment models in\nFilipino by performing global and local model interpretation to understand the\ncontributions of varying linguistic features and discuss its implications in\nthe context of the Filipino language. Results show that using a model trained\nwith top features from global interpretation obtained higher performance than\nthe ones using features selected by Spearman correlation. Likewise, we also\nempirically observed local feature weight boundaries for discriminating reading\ndifficulty at an extremely fine-grained level and their corresponding effects\nif values are perturbed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1\">Joseph Marvin Imperial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1\">Ethel Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building an Efficient and Effective Retrieval-based Dialogue System via Mutual Learning. (arXiv:2110.00159v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00159","description":"<p>Establishing retrieval-based dialogue systems that can select appropriate\nresponses from the pre-built index has gained increasing attention from\nresearchers. For this task, the adoption of pre-trained language models (such\nas BERT) has led to remarkable progress in a number of benchmarks. There exist\ntwo common approaches, including cross-encoders which perform full attention\nover the inputs, and bi-encoders that encode the context and response\nseparately. The former gives considerable improvements in accuracy but is often\ninapplicable in practice for large-scale retrieval given the cost of the full\nattention required for each sample at test time. The latter is efficient for\nbillions of indexes but suffers from sub-optimal performance. In this work, we\npropose to combine the best of both worlds to build a retrieval system.\nSpecifically, we employ a fast bi-encoder to replace the traditional\nfeature-based pre-retrieval model (such as BM25) and set the response\nre-ranking model as a more complicated architecture (such as cross-encoder). To\nfurther improve the effectiveness of our framework, we train the pre-retrieval\nmodel and the re-ranking model at the same time via mutual learning, which\nenables two models to learn from each other throughout the training process. We\nconduct experiments on two benchmarks and evaluation results demonstrate the\nefficiency and effectiveness of our proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiazhan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation by Self- and Semi-supervised Learning. (arXiv:2110.00165v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and Semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT4GCN: Using BERT Intermediate Layers to Augment GCN for Aspect-based Sentiment Classification. (arXiv:2110.00171v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00171","description":"<p>Graph-based Aspect-based Sentiment Classification (ABSC) approaches have\nyielded state-of-the-art results, expecially when equipped with contextual word\nembedding from pre-training language models (PLMs). However, they ignore\nsequential features of the context and have not yet made the best of PLMs. In\nthis paper, we propose a novel model, BERT4GCN, which integrates the\ngrammatical sequential features from the PLM of BERT, and the syntactic\nknowledge from dependency graphs. BERT4GCN utilizes outputs from intermediate\nlayers of BERT and positional information between words to augment GCN (Graph\nConvolutional Network) to better encode the dependency graphs for the\ndownstream classification. Experimental results demonstrate that the proposed\nBERT4GCN outperforms all state-of-the-art baselines, justifying that augmenting\nGCN with the grammatical features from intermediate layers of BERT can\nsignificantly empower ABSC models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zeguan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiarun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Congjian Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00269","description":"<p>Pre-trained models learn contextualized word representations on large-scale\ntext corpus through a self-supervised learning method, which has achieved\npromising performance after fine-tuning. These models, however, suffer from\npoor robustness and lack of interpretability. Pre-trained models with knowledge\ninjection, which we call knowledge enhanced pre-trained models (KEPTMs),\npossess deep understanding and logical reasoning and introduce interpretability\nto some extent. In this survey, we provide a comprehensive overview of KEPTMs\nfor natural language processing. We first introduce the progress of pre-trained\nmodels and knowledge representation learning. Then we systematically categorize\nexisting KEPTMs from three different perspectives. Finally, we outline some\npotential directions of KEPTMs for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_G/0/1/0/all/0/1\">Gang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinghui Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Harmful Memes and Their Targets. (arXiv:2110.00413v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00413","description":"<p>Among the various modes of communication in social media, the use of Internet\nmemes has emerged as a powerful means to convey political, psychological, and\nsocio-cultural opinions. Although memes are typically humorous in nature,\nrecent days have witnessed a proliferation of harmful memes targeted to abuse\nvarious social entities. As most harmful memes are highly satirical and\nabstruse without appropriate contexts, off-the-shelf multimodal models may not\nbe adequate to understand their underlying semantics. In this work, we propose\ntwo novel problem formulations: detecting harmful memes and the social entities\nthat these harmful memes target. To this end, we present HarMeme, the first\nbenchmark dataset, containing 3,544 memes related to COVID-19. Each meme went\nthrough a rigorous two-stage annotation process. In the first stage, we labeled\na meme as very harmful, partially harmful, or harmless; in the second stage, we\nfurther annotated the type of target(s) that each harmful meme points to:\nindividual, organization, community, or society/general public/other. The\nevaluation results using ten unimodal and multimodal models highlight the\nimportance of using multimodal signals for both tasks. We further discuss the\nlimitations of these models and we argue that more research is needed to\naddress these problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1\">Rituparna Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md. Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiLMing Multimodal Sarcasm Detection with Attention. (arXiv:2110.00416v1 [cs.MM])","link":"http://arxiv.org/abs/2110.00416","description":"<p>Sarcasm detection identifies natural language expressions whose intended\nmeaning is different from what is implied by its surface meaning. It finds\napplications in many NLP tasks such as opinion mining, sentiment analysis, etc.\nToday, social media has given rise to an abundant amount of multimodal data\nwhere users express their opinions through text and images. Our paper aims to\nleverage multimodal data to improve the performance of the existing systems for\nsarcasm detection. So far, various approaches have been proposed that uses text\nand image modality and a fusion of both. We propose a novel architecture that\nuses the RoBERTa model with a co-attention layer on top to incorporate context\nincongruity between input text and image attributes. Further, we integrate\nfeature-wise affine transformation by conditioning the input image through\nFiLMed ResNet blocks with the textual features using the GRU network to capture\nthe multimodal information. The output from both the models and the CLS token\nfrom RoBERTa is concatenated and used for the final prediction. Our results\ndemonstrate that our proposed model outperforms the existing state-of-the-art\nmethod by 6.14% F1 score on the public Twitter multimodal sarcasm detection\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sundesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Aditya Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Miten Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syiemlieh_L/0/1/0/all/0/1\">Laribok Syiemlieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurya_C/0/1/0/all/0/1\">Chandresh Maurya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Non-Negative Matrix Factorization and n-stage Latent Dirichlet Allocation for Emotion Analysis in Turkish Tweets. (arXiv:2110.00418v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00418","description":"<p>With the development of technology, the use of social media has become quite\ncommon. Analyzing comments on social media in areas such as media and\nadvertising plays an important role today. For this reason, new and traditional\nnatural language processing methods are used to detect the emotion of these\nshares. In this paper, the Latent Dirichlet Allocation, namely LDA, and\nNon-Negative Matrix Factorization methods in topic modeling were used to\ndetermine which emotion the Turkish tweets posted via Twitter. In addition, the\naccuracy of a proposed n-level method based on LDA was analyzed. Dataset\nconsists of 5 emotions, namely angry, fear, happy, sad and confused. NMF was\nthe most successful method among all topic modeling methods in this study.\nThen, the F1-measure of Random Forest, Naive Bayes and Support Vector Machine\nmethods was analyzed by obtaining a file suitable for Weka by using the word\nweights and class labels of the topics. Among the Weka results, the most\nsuccessful method was n-stage LDA, and the most successful algorithm was Random\nForest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guven_Z/0/1/0/all/0/1\">Zekeriya Anil Guven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diri_B/0/1/0/all/0/1\">Banu Diri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cakaloglu_T/0/1/0/all/0/1\">Tolgahan Cakaloglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Web Scale Entity Extraction System. (arXiv:2110.00423v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00423","description":"<p>Understanding the semantic meaning of content on the web through the lens of\nentities and concepts has many practical advantages. However, when building\nlarge-scale entity extraction systems, practitioners are facing unique\nchallenges involving finding the best ways to leverage the scale and variety of\ndata available on internet platforms. We present learnings from our efforts in\nbuilding an entity extraction system for multiple document types at large scale\nusing multi-modal Transformers. We empirically demonstrate the effectiveness of\nmulti-lingual, multi-task and cross-document type learning. We also discuss the\nlabel collection schemes that help to minimize the amount of noise in the\ncollected data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xuanting Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Quanbin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengkan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_P/0/1/0/all/0/1\">Pushkar Tripathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rumor Detection on Social Media with Hierarchical Adversarial Training. (arXiv:2110.00425v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00425","description":"<p>The proliferation of rumors on social media has a huge impact on society.\nHowever, natural language text is high-dimensional and sparse, and the same\nrumor may be expressed in hundreds of ways on social media. As such, the\nrobustness and generalization of the current rumor detection model are put into\nquestion. We propose a new hierarchical model called HAT-RD, which is divided\ninto two categories: post-level modules and event-level modules. HAT-RD adopts\na novel hierarchical adversarial training method based on gradient ascent by\nadding adversarial perturbations to the embedding layers both of post-level\nmodules and event-level modules to deceive the detector. At the same time, the\ndetector uses stochastic gradient descent to minimize the adversarial risk to\nlearn a more robust model. In this way, the post-level and event-level sample\nspaces are enhanced, and experiments indicate that the model drift into an area\nwith a flat loss landscape that leads to better generalization. Experiments on\ntwo real-world datasets demonstrate that our model achieves better results than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shiwen Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_H/0/1/0/all/0/1\">Hung-Yu Kao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Natural Language Video Localization. (arXiv:2110.00428v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00428","description":"<p>Understanding videos to localize moments with natural language often requires\nlarge expensive annotated video regions paired with language queries. To\neliminate the annotation costs, we make a first attempt to train a natural\nlanguage video localization model in zero-shot manner. Inspired by unsupervised\nimage captioning setup, we merely require random text corpora, unlabeled video\ncollections, and an off-the-shelf object detector to train a model. With the\nunpaired data, we propose to generate pseudo-supervision of candidate temporal\nregions and corresponding query sentences, and develop a simple NLVL model to\ntrain with the pseudo-supervision. Our empirical validations show that the\nproposed pseudo-supervised method outperforms several baseline approaches and a\nnumber of methods using stronger supervision on Charades-STA and\nActivityNet-Captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Jinwoo Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1\">Daechul Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Seong Jong Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"External knowledge transfer deployment inside a simple double agent Viterbi algorithm. (arXiv:2110.00433v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00433","description":"<p>We consider in this paper deploying external knowledge transfer inside a\nsimple double agent Viterbi algorithm which is an algorithm firstly introduced\nby the author in his preprint \"Hidden Markov Based Mathematical Model dedicated\nto Extract Ingredients from Recipe Text\". The key challenge of this work lies\nin discovering the reason why our old model does have bad performances when it\nis confronted with estimating ingredient state for unknown words and see if\ndeploying external knowledge transfer directly on calculating state matrix\ncould be the solution instead of deploying it only on back propagating step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baklouti_Z/0/1/0/all/0/1\">Zied Baklouti</a> (ENIT, UP)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention based Sequence to Sequence Learning for Machine Translation of Low Resourced Indic Languages -- A case of Sanskrit to Hindi. (arXiv:2110.00435v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00435","description":"<p>Deep Learning techniques are powerful in mimicking humans in a particular set\nof problems. They have achieved a remarkable performance in complex learning\ntasks. Deep learning inspired Neural Machine Translation (NMT) is a proficient\ntechnique that outperforms traditional machine translation. Performing\nmachine-aided translation on Indic languages has always been a challenging task\nconsidering their rich and diverse grammar. The neural machine translation has\nshown quality results compared to the traditional machine translation\napproaches. The fully automatic machine translation becomes problematic when it\ncomes to low-resourced languages, especially with Sanskrit. This paper presents\nattention mechanism based neural machine translation by selectively focusing on\na particular part of language sentences during translation. The work shows the\nconstruction of Sanskrit to Hindi bilingual parallel corpus with nearly 10K\nsamples and having 178,000 tokens. The neural translation model equipped with\nan attention mechanism has been trained on Sanskrit to Hindi parallel corpus.\nThe approach has shown the significance of attention mechanisms to overcome\nlong-term dependencies, primarily associated with low resources Indic\nlanguages. The paper shows the attention plots on testing data to demonstrate\nthe alignment between source and translated words. For the evaluation of the\ntranslated sentences, manual score based human evaluation and automatic\nevaluation metric based techniques have been adopted. The attention mechanism\nbased neural translation has achieved 88% accuracy in human evaluation and a\nBLEU score of 0.92 on Sanskrit to Hindi translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakarola_V/0/1/0/all/0/1\">Vishvajit Bakarola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasriwala_J/0/1/0/all/0/1\">Jitendra Nasriwala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phonology Recognition in American Sign Language. (arXiv:2110.00453v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00453","description":"<p>Inspired by recent developments in natural language processing, we propose a\nnovel approach to sign language processing based on phonological properties\nvalidated by American Sign Language users. By taking advantage of datasets\ncomposed of phonological data and people speaking sign language, we use a\npretrained deep model based on mesh reconstruction to extract the 3D\ncoordinates of the signers keypoints. Then, we train standard statistical and\ndeep machine learning models in order to assign phonological classes to each\ntemporal sequence of coordinates.\n</p>\n<p>Our paper introduces the idea of exploiting the phonological properties\nmanually assigned by sign language users to classify videos of people\nperforming signs by regressing a 3D mesh. We establish a new baseline for this\nproblem based on the statistical distribution of 725 different signs. Our\nbest-performing models achieve a micro-averaged F1-score of 58% for the major\nlocation class and 70% for the sign type using statistical and deep learning\nalgorithms, compared to their corresponding baselines of 35% and 39%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tavella_F/0/1/0/all/0/1\">Federico Tavella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galata_A/0/1/0/all/0/1\">Aphrodite Galata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cangelosi_A/0/1/0/all/0/1\">Angelo Cangelosi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Ask for Data-Efficient Event Argument Extraction. (arXiv:2110.00479v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00479","description":"<p>Event argument extraction (EAE) is an important task for information\nextraction to discover specific argument roles. In this study, we cast EAE as a\nquestion-based cloze task and empirically analyze fixed discrete token template\nperformance. As generating human-annotated question templates is often\ntime-consuming and labor-intensive, we further propose a novel approach called\n\"Learning to Ask,\" which can learn optimized question templates for EAE without\nhuman annotations. Experiments using the ACE-2005 dataset demonstrate that our\nmethod based on optimized questions achieves state-of-the-art performance in\nboth the few-shot and supervised settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEMON: Explainable Entity Matching. (arXiv:2110.00516v1 [cs.DB])","link":"http://arxiv.org/abs/2110.00516","description":"<p>State-of-the-art entity matching (EM) methods are hard to interpret, and\nthere is significant value in bringing explainable AI to EM. Unfortunately,\nmost popular explainability methods do not work well out of the box for EM and\nneed adaptation. In this paper, we identify three challenges of applying local\npost hoc feature attribution methods to entity matching: cross-record\ninteraction effects, non-match explanations, and variation in sensitivity. We\npropose our novel model-agnostic and schema-flexible method LEMON that\naddresses all three challenges by (i) producing dual explanations to avoid\ncross-record interaction effects, (ii) introducing the novel concept of\nattribution potential to explain how two records could have matched, and (iii)\nautomatically choosing explanation granularity to match the sensitivity of the\nmatcher and record pair in question. Experiments on public datasets demonstrate\nthat the proposed method is more faithful to the matcher and does a better job\nof helping users understand the decision boundary of the matcher than previous\nwork. Furthermore, user studies show that the rate at which human subjects can\nconstruct counterfactual examples after seeing an explanation from our proposed\nmethod increases from 54% to 64% for matches and from 15% to 49% for\nnon-matches compared to explanations from a standard adaptation of LIME.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barlaug_N/0/1/0/all/0/1\">Nils Barlaug</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images. (arXiv:2110.00519v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00519","description":"<p>While neural symbolic methods demonstrate impressive performance in visual\nquestion answering on synthetic images, their performance suffers on real\nimages. We identify that the long-tail distribution of visual concepts and\nunequal importance of reasoning steps in real data are the two key obstacles\nthat limit the models' real-world potentials. To address these challenges, we\npropose a new paradigm, Calibrating Concepts and Operations (CCO), which\nenables neural symbolic models to capture underlying data characteristics and\nto reason with hierarchical importance. Specifically, we introduce an executor\nwith learnable concept embedding magnitudes for handling distribution\nimbalance, and an operation calibrator for highlighting important operations\nand suppressing redundant ones. Our experiments show CCO substantially boosts\nthe performance of neural symbolic methods on real images. By evaluating models\non the real world dataset GQA, CCO helps the neural symbolic method NSCL\noutperforms its vanilla counterpart by 9.1% (from 47.0% to 56.1%); this result\nalso largely reduces the performance gap between symbolic and non-symbolic\nmethods. Additionally, we create a perturbed test set for better understanding\nand analyzing model performance on real images. Code is available at\nhttps://github.com/Lizw14/CaliCO.git .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stengel_Eskin_E/0/1/0/all/0/1\">Elias Stengel-Eskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yixiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Cihang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens. (arXiv:2110.00521v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00521","description":"<p>Much of the world's population experiences some form of disability during\ntheir lifetime. Caution must be exercised while designing natural language\nprocessing (NLP) systems to prevent systems from inadvertently perpetuating\nableist bias against people with disabilities, i.e., prejudice that favors\nthose with typical abilities. We report on various analyses based on word\npredictions of a large-scale BERT language model. Statistically significant\nresults demonstrate that people with disabilities can be disadvantaged.\nFindings also explore overlapping forms of discrimination related to\ninterconnected gender and race identities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Saad Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huenerfauth_M/0/1/0/all/0/1\">Matt Huenerfauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alm_C/0/1/0/all/0/1\">Cecilia Ovesdotter Alm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v1 [cs.CV])","link":"http://arxiv.org/abs/2110.00534","description":"<p>Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human--human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\",\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models' abilities in dialogue understanding, language\ngrounding, and task execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramithu_R/0/1/0/all/0/1\">Robinson Piramithu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural language understanding for logical games. (arXiv:2110.00558v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00558","description":"<p>We developed a system able to automatically solve logical puzzles in natural\nlanguage. Our solution is composed by a parser and an inference module. The\nparser translates the text into first order logic (FOL), while the MACE4 model\nfinder is used to compute the models of the given FOL theory. We also empower\nour software agent with the capability to provide Yes/No answers to natural\nlanguage questions related to each puzzle. Moreover, in line with Explainalbe\nArtificial Intelligence (XAI), the agent can back its answer, providing a\ngraphical representation of the proof. The advantage of using reasoning for\nNatural Language Understanding (NLU) instead of Machine learning is that the\nuser can obtain an explanation of the reasoning chain. We illustrate how the\nsystem performs on various types of natural language puzzles, including 382\nknights and knaves puzzles. These features together with the overall\nperformance rate of 80.89\\% makes the proposed solution an improvement upon\nsimilar solvers for natural language understanding in the puzzles domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Groza_A/0/1/0/all/0/1\">Adrian Groza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nitu_C/0/1/0/all/0/1\">Cristian Nitu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Punctuation Restoration for Speech Transcripts via External Data. (arXiv:2110.00560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00560","description":"<p>Automatic Speech Recognition (ASR) systems generally do not produce\npunctuated transcripts. To make transcripts more readable and follow the\nexpected input format for downstream language models, it is necessary to add\npunctuation marks. In this paper, we tackle the punctuation restoration problem\nspecifically for the noisy text (e.g., phone conversation scenarios). To\nleverage the available written text datasets, we introduce a data sampling\ntechnique based on an n-gram language model to sample more training data that\nare similar to our in-domain data. Moreover, we propose a two-stage fine-tuning\napproach that utilizes the sampled external data as well as our in-domain\ndataset for models based on BERT. Extensive experiments show that the proposed\napproach outperforms the baseline with an improvement of 1:12% F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xue-Yong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_M/0/1/0/all/0/1\">Md Tahmid Rahman Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+TN_S/0/1/0/all/0/1\">Shashi Bhushan TN</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corston_Oliver_S/0/1/0/all/0/1\">Simon Corston-Oliver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paraphrases as Foreign Languages in Multilingual Neural Machine Translation. (arXiv:1808.08438v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1808.08438","description":"<p>Paraphrases, the rewordings of the same semantic meaning, are useful for\nimproving generalization and translation. However, prior works only explore\nparaphrases at the word or phrase level, not at the sentence or corpus level.\nUnlike previous works that only explore paraphrases at the word or phrase\nlevel, we use different translations of the whole training data that are\nconsistent in structure as paraphrases at the corpus level. We train on\nparallel paraphrases in multiple languages from various sources. We treat\nparaphrases as foreign languages, tag source sentences with paraphrase labels,\nand train on parallel paraphrases in the style of multilingual Neural Machine\nTranslation (NMT). Our multi-paraphrase NMT that trains only on two languages\noutperforms the multilingual baselines. Adding paraphrases improves the rare\nword translation and increases entropy and diversity in lexical choice. Adding\nthe source paraphrases boosts performance better than adding the target ones.\nCombining both the source and the target paraphrases lifts performance further;\ncombining paraphrases with multilingual data helps but has mixed performance.\nWe achieve a BLEU score of 57.2 for French-to-English translation using 24\ncorpus-level paraphrases of the Bible, which outperforms the multilingual\nbaselines and is +34.7 above the single-source single-target NMT baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sperber_M/0/1/0/all/0/1\">Matthias Sperber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergence of Pragmatics from Referential Game between Theory of Mind Agents. (arXiv:2001.07752v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2001.07752","description":"<p>Pragmatics studies how context can contribute to language meanings. In human\ncommunication, language is never interpreted out of context, and sentences can\nusually convey more information than their literal meanings. However, this\nmechanism is missing in most multi-agent systems, restricting the communication\nefficiency and the capability of human-agent interaction. In this paper, we\npropose an algorithm, using which agents can spontaneously learn the ability to\n\"read between lines\" without any explicit hand-designed rules. We integrate the\ntheory of mind (ToM) in a cooperative multi-agent pedagogical situation and\npropose an adaptive reinforcement learning (RL) algorithm to develop a\ncommunication protocol. ToM is a profound cognitive science concept, claiming\nthat people regularly reason about other's mental states, including beliefs,\ngoals, and intentions, to obtain performance advantage in competition,\ncooperation or coalition. With this ability, agents consider language as not\nonly messages but also rational acts reflecting others' hidden states. Our\nexperiments demonstrate the advantage of pragmatic protocols over non-pragmatic\nprotocols. We also show the teaching complexity following the pragmatic\nprotocol empirically approximates to recursive teaching dimension (RTD).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Luyao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zipeng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jingyue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Junhong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Self-Training for Sentiment Analysis of Code-Switched Data. (arXiv:2103.14797v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14797","description":"<p>Sentiment analysis is an important task in understanding social media content\nlike customer reviews, Twitter and Facebook feeds etc. In multilingual\ncommunities around the world, a large amount of social media text is\ncharacterized by the presence of Code-Switching. Thus, it has become important\nto build models that can handle code-switched data. However, annotated\ncode-switched data is scarce and there is a need for unsupervised models and\nalgorithms. We propose a general framework called Unsupervised Self-Training\nand show its applications for the specific use case of sentiment analysis of\ncode-switched data. We use the power of pre-trained BERT models for\ninitialization and fine-tune them in an unsupervised manner, only using pseudo\nlabels produced by zero-shot transfer. We test our algorithm on multiple\ncode-switched languages and provide a detailed analysis of the learning\ndynamics of the algorithm with the aim of answering the question - `Does our\nunsupervised model understand the Code-Switched languages or does it just learn\nits representations?'. Our unsupervised models compete well with their\nsupervised counterparts, with their performance reaching within 1-7\\% (weighted\nF1 scores) when compared to supervised models trained for a two class problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menghani_S/0/1/0/all/0/1\">Sargam Menghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sai Krishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers. (arXiv:2104.01604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01604","description":"<p>This paper introduces Timers and Such, a new open source dataset of spoken\nEnglish commands for common voice control use cases involving numbers. We\ndescribe the gap in existing spoken language understanding datasets that Timers\nand Such fills, the design and creation of the dataset, and experiments with a\nnumber of ASR-based and end-to-end baseline models, the code for which has been\nmade available as part of the SpeechBrain toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lugosch_L/0/1/0/all/0/1\">Loren Lugosch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papreja_P/0/1/0/all/0/1\">Piyush Papreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanelli_M/0/1/0/all/0/1\">Mirco Ravanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heba_A/0/1/0/all/0/1\">Abdelwahab Heba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FRAKE: Fusional Real-time Automatic Keyword Extraction. (arXiv:2104.04830v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04830","description":"<p>Keyword extraction is the process of identifying the words or phrases that\nexpress the main concepts of text to the best of one's ability. Electronic\ninfrastructure creates a considerable amount of text every day and at all\ntimes. This massive volume of documents makes it practically impossible for\nhuman resources to study and manage them. Nevertheless, the need for these\ndocuments to be accessed efficiently and effectively is evident in numerous\npurposes. A blog, news article, or technical note is considered a relatively\nlong text since the reader aims to learn the subject based on keywords or\ntopics. Our approach consists of a combination of two models: graph centrality\nfeatures and textural features. The proposed method has been used to extract\nthe best keyword among the candidate keywords with an optimal combination of\ngraph centralities, such as degree, betweenness, eigenvector, closeness\ncentrality and etc, and textural, such as Casing, Term position, Term frequency\nnormalization, Term different sentence, Part Of Speech tagging. There have also\nbeen attempts to distinguish keywords from candidate phrases and consider them\non separate keywords. For evaluating the proposed method, seven datasets were\nused: Semeval2010, SemEval2017, Inspec, fao30, Thesis100, pak2018, and\nWikinews, with results reported as Precision, Recall, and F- measure. Our\nproposed method performed much better in terms of evaluation metrics in all\nreviewed datasets compared with available methods in literature. An approximate\n16.9% increase was witnessed in F-score metric and this was much more for the\nInspec in English datasets and WikiNews in forgone languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zehtab_Salmasi_A/0/1/0/all/0/1\">Aidin Zehtab-Salmasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1\">Mohamad-Ali Balafar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Machine Translation. (arXiv:2104.05848v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05848","description":"<p>We translate a closed text that is known in advance into a severely low\nresource language by leveraging massive source parallelism. In other words,\ngiven a text in 124 source languages, we translate it into a severely low\nresource language using only ~1,000 lines of low resource data without any\nexternal help. Firstly, we propose a systematic method to rank and choose\nsource languages that are close to the low resource language. We call the\nlinguistic definition of language family Family of Origin (FAMO), and we call\nthe empirical definition of higher-ranked languages using our metrics Family of\nChoice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual\nOrder-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines\n(~3.5%) of low resource data. To translate named entities correctly, we build a\nmassive lexicon table for 2,939 Bible named entities in 124 source languages,\nand include many that occur once and covers more than 66 severely low resource\nlanguages. Moreover, we also build a novel method of combining translations\nfrom different source languages into one. Using English as a hypothetical low\nresource language, we get a +23.9 BLEU increase over a multilingual baseline,\nand a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We\nget a 42.8 BLEU score for Portuguese-English translation on the medical EMEA\ndataset. We also have good results for a real severely low resource Mayan\nlanguage, Eastern Pokomchi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alex Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moving on from OntoNotes: Coreference Resolution Model Transfer. (arXiv:2104.08457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08457","description":"<p>Academic neural models for coreference resolution (coref) are typically\ntrained on a single dataset, OntoNotes, and model improvements are benchmarked\non that same dataset. However, real-world applications of coref depend on the\nannotation guidelines and the domain of the target dataset, which often differ\nfrom those of OntoNotes. We aim to quantify transferability of coref models\nbased on the number of annotated documents available in the target dataset. We\nexamine eleven target datasets and find that continued training is consistently\neffective and especially beneficial when there are few target documents. We\nestablish new benchmarks across several datasets, including state-of-the-art\nresults on PreCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Patrick Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP. (arXiv:2104.08835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08835","description":"<p>Humans can learn a new language task efficiently with only few examples, by\nleveraging their knowledge obtained when learning prior tasks. In this paper,\nwe explore whether and how such cross-task generalization ability can be\nacquired, and further applied to build better few-shot learners across diverse\nNLP tasks. We introduce CrossFit, a problem setup for studying cross-task\ngeneralization ability, which standardizes seen/unseen task partitions, data\naccess during different learning stages, and the evaluation protocols. To\ninstantiate different seen/unseen task partitions in CrossFit and facilitate\nin-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse\nfew-shot NLP tasks created from open-access NLP datasets and converted to a\nunified text-to-text format. Our analysis reveals that the few-shot learning\nability on unseen tasks can be improved via an upstream learning stage using a\nset of seen tasks. We also observe that the selection of upstream learning\ntasks can significantly influence few-shot performance on unseen tasks, asking\nfurther analysis on task similarity and transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Influence of Masking Policies in Intermediate Pre-training. (arXiv:2104.08840v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08840","description":"<p>Current NLP models are predominantly trained through a two-stage \"pre-train\nthen fine-tune\" pipeline. Prior work has shown that inserting an intermediate\npre-training stage, using heuristic masking policies for masked language\nmodeling (MLM), can significantly improve final performance. However, it is\nstill unclear (1) in what cases such intermediate pre-training is helpful, (2)\nwhether hand-crafted heuristic objectives are optimal for a given task, and (3)\nwhether a masking policy designed for one task is generalizable beyond that\ntask. In this paper, we perform a large-scale empirical study to investigate\nthe effect of various masking policies in intermediate pre-training with nine\nselected tasks across three categories. Crucially, we introduce methods to\nautomate the discovery of optimal masking policies via direct supervision or\nmeta-learning. We conclude that the success of intermediate pre-training is\ndependent on appropriate pre-train corpus, selection of output format (i.e.,\nmasked spans or full sentence), and clear understanding of the role that MLM\nplays for the downstream task. In addition, we find our learned masking\npolicies outperform the heuristic of masking named entities on TriviaQA, and\npolicies learned from one task can positively transfer to other tasks in\ncertain cases, inviting future research in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Belinda Z. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_B/0/1/0/all/0/1\">Benjamin Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Clinical Note Summarization. (arXiv:2104.08942v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08942","description":"<p>The trend of deploying digital systems in numerous industries has induced a\nhike in recording digital information. The health sector has observed an\nextensive adoption of digital devices and systems that generate large volumes\nof personal medical records. Electronic health records contain valuable\ninformation for retrospective and prospective analysis that is often not\nentirely exploited because of the dense information storage. The crude purpose\nof condensing health records is to select the information that holds most\ncharacteristics of the original documents based on reported disease. These\nsummaries may boost diagnosis and extend a doctor's time with the patient\nduring a high workload situation like the COVID-19 pandemic. In this paper, we\npropose applying a multi-head attention-based mechanism to perform extractive\nsummarization of meaningful phrases in clinical notes. This method finds major\nsentences for a summary by correlating tokens, segments, and positional\nembeddings. The model outputs attention scores that are statistically\ntransformed to extract key phrases and can be used to projection on the\nheat-mapping tool for visual and human use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanwal_N/0/1/0/all/0/1\">Neel Kanwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizzo_G/0/1/0/all/0/1\">Giuseppe Rizzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect: Fast Error Correction with Edit Alignment for Automatic Speech Recognition. (arXiv:2105.03842v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03842","description":"<p>Error correction techniques have been used to refine the output sentences\nfrom automatic speech recognition (ASR) models and achieve a lower word error\nrate (WER) than original ASR outputs. Previous works usually use a\nsequence-to-sequence model to correct an ASR output sentence autoregressively,\nwhich causes large latency and cannot be deployed in online ASR services. A\nstraightforward solution to reduce latency, inspired by non-autoregressive\n(NAR) neural machine translation, is to use an NAR sequence generation model\nfor ASR error correction, which, however, comes at the cost of significantly\nincreased ASR error rate. In this paper, observing distinctive error patterns\nand correction operations (i.e., insertion, deletion, and substitution) in ASR,\nwe propose FastCorrect, a novel NAR error correction model based on edit\nalignment. In training, FastCorrect aligns each source token from an ASR output\nsentence to the target tokens from the corresponding ground-truth sentence\nbased on the edit distance between the source and target sentences, and\nextracts the number of target tokens corresponding to each source token during\nedition/correction, which is then used to train a length predictor and to\nadjust the source tokens to match the length of the target sentence for\nparallel generation. In inference, the token number predicted by the length\npredictor is used to adjust the source tokens for target sequence generation.\nExperiments on the public AISHELL-1 dataset and an internal industrial-scale\nASR dataset show the effectiveness of FastCorrect for ASR error correction: 1)\nit speeds up the inference by 6-9 times and maintains the accuracy (8-14% WER\nreduction) compared with the autoregressive correction model; and 2) it\noutperforms the popular NAR models adopted in neural machine translation and\ntext edition by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Ed Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding. (arXiv:2105.09996v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09996","description":"<p>We present a simplified, task-agnostic multi-modal pre-training approach that\ncan accept either video or text input, or both for a variety of end tasks.\nExisting pre-training are task-specific by adopting either a single cross-modal\nencoder that requires both modalities, limiting their use for retrieval-style\nend tasks or more complex multitask learning with two unimodal encoders,\nlimiting early cross-modal fusion. We instead introduce new pretraining masking\nschemes that better mix across modalities (e.g. by forcing masks for text to\npredict the closest video embeddings) while also maintaining separability (e.g.\nunimodal predictions are sometimes required, without using all the input).\nExperimental results show strong performance across a wider range of tasks than\nany previous methods, often outperforming task-specific pre-training. Code is\nmade available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1\">Prahal Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminzadeh_M/0/1/0/all/0/1\">Masoumeh Aminzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Inconsistency and Entity Bias in MultiWOZ. (arXiv:2105.14150v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14150","description":"<p>MultiWOZ is one of the most popular multi-domain task-oriented dialog\ndatasets, containing 10K+ annotated dialogs covering eight domains. It has been\nwidely accepted as a benchmark for various dialog tasks, e.g., dialog state\ntracking (DST), natural language generation (NLG), and end-to-end (E2E) dialog\nmodeling. In this work, we identify an overlooked issue with dialog state\nannotation inconsistencies in the dataset, where a slot type is tagged\ninconsistently across similar dialogs leading to confusion for DST modeling. We\npropose an automated correction for this issue, which is present in a whopping\n70% of the dialogs. Additionally, we notice that there is significant entity\nbias in the dataset (e.g., \"cambridge\" appears in 50% of the destination cities\nin the train domain). The entity bias can potentially lead to named entity\nmemorization in generative models, which may go unnoticed as the test set\nsuffers from a similar entity bias as well. We release a new test set with all\nentities replaced with unseen entities. Finally, we benchmark joint goal\naccuracy (JGA) of the state-of-the-art DST baselines on these modified versions\nof the data. Our experiments show that the annotation inconsistency corrections\nlead to 7-10% improvement in JGA. On the other hand, we observe a 29% drop in\nJGA when models are evaluated on the new test set with unseen entities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1\">Ahmad Beirami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Ankita De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1\">Chinnadhurai Sankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-utterance Reranking Models with BERT and Graph Convolutional Networks for Conversational Speech Recognition. (arXiv:2106.06922v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06922","description":"<p>How to effectively incorporate cross-utterance information cues into a neural\nlanguage model (LM) has emerged as one of the intriguing issues for automatic\nspeech recognition (ASR). Existing research efforts on improving\ncontextualization of an LM typically regard previous utterances as a sequence\nof additional input and may fail to capture complex global structural\ndependencies among these utterances. In view of this, we in this paper seek to\nrepresent the historical context information of an utterance as\ngraph-structured data so as to distill cross-utterances, global word\ninteraction relationships. To this end, we apply a graph convolutional network\n(GCN) on the resulting graph to obtain the corresponding GCN embeddings of\nhistorical words. GCN has recently found its versatile applications on\nsocial-network analysis, text summarization, and among others due mainly to its\nability of effectively capturing rich relational information among elements.\nHowever, GCN remains largely underexplored in the context of ASR, especially\nfor dealing with conversational speech. In addition, we frame ASR N-best\nreranking as a prediction problem, leveraging bidirectional encoder\nrepresentations from transformers (BERT) as the vehicle to not only seize the\nlocal intrinsic word regularity patterns inherent in a candidate hypothesis but\nalso incorporate the cross-utterance, historical word interaction cues\ndistilled by GCN for promoting performance. Extensive experiments conducted on\nthe AMI benchmark dataset seem to confirm the pragmatic utility of our methods,\nin relation to some current top-of-the-line methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1\">Tien-Hong Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fu-An Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation of Low-Resource Indo-European Languages. (arXiv:2108.03739v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.03739","description":"<p>In this work, we investigate methods for the challenging task of translating\nbetween low-resource language pairs that exhibit some level of similarity. In\nparticular, we consider the utility of transfer learning for translating\nbetween several Indo-European low-resource languages from the Germanic and\nRomance language families. In particular, we build two main classes of\ntransfer-based systems to study how relatedness can benefit the translation\nperformance. The primary system fine-tunes a model pre-trained on a related\nlanguage pair and the contrastive system fine-tunes one pre-trained on an\nunrelated language pair. Our experiments show that although relatedness is not\nnecessary for transfer learning to work, it does benefit model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition. (arXiv:2108.07789v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07789","description":"<p>Language models (LMs) pre-trained on massive amounts of text, in particular\nbidirectional encoder representations from Transformers (BERT), generative\npre-training (GPT), and GPT-2, have become a key technology for many natural\nlanguage processing tasks. In this paper, we present results using fine-tuned\nGPT, GPT-2, and their combination for automatic speech recognition (ASR).\nUnlike unidirectional LM GPT and GPT-2, BERT is bidirectional whose direct\nproduct of the output probabilities is no longer a valid language prior\nprobability. A conversion method is proposed to compute the correct language\nprior probability based on bidirectional LM outputs in a mathematically exact\nway. Experimental results on the widely used AMI and Switchboard ASR tasks\nshowed that the combination of the fine-tuned GPT and GPT-2 outperformed the\ncombination of three neural LMs with different architectures trained from\nscratch on the in-domain text by up to a 12% relative word error rate reduction\n(WERR). Furthermore, on the AMI corpus, the proposed conversion for language\nprior probabilities enables BERT to obtain an extra 3% relative WERR, and the\ncombination of BERT, GPT and GPT-2 results in further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xianrui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02401","description":"<p>Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2109.08186","description":"<p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.\nFaST-VGS is a Transformer-based model for learning the associations between raw\nspeech waveforms and visual images. The model unifies dual-encoder and\ncross-attention architectures into a single model, reaping the superior\nretrieval speed of the former along with the accuracy of the latter. FaST-VGS\nachieves state-of-the-art speech-image retrieval accuracy on benchmark\ndatasets, and its learned representations exhibit strong performance on the\nZeroSpeech 2021 phonetic and semantic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Invariant Properties in Natural Language Processing. (arXiv:2109.13037v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13037","description":"<p>Meaning is context-dependent, but many properties of language (should) remain\nthe same even if we transform the context. For example, sentiment, entailment,\nor speaker properties should be the same in a translation and original of a\ntext. We introduce language invariant properties: i.e., properties that should\nnot change when we transform text, and how they can be used to quantitatively\nevaluate the robustness of transformation algorithms. We use translation and\nparaphrasing as transformation examples, but our findings apply more broadly to\nany transformation. Our results indicate that many NLP transformations change\nproperties like author characteristics, i.e., make them sound more male. We\nbelieve that studying these properties will allow NLP to address both social\nfactors and pragmatic aspects of language. We also release an application suite\nthat can be used to evaluate the invariance of transformation applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Federico Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nozza_D/0/1/0/all/0/1\">Debora Nozza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1\">Dirk Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition. (arXiv:2109.13226v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2109.13226","description":"<p>We summarize the results of a host of efforts using giant automatic speech\nrecognition (ASR) models pre-trained using large, diverse unlabeled datasets\ncontaining approximately a million hours of audio. We find that the combination\nof pre-training, self-training and scaling up model size greatly increases data\nefficiency, even for extremely large tasks with tens of thousands of hours of\nlabeled data. In particular, on an ASR task with 34k hours of labeled data, by\nfine-tuning an 8 billion parameter pre-trained Conformer model we can match\nstate-of-the-art (SoTA) performance with only 3% of the training data and\nsignificantly improve SoTA with the full training set. We also report on the\nuniversal benefits gained from using big pre-trained and self-trained models\nfor a large set of downstream tasks that cover a wide range of speech domains\nand span multiple orders of magnitudes of dataset sizes, including obtaining\nSoTA performance on many public benchmarks. In addition, we utilize the learned\nrepresentation of pre-trained networks to achieve SoTA results on non-ASR\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_D/0/1/0/all/0/1\">Daniel S. Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gulati_A/0/1/0/all/0/1\">Anmol Gulati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shor_J/0/1/0/all/0/1\">Joel Shor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jansen_A/0/1/0/all/0/1\">Aren Jansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanzhong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yanping Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shibo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zongwei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yongqiang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangliang Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhifeng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation. (arXiv:2109.13318v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13318","description":"<p>Automating sign language translation (SLT) is a challenging real world\napplication. Despite its societal importance, though, research progress in the\nfield remains rather poor. Crucially, existing methods that yield viable\nperformance necessitate the availability of laborious to obtain gloss sequence\ngroundtruth. In this paper, we attenuate this need, by introducing an\nend-to-end SLT model that does not entail explicit use of glosses; the model\nonly needs text groundtruth. This is in stark contrast to existing end-to-end\nmodels that use gloss sequence groundtruth, either in the form of a modality\nthat is recognized at an intermediate model stage, or in the form of a parallel\noutput process, jointly trained with the SLT model. Our approach constitutes a\nTransformer network with a novel type of layers that combines: (i) local\nwinner-takes-all (LWTA) layers with stochastic winner sampling, instead of\nconventional ReLU layers, (ii) stochastic weights with posterior distributions\nestimated via variational inference, and (iii) a weight compression technique\nat inference time that exploits estimated posterior variance to perform\nmassive, almost lossless compression. We demonstrate that our approach can\nreach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark,\nbut without making use of glosses for model training, and with a memory\nfootprint reduced by more than 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voskou_A/0/1/0/all/0/1\">Andreas Voskou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosmopoulos_D/0/1/0/all/0/1\">Dimitrios Kosmopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. (arXiv:2109.14084v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14084","description":"<p>We present VideoCLIP, a contrastive approach to pre-train a unified model for\nzero-shot video and text understanding, without using any labels on downstream\ntasks. VideoCLIP trains a transformer for video and text by contrasting\ntemporally overlapping positive video-text pairs with hard negatives from\nnearest neighbor retrieval. Our experiments on a diverse series of downstream\ntasks, including sequence-level text-video retrieval, VideoQA, token-level\naction localization, and action segmentation reveal state-of-the-art\nperformance, surpassing prior work, and in some cases even outperforming\nsupervised approaches. Code is made available at\nhttps://github.com/pytorch/fairseq/tree/main/examples/MMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_G/0/1/0/all/0/1\">Gargi Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Yao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Fact Linking. (arXiv:2109.14364v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14364","description":"<p>Knowledge-intensive NLP tasks can benefit from linking natural language text\nwith facts from a Knowledge Graph (KG). Although facts themselves are\nlanguage-agnostic, the fact labels (i.e., language-specific representation of\nthe fact) in the KG are often present only in a few languages. This makes it\nchallenging to link KG facts to sentences in languages other than the limited\nset of languages. To address this problem, we introduce the task of\nMultilingual Fact Linking (MFL) where the goal is to link fact expressed in a\nsentence to corresponding fact in the KG, even when the fact label in the KG is\nnot available in the language of the sentence. To facilitate research in this\narea, we present a new evaluation dataset, IndicLink. This dataset contains\n11,293 linked WikiData facts and 6,429 sentences spanning English and six\nIndian languages. We propose a Retrieval+Generation model, ReFCoG, that can\nscale to millions of KG facts by combining Dual Encoder based retrieval with a\nSeq2Seq based generation model which is constrained to output only valid KG\nfacts. ReFCoG outperforms standard Retrieval+Re-ranking models by 10.7 pts in\nPrecision@1. In spite of this gain, the model achieves an overall score of\n52.1, showing ample scope for improvement in the task.ReFCoG code and IndicLink\ndata are available at https://github.com/SaiKeshav/mfl\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolluru_K/0/1/0/all/0/1\">Keshav Kolluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezk_M/0/1/0/all/0/1\">Martin Rezk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verga_P/0/1/0/all/0/1\">Pat Verga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EDGAR-CORPUS: Billions of Tokens Make The World Go Round. (arXiv:2109.14394v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14394","description":"<p>We release EDGAR-CORPUS, a novel corpus comprising annual reports from all\nthe publicly traded companies in the US spanning a period of more than 25\nyears. To the best of our knowledge, EDGAR-CORPUS is the largest financial NLP\ncorpus available to date. All the reports are downloaded, split into their\ncorresponding items (sections), and provided in a clean, easy-to-use JSON\nformat. We use EDGAR-CORPUS to train and release EDGAR-W2V, which are WORD2VEC\nembeddings for the financial domain. We employ these embeddings in a battery of\nfinancial NLP tasks and showcase their superiority over generic GloVe\nembeddings and other existing financial word embeddings. We also open-source\nEDGAR-CRAWLER, a toolkit that facilitates downloading and extracting future\nannual reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1\">Lefteris Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malakasiotis_P/0/1/0/all/0/1\">Prodromos Malakasiotis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition. (arXiv:2109.14420v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14420","description":"<p>Error correction is widely used in automatic speech recognition (ASR) to\npost-process the generated sentence, and can further reduce the word error rate\n(WER). Although multiple candidates are generated by an ASR system through beam\nsearch, current error correction approaches can only correct one sentence at a\ntime, failing to leverage the voting effect from multiple candidates to better\ndetect and correct error tokens. In this work, we propose FastCorrect 2, an\nerror correction model that takes multiple ASR candidates as input for better\ncorrection accuracy. FastCorrect 2 adopts non-autoregressive generation for\nfast inference, which consists of an encoder that processes multiple source\nsentences and a decoder that generates the target sentence in parallel from the\nadjusted source sentence, where the adjustment is based on the predicted\nduration of each source token. However, there are some issues when handling\nmultiple source sentences. First, it is non-trivial to leverage the voting\neffect from multiple source sentences since they usually vary in length. Thus,\nwe propose a novel alignment algorithm to maximize the degree of token\nalignment among multiple sentences in terms of token and pronunciation\nsimilarity. Second, the decoder can only take one adjusted source sentence as\ninput, while there are multiple source sentences. Thus, we develop a candidate\npredictor to detect the most suitable candidate for the decoder. Experiments on\nour inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce\nthe WER over the previous correction model with single candidate by 3.2% and\n2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR\nerror correction. FastCorrect 2 achieves better performance than the cascaded\nre-scoring and correction pipeline and can serve as a unified post-processing\nmodule for ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Y/0/1/0/all/0/1\">Yichong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang-Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_E/0/1/0/all/0/1\">Edward Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Fake News Detection Using Bidirectional Encoder Representations from Transformers Based Models. (arXiv:2109.14816v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14816","description":"<p>Nowadays, the development of social media allows people to access the latest\nnews easily. During the COVID-19 pandemic, it is important for people to access\nthe news so that they can take corresponding protective measures. However, the\nfake news is flooding and is a serious issue especially under the global\npandemic. The misleading fake news can cause significant loss in terms of the\nindividuals and the society. COVID-19 fake news detection has become a novel\nand important task in the NLP field. However, fake news always contain the\ncorrect portion and the incorrect portion. This fact increases the difficulty\nof the classification task. In this paper, we fine tune the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model as our\nbase model. We add BiLSTM layers and CNN layers on the top of the finetuned\nBERT model with frozen parameters or not frozen parameters methods\nrespectively. The model performance evaluation results showcase that our best\nmodel (BERT finetuned model with frozen parameters plus BiLSTM layers) achieves\nstate-of-the-art results towards COVID-19 fake news detection task. We also\nexplore keywords evaluation methods using our best model and evaluate the model\nperformance after removing keywords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuebo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prose2Poem: The Blessing of Transformers in Translating Prose to Persian Poetry. (arXiv:2109.14934v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14934","description":"<p>Persian Poetry has consistently expressed its philosophy, wisdom, speech, and\nrationale on the basis of its couplets, making it an enigmatic language on its\nown to both native and non-native speakers. Nevertheless, the notice able gap\nbetween Persian prose and poem has left the two pieces of literature\nmedium-less. Having curated a parallel corpus of prose and their equivalent\npoems, we introduce a novel Neural Machine Translation (NMT) approach to\ntranslate prose to ancient Persian poetry using transformer-based Language\nModels in an extremely low-resource setting. More specifically, we trained a\nTransformer model from scratch to obtain initial translations and pretrained\ndifferent variations of BERT to obtain final translations. To address the\nchallenge of using masked language modelling under poeticness criteria, we\nheuristically joined the two models and generated valid poems in terms of\nautomatic and human assessments. Final results demonstrate the eligibility and\ncreativity of our novel heuristically aided approach among Literature\nprofessionals and non-professionals in generating novel Persian poems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirshafiee_M/0/1/0/all/0/1\">Mitra Sadat Mirshafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouryabi_Y/0/1/0/all/0/1\">Yazdan Rezaee Jouryabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}