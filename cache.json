{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v1 [eess.AS])","link":"http://arxiv.org/abs/2109.08186","description":"<p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.\nFaST-VGS is a Transformer-based model for learning the associations between raw\nspeech waveforms and visual images. The model unifies dual-encoder and\ncross-attention architectures into a single model, reaping the superior\nretrieval speed of the former along with the accuracy of the latter. FaST-VGS\nachieves state-of-the-art speech-image retrieval accuracy on benchmark\ndatasets, and its learned representations exhibit strong performance on the\nZeroSpeech 2021 phonetic and semantic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Numerical reasoning in machine reading comprehension tasks: are we there yet?. (arXiv:2109.08207v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08207","description":"<p>Numerical reasoning based machine reading comprehension is a task that\ninvolves reading comprehension along with using arithmetic operations such as\naddition, subtraction, sorting, and counting. The DROP benchmark (Dua et al.,\n2019) is a recent dataset that has inspired the design of NLP models aimed at\nsolving this task. The current standings of these models in the DROP\nleaderboard, over standard metrics, suggest that the models have achieved\nnear-human performance. However, does this mean that these models have learned\nto reason? In this paper, we present a controlled study on some of the\ntop-performing model architectures for the task of numerical reasoning. Our\nobservations suggest that the standard metrics are incapable of measuring\nprogress towards such tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Negheimish_H/0/1/0/all/0/1\">Hadeel Al-Negheimish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08214","description":"<p>When humans conceive how to perform a particular task, they do so\nhierarchically: splitting higher-level tasks into smaller sub-tasks. However,\nin the literature on natural language (NL) command of situated agents, most\nworks have treated the procedures to be executed as flat sequences of simple\nactions, or any hierarchies of procedures have been shallow at best. In this\npaper, we propose a formalism of procedures as programs, a powerful yet\nintuitive method of representing hierarchical procedural knowledge for agent\ncommand and control. We further propose a modeling paradigm of hierarchical\nmodular networks, which consist of a planner and reactors that convert NL\nintents to predictions of executable programs and probe the environment for\ninformation necessary to complete the program execution. We instantiate this\nframework on the IQA and ALFRED datasets for NL instruction following. Our\nmodel outperforms reactive baselines by a large margin on both datasets. We\nalso demonstrate that our framework is more data-efficient, and that it allows\nfor fast iterative development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bag of Tricks for Dialogue Summarization. (arXiv:2109.08232v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08232","description":"<p>Dialogue summarization comes with its own peculiar challenges as opposed to\nnews or scientific articles summarization. In this work, we explore four\ndifferent challenges of the task: handling and differentiating parts of the\ndialogue belonging to multiple speakers, negation understanding, reasoning\nabout the situation, and informal language understanding. Using a pretrained\nsequence-to-sequence language model, we explore speaker name substitution,\nnegation scope highlighting, multi-task learning with relevant tasks, and\npretraining on in-domain data. Our experiments show that our proposed\ntechniques indeed improve summarization performance, outperforming strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularized Training of Nearest Neighbor Language Models. (arXiv:2109.08249v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08249","description":"<p>Including memory banks in a natural language processing architecture\nincreases model capacity by equipping it with additional data at inference\ntime. In this paper, we build upon $k$NN-LM \\citep{khandelwal20generalization},\nwhich uses a pre-trained language model together with an exhaustive $k$NN\nsearch through the training data (memory bank) to achieve state-of-the-art\nresults. We investigate whether we can improve the $k$NN-LM performance by\ninstead training a LM with the knowledge that we will be using a $k$NN\npost-hoc. We achieved significant improvement using our method on language\nmodeling tasks on \\texttt{WIKI-2} and \\texttt{WIKI-103}. The main phenomenon\nthat we encounter is that adding a simple L2 regularization on the activations\n(not weights) of the model, a transformer, improves the post-hoc $k$NN\nclassification performance. We explore some possible reasons for this\nimprovement. In particular, we find that the added L2 regularization seems to\nimprove the performance for high-frequency words without deteriorating the\nperformance for low frequency ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ton_J/0/1/0/all/0/1\">Jean-Francois Ton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing out Bias: Achieving Fairness Through Training Reweighting. (arXiv:2109.08253v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08253","description":"<p>Bias in natural language processing arises primarily from models learning\ncharacteristics of the author such as gender and race when modelling tasks such\nas sentiment and syntactic parsing. This problem manifests as disparities in\nerror rates across author demographics, typically disadvantaging minority\ngroups. Existing methods for mitigating and measuring bias do not directly\naccount for correlations between author demographics and linguistic variables.\nMoreover, evaluation of bias has been inconsistent in previous work, in terms\nof dataset balance and evaluation methods. This paper introduces a very simple\nbut highly effective method for countering bias using instance reweighting,\nbased on the frequency of both task labels and author demographics. We extend\nthe method in the form of a gated model which incorporates the author\ndemographic as an input, and show that while it is highly vulnerable to input\ndata bias, it provides debiased predictions through demographic input\nperturbation, and outperforms all other bias mitigation techniques when\ncombined with instance reweighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08256","description":"<p>The importance and pervasiveness of emotions in our lives makes affective\ncomputing a tremendously important and vibrant line of work. Systems for\nautomatic emotion recognition (AER) and sentiment analysis can be facilitators\nof enormous progress (e.g., in improving public health and commerce) but also\nenablers of great harm (e.g., for suppressing dissidents and manipulating\nvoters). Thus, it is imperative that the affective computing community actively\nengage with the ethical ramifications of their creations. In this paper, I have\nsynthesized and organized information from AI Ethics and Emotion Recognition\nliterature to present fifty ethical considerations relevant to AER. Notably,\nthe sheet fleshes out assumptions hidden in how AER is commonly framed, and in\nthe choices often made regarding the data, method, and evaluation. Special\nattention is paid to the implications of AER on privacy and social groups. The\nobjective of the sheet is to facilitate and encourage more thoughtfulness on\nwhy to automate, how to automate, and how to judge success well before the\nbuilding of AER systems. Additionally, the sheet acts as a useful introductory\ndocument on emotion recognition (complementing survey articles).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-training with Few-shot Rationalization: Teacher Explanations Aid Student in Few-shot NLU. (arXiv:2109.08259v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08259","description":"<p>While pre-trained language models have obtained state-of-the-art performance\nfor several natural language understanding tasks, they are quite opaque in\nterms of their decision-making process. While some recent works focus on\nrationalizing neural predictions by highlighting salient concepts in the text\nas justifications or rationales, they rely on thousands of labeled training\nexamples for both task labels as well as an-notated rationales for every\ninstance. Such extensive large-scale annotations are infeasible to obtain for\nmany tasks. To this end, we develop a multi-task teacher-student framework\nbased on self-training language models with limited task-specific labels and\nrationales, and judicious sample selection to learn from informative\npseudo-labeled examples1. We study several characteristics of what constitutes\na good rationale and demonstrate that the neural model performance can be\nsignificantly improved by making it aware of its rationalized predictions,\nparticularly in low-resource settings. Extensive experiments in several\nbench-mark datasets demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Meghana Moorthy Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v1 [cs.AI])","link":"http://arxiv.org/abs/2109.08270","description":"<p>Language models (LMs) are sentence-completion engines trained on massive\ncorpora. LMs have emerged as a significant breakthrough in natural-language\nprocessing, providing capabilities that go far beyond sentence completion\nincluding question answering, summarization, and natural-language inference.\nWhile many of these capabilities have potential application to cognitive\nsystems, exploiting language models as a source of task knowledge, especially\nfor task learning, offers significant, near-term benefits. We introduce\nlanguage models and the various tasks to which they have been applied and then\nreview methods of knowledge extraction from language models. The resulting\nanalysis outlines both the challenges and opportunities for using language\nmodels as a new knowledge source for cognitive systems. It also identifies\npossible ways to improve knowledge extraction from language models using the\ncapabilities provided by cognitive systems. Central to success will be the\nability of a cognitive agent to itself learn an abstract model of the knowledge\nimplicit in the LM as well as methods to extract high-quality knowledge\neffectively and efficiently. To illustrate, we introduce a hypothetical robot\nagent and describe how language models could extend its task knowledge and\nimprove its performance and the kinds of knowledge and methods the agent can\nuse to exploit the knowledge within a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wray_R/0/1/0/all/0/1\">Robert E. Wray, III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_J/0/1/0/all/0/1\">James R. Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_J/0/1/0/all/0/1\">John E. Laird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis. (arXiv:2109.08306v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08306","description":"<p>Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment\nanalysis task that aims to extract aspects, classify corresponding sentiment\npolarities and find opinions as the causes of sentiment. The latest research\ntends to solve the ABSA task in a unified way with end-to-end frameworks. Yet,\nthese frameworks get fine-tuned from downstream tasks without any task-adaptive\nmodification. Specifically, they do not use task-related knowledge well or\nexplicitly model relations between aspect and opinion terms, hindering them\nfrom better performance. In this paper, we propose SentiPrompt to use sentiment\nknowledge enhanced prompts to tune the language model in the unified framework.\nWe inject sentiment knowledge regarding aspects, opinions, and polarities into\nprompt and explicitly model term relations via constructing consistency and\npolarity judgment templates from the ground truth triplets. Experimental\nresults demonstrate that our approach can outperform strong baselines on\nTriplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment\nClassification by a notable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feiyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zirui Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongpan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal Sentiment Dataset for Video Recommendation. (arXiv:2109.08333v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08333","description":"<p>Recently, multimodal sentiment analysis has seen remarkable advance and a lot\nof datasets are proposed for its development. In general, current multimodal\nsentiment analysis datasets usually follow the traditional system of\nsentiment/emotion, such as positive, negative and so on. However, when applied\nin the scenario of video recommendation, the traditional sentiment/emotion\nsystem is hard to be leveraged to represent different contents of videos in the\nperspective of visual senses and language understanding. Based on this, we\npropose a multimodal sentiment analysis dataset, named baiDu Video Sentiment\ndataset (DuVideoSenti), and introduce a new sentiment system which is designed\nto describe the sentimental style of a video on recommendation scenery.\nSpecifically, DuVideoSenti consists of 5,630 videos which displayed on Baidu,\neach video is manually annotated with a sentimental style label which describes\nthe user's real feeling of a video. Furthermore, we propose UNIMO as our\nbaseline for DuVideoSenti. Experimental results show that DuVideoSenti brings\nnew challenges to multimodal sentiment analysis, and could be used as a new\nbenchmark for evaluating approaches designed for video understanding and\nmultimodal fusion. We also expect our proposed DuVideoSenti could further\nimprove the development of multimodal sentiment analysis and its application to\nvideo recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-adaptive Pre-training of Language Models with Word Embedding Regularization. (arXiv:2109.08354v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08354","description":"<p>Pre-trained language models (PTLMs) acquire domain-independent linguistic\nknowledge through pre-training with massive textual resources. Additional\npre-training is effective in adapting PTLMs to domains that are not well\ncovered by the pre-training corpora. Here, we focus on the static word\nembeddings of PTLMs for domain adaptation to teach PTLMs domain-specific\nmeanings of words. We propose a novel fine-tuning process: task-adaptive\npre-training with word embedding regularization (TAPTER). TAPTER runs\nadditional pre-training by making the static word embeddings of a PTLM close to\nthe word embeddings obtained in the target domain with fastText. TAPTER\nrequires no additional corpus except for the training data of the downstream\ntask. We confirmed that TAPTER improves the performance of the standard\nfine-tuning and the task-adaptive pre-training on BioASQ (question answering in\nthe biomedical domain) and on SQuAD (the Wikipedia domain) when their\npre-training corpora were not dominated by in-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kyosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_S/0/1/0/all/0/1\">Sen Yoshida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Linguistic Context for Language Model Compression. (arXiv:2109.08359v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08359","description":"<p>A computationally expensive and memory intensive neural network lies behind\nthe recent success of language representation learning. Knowledge distillation,\na major technique for deploying such a vast language model in resource-scarce\nenvironments, transfers the knowledge on individual word representations\nlearned without restrictions. In this paper, inspired by the recent\nobservations that language representations are relatively positioned and have\nmore semantic knowledge as a whole, we present a new knowledge distillation\nobjective for language representation learning that transfers the contextual\nknowledge via two types of relationships across representations: Word Relation\nand Layer Transforming Relation. Unlike other recent distillation techniques\nfor the language models, our contextual distillation does not have any\nrestrictions on architectural changes between teacher and student. We validate\nthe effectiveness of our method on challenging benchmarks of language\nunderstanding tasks, not only in architectures of various sizes, but also in\ncombination with DynaBERT, the recently proposed adaptive size pruning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Geondo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongman Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeQA: A Question Answering Dataset for Source Code Comprehension. (arXiv:2109.08365v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08365","description":"<p>We propose CodeQA, a free-form question answering dataset for the purpose of\nsource code comprehension: given a code snippet and a question, a textual\nanswer is required to be generated. CodeQA contains a Java dataset with 119,778\nquestion-answer pairs and a Python dataset with 70,085 question-answer pairs.\nTo obtain natural and faithful questions and answers, we implement syntactic\nrules and semantic analysis to transform code comments into question-answer\npairs. We present the construction process and conduct systematic analysis of\nour dataset. Experiment results achieved by several neural baselines on our\ndataset are shown and discussed. While research on question-answering and\nmachine reading comprehension develops rapidly, few prior work has drawn\nattention to code question answering. This new dataset can serve as a useful\nresearch benchmark for source code comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To be Closer: Learning to Link up Aspects with Opinions. (arXiv:2109.08382v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08382","description":"<p>Dependency parse trees are helpful for discovering the opinion words in\naspect-based sentiment analysis (ABSA). However, the trees obtained from\noff-the-shelf dependency parsers are static, and could be sub-optimal in ABSA.\nThis is because the syntactic trees are not designed for capturing the\ninteractions between opinion words and aspect words. In this work, we aim to\nshorten the distance between aspects and corresponding opinion words by\nlearning an aspect-centric tree structure. The aspect and opinion words are\nexpected to be closer along such tree structure compared to the standard\ndependency parse tree. The learning process allows the tree structure to\nadaptively correlate the aspect and opinion words, enabling us to better\nidentify the polarity in the ABSA task. We conduct experiments on five\naspect-based sentiment datasets, and the proposed model significantly\noutperforms recent strong baselines. Furthermore, our thorough analysis\ndemonstrates the average distance between aspect and opinion words are\nshortened by at least 19% on the standard SemEval Restaurant14 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lejian Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"reproducing \"ner and pos when nothing is capitalized\". (arXiv:2109.08396v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08396","description":"<p>Capitalization is an important feature in many NLP tasks such as Named Entity\nRecognition (NER) or Part of Speech Tagging (POS). We are trying to reproduce\nresults of paper which shows how to mitigate a significant performance drop\nwhen casing is mismatched between training and testing data. In particular we\nshow that lowercasing 50% of the dataset provides the best performance,\nmatching the claims of the original paper. We also show that we got slightly\nlower performance in almost all experiments we have tried to reproduce,\nsuggesting that there might be some hidden factors impacting our performance.\nLastly, we make all of our work available in a public github repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuster_A/0/1/0/all/0/1\">Andreas Kuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipek_J/0/1/0/all/0/1\">Jakub Filipek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muppirala_V/0/1/0/all/0/1\">Viswa Virinchi Muppirala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08406","description":"<p>Despite the success of fine-tuning pretrained language encoders like BERT for\ndownstream natural language understanding (NLU) tasks, it is still poorly\nunderstood how neural networks change after fine-tuning. In this work, we use\ncentered kernel alignment (CKA), a method for comparing learned\nrepresentations, to measure the similarity of representations in task-tuned\nmodels across layers. In experiments across twelve NLU tasks, we discover a\nconsistent block diagonal structure in the similarity of representations within\nfine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of\nearlier and later layers, but not between them. The similarity of later layer\nrepresentations implies that later layers only marginally contribute to task\nperformance, and we verify in experiments that the top few layers of fine-tuned\nTransformers can be discarded without hurting performance, even with no further\ntuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haokun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis. (arXiv:2109.08412v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08412","description":"<p>Chatbot is increasingly thriving in different domains, however, because of\nunexpected discourse complexity and training data sparseness, its potential\ndistrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff\n(MHCH), predicting chatbot failure and enabling human-algorithm collaboration\nto enhance chatbot quality, has attracted increasing attention from industry\nand academia. In this study, we propose a novel model, Role-Selected Sharing\nNetwork (RSSN), which integrates both dialogue satisfaction estimation and\nhandoff prediction in one multi-task learning framework. Unlike prior efforts\nin dialog mining, by utilizing local user satisfaction as a bridge, global\nsatisfaction detector and handoff predictor can effectively exchange critical\ninformation. Specifically, we decouple the relation and interaction between the\ntwo tasks by the role information after the shared encoder. Extensive\nexperiments on two public datasets demonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaisong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Guoxiu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoren Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Students on Sesame Street: What Order-Aware Matrix Embeddings Can Learn from BERT. (arXiv:2109.08449v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08449","description":"<p>Large-scale pretrained language models (PreLMs) are revolutionizing natural\nlanguage processing across all benchmarks. However, their sheer size is\nprohibitive in low-resource or large-scale applications. While common\napproaches reduce the size of PreLMs via same-architecture distillation or\npruning, we explore distilling PreLMs into more efficient order-aware embedding\nmodels. Our results on the GLUE benchmark show that embedding-centric students,\nwhich have learned from BERT, yield scores comparable to DistilBERT on QQP and\nRTE, often match or exceed the scores of ELMo, and only fall behind on\ndetecting linguistic acceptability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuber_I/0/1/0/all/0/1\">Isabelle Cuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1\">Christoph Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nolscher_H/0/1/0/all/0/1\">Henrik Ferdinand N&#xf6;lscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonderecker_A/0/1/0/all/0/1\">Angelina Sonderecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Unification for Logic Reasoning over Natural Language. (arXiv:2109.08460v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08460","description":"<p>Automated Theorem Proving (ATP) deals with the development of computer\nprograms being able to show that some conjectures (queries) are a logical\nconsequence of a set of axioms (facts and rules). There exists several\nsuccessful ATPs where conjectures and axioms are formally provided (e.g.\nformalised as First Order Logic formulas). Recent approaches, such as (Clark et\nal., 2020), have proposed transformer-based architectures for deriving\nconjectures given axioms expressed in natural language (English). The\nconjecture is verified through a binary text classifier, where the transformers\nmodel is trained to predict the truth value of a conjecture given the axioms.\nThe RuleTaker approach of (Clark et al., 2020) achieves appealing results both\nin terms of accuracy and in the ability to generalize, showing that when the\nmodel is trained with deep enough queries (at least 3 inference steps), the\ntransformers are able to correctly answer the majority of queries (97.6%) that\nrequire up to 5 inference steps. In this work we propose a new architecture,\nnamely the Neural Unifier, and a relative training procedure, which achieves\nstate-of-the-art results in term of generalisation, showing that mimicking a\nwell-known inference procedure, the backward chaining, it is possible to answer\ndeep queries even when the model is trained only on shallow ones. The approach\nis demonstrated in experiments using a diverse set of benchmark data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picco_G/0/1/0/all/0/1\">Gabriele Picco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1\">Hoang Thanh Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbodio_M/0/1/0/all/0/1\">Marco Luca Sbodio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_V/0/1/0/all/0/1\">Vanessa Lopez Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08475","description":"<p>Visual dialog, which aims to hold a meaningful conversation with humans about\na given image, is a challenging task that requires models to reason the complex\ndependencies among visual content, dialog history, and current questions. Graph\nneural networks are recently applied to model the implicit relations between\nobjects in an image or dialog. However, they neglect the importance of 1)\ncoreference relations among dialog history and dependency relations between\nwords for the question representation; and 2) the representation of the image\nbased on the fully represented question. Therefore, we propose a novel\nrelation-aware graph-over-graph network (GoG) for visual dialog. Specifically,\nGoG consists of three sequential graphs: 1) H-Graph, which aims to capture\ncoreference relations among dialog history; 2) History-aware Q-Graph, which\naims to fully understand the question through capturing dependency relations\nbetween words based on coreference resolution on the dialog history; and 3)\nQuestion-aware I-Graph, which aims to capture the relations between objects in\nan image based on fully question representation. As an additional feature\nrepresentation module, we add GoG to the existing visual dialogue model.\nExperimental results show that our model outperforms the strong baseline in\nboth generative and discriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation. (arXiv:2109.08478v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08478","description":"<p>Visual dialogue is a challenging task since it needs to answer a series of\ncoherent questions on the basis of understanding the visual environment.\nPrevious studies focus on the implicit exploration of multimodal co-reference\nby implicitly attending to spatial image features or object-level image\nfeatures but neglect the importance of locating the objects explicitly in the\nvisual content, which is associated with entities in the textual content.\nTherefore, in this paper we propose a {\\bf M}ultimodal {\\bf I}ncremental {\\bf\nT}ransformer with {\\bf V}isual {\\bf G}rounding, named MITVG, which consists of\ntwo key parts: visual grounding and multimodal incremental transformer. Visual\ngrounding aims to explicitly locate related objects in the image guided by\ntextual entities, which helps the model exclude the visual content that does\nnot need attention. On the basis of visual grounding, the multimodal\nincremental transformer encodes the multi-turn dialogue history combined with\nvisual scene step by step according to the order of the dialogue and then\ngenerates a contextually and visually coherent response. Experimental results\non the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the\nproposed model, which achieves comparable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08535","description":"<p>Open-domain question answering has exploded in popularity recently due to the\nsuccess of dense retrieval models, which have surpassed sparse models using\nonly a few supervised training examples. However, in this paper, we demonstrate\ncurrent dense models are not yet the holy grail of retrieval. We first\nconstruct EntityQuestions, a set of simple, entity-rich questions based on\nfacts from Wikidata (e.g., \"Where was Arve Furset born?\"), and observe that\ndense retrievers drastically underperform sparse methods. We investigate this\nissue and uncover that dense retrievers can only generalize to common entities\nunless the question pattern is explicitly observed during training. We discuss\ntwo simple solutions towards addressing this critical problem. First, we\ndemonstrate that data augmentation is unable to fix the generalization problem.\nSecond, we argue a more robust passage encoder helps facilitate better question\nadaptation using specialized question encoders. We hope our work can shed light\non the challenges in creating a robust, universal dense retriever that works\nwell across different input distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sciavolino_C/0/1/0/all/0/1\">Christopher Sciavolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules. (arXiv:2109.08544v1 [cs.AI])","link":"http://arxiv.org/abs/2109.08544","description":"<p>One of the challenges faced by conversational agents is their inability to\nidentify unstated presumptions of their users' commands, a task trivial for\nhumans due to their common sense. In this paper, we propose a zero-shot\ncommonsense reasoning system for conversational agents in an attempt to achieve\nthis. Our reasoner uncovers unstated presumptions from user commands satisfying\na general template of if-(state), then-(action), because-(goal). Our reasoner\nuses a state-of-the-art transformer-based generative commonsense knowledge base\n(KB) as its source of background knowledge for reasoning. We propose a novel\nand iterative knowledge query mechanism to extract multi-hop reasoning chains\nfrom the neural KB which uses symbolic logic rules to significantly reduce the\nsearch space. Similar to any KBs gathered to date, our commonsense KB is prone\nto missing knowledge. Therefore, we propose to conversationally elicit the\nmissing knowledge from human users with our novel dynamic question generation\nstrategy, which generates and presents contextualized queries to human users.\nWe evaluate the model with a user study with human users that achieves a 35%\nhigher success rate compared to SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1\">Forough Arabshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jennifer Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slot Filling for Biomedical Information Extraction. (arXiv:2109.08564v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08564","description":"<p>Information Extraction (IE) from text refers to the task of extracting\nstructured knowledge from unstructured text. The task typically consists of a\nseries of sub-tasks such as Named Entity Recognition and Relation Extraction.\nSourcing entity and relation type specific training data is a major bottleneck\nin the above sub-tasks.In this work we present a slot filling approach to the\ntask of biomedical IE, effectively replacing the need for entity and\nrelation-specific training data, allowing to deal with zero-shot settings. We\nfollow the recently proposed paradigm of coupling a Tranformer-based\nbi-encoder, Dense Passage Retrieval, with a Transformer-based reader model to\nextract relations from biomedical text. We assemble a biomedical slot filling\ndataset for both retrieval and reading comprehension and conduct a series of\nexperiments demonstrating that our approach outperforms a number of simpler\nbaselines. We also evaluate our approach end-to-end for standard as well as\nzero-shot settings. Our work provides a fresh perspective on how to solve\nbiomedical IE tasks, in the absence of relevant training data. Our code, models\nand pretrained data are available at\nhttps://github.com/healx/biomed-slot-filling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papanikolaou_Y/0/1/0/all/0/1\">Yannis Papanikolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_F/0/1/0/all/0/1\">Francine Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Multitask Learning for Low-Resource AbstractiveSummarization. (arXiv:2109.08565v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08565","description":"<p>This paper explores the effect of using multitask learning for abstractive\nsummarization in the context of small training corpora. In particular, we\nincorporate four different tasks (extractive summarization, language modeling,\nconcept detection, and paraphrase detection) both individually and in\ncombination, with the goal of enhancing the target task of abstractive\nsummarization via multitask learning. We show that for many task combinations,\na model trained in a multitask setting outperforms a model trained only for\nabstractive summarization, with no additional summarization data introduced.\nAdditionally, we do a comprehensive search and find that certain tasks (e.g.\nparaphrase detection) consistently benefit abstractive summarization, not only\nwhen combined with other tasks but also when using different architectures and\ntraining corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magooda_A/0/1/0/all/0/1\">Ahmed Magooda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elaraby_M/0/1/0/all/0/1\">Mohamed Elaraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization. (arXiv:2109.08569v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08569","description":"<p>This paper explores three simple data manipulation techniques (synthesis,\naugmentation, curriculum) for improving abstractive summarization models\nwithout the need for any additional data. We introduce a method of data\nsynthesis with paraphrasing, a data augmentation technique with sample mixing,\nand curriculum learning with two new difficulty metrics based on specificity\nand abstractiveness. We conduct experiments to show that these three techniques\ncan help improve abstractive summarization across two summarization models and\ntwo different small datasets. Furthermore, we show that these techniques can\nimprove performance when applied in isolation and when combined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magooda_A/0/1/0/all/0/1\">Ahmed Magooda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchy-Aware T5 with Path-Adaptive Mask Mechanism for Hierarchical Text Classification. (arXiv:2109.08585v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08585","description":"<p>Hierarchical Text Classification (HTC), which aims to predict text labels\norganized in hierarchical space, is a significant task lacking in investigation\nin natural language processing. Existing methods usually encode the entire\nhierarchical structure and fail to construct a robust label-dependent model,\nmaking it hard to make accurate predictions on sparse lower-level labels and\nachieving low Macro-F1. In this paper, we propose a novel PAMM-HiA-T5 model for\nHTC: a hierarchy-aware T5 model with path-adaptive mask mechanism that not only\nbuilds the knowledge of upper-level labels into low-level ones but also\nintroduces path dependency information in label prediction. Specifically, we\ngenerate a multi-level sequential label structure to exploit hierarchical\ndependency across different levels with Breadth-First Search (BFS) and T5\nmodel. To further improve label dependency prediction within each path, we then\npropose an original path-adaptive mask mechanism (PAMM) to identify the label's\npath information, eliminating sources of noises from other paths. Comprehensive\nexperiments on three benchmark datasets show that our novel PAMM-HiA-T5 model\ngreatly outperforms all state-of-the-art HTC approaches especially in Macro-F1.\nThe ablation studies show that the improvements mainly come from our innovative\napproach instead of T5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yihua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhaoming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guiquan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Commonsense help in detecting Sarcasm?. (arXiv:2109.08588v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08588","description":"<p>Sarcasm detection is important for several NLP tasks such as sentiment\nidentification in product reviews, user feedback, and online forums. It is a\nchallenging task requiring a deep understanding of language, context, and world\nknowledge. In this paper, we investigate whether incorporating commonsense\nknowledge helps in sarcasm detection. For this, we incorporate commonsense\nknowledge into the prediction process using a graph convolution network with\npre-trained language model embeddings as input. Our experiments with three\nsarcasm detection datasets indicate that the approach does not outperform the\nbaseline model. We perform an exhaustive set of experiments to analyze where\ncommonsense support adds value and where it hurts classification. Our\nimplementation is publicly available at:\nhttps://github.com/brcsomnath/commonsense-sarcasm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Transformers for Job Expression Extraction and Classification in a Low-Resource Setting. (arXiv:2109.08597v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08597","description":"<p>In this paper, we explore possible improvements of transformer models in a\nlow-resource setting. In particular, we present our approaches to tackle the\nfirst two of three subtasks of the MEDDOPROF competition, i.e., the extraction\nand classification of job expressions in Spanish clinical texts. As neither\nlanguage nor domain experts, we experiment with the multilingual XLM-R\ntransformer model and tackle these low-resource information extraction tasks as\nsequence-labeling problems. We explore domain- and language-adaptive\npretraining, transfer learning and strategic datasplits to boost the\ntransformer model. Our results show strong improvements using these methods by\nup to 5.3 F1 points compared to a fine-tuned XLM-R model. Our best models\nachieve 83.2 and 79.3 F1 for the first two tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The futility of STILTs for the classification of lexical borrowings in Spanish. (arXiv:2109.08607v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08607","description":"<p>The first edition of the IberLEF 2021 shared task on automatic detection of\nborrowings (ADoBo) focused on detecting lexical borrowings that appeared in the\nSpanish press and that have recently been imported into the Spanish language.\nIn this work, we tested supplementary training on intermediate labeled-data\ntasks (STILTs) from part of speech (POS), named entity recognition (NER),\ncode-switching, and language identification approaches to the classification of\nborrowings at the token level using existing pre-trained transformer-based\nlanguage models. Our extensive experimental results suggest that STILTs do not\nprovide any improvement over direct fine-tuning of multilingual models.\nHowever, multilingual models trained on small subsets of languages perform\nreasonably better than multilingual BERT but not as good as multilingual\nRoBERTa for the given dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_J/0/1/0/all/0/1\">Javier de la Rosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Scrubbing of Demographic Information for Text Classification. (arXiv:2109.08613v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08613","description":"<p>Contextual representations learned by language models can often encode\nundesirable attributes, like demographic associations of the users, while being\ntrained for an unrelated target task. We aim to scrub such undesirable\nattributes and learn fair representations while maintaining performance on the\ntarget task. In this paper, we present an adversarial learning framework\n\"Adversarial Scrubber\" (ADS), to debias contextual representations. We perform\ntheoretical analysis to show that our framework converges without leaking\ndemographic information under certain conditions. We extend previous evaluation\ntechniques by evaluating debiasing performance using Minimum Description Length\n(MDL) probing. Experimental evaluations on 8 datasets show that ADS generates\nrepresentations with minimal information about demographic attributes while\nbeing maximally informative about the target task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier B. Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CKMorph: A Comprehensive Morphological Analyzer for Central Kurdish. (arXiv:2109.08615v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08615","description":"<p>A morphological analyzer, which is a significant component of many natural\nlanguage processing applications especially for morphologically rich languages,\ndivides an input word into all its composing morphemes and identifies their\nmorphological roles. In this paper, we introduce a comprehensive morphological\nanalyzer for Central Kurdish (CK), a low-resourced language with a rich\nmorphology. Building upon the limited existing literature, we first assembled\nand systematically categorized a comprehensive collection of the morphological\nand morphophonological rules of the language. Additionally, we collected and\nmanually labeled a generative lexicon containing nearly 10,000 verb, noun and\nadjective stems, named entities, and other types of word stems. We used these\nrule sets and resources to implement CKMorph Analyzer based on finite-state\ntransducers. In order to provide a benchmark for future research, we collected,\nmanually labeled, and publicly shared test sets for evaluating accuracy and\ncoverage of the analyzer. CKMorph was able to correctly analyze 95.9% of the\naccuracy test set, containing 1,000 CK words morphologically analyzed according\nto the context. Moreover, CKMorph gave at least one analysis for 95.5% of 4.22M\nCK tokens of the coverage test set. The demonstration of the application and\nresources including CK verb database and test sets are openly accessible at\nhttps://github.com/CKMorph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naserzade_M/0/1/0/all/0/1\">Morteza Naserzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmudi_A/0/1/0/all/0/1\">Aso Mahmudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1\">Hawre Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MohammadAmini_M/0/1/0/all/0/1\">Mohammad MohammadAmini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications. (arXiv:2109.08627v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08627","description":"<p>Sentence-level Quality estimation (QE) of machine translation is\ntraditionally formulated as a regression task, and the performance of QE models\nis typically measured by Pearson correlation with human labels. Recent QE\nmodels have achieved previously-unseen levels of correlation with human\njudgments, but they rely on large multilingual contextualized language models\nthat are computationally expensive and make them infeasible for real-world\napplications. In this work, we evaluate several model compression techniques\nfor QE and find that, despite their popularity in other NLP tasks, they lead to\npoor performance in this regression setting. We observe that a full model\nparameterization is required to achieve SoTA results in a regression task.\nHowever, we argue that the level of expressiveness of a model in a continuous\nrange is unnecessary given the downstream applications of QE, and show that\nreframing QE as a classification problem and evaluating QE models using\nclassification metrics would better reflect their actual performance in\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?. (arXiv:2109.08634v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08634","description":"<p>Models designed for intelligent process automation are required to be capable\nof grounding user interface elements. This task of interface element grounding\nis centred on linking instructions in natural language to their target\nreferents. Even though BERT and similar pre-trained language models have\nexcelled in several NLP tasks, their use has not been widely explored for the\nUI grounding domain. This work concentrates on testing and probing the\ngrounding abilities of three different transformer-based models: BERT, RoBERTa\nand LayoutLM. Our primary focus is on these models' spatial reasoning skills,\ngiven their importance in this domain. We observe that LayoutLM has a promising\nadvantage for applications in this domain, even though it was created for a\ndifferent original purpose (representing scanned documents): the learned\nspatial features appear to be transferable to the UI grounding setting,\nespecially as they demonstrate the ability to discriminate between target\ndirections in natural language instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubba_K/0/1/0/all/0/1\">Krishna Dubba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Weiwei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Measuring of Readability to Improve Documents Accessibility for Arabic Language Learners. (arXiv:2109.08648v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08648","description":"<p>This paper presents an approach based on supervised machine learning methods\nto build a classifier that can identify text complexity in order to present\nArabic language learners with texts suitable to their levels. The approach is\nbased on machine learning classification methods to discriminate between the\ndifferent levels of difficulty in reading and understanding a text. Several\nmodels were trained on a large corpus mined from online Arabic websites and\nmanually annotated. The model uses both Count and TF-IDF representations and\napplies five machine learning algorithms; Multinomial Naive Bayes, Bernoulli\nNaive Bayes, Logistic Regression, Support Vector Machine and Random Forest,\nusing unigrams and bigrams features. With the goal of extracting the text\ncomplexity, the problem is usually addressed by formulating the level\nidentification as a classification task. Experimental results showed that\nn-gram features could be indicative of the reading level of a text and could\nsubstantially improve performance, and showed that SVM and Multinomial Naive\nBayes are the most accurate in predicting the complexity level. Best results\nwere achieved using TF-IDF Vectors trained by a combination of word-based\nunigrams and bigrams with an overall accuracy of 87.14% over four classes of\ncomplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bessou_S/0/1/0/all/0/1\">Sadik Bessou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenni_G/0/1/0/all/0/1\">Ghozlane Chenni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Handling Unconstrained User Preferences in Dialogue. (arXiv:2109.08650v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08650","description":"<p>A user input to a schema-driven dialogue information navigation system, such\nas venue search, is typically constrained by the underlying database which\nrestricts the user to specify a predefined set of preferences, or slots,\ncorresponding to the database fields. We envision a more natural information\nnavigation dialogue interface where a user has flexibility to specify\nunconstrained preferences that may not match a predefined schema. We propose to\nuse information retrieval from unstructured knowledge to identify entities\nrelevant to a user request. We update the Cambridge restaurants database with\nunstructured knowledge snippets (reviews and information from the web) for each\nof the restaurants and annotate a set of query-snippet pairs with a relevance\nlabel. We use the annotated dataset to train and evaluate snippet relevance\nclassifiers, as a proxy to evaluating recommendation accuracy. We show that\nwith a pretrained transformer model as an encoder, an unsupervised/supervised\nclassifier achieves a weighted F1 of .661/.856.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Suraj Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanchev_S/0/1/0/all/0/1\">Svetlana Stoyanchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Primer: Searching for Efficient Transformers for Language Modeling. (arXiv:2109.08668v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08668","description":"<p>Large Transformer models have been central to recent advances in natural\nlanguage processing. The training and inference costs of these models, however,\nhave grown rapidly and become prohibitively expensive. Here we aim to reduce\nthe costs of Transformers by searching for a more efficient variant. Compared\nto previous approaches, our search is performed at a lower level, over the\nprimitives that define a Transformer TensorFlow program. We identify an\narchitecture, named Primer, that has a smaller training cost than the original\nTransformer and other variants for auto-regressive language modeling. Primer's\nimprovements can be mostly attributed to two simple modifications: squaring\nReLU activations and adding a depthwise convolution layer after each Q, K, and\nV projection in self-attention.\n</p>\n<p>Experiments show Primer's gains over Transformer increase as compute scale\ngrows and follow a power law with respect to quality at optimal model sizes. We\nalso verify empirically that Primer can be dropped into different codebases to\nsignificantly speed up training without additional tuning. For example, at a\n500M parameter size, Primer improves the original T5 architecture on C4\nauto-regressive language modeling, reducing the training cost by 4X.\nFurthermore, the reduced training cost means Primer needs much less compute to\nreach a target one-shot performance. For instance, in a 1.9B parameter\nconfiguration similar to GPT-3 XL, Primer uses 1/3 of the training compute to\nachieve the same one-shot performance as Transformer. We open source our models\nand several comparisons in T5 to help with reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manke_W/0/1/0/all/0/1\">Wojciech Ma&#x144;ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. (arXiv:2109.08678v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08678","description":"<p>Existing KBQA approaches, despite achieving strong performance on i.i.d. test\ndata, often struggle in generalizing to questions involving unseen KB schema\nitems. Prior ranking-based approaches have shown some success in\ngeneralization, but suffer from the coverage issue. We present RnG-KBQA, a\nRank-and-Generate approach for KBQA, which remedies the coverage issue with a\ngeneration model while preserving a strong generalization capability. Our\napproach first uses a contrastive ranker to rank a set of candidate logical\nforms obtained by searching over the knowledge graph. It then introduces a\ntailored generation model conditioned on the question and the top-ranked\ncandidates to compose the final logical form. We achieve new state-of-the-art\nresults on GrailQA and WebQSP datasets. In particular, our method surpasses the\nprior state-of-the-art by a large margin on the GrailQA leaderboard. In\naddition, RnG-KBQA outperforms all prior approaches on the popular WebQSP\nbenchmark, even including the ones that use the oracle entity linking. The\nexperimental results demonstrate the effectiveness of the interplay between\nranking and generation, which leads to the superior performance of our proposed\napproach across all settings with especially strong improvements in zero-shot\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Global Informativeness in Open Domain Keyphrase Extraction. (arXiv:2004.13639v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13639","description":"<p>Open-domain KeyPhrase Extraction (KPE) aims to extract keyphrases from\ndocuments without domain or quality restrictions, e.g., web pages with variant\ndomains and qualities. Recently, neural methods have shown promising results in\nmany KPE tasks due to their powerful capacity for modeling contextual semantics\nof the given documents. However, we empirically show that most neural KPE\nmethods prefer to extract keyphrases with good phraseness, such as short and\nentity-style n-grams, instead of globally informative keyphrases from\nopen-domain documents. This paper presents JointKPE, an open-domain KPE\narchitecture built on pre-trained language models, which can capture both local\nphraseness and global informativeness when extracting keyphrases. JointKPE\nlearns to rank keyphrases by estimating their informativeness in the entire\ndocument and is jointly trained on the keyphrase chunking task to guarantee the\nphraseness of keyphrase candidates. Experiments on two large KPE datasets with\ndiverse domains, OpenKP and KP20k, demonstrate the effectiveness of JointKPE on\ndifferent pre-trained variants in open-domain scenarios. Further analyses\nreveal the significant advantages of JointKPE in predicting long and non-entity\nkeyphrases, which are challenging for previous neural KPE methods. Our code is\npublicly available at https://github.com/thunlp/BERT-KPE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Si Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jie Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.15779","description":"<p>Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning\nBenchmark) at https://aka.ms/BLURB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Michael Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Document Clustering Based on BERT with Data Augment. (arXiv:2011.08523v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.08523","description":"<p>Contrastive learning is a promising approach to unsupervised learning, as it\ninherits the advantages of well-studied deep models without a dedicated and\ncomplex model design. In this paper, based on bidirectional encoder\nrepresentations from transformers, we propose self-supervised contrastive\nlearning (SCL) as well as few-shot contrastive learning (FCL) with unsupervised\ndata augmentation (UDA) for text clustering. SCL outperforms state-of-the-art\nunsupervised clustering approaches for short texts and those for long texts in\nterms of several clustering evaluation measures. FCL achieves performance close\nto supervised learning, and FCL with UDA further improves the performance for\nshort texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To what extent do human explanations of model behavior align with actual model behavior?. (arXiv:2012.13354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.13354","description":"<p>Given the increasingly prominent role NLP models (will) play in our lives, it\nis important for human expectations of model behavior to align with actual\nmodel behavior. Using Natural Language Inference (NLI) as a case study, we\ninvestigate the extent to which human-generated explanations of models'\ninference decisions align with how models actually make these decisions. More\nspecifically, we define three alignment metrics that quantify how well natural\nlanguage explanations align with model sensitivity to input words, as measured\nby integrated gradients. Then, we evaluate eight different models (the base and\nlarge versions of BERT, RoBERTa and ELECTRA, as well as anRNN and bag-of-words\nmodel), and find that the BERT-base model has the highest alignment with\nhuman-generated explanations, for all alignment metrics. Focusing in on\ntransformers, we find that the base versions tend to have higher alignment with\nhuman-generated explanations than their larger counterparts, suggesting that\nincreasing the number of model parameters leads, in some cases, to worse\nalignment with human explanations. Finally, we find that a model's alignment\nwith human explanations is not predicted by the model's accuracy, suggesting\nthat accuracy and alignment are complementary ways to evaluate models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1\">Grusha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yixin Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15283","description":"<p>While pre-trained language models (PTLMs) have achieved noticeable success on\nmany NLP tasks, they still struggle for tasks that require event temporal\nreasoning, which is essential for event-centric applications. We present a\ncontinual pre-training approach that equips PTLMs with targeted knowledge about\nevent temporal relations. We design self-supervised learning objectives to\nrecover masked-out event and temporal indicators and to discriminate sentences\nfrom their corrupted counterparts (where event or temporal indicators got\nreplaced). By further pre-training a PTLM with these objectives jointly, we\nreinforce its attention to event and temporal information, yielding enhanced\ncapability on event temporal reasoning. This effective continual pre-training\nframework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning\nperformances across five relation extraction and question answering tasks and\nachieves new or on-par state-of-the-art performances in most of our downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora. (arXiv:2012.15674v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15674","description":"<p>Recent studies have demonstrated that pre-trained cross-lingual models\nachieve impressive performance in downstream cross-lingual tasks. This\nimprovement benefits from learning a large amount of monolingual and parallel\ncorpora. Although it is generally acknowledged that parallel corpora are\ncritical for improving the model performance, existing methods are often\nconstrained by the size of parallel corpora, especially for low-resource\nlanguages. In this paper, we propose ERNIE-M, a new training method that\nencourages the model to align the representation of multiple languages with\nmonolingual corpora, to overcome the constraint that the parallel corpus size\nplaces on the model performance. Our key insight is to integrate\nback-translation into the pre-training process. We generate pseudo-parallel\nsentence pairs on a monolingual corpus to enable the learning of semantic\nalignments between different languages, thereby enhancing the semantic modeling\nof cross-lingual models. Experimental results show that ERNIE-M outperforms\nexisting cross-lingual models and delivers new state-of-the-art results in\nvarious cross-lingual downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xuan Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Politics via Contextualized Discourse Processing. (arXiv:2012.15784v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15784","description":"<p>Politicians often have underlying agendas when reacting to events. Arguments\nin contexts of various events reflect a fairly consistent set of agendas for a\ngiven entity. In spite of recent advances in Pretrained Language Models (PLMs),\nthose text representations are not designed to capture such nuanced patterns.\nIn this paper, we propose a Compositional Reader model consisting of encoder\nand composer modules, that attempts to capture and leverage such information to\ngenerate more effective representations for entities, issues, and events. These\nrepresentations are contextualized by tweets, press releases, issues, news\narticles, and participating entities. Our model can process several documents\nat once and generate composed representations for multiple entities over\nseveral issues or events. Via qualitative and quantitative empirical analysis,\nwe show that these representations are meaningful and effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pujari_R/0/1/0/all/0/1\">Rajkumar Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Everything in Order? A Simple Way to Order Sentences. (arXiv:2104.07064v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07064","description":"<p>The task of organizing a shuffled set of sentences into a coherent text has\nbeen used to evaluate a machine's understanding of causal and temporal\nrelations. We formulate the sentence ordering task as a conditional\ntext-to-marker generation problem. We present Reorder-BART (Re-BART) that\nleverages a pre-trained Transformer-based model to identify a coherent order\nfor a given set of shuffled sentences. The model takes a set of shuffled\nsentences with sentence-specific markers as input and generates a sequence of\nposition markers of the sentences in the ordered text. Re-BART achieves the\nstate-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and\nKendall's tau ($\\tau$). We perform evaluations in a zero-shot setting,\nshowcasing that our model is able to generalize well across other datasets. We\nadditionally perform several experiments to understand the functioning and\nlimitations of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Learning for Generation with Long Source Sequences. (arXiv:2104.07545v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07545","description":"<p>One of the challenges for current sequence to sequence (seq2seq) models is\nprocessing long sequences, such as those in summarization and document level\nmachine translation tasks. These tasks require the model to reason at the token\nlevel as well as the sentence and paragraph level. We design and study a new\nHierarchical Attention Transformer-based architecture (HAT) that outperforms\nstandard Transformers on several sequence to sequence tasks. Furthermore, our\nmodel achieves state-of-the-art ROUGE scores on four summarization tasks,\nincluding PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms\ndocument-level machine translation baseline on the WMT20 English to German\ntranslation task. We investigate what the hierarchical layers learn by\nvisualizing the hierarchical encoder-decoder attention. Finally, we study\nhierarchical learning on encoder-only pre-training and analyze its performance\non classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohde_T/0/1/0/all/0/1\">Tobias Rohde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinhan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04484","description":"<p>Deep learning algorithms have shown promising results in visual question\nanswering (VQA) tasks, but a more careful look reveals that they often do not\nunderstand the rich signal they are being fed with. To understand and better\nmeasure the generalization capabilities of VQA systems, we look at their\nrobustness to counterfactually augmented data. Our proposed augmentations are\ndesigned to make a focused intervention on a specific property of the question\nsuch that the answer changes. Using these augmentations, we propose a new\nrobustness measure, Robustness to Augmented Data (RAD), which measures the\nconsistency of model predictions between original and augmented examples.\nThrough extensive experimentation, we show that RAD, unlike classical accuracy\nmeasures, can quantify when state-of-the-art systems are not robust to\ncounterfactuals. We find substantial failure cases which reveal that current\nVQA systems are still brittle. Finally, we connect between robustness and\ngeneralization, demonstrating the predictive power of RAD for performance on\nunseen augmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1\">Daniel Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2106.13375","description":"<p>Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1\">Richard Rogahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheets for AI Tasks. (arXiv:2107.01183v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2107.01183","description":"<p>Recent innovations such as Datasheets for Datasets and Model Cards for Model\nReporting have made useful contributions to furthering ethical research. Yet,\nseveral high-profile events, such as the mass testing of emotion recognition\nsystems on vulnerable sub-populations, have highlighted how technology will\noften lead to more adverse outcomes for those that are already marginalized. In\nthis paper, I will make a case for thinking about ethical considerations not\njust at the level of individual models and datasets, but also at the level of\nAI tasks. I will present a new form of such an effort, Ethics Sheets for AI\nTasks, dedicated to fleshing out the assumptions and ethical considerations\nhidden in how a task is commonly framed and in the choices we make regarding\nthe data, method, and evaluation. Finally, I will provide an example ethics\nsheet for automatic emotion recognition. Ethics sheets are a mechanism to\ndocument ethical considerations \\textit{before} building datasets and systems.\nSuch pre-production activities (e.g., ethics analyses) and associated artifacts\n(e.g., accessible documentation) are crucial for responsible AI: for\ncommunicating risks to all stakeholders, to help decision and policy making,\nand for developing more effective post-production documents such as Data Sheets\nand Model Cards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pattern-based Acquisition of Scientific Entities from Scholarly Article Titles. (arXiv:2109.00199v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.00199","description":"<p>We describe a rule-based approach for the automatic acquisition of salient\nscientific entities from Computational Linguistics (CL) scholarly article\ntitles. Two observations motivated the approach: (i) noting salient aspects of\nan article's contribution in its title; and (ii) pattern regularities capturing\nthe salient terms that could be expressed in a set of rules. Only those\nlexico-syntactic patterns were selected that were easily recognizable, occurred\nfrequently, and positionally indicated a scientific entity type. The rules were\ndeveloped on a collection of 50,237 CL titles covering all articles in the ACL\nAnthology. In total, 19,799 research problems, 18,111 solutions, 20,033\nresources, 1,059 languages, 6,878 tools, and 21,687 methods were extracted at\nan average precision of 75%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">Soeren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00627","description":"<p>Contextual knowledge is important for real-world automatic speech recognition\n(ASR) applications. In this paper, a novel tree-constrained pointer generator\n(TCPGen) component is proposed that incorporates such knowledge as a list of\nbiasing words into both attention-based encoder-decoder and transducer\nend-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing\nwords into an efficient prefix tree to serve as its symbolic input and creates\na neural shortcut between the tree and the final ASR output distribution to\nfacilitate recognising biasing words during decoding. Systems were trained and\nevaluated on the Librispeech corpus where biasing words were extracted at the\nscales of an utterance, a chapter, or a book to simulate different application\nscenarios. Experimental results showed that TCPGen consistently improved word\nerror rates (WERs) compared to the baselines, and in particular, achieved\nsignificant WER reductions on the biasing words. TCPGen is highly efficient: it\ncan handle 5,000 biasing words and distractors and only add a small overhead to\nmemory use and computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03570","description":"<p>This work presents biomedical and clinical language models for Spanish by\nexperimenting with different pretraining choices, such as masking at word and\nsubword level, varying the vocabulary size and testing with domain data,\nlooking for better language representations. Interestingly, in the absence of\nenough clinical data to train a model from scratch, we applied mixed-domain\npretraining and cross-domain transfer approaches to generate a performant\nbio-clinical model suitable for real-world clinical data. We evaluated our\nmodels on Named Entity Recognition (NER) tasks for biomedical documents and\nchallenging hospital discharge reports. When compared against the competitive\nmBERT and BETO models, we outperform them in all NER tasks by a significant\nmargin. Finally, we studied the impact of the model's vocabulary on the NER\nperformances by offering an interesting vocabulary-centric analysis. The\nresults confirm that domain-specific pretraining is fundamental to achieving\nhigher performances in downstream NER tasks, even within a mid-resource\nscenario. To the best of our knowledge, we provide the first biomedical and\nclinical transformer-based pretrained language models for Spanish, intending to\nboost native Spanish NLP applications in biomedicine. Our best models are\nfreely available in the HuggingFace hub: https://huggingface.co/BSC-TeMU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04114","description":"<p>We apply imitation learning (IL) to tackle the NMT exposure bias problem with\nerror-correcting oracles, and evaluate an SMT lattice-based oracle which,\ndespite its excellent performance in an unconstrained oracle translation task,\nturned out to be too pruned and idiosyncratic to serve as the oracle for IL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hormann_L/0/1/0/all/0/1\">Luca Hormann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartography Active Learning. (arXiv:2109.04282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04282","description":"<p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)\nalgorithm that exploits the behavior of the model on individual instances\nduring training as a proxy to find the most informative instances for labeling.\nCAL is inspired by data maps, which were recently proposed to derive insights\ninto dataset quality (Swayamdipta et al., 2020). We compare our method on\npopular text classification tasks to commonly used AL strategies, which instead\nrely on post-training behavior. We demonstrate that CAL is competitive to other\ncommon AL methods, showing that training dynamics derived from small seed data\ncan be successfully used for AL. We provide insights into our new AL method by\nanalyzing batch-level statistics utilizing the data maps. Our results further\nshow that CAL results in a more data-efficient learning strategy, achieving\ncomparable or better results with considerably less training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05179","description":"<p>Generating high quality question-answer pairs is a hard but meaningful task.\nAlthough previous works have achieved great results on answer-aware question\ngeneration, it is difficult to apply them into practical application in the\neducation field. This paper for the first time addresses the question-answer\npair generation task on the real-world examination data, and proposes a new\nunified framework on RACE. To capture the important information of the input\npassage we first automatically generate(rather than extracting) keyphrases,\nthus this task is reduced to keyphrase-question-answer triplet joint\ngeneration. Accordingly, we propose a multi-agent communication model to\ngenerate and optimize the question and keyphrases iteratively, and then apply\nthe generated question and keyphrases to guide the generation of answers. To\nestablish a solid benchmark, we build our model on the strong generative\npre-training model. Experimental results show that our model makes great\nbreakthroughs in the question-answer pair generation task. Moreover, we make a\ncomprehensive analysis on our model, suggesting new directions for this\nchallenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_F/0/1/0/all/0/1\">Fanyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. (arXiv:2109.07383v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07383","description":"<p>This paper addresses the efficiency challenge of Neural Architecture Search\n(NAS) by formulating the task as a ranking problem. Previous methods require\nnumerous training examples to estimate the accurate performance of\narchitectures, although the actual goal is to find the distinction between\n\"good\" and \"bad\" candidates. Here we do not resort to performance predictors.\nInstead, we propose a performance ranking method (RankNAS) via pairwise\nranking. It enables efficient architecture search using much fewer training\nexamples. Moreover, we develop an architecture selection method to prune the\nsearch space and concentrate on more promising candidates. Extensive\nexperiments on machine translation and language modeling tasks show that\nRankNAS can design high-performance architectures while being orders of\nmagnitude faster than state-of-the-art NAS systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiangnan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xia Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changliang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}