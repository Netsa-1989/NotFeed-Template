{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DPUV3INT8: A Compiler View to programmable FPGA Inference Engines. (arXiv:2110.04327v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04327","description":"<p>We have a FPGA design, we make it fast, efficient, and tested for a few\nimportant examples. Now we must infer a general solution to deploy in the data\ncenter. Here, we describe the FPGA DPUV3INT8 design and our compiler effort.\nThe hand-tuned SW-HW solution for Resnet50\\_v1 has (close to) 2 times better\nimages per second (throughput) than our best FPGA implementation; the compiler\ngeneralizes the hand written techniques achieving about 1.5 times better\nperformance for the same example, the compiler generalizes the optimizations to\na model zoo of networks, and it achieves 80+\\% HW efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DAlberto_P/0/1/0/all/0/1\">Paolo D&#x27;Alberto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiangsha Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollavaram_M/0/1/0/all/0/1\">Manasa Bollavaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shaoxia Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering. (arXiv:2110.04330v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04330","description":"<p>Current Open-Domain Question Answering (ODQA) model paradigm often contains a\nretrieving module and a reading module. Given an input question, the reading\nmodule predicts the answer from the relevant passages which are retrieved by\nthe retriever. The recent proposed Fusion-in-Decoder (FiD), which is built on\ntop of the pretrained generative model T5, achieves the state-of-the-art\nperformance in the reading module. Although being effective, it remains\nconstrained by inefficient attention on all retrieved passages which contain a\nlot of noise. In this work, we propose a novel method KG-FiD, which filters\nnoisy passages by leveraging the structural relationship among the retrieved\npassages with a knowledge graph. We initiate the passage node embedding from\nthe FiD encoder and then use graph neural network (GNN) to update the\nrepresentation for reranking. To improve the efficiency, we build the GNN on\ntop of the intermediate layer output of the FiD encoder and only pass a few top\nreranked passages into the higher layers of encoder and decoder for answer\ngeneration. We also apply the proposed GNN based reranking method to enhance\nthe passage retrieval results in the retrieving module. Extensive experiments\non common ODQA benchmark datasets (Natural Question and TriviaQA) demonstrate\nthat KG-FiD can improve vanilla FiD by up to 1.5% on answer exact match score\nand achieve comparable performance with FiD with only 40% of computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Donghan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Describe Solutions for Bug Reports Based on Developer Discussions. (arXiv:2110.04353v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04353","description":"<p>When a software bug is reported, developers engage in a discussion to\ncollaboratively resolve it. While the solution is likely formulated within the\ndiscussion, it is often buried in a large amount of text, making it difficult\nto comprehend, which delays its implementation. To expedite bug resolution, we\npropose generating a concise natural language description of the solution by\nsynthesizing relevant content within the discussion, which encompasses both\nnatural language and source code. Furthermore, to support generating an\ninformative description during an ongoing discussion, we propose a secondary\ntask of determining when sufficient context about the solution emerges in\nreal-time. We construct a dataset for these tasks with a novel technique for\nobtaining noisy supervision from repository changes linked to bug reports. We\nestablish baselines for generating solution descriptions, and develop a\nclassifier which makes a prediction following each new utterance on whether or\nnot the necessary context for performing generation is available. Through\nautomated and human evaluation, we find these tasks to form an ideal testbed\nfor complex reasoning in long, bimodal dialogue context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panthaplackel_S/0/1/0/all/0/1\">Sheena Panthaplackel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligoric_M/0/1/0/all/0/1\">Milos Gligoric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond J. Mooney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Unified View of Parameter-Efficient Transfer Learning. (arXiv:2110.04366v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04366","description":"<p>Fine-tuning large pre-trained language models on downstream tasks has become\nthe de-facto learning paradigm in NLP. However, conventional approaches\nfine-tune all the parameters of the pre-trained model, which becomes\nprohibitive as the model size and the number of tasks grow. Recent work has\nproposed a variety of parameter-efficient transfer learning methods that only\nfine-tune a small number of (extra) parameters to attain strong performance.\nWhile effective, the critical ingredients for success and the connections among\nthe various methods are poorly understood. In this paper, we break down the\ndesign of state-of-the-art parameter-efficient transfer learning methods and\npresent a unified framework that establishes connections between them.\nSpecifically, we re-frame them as modifications to specific hidden states in\npre-trained models, and define a set of design dimensions along which different\nmethods vary, such as the function to compute the modification and the position\nto apply the modification. Through comprehensive empirical studies across\nmachine translation, text summarization, language understanding, and text\nclassification benchmarks, we utilize the unified view to identify important\ndesign choices in previous methods. Furthermore, our unified framework enables\nthe transfer of design elements across different approaches, and as a result we\nare able to instantiate new parameter-efficient fine-tuning methods that tune\nless parameters than previous methods while being more effective, achieving\ncomparable results to fine-tuning all parameters on all four tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuezhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Few More Examples May Be Worth Billions of Parameters. (arXiv:2110.04374v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04374","description":"<p>We investigate the dynamics of increasing the number of model parameters\nversus the number of labeled examples across a wide variety of tasks. Our\nexploration reveals that while scaling parameters consistently yields\nperformance improvements, the contribution of additional examples highly\ndepends on the task's format. Specifically, in open question answering tasks,\nenlarging the training set does not improve performance. In contrast,\nclassification, extractive question answering, and multiple choice tasks\nbenefit so much from additional examples that collecting a few hundred examples\nis often \"worth\" billions of parameters. We hypothesize that unlike open\nquestion answering, which involves recalling specific information, solving\nstrategies for tasks with a more restricted output space transfer across\nexamples, and can therefore be learned with small amounts of labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirstain_Y/0/1/0/all/0/1\">Yuval Kirstain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Summarization Systems across Gender, Age, and Race. (arXiv:2110.04384v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04384","description":"<p>Summarization systems are ultimately evaluated by human annotators and\nraters. Usually, annotators and raters do not reflect the demographics of end\nusers, but are recruited through student populations or crowdsourcing platforms\nwith skewed demographics. For two different evaluation scenarios -- evaluation\nagainst gold summaries and system output ratings -- we show that summary\nevaluation is sensitive to protected attributes. This can severely bias system\ndevelopment and evaluation, leading us to build models that cater for some\ngroups rather than others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_A/0/1/0/all/0/1\">Anna J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Eval4NLP Shared Task on Explainable Quality Estimation: Overview and Results. (arXiv:2110.04392v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04392","description":"<p>In this paper, we introduce the Eval4NLP-2021shared task on explainable\nquality estimation. Given a source-translation pair, this shared task requires\nnot only to provide a sentence-level score indicating the overall quality of\nthe translation, but also to explain this score by identifying the words that\nnegatively impact translation quality. We present the data, annotation\nguidelines and evaluation setup of the shared task, describe the six\nparticipating systems, and analyze the results. To the best of our knowledge,\nthis is the first shared task on explainable NLP evaluation metrics. Datasets\nand results are available at https://github.com/eval4nlp/SharedTask2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lertvittayakumjorn_P/0/1/0/all/0/1\">Piyawat Lertvittayakumjorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Explainability of BERT-Based Evaluation Metrics by Disentangling along Linguistic Factors. (arXiv:2110.04399v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04399","description":"<p>Evaluation metrics are a key ingredient for progress of text generation\nsystems. In recent years, several BERT-based evaluation metrics have been\nproposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much\nbetter with human assessment of text generation quality than BLEU or ROUGE,\ninvented two decades ago. However, little is known what these metrics, which\nare based on black-box language model representations, actually capture (it is\ntypically assumed they model semantic similarity). In this work, we \\wei{use a\nsimple regression based global explainability technique to} disentangle metric\nscores along linguistic factors, including semantics, syntax, morphology, and\nlexical overlap. We show that the different metrics capture all aspects to some\ndegree, but that they are all substantially sensitive to lexical overlap, just\nlike BLEU and ROUGE. This exposes limitations of these novelly proposed\nmetrics, which we also highlight in an adversarial test scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaster_M/0/1/0/all/0/1\">Marvin Kaster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HydraSum -- Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04400","description":"<p>Existing abstractive summarization models lack explicit control mechanisms\nthat would allow users to influence the stylistic features of the model\noutputs. This results in generating generic summaries that do not cater to the\nusers needs or preferences. To address this issue we introduce HydraSum, a new\nsummarization architecture that extends the single decoder framework of current\nmodels, e.g. BART, to a mixture-of-experts version consisting of multiple\ndecoders. Our proposed model encourages each expert, i.e. decoder, to learn and\ngenerate stylistically-distinct summaries along dimensions such as\nabstractiveness, length, specificity, and others. At each time step, HydraSum\nemploys a gating mechanism that decides the contribution of each individual\ndecoder to the next token's output probability distribution. Through\nexperiments on three summarization datasets (CNN, Newsroom, XSum), we\ndemonstrate that this gating mechanism automatically learns to assign\ncontrasting summary styles to different HydraSum decoders under the standard\ntraining objective without the need for additional supervision. We further show\nthat a guided version of the training process can explicitly govern which\nsummary style is partitioned between decoders, e.g. high abstractiveness vs.\nlow abstractiveness or high specificity vs. low specificity, and also increase\nthe stylistic-difference between individual decoders. Finally, our experiments\ndemonstrate that our decoder framework is highly flexible: during inference, we\ncan sample from individual decoders or mixtures of different subsets of the\ndecoders to yield a diverse set of summaries and enforce single- and\nmulti-style control over summary generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content. (arXiv:2110.04406v1 [cs.HC])","link":"http://arxiv.org/abs/2110.04406","description":"<p>Natural language descriptions sometimes accompany visualizations to better\ncommunicate and contextualize their insights, and to improve their\naccessibility for readers with disabilities. However, it is difficult to\nevaluate the usefulness of these descriptions, and how effectively they improve\naccess to meaningful information, because we have little understanding of the\nsemantic content they convey, and how different readers receive this content.\nIn response, we introduce a conceptual model for the semantic content conveyed\nby natural language descriptions of visualizations. Developed through a\ngrounded theory analysis of 2,147 sentences, our model spans four levels of\nsemantic content: enumerating visualization construction properties (e.g.,\nmarks and encodings); reporting statistical concepts and relations (e.g.,\nextrema and correlations); identifying perceptual and cognitive phenomena\n(e.g., complex trends and patterns); and elucidating domain-specific insights\n(e.g., social and political context). To demonstrate how our model can be\napplied to evaluate the effectiveness of visualization descriptions, we conduct\na mixed-methods evaluation with 30 blind and 90 sighted readers, and find that\nthese reader groups differ significantly on which semantic content they rank as\nmost useful. Together, our model and findings suggest that access to meaningful\ninformation is strongly reader-specific, and that research in automatic\nvisualization captioning should orient toward descriptions that more richly\ncommunicate overall trends and statistics, sensitive to reader preferences. Our\nwork further opens a space of research on natural language as a data interface\ncoequal with visualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lundgard_A/0/1/0/all/0/1\">Alan Lundgard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1\">Arvind Satyanarayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Community Sensitive Norm Violations in Online Conversations. (arXiv:2110.04419v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04419","description":"<p>Online platforms and communities establish their own norms that govern what\nbehavior is acceptable within the community. Substantial effort in NLP has\nfocused on identifying unacceptable behaviors and, recently, on forecasting\nthem before they occur. However, these efforts have largely focused on toxicity\nas the sole form of community norm violation. Such focus has overlooked the\nmuch larger set of rules that moderators enforce. Here, we introduce a new\ndataset focusing on a more complete spectrum of community norms and their\nviolations in the local conversational and global community contexts. We\nintroduce a series of models that use this data to develop context- and\ncommunity-sensitive norm violation detection, showing that these changes give\nhigh performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendelsohn_J/0/1/0/all/0/1\">Julia Mendelsohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_K/0/1/0/all/0/1\">Karthik Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_K/0/1/0/all/0/1\">Kinjal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanakagiri_T/0/1/0/all/0/1\">Tushar Kanakagiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning. (arXiv:2110.04429v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04429","description":"<p>Distantly supervised named entity recognition (DS-NER) efficiently reduces\nlabor costs but meanwhile intrinsically suffers from the label noise due to the\nstrong assumption of distant supervision. Typically, the wrongly labeled\ninstances comprise numbers of incomplete and inaccurate annotation noise, while\nmost prior denoising works are only concerned with one kind of noise and fail\nto fully explore useful information in the whole training set. To address this\nissue, we propose a robust learning paradigm named Self-Collaborative Denoising\nLearning (SCDL), which jointly trains two teacher-student networks in a\nmutually-beneficial manner to iteratively perform noisy label refinery. Each\nnetwork is designed to exploit reliable labels via self denoising, and two\nnetworks communicate with each other to explore unreliable annotations by\ncollaborative denoising. Extensive experimental results on five real-world\ndatasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinghua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bowen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tingwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1\">Jiawei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Mengge Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage Visual Cues Enhancement Network for Referring Image Segmentation. (arXiv:2110.04435v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04435","description":"<p>Referring Image Segmentation (RIS) aims at segmenting the target object from\nan image referred by one given natural language expression. The diverse and\nflexible expressions as well as complex visual contents in the images raise the\nRIS model with higher demands for investigating fine-grained matching behaviors\nbetween words in expressions and objects presented in images. However, such\nmatching behaviors are hard to be learned and captured when the visual cues of\nreferents (i.e. referred objects) are insufficient, as the referents with weak\nvisual cues tend to be easily confused by cluttered background at boundary or\neven overwhelmed by salient objects in the image. And the insufficient visual\ncues issue can not be handled by the cross-modal fusion mechanisms as done in\nprevious work. In this paper, we tackle this problem from a novel perspective\nof enhancing the visual information for the referents by devising a Two-stage\nVisual cues enhancement Network (TV-Net), where a novel Retrieval and\nEnrichment Scheme (RES) and an Adaptive Multi-resolution feature Fusion (AMF)\nmodule are proposed. Through the two-stage enhancement, our proposed TV-Net\nenjoys better performances in learning fine-grained matching behaviors between\nthe natural language expression and image, especially when the visual\ninformation of the referent is inadequate, thus produces better segmentation\nresults. Extensive experiments are conducted to validate the effectiveness of\nthe proposed method on the RIS task, with our proposed TV-Net surpassing the\nstate-of-the-art approaches on four benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weixin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language for Human-Robot Collaboration: Problems Beyond Language Grounding. (arXiv:2110.04441v1 [cs.AI])","link":"http://arxiv.org/abs/2110.04441","description":"<p>To enable robots to instruct humans in collaborations, we identify several\naspects of language processing that are not commonly studied in this context.\nThese include location, planning, and generation. We suggest evaluations for\neach task, offer baselines for simple methods, and close by discussing\nchallenges and opportunities in studying language for collaboration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pate_S/0/1/0/all/0/1\">Seth Pate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Love_M/0/1/0/all/0/1\">Maxwell Love</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguri_S/0/1/0/all/0/1\">Siddarth Ganguri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1\">Lawson L.S. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging recent advances in Pre-Trained Language Models forEye-Tracking Prediction. (arXiv:2110.04475v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04475","description":"<p>Cognitively inspired Natural Language Pro-cessing uses human-derived\nbehavioral datalike eye-tracking data, which reflect the seman-tic\nrepresentations of language in the humanbrain to augment the neural nets to\nsolve arange of tasks spanning syntax and semanticswith the aim of teaching\nmachines about lan-guage processing mechanisms. In this paper,we use the ZuCo\n1.0 and ZuCo 2.0 dataset con-taining the eye-gaze features to explore\ndiffer-ent linguistic models to directly predict thesegaze features for each\nword with respect to itssentence. We tried different neural networkmodels with\nthe words as inputs to predict thetargets. And after lots of experimentation\nandfeature engineering finally devised a novel ar-chitecture consisting of\nRoBERTa Token Clas-sifier with a dense layer on top for languagemodeling and a\nstand-alone model consistingof dense layers followed by a transformer layerfor\nthe extra features we engineered. Finally,we took the mean of the outputs of\nboth thesemodels to make the final predictions. We eval-uated the models using\nmean absolute error(MAE) and the R2 score for each target.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madhavan_V/0/1/0/all/0/1\">Varun Madhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawate_A/0/1/0/all/0/1\">Aditya Girish Pawate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shraman Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Abhranil Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Active Summarization. (arXiv:2110.04480v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04480","description":"<p>Bayesian Active Learning has had significant impact to various NLP problems,\nbut nevertheless it's application to text summarization has been explored very\nlittle. We introduce Bayesian Active Summarization (BAS), as a method of\ncombining active learning methods with state-of-the-art summarization models.\nOur findings suggest that BAS achieves better and more robust performance,\ncompared to random selection, particularly for small and very small data\nannotation budgets. Using BAS we showcase it is possible to leverage large\nsummarization models to effectively solve real-world problems with very limited\nannotated data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gidiotis_A/0/1/0/all/0/1\">Alexios Gidiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lifelong Learning of Multilingual Text-To-Speech Synthesis. (arXiv:2110.04482v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04482","description":"<p>This work presents a lifelong learning approach to train a multilingual\nText-To-Speech (TTS) system, where each language was seen as an individual task\nand was learned sequentially and continually. It does not require pooled data\nfrom all languages altogether, and thus alleviates the storage and computation\nburden. One of the challenges of lifelong learning methods is \"catastrophic\nforgetting\": in TTS scenario it means that model performance quickly degrades\non previous languages when adapted to a new language. We approach this problem\nvia a data-replay-based lifelong learning method. We formulate the replay\nprocess as a supervised learning problem, and propose a simple yet effective\ndual-sampler framework to tackle the heavily language-imbalanced training\nsamples. Through objective and subjective evaluations, we show that this\nsupervised learning formulation outperforms other gradient-based and\nregularization-based lifelong learning methods, achieving 43% Mel-Cepstral\nDistortion reduction compared to a fine-tuning baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Mu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1\">Shaojin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav2vec-S: Semi-Supervised Pre-Training for Speech Recognition. (arXiv:2110.04484v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04484","description":"<p>Self-supervised pre-training has dramatically improved the performance of\nautomatic speech recognition (ASR). However, most existing self-supervised\npre-training approaches are task-agnostic, i.e., could be applied to various\ndownstream tasks. And there is a gap between the task-agnostic pre-training and\nthe task-specific downstream fine-tuning, which may degrade the downstream\nperformance. In this work, we propose a novel pre-training paradigm called\nwav2vec-S, where we use task-specific semi-supervised pre-training to bridge\nthis gap. Specifically, the semi-supervised pre-training is conducted on the\nbasis of self-supervised pre-training such as wav2vec 2.0. Experiments on ASR\nshow that compared to wav2vec 2.0, wav2vec-S only requires marginal increment\nof pre-training time but could significantly improve ASR performance on\nin-domain, cross-domain and cross-lingual datasets. The average relative WER\nreductions are 26.3% and 6.3% for 1h and 10h fine-tuning, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Han Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1\">Ying Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_G/0/1/0/all/0/1\">Gaofeng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yonghong Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control. (arXiv:2110.04486v1 [cs.SD])","link":"http://arxiv.org/abs/2110.04486","description":"<p>Sequence expansion between encoder and decoder is a critical challenge in\nsequence-to-sequence TTS. Attention-based methods achieve great naturalness but\nsuffer from unstable issues like missing and repeating phonemes, not to mention\naccurate duration control. Duration-informed methods, on the contrary, seem to\neasily adjust phoneme duration but show obvious degradation in speech\nnaturalness. This paper proposes PAMA-TTS to address the problem. It takes the\nadvantage of both flexible attention and explicit duration models. Based on the\nmonotonic attention mechanism, PAMA-TTS also leverages token duration and\nrelative position of a frame, especially countdown information, i.e. in how\nmany future frames the present phoneme will end. They help the attention to\nmove forward along the token sequence in a soft but reliable control.\nExperimental results prove that PAMA-TTS achieves the highest naturalness,\nwhile has on-par or even better duration controllability than the\nduration-informed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yunchao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Isotropy Analysis in the Multilingual BERT Embedding Space. (arXiv:2110.04504v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04504","description":"<p>Several studies have explored various advantages of multilingual pre-trained\nmodels (e.g., multilingual BERT) in capturing shared linguistic knowledge.\nHowever, their limitations have not been paid enough attention. In this paper,\nwe investigate the representation degeneration problem in multilingual\ncontextual word representations (CWRs) of BERT and show that the embedding\nspaces of the selected languages suffer from anisotropy problem. Our\nexperimental results demonstrate that, similarly to their monolingual\ncounterparts, increasing the isotropy of multilingual embedding space can\nsignificantly improve its representation power and performance. Our analysis\nindicates that although the degenerated directions vary in different languages,\nthey encode similar linguistic knowledge, suggesting a shared linguistic space\namong languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extending Multi-Text Sentence Fusion Resources via Pyramid Annotations. (arXiv:2110.04517v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04517","description":"<p>NLP models that compare or consolidate information across multiple documents\noften struggle when challenged with recognizing substantial information\nredundancies across the texts. For example, in multi-document summarization it\nis crucial to identify salient information across texts and then generate a\nnon-redundant summary, while facing repeated and usually differently-phrased\nsalient content. To facilitate researching such challenges, the sentence-level\ntask of \\textit{sentence fusion} was proposed, yet previous datasets for this\ntask were very limited in their size and scope. In this paper, we revisit and\nsubstantially extend previous dataset creation efforts. With careful\nmodifications, relabeling and employing complementing data sources, we were\nable to triple the size of a notable earlier dataset. Moreover, we show that\nour extended version uses more representative texts for multi-document tasks\nand provides a larger and more diverse training set, which substantially\nimproves model training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1\">Daniela Brook Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Ori Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing. (arXiv:2110.04518v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04518","description":"<p>Text discourse parsing weighs importantly in understanding information flow\nand argumentative structure in natural language, making it beneficial for\ndownstream tasks. While previous work significantly improves the performance of\nRST discourse parsing, they are not readily applicable to practical use cases:\n(1) EDU segmentation is not integrated into most existing tree parsing\nframeworks, thus it is not straightforward to apply such models on newly-coming\ndata. (2) Most parsers cannot be used in multilingual scenarios, because they\nare developed only in English. (3) Parsers trained from single-domain treebanks\ndo not generalize well on out-of-domain inputs. In this work, we propose a\ndocument-level multilingual RST discourse parsing framework, which conducts EDU\nsegmentation and discourse tree parsing jointly. Moreover, we propose a\ncross-translation augmentation strategy to enable the framework to support\nmultilingual parsing and improve its domain generality. Experimental results\nshow that our model achieves state-of-the-art performance on document-level\nmultilingual RST parsing in all sub-tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks. (arXiv:2110.04522v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04522","description":"<p>Rumors are rampant in the era of social media. Conversation structures\nprovide valuable clues to differentiate between real and fake claims. However,\nexisting rumor detection methods are either limited to the strict relation of\nuser responses or oversimplify the conversation structure. In this study, to\nsubstantially reinforces the interaction of user opinions while alleviating the\nnegative impact imposed by irrelevant posts, we first represent the\nconversation thread as an undirected interaction graph. We then present a\nClaim-guided Hierarchical Graph Attention Network for rumor classification,\nwhich enhances the representation learning for responsive posts considering the\nentire social contexts and attends over the posts that can semantically infer\nthe target claim. Extensive experiments on three Twitter datasets demonstrate\nthat our rumor detection method achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for detecting rumors\nat early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingfei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Disentangled Arguments with Prompts: A Simple Event Extraction Framework that Works. (arXiv:2110.04525v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04525","description":"<p>Event Extraction bridges the gap between text and event signals. Based on the\nassumption of trigger-argument dependency, existing approaches have achieved\nstate-of-the-art performance with expert-designed templates or complicated\ndecoding constraints. In this paper, for the first time we introduce the\nprompt-based learning strategy to the domain of Event Extraction, which\nempowers the automatic exploitation of label semantics on both input and output\nsides. To validate the effectiveness of the proposed generative method, we\nconduct extensive experiments with 11 diverse baselines. Empirical results show\nthat, in terms of F1 score on Argument Extraction, our simple architecture is\nstronger than any other generative counterpart and even competitive with\nalgorithms that require template engineering. Regarding the measure of recall,\nit sets new overall records for both Argument and Trigger Extractions. We\nhereby recommend this framework to the community, with the code publicly\navailable at https://git.io/GDAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1\">Jinghui Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haotian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multi-Party Dialogue Discourse Parsing via Domain Integration. (arXiv:2110.04526v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04526","description":"<p>While multi-party conversations are often less structured than monologues and\ndocuments, they are implicitly organized by semantic level correlations across\nthe interactive turns, and dialogue discourse analysis can be applied to\npredict the dependency structure and relations between the elementary discourse\nunits, and provide feature-rich structural information for downstream tasks.\nHowever, the existing corpora with dialogue discourse annotation are collected\nfrom specific domains with limited sample sizes, rendering the performance of\ndata-driven approaches poor on incoming dialogues without any domain\nadaptation. In this paper, we first introduce a Transformer-based parser, and\nassess its cross-domain performance. We next adopt three methods to gain domain\nintegration from both data and language modeling perspectives to improve the\ngeneralization capability. Empirical results show that the neural parser can\nbenefit from our proposed methods, and performs better on cross-domain dialogue\nsamples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04541","description":"<p>Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pretraining example\nyields improved sentence representations and open domain question answering\nabilities. This theoretically motivated degree of freedom for \"pretraining\nexample design\" indicates new training schemes for self-improving\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1\">Daniel Jannai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_D/0/1/0/all/0/1\">Dan Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Adapter: Better Vision-Language Models with Feature Adapters. (arXiv:2110.04544v1 [cs.CV])","link":"http://arxiv.org/abs/2110.04544","description":"<p>Large-scale contrastive vision-language pre-training has shown significant\nprogress in visual representation learning. Unlike traditional visual systems\ntrained by a fixed set of discrete labels, a new paradigm was introduced in\n\\cite{radford2021learning} to directly learn to align images with raw texts in\nan open-vocabulary setting. On downstream tasks, a carefully chosen text prompt\nis employed to make zero-shot predictions.~To avoid non-trivial prompt\nengineering, context optimization \\cite{zhou2021coop} has been proposed to\nlearn continuous vectors as task-specific prompts with few-shot training\nexamples.~In this paper, we show that there is an alternative path to achieve\nbetter vision-language models other than prompt tuning.~While prompt tuning is\nfor the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with\nfeature adapters on either visual or language branch. Specifically,\nCLIP-Adapter adopts an additional bottleneck layer to learn new features and\nperforms residual-style feature blending with the original pre-trained\nfeatures.~As a consequence, CLIP-Adapter is able to outperform context\noptimization while maintains a simple design. Experiments and extensive\nablation studies on various visual classification tasks demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1\">Rongyao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploration of Self-Supervised Pretrained Representations for End-to-End Speech Recognition. (arXiv:2110.04590v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04590","description":"<p>Self-supervised pretraining on speech data has achieved a lot of progress.\nHigh-fidelity representation of the speech signal is learned from a lot of\nuntranscribed data and shows promising performance. Recently, there are several\nworks focusing on evaluating the quality of self-supervised pretrained\nrepresentations on various tasks without domain restriction, e.g. SUPERB.\nHowever, such evaluations do not provide a comprehensive comparison among many\nASR benchmark corpora. In this paper, we focus on the general applications of\npretrained speech representations, on advanced end-to-end automatic speech\nrecognition (E2E-ASR) models. We select several pretrained speech\nrepresentations and present the experimental results on various open-source and\npublicly available corpora for E2E-ASR. Without any modification of the\nback-end model architectures or training strategy, some of the experiments with\npretrained representations, e.g., WSJ, WSJ0-2mix with HuBERT, reach or\noutperform current state-of-the-art (SOTA) recognition performance. Moreover,\nwe further explore more scenarios for whether the pretraining representations\nare effective, such as the cross-language or overlapped speech. The scripts,\nconfiguratons and the trained models have been released in ESPnet to let the\ncommunity reproduce our experiments and improve them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xuankai Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maekaku_T/0/1/0/all/0/1\">Takashi Maekaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yen-Ju Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Aswin Shanmugam Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianzi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Automatic Speech Recognition Trained on Small Disordered Speech Datasets. (arXiv:2110.04612v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04612","description":"<p>This study investigates the performance of personalized automatic speech\nrecognition (ASR) for recognizing disordered speech using small amounts of\nper-speaker adaptation data. We trained personalized models for 195 individuals\nwith different types and severities of speech impairment with training sets\nranging in size from &lt;1 minute to 18-20 minutes of speech data. Word error rate\n(WER) thresholds were selected to determine Success Percentage (the percentage\nof personalized models reaching the target WER) in different application\nscenarios. For the home automation scenario, 79% of speakers reached the target\nWER with 18-20 minutes of speech; but even with only 3-4 minutes of speech, 63%\nof speakers reached the target WER. Further evaluation found similar\nimprovement on test sets with conversational and out-of-domain, unprompted\nphrases. Our results demonstrate that with only a few minutes of recordings,\nindividuals with disordered speech could benefit from personalized ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tobin_J/0/1/0/all/0/1\">Jimmy Tobin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomanek_K/0/1/0/all/0/1\">Katrin Tomanek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Response Generation through Graph-based Multi-hop Reasoning on Emotional Causality. (arXiv:2110.04614v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04614","description":"<p>Empathetic response generation aims to comprehend the user emotion and then\nrespond to it appropriately. Most existing works merely focus on what the\nemotion is and ignore how the emotion is evoked, thus weakening the capacity of\nthe model to understand the emotional experience of the user for generating\nempathetic responses. To tackle this problem, we consider the emotional\ncausality, namely, what feelings the user expresses (i.e., emotion) and why the\nuser has such feelings (i.e., cause). Then, we propose a novel graph-based\nmodel with multi-hop reasoning to model the emotional causality of the\nempathetic conversation. Finally, we demonstrate the effectiveness of our model\non EMPATHETICDIALOGUES in comparison with several competitive models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiashuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LI_W/0/1/0/all/0/1\">Wenjie LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Peiqin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_F/0/1/0/all/0/1\">Feiteng Mu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Rationale Extraction for Deep QA models. (arXiv:2110.04620v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04620","description":"<p>As neural-network-based QA models become deeper and more complex, there is a\ndemand for robust frameworks which can access a model's rationale for its\nprediction. Current techniques that provide insights on a model's working are\neither dependent on adversarial datasets or are proposing models with explicit\nexplanation generation components. These techniques are time-consuming and\nchallenging to extend to existing models and new datasets. In this work, we use\n`Integrated Gradients' to extract rationale for existing state-of-the-art\nmodels in the task of Reading Comprehension based Question Answering (RCQA). On\ndetailed analysis and comparison with collected human rationales, we find that\nthough ~40-80% words of extracted rationale coincide with the human rationale\n(precision), only 6-19% of human rationale is present in the extracted\nrationale (recall).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_S/0/1/0/all/0/1\">Sahana Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nema_P/0/1/0/all/0/1\">Preksha Nema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahni_D/0/1/0/all/0/1\">Deep Sahni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Relation between Syntactic Divergence and Zero-Shot Performance. (arXiv:2110.04644v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04644","description":"<p>We explore the link between the extent to which syntactic relations are\npreserved in translation and the ease of correctly constructing a parse tree in\na zero-shot setting. While previous work suggests such a relation, it tends to\nfocus on the macro level and not on the level of individual edges-a gap we aim\nto address. As a test case, we take the transfer of Universal Dependencies (UD)\nparsing from English to a diverse set of languages and conduct two sets of\nexperiments. In one, we analyze zero-shot performance based on the extent to\nwhich English source edges are preserved in translation. In another, we apply\nthree linguistically motivated transformations to UD, creating more\ncross-lingually stable versions of it, and assess their zero-shot parsability.\nIn order to compare parsing performance across different schemes, we perform\nextrinsic evaluation on the downstream task of cross-lingual relation\nextraction (RE) using a subset of a popular English RE benchmark translated to\nRussian and Korean. In both sets of experiments, our results suggest a strong\nrelation between cross-lingual stability and zero-shot parsing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arviv_O/0/1/0/all/0/1\">Ofir Arviv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1\">Dmitry Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1\">Taelin Karidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Follow Language Instructions with Compositional Policies. (arXiv:2110.04647v1 [cs.LG])","link":"http://arxiv.org/abs/2110.04647","description":"<p>We propose a framework that learns to execute natural language instructions\nin an environment consisting of goal-reaching tasks that share components of\ntheir task descriptions. Our approach leverages the compositionality of both\nvalue functions and language, with the aim of reducing the sample complexity of\nlearning novel tasks. First, we train a reinforcement learning agent to learn\nvalue functions that can be subsequently composed through a Boolean algebra to\nsolve novel tasks. Second, we fine-tune a seq2seq model pretrained on web-scale\ncorpora to map language to logical expressions that specify the required value\nfunction compositions. Evaluating our agent in the BabyAI domain, we observe a\ndecrease of 86% in the number of training steps needed to learn a second task\nafter mastering a single task. Results from ablation studies further indicate\nthat it is the combination of compositional value functions and language\nrepresentations that allows the agent to quickly generalize to new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_V/0/1/0/all/0/1\">Vanya Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tasse_G/0/1/0/all/0/1\">Geraud Nangue Tasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1\">Nakul Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Steven James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1\">Matthew Gombolay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosman_B/0/1/0/all/0/1\">Benjamin Rosman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Sequence to Sequence Learning for Compositional Generalization. (arXiv:2110.04655v1 [cs.CL])","link":"http://arxiv.org/abs/2110.04655","description":"<p>There is mounting evidence that existing neural network models, in particular\nthe very popular sequence-to-sequence architecture, struggle with compositional\ngeneralization, i.e., the ability to systematically generalize to unseen\ncompositions of seen components. In this paper we demonstrate that one of the\nreasons hindering compositional generalization relates to the representations\nbeing entangled. We propose an extension to sequence-to-sequence models which\nallows us to learn disentangled representations by adaptively re-encoding (at\neach time step) the source input. Specifically, we condition the source\nrepresentations on the newly decoded target context which makes it easier for\nthe encoder to exploit specialized information for each prediction rather than\ncapturing all source information in a single forward pass. Experimental results\non semantic parsing and machine translation empirically show that our proposal\nyields more disentangled representations and better generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Audio Captions Be Evaluated with Image Caption Metrics?. (arXiv:2110.04684v1 [cs.SD])","link":"http://arxiv.org/abs/2110.04684","description":"<p>Automated audio captioning aims at generating textual descriptions for an\naudio clip. To evaluate the quality of generated audio captions, previous works\ndirectly adopt image captioning metrics like SPICE and CIDEr, without\njustifying their suitability in this new domain, which may mislead the\ndevelopment of advanced models. This problem is still unstudied due to the lack\nof human judgment datasets on caption quality. Therefore, we firstly construct\ntwo evaluation benchmarks, AudioCaps-Eval and Clotho-Eval. They are established\nwith pairwise comparison instead of absolute rating to achieve better\ninter-annotator agreement. Current metrics are found in poor correlation with\nhuman annotations on these datasets. To overcome their limitations, we propose\na metric named FENSE, where we combine the strength of Sentence-BERT in\ncapturing similarity, and a novel Error Detector to penalize erroneous\nsentences for robustness. On the newly established benchmarks, FENSE\noutperforms current metrics by 14-25% accuracy. Code, data and web demo\navailable at: https://github.com/blmoistawinde/fense\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zelin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zeyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kenny Q. Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Channel End-to-End Neural Diarization with Distributed Microphones. (arXiv:2110.04694v1 [eess.AS])","link":"http://arxiv.org/abs/2110.04694","description":"<p>Recent progress on end-to-end neural diarization (EEND) has enabled\noverlap-aware speaker diarization with a single neural network. This paper\nproposes to enhance EEND by using multi-channel signals from distributed\nmicrophones. We replace Transformer encoders in EEND with two types of encoders\nthat process a multi-channel input: spatio-temporal and co-attention encoders.\nBoth are independent of the number and geometry of microphones and suitable for\ndistributed microphone settings. We also propose a model adaptation method\nusing only single-channel recordings. With simulated and real-recorded\ndatasets, we demonstrated that the proposed method outperformed conventional\nEEND when a multi-channel input was given while maintaining comparable\nperformance with a single-channel input. We also showed that the proposed\nmethod performed well even when spatial information is inoperative given\nmulti-channel inputs, such as in hybrid meetings in which the utterances of\nmultiple remote participants are played back from the same loudspeaker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_Y/0/1/0/all/0/1\">Yuki Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1\">Yohei Kawaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention in Natural Language Processing. (arXiv:1902.02181v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1902.02181","description":"<p>Attention is an increasingly popular mechanism used in a wide range of neural\narchitectures. The mechanism itself has been realized in a variety of formats.\nHowever, because of the fast-paced advances in this domain, a systematic\noverview of attention is still missing. In this article, we define a unified\nmodel for attention architectures in natural language processing, with a focus\non those designed to work with vector representations of the textual data. We\npropose a taxonomy of attention models according to four dimensions: the\nrepresentation of the input, the compatibility function, the distribution\nfunction, and the multiplicity of the input and/or output. We present the\nexamples of how prior information can be exploited in attention models and\ndiscuss ongoing research efforts and open challenges in the area, providing the\nfirst extensive categorization of the vast body of literature in this exciting\ndomain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippi_M/0/1/0/all/0/1\">Marco Lippi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Relational Image Captioning via Multi-task Triple-Stream Networks. (arXiv:2010.03855v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.03855","description":"<p>We introduce dense relational captioning, a novel image captioning task which\naims to generate multiple captions with respect to relational information\nbetween objects in a visual scene. Relational captioning provides explicit\ndescriptions for each relationship between object combinations. This framework\nis advantageous in both diversity and amount of information, leading to a\ncomprehensive image understanding based on relationships, e.g., relational\nproposal generation. For relational understanding between objects, the\npart-of-speech (POS; i.e., subject-object-predicate categories) can be a\nvaluable prior information to guide the causal sequence of words in a caption.\nWe enforce our framework to learn not only to generate captions but also to\nunderstand the POS of each word. To this end, we propose the multi-task\ntriple-stream network (MTTSNet) which consists of three recurrent units\nresponsible for each POS which is trained by jointly predicting the correct\ncaptions and POS for each word. In addition, we found that the performance of\nMTTSNet can be improved by modulating the object embeddings with an explicit\nrelational module. We demonstrate that our proposed model can generate more\ndiverse and richer captions, via extensive experimental analysis on large scale\ndatasets and several metrics. Then, we present applications of our framework to\nholistic image captioning, scene graph generation, and retrieval tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset. (arXiv:2010.04480v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.04480","description":"<p>We present MLQE-PE, a new dataset for Machine Translation (MT) Quality\nEstimation (QE) and Automatic Post-Editing (APE). The dataset contains eleven\nlanguage pairs, with human labels for up to 10,000 translations per language\npair in the following formats: sentence-level direct assessments and\npost-editing effort, and word-level good/bad labels. It also contains the\npost-edited sentences, as well as titles of the articles where the sentences\nwere extracted from, and the neural MT models used to translate the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Erick Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopatina_N/0/1/0/all/0/1\">Nina Lopatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemMT: A Semantic-based Testing Approach for Machine Translation Systems. (arXiv:2012.01815v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2012.01815","description":"<p>Machine translation has wide applications in daily life. In mission-critical\napplications such as translating official documents, incorrect translation can\nhave unpleasant or sometimes catastrophic consequences. This motivates recent\nresearch on testing methodologies for machine translation systems. Existing\nmethodologies mostly rely on metamorphic relations designed at the textual\nlevel (e.g., Levenshtein distance) or syntactic level (e.g., the distance\nbetween grammar structures) to determine the correctness of translation\nresults. However, these metamorphic relations do not consider whether the\noriginal and translated sentences have the same meaning (i.e., Semantic\nsimilarity). Therefore, in this paper, we propose SemMT, an automatic testing\napproach for machine translation systems based on semantic similarity checking.\nSemMT applies round-trip translation and measures the semantic similarity\nbetween the original and translated sentences. Our insight is that the\nsemantics expressed by the logic and numeric constraint in sentences can be\ncaptured using regular expressions (or deterministic finite automata) where\nefficient equivalence/similarity checking algorithms are available. Leveraging\nthe insight, we propose three semantic similarity metrics and implement them in\nSemMT. The experiment result reveals SemMT can achieve higher effectiveness\ncompared with state-of-the-art works, achieving an increase of 21% and 23% on\naccuracy and F-Score, respectively. We also explore potential improvements that\ncan be achieved when proper combinations of metrics are adopted. Finally, we\ndiscuss a solution to locate the suspicious trip in round-trip translation,\nwhich may shed lights on further exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jialun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meiziniu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yeting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1\">Ming Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_S/0/1/0/all/0/1\">Shing-Chi Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximating How Single Head Attention Learns. (arXiv:2103.07601v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07601","description":"<p>Why do models often attend to salient words, and how does this evolve\nthroughout training? We approximate model training as a two stage process:\nearly on in training when the attention weights are uniform, the model learns\nto translate individual input word `i` to `o` if they co-occur frequently.\nLater, the model learns to attend to `i` while the correct output is $o$\nbecause it knows `i` translates to `o`. To formalize, we define a model\nproperty, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`\ntranslates to `o`), and claim that it drives the learning of the attention.\nThis claim is supported by the fact that before the attention mechanism is\nlearned, KTIW can be learned from word co-occurrence statistics, but not the\nother way around. Particularly, we can construct a training distribution that\nmakes KTIW hard to learn, the learning of the attention fails, and the model\ncannot even learn the simple task of copying the input words to the output. Our\napproximation explains why models sometimes attend to salient words, and\ninspires a toy example where a multi-head attention model can overcome the\nabove hard training distribution by improving learning dynamics rather than\nexpressiveness. We end by discussing the limitation of our approximation\nframework and suggest future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LSTM Based Sentiment Analysis for Cryptocurrency Prediction. (arXiv:2103.14804v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14804","description":"<p>Recent studies in big data analytics and natural language processing develop\nautomatic techniques in analyzing sentiment in the social media information. In\naddition, the growing user base of social media and the high volume of posts\nalso provide valuable sentiment information to predict the price fluctuation of\nthe cryptocurrency. This research is directed to predicting the volatile price\nmovement of cryptocurrency by analyzing the sentiment in social media and\nfinding the correlation between them. While previous work has been developed to\nanalyze sentiment in English social media posts, we propose a method to\nidentify the sentiment of the Chinese social media posts from the most popular\nChinese social media platform Sina-Weibo. We develop the pipeline to capture\nWeibo posts, describe the creation of the crypto-specific sentiment dictionary,\nand propose a long short-term memory (LSTM) based recurrent neural network\nalong with the historical cryptocurrency price movement to predict the price\ntrend for future time frames. The conducted experiments demonstrate the\nproposed approach outperforms the state of the art auto regressive based model\nby 18.5% in precision and 15.4% in recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surbiryala_J/0/1/0/all/0/1\">Jayachander Surbiryala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_V/0/1/0/all/0/1\">Vasileios Iosifidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. (arXiv:2104.06378v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06378","description":"<p>The problem of answering questions using knowledge from pre-trained language\nmodels (LMs) and knowledge graphs (KGs) presents two challenges: given a QA\ncontext (question and answer choice), methods need to (i) identify relevant\nknowledge from large KGs, and (ii) perform joint reasoning over the QA context\nand KG. In this work, we propose a new model, QA-GNN, which addresses the above\nchallenges through two key innovations: (i) relevance scoring, where we use LMs\nto estimate the importance of KG nodes relative to the given QA context, and\n(ii) joint reasoning, where we connect the QA context and KG to form a joint\ngraph, and mutually update their representations through graph neural networks.\nWe evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its\nimprovement over existing LM and LM+KG models, as well as its capability to\nperform interpretable and structured reasoning, e.g., correctly handling\nnegation in questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph. (arXiv:2104.07302v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07302","description":"<p>Multi-hop Question Answering (QA) is a challenging task because it requires\nprecise reasoning with entity relations at every step towards the answer. The\nrelations can be represented in terms of labels in knowledge graph (e.g.,\n\\textit{spouse}) or text in text corpus (e.g., \\textit{they have been married\nfor 26 years}). Existing models usually infer the answer by predicting the\nsequential relation path or aggregating the hidden graph features. The former\nis hard to optimize, and the latter lacks interpretability. In this paper, we\npropose TransferNet, an effective and transparent model for multi-hop QA, which\nsupports both label and text relations in a unified framework. TransferNet\njumps across entities at multiple steps. At each step, it attends to different\nparts of the question, computes activated scores for relations, and then\ntransfer the previous entity scores along activated relations in a\ndifferentiable way. We carry out extensive experiments on three datasets and\ndemonstrate that TransferNet surpasses the state-of-the-art models by a large\nmargin. In particular, on MetaQA, it achieves 100\\% accuracy in 2-hop and 3-hop\nquestions. By qualitative analysis, we show that TransferNet has transparent\nand interpretable intermediate results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation. (arXiv:2104.08200v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08200","description":"<p>Natural language generation (NLG) benchmarks provide an important avenue to\nmeasure progress and develop better NLG systems. Unfortunately, the lack of\npublicly available NLG benchmarks for low-resource languages poses a\nchallenging barrier for building NLG systems that work well for languages with\nlimited amounts of data. Here we introduce IndoNLG, the first benchmark to\nmeasure natural language generation (NLG) progress in three low-resource -- yet\nwidely spoken -- languages of Indonesia: Indonesian, Javanese, and Sundanese.\nAltogether, these languages are spoken by more than 100 million native\nspeakers, and hence constitute an important use case of NLG systems today.\nConcretely, IndoNLG covers six tasks: summarization, question answering,\nchit-chat, and three different pairs of machine translation (MT) tasks. We\ncollate a clean pretraining corpus of Indonesian, Sundanese, and Javanese\ndatasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and\nIndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on\nall tasks -- despite using only one-fifth the parameters of a larger\nmultilingual model, mBART-LARGE (Liu et al., 2020). This finding emphasizes the\nimportance of pretraining on closely related, local languages to achieve more\nefficient learning and faster inference for very low-resource languages like\nJavanese and Sundanese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentio_K/0/1/0/all/0/1\">Karissa Vincentio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1\">Zhi Yuan Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahar_S/0/1/0/all/0/1\">Syafri Bahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodra_M/0/1/0/all/0/1\">Masayu Leylia Khodra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08808","description":"<p>The ability to continuously expand knowledge over time and utilize it to\nrapidly generalize to new tasks is a key feature of human linguistic\nintelligence. Existing models that pursue rapid generalization to new tasks\n(e.g., few-shot learning methods), however, are mostly trained in a single shot\non fixed datasets, unable to dynamically expand their knowledge; while\ncontinual learning algorithms are not specifically designed for rapid\ngeneralization. We present a new learning setup, Continual Learning of Few-Shot\nLearners (CLIF), to address the challenges of both learning settings in a\nunified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks\narriving sequentially, accumulating knowledge for improved generalization to\nnew tasks, while also retaining performance on the tasks learned earlier. We\nexamine how the generalization ability is affected in the continual learning\nsetup, evaluate a number of continual learning algorithms, and propose a novel\nregularized adapter generation approach. We find that catastrophic forgetting\naffects generalization ability to a less degree than performance on seen tasks;\nwhile continual learning algorithms can still bring considerable benefit to the\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoFormer: Enhanced Transformer with Rotary Position Embedding. (arXiv:2104.09864v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09864","description":"<p>Position encoding in transformer architecture provides supervision for\ndependency modeling between elements at different positions in the sequence. We\ninvestigate various methods to encode positional information in\ntransformer-based language models and propose a novel implementation named\nRotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional\ninformation with rotation matrix and naturally incorporates explicit relative\nposition dependency in self-attention formulation. Notably, RoPE comes with\nvaluable properties such as flexibility of being expand to any sequence\nlengths, decaying inter-token dependency with increasing relative distances,\nand capability of equipping the linear self-attention with relative position\nencoding. As a result, the enhanced transformer with rotary position embedding,\nor RoFormer, achieves superior performance in tasks with long texts. We release\nthe theoretical analysis along with some preliminary experiment results on\nChinese data. The undergoing experiment for English benchmark will soon be\nupdated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bo Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12227","description":"<p>Classic information extraction techniques consist in building questions and\nanswers about the facts. Indeed, it is still a challenge to subjective\ninformation extraction systems to identify opinions and feelings in context. In\nsentiment-based NLP tasks, there are few resources to information extraction,\nabove all offensive or hateful opinions in context. To fill this important gap,\nthis short paper provides a new cross-lingual and contextual offensive lexicon,\nwhich consists of explicit and implicit offensive and swearing expressions of\nopinion, which were annotated in two different classes: context dependent and\ncontext-independent offensive. In addition, we provide markers to identify hate\nspeech. Annotation approach was evaluated at the expression-level and achieves\nhigh human inter-annotator agreement. The provided offensive lexicon is\navailable in Portuguese and English languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12265","description":"<p>This paper provides a new approach for offensive language and hate speech\ndetection on social media. Our approach incorporates an offensive lexicon\ncomposed of implicit and explicit offensive and swearing expressions annotated\nwith binary classes: context-dependent and context-independent offensive. Due\nto the severity of the hate speech and offensive comments in Brazil, and the\nlack of research in Portuguese, Brazilian Portuguese is the language used to\nvalidate the proposed method. Nevertheless, our proposal may be applied to any\nother language or domain. Based on the obtained results, the proposed approach\nshowed high-performance overcoming the current baselines for European and\nBrazilian Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1\">Fabr&#xed;cio Benevenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_T/0/1/0/all/0/1\">Thiago Alexandre Salgueiro Pardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WhatTheWikiFact: Fact-Checking Claims Against Wikipedia. (arXiv:2105.00826v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00826","description":"<p>The rise of Internet has made it a major source of information.\nUnfortunately, not all information online is true, and thus a number of\nfact-checking initiatives have been launched, both manual and automatic, to\ndeal with the problem. Here, we present our contribution in this regard:\n\\emph{WhatTheWikiFact}, a system for automatic claim verification using\nWikipedia. The system can predict the veracity of an input claim, and it\nfurther shows the evidence it has retrieved as part of the verification\nprocess. It shows confidence scores and a list of relevant Wikipedia articles,\ntogether with detailed information about each article, including the phrase\nused to retrieve it, the most relevant sentences extracted from it and their\nstance with respect to the input claim, as well as the associated\nprobabilities. The system supports several languages: Bulgarian, English, and\nRussian.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Anton Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilvovsky_D/0/1/0/all/0/1\">Dmitry Ilvovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Black or White but never neutral: How readers perceive identity from yellow or skin-toned emoji. (arXiv:2105.05887v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.05887","description":"<p>Research in sociology and linguistics shows that people use language not only\nto express their own identity but to understand the identity of others. Recent\nwork established a connection between expression of identity and emoji usage on\nsocial media, through use of emoji skin tone modifiers. Motivated by that\nfinding, this work asks if, as with language, readers are sensitive to such\nacts of self-expression and use them to understand the identity of authors. In\nbehavioral experiments (n=488), where text and emoji content of social media\nposts were carefully controlled before being presented to participants, we find\nin the affirmative -- emoji are a salient signal of author identity. That\nsignal is distinct from, and complementary to, the one encoded in language.\nParticipant groups (based on self-identified ethnicity) showed no differences\nin how they perceive this signal, except in the case of the default yellow\nemoji. While both groups associate this with a White identity, the effect was\nstronger in White participants. Our finding that emoji can index social\nvariables will have experimental applications for researchers but also\nimplications for designers: supposedly ``neutral`` defaults may be more\nrepresentative of some users than others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robertson_A/0/1/0/all/0/1\">Alexander Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magdy_W/0/1/0/all/0/1\">Walid Magdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doc2Dict: Information Extraction as Text Generation. (arXiv:2105.07510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07510","description":"<p>Typically, information extraction (IE) requires a pipeline approach: first, a\nsequence labeling model is trained on manually annotated documents to extract\nrelevant spans; then, when a new document arrives, a model predicts spans which\nare then post-processed and standardized to convert the information into a\ndatabase entry. We replace this labor-intensive workflow with a transformer\nlanguage model trained on existing database records to directly generate\nstructured JSON. Our solution removes the workload associated with producing\ntoken-level annotations and takes advantage of a data source which is generally\nquite plentiful (e.g. database records). As long documents are common in\ninformation extraction tasks, we use gradient checkpointing and chunked\nencoding to apply our method to sequences of up to 32,000 tokens on a single\nGPU. Our Doc2Dict approach is competitive with more complex, hand-engineered\npipelines and offers a simple but effective baseline for document-level\ninformation extraction. We release our Doc2Dict model and code to reproduce our\nexperiments and facilitate future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Townsend_B/0/1/0/all/0/1\">Benjamin Townsend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_Fisher_E/0/1/0/all/0/1\">Eamon Ito-Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lily Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_M/0/1/0/all/0/1\">Madison May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable agent communication from scratch (with a generic visual processor emerging on the side). (arXiv:2106.04258v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04258","description":"<p>As deep networks begin to be deployed as autonomous agents, the issue of how\nthey can communicate with each other becomes important. Here, we train two deep\nnets from scratch to perform realistic referent identification through\nunsupervised emergent communication. We show that the largely interpretable\nemergent protocol allows the nets to successfully communicate even about object\ntypes they did not see at training time. The visual representations induced as\na by-product of our training regime, moreover, show comparable quality, when\nre-used as generic visual features, to a recent self-supervised learning model.\nOur results provide concrete evidence of the viability of (interpretable)\nemergent deep net communication in a more realistic scenario than previously\nconsidered, as well as establishing an intriguing link between this field and\nself-supervised visual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Spatio-Temporal Language with Transformers. (arXiv:2106.08858v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.08858","description":"<p>Language is an interface to the outside world. In order for embodied agents\nto use it, language must be grounded in other, sensorimotor modalities. While\nthere is an extended literature studying how machines can learn grounded\nlanguage, the topic of how to learn spatio-temporal linguistic concepts is\nstill largely uncharted. To make progress in this direction, we here introduce\na novel spatio-temporal language grounding task where the goal is to learn the\nmeaning of spatio-temporal descriptions of behavioral traces of an embodied\nagent. This is achieved by training a truth function that predicts if a\ndescription matches a given history of observations. The descriptions involve\ntime-extended predicates in past and present tense as well as spatio-temporal\nreferences to objects in the scene. To study the role of architectural biases\nin this task, we train several models including multimodal Transformer\narchitectures; the latter implement different attention computations between\nwords and objects across space and time. We test models on two classes of\ngeneralization: 1) generalization to randomly held-out sentences; 2)\ngeneralization to grammar primitives. We observe that maintaining object\nidentity in the attention computation of our Transformers is instrumental to\nachieving good performance on generalization overall, and that summarizing\nobject traces in a single token has little influence on performance. We then\ndiscuss how this opens new perspectives for language-guided autonomous embodied\nagents. We also release our code under open-source license as well as\npretrained models and datasets to encourage the wider community to build upon\nand extend our work in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_L/0/1/0/all/0/1\">Laetitia Teodorescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1\">Cl&#xe9;ment Moulin-Frier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Analysis of a Self-supervised Speech Representation Model. (arXiv:2107.04734v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.04734","description":"<p>Recently proposed self-supervised learning approaches have been successful\nfor pre-training speech representation models. The utility of these learned\nrepresentations has been observed empirically, but not much has been studied\nabout the type or extent of information encoded in the pre-trained\nrepresentations themselves. Developing such insights can help understand the\ncapabilities and limits of these models and enable the research community to\nmore efficiently develop their usage for downstream applications. In this work,\nwe begin to fill this gap by examining one recent and successful pre-trained\nmodel (wav2vec 2.0), via its intermediate representation vectors, using a suite\nof analysis tools. We use the metrics of canonical correlation, mutual\ninformation, and performance on simple downstream tasks with non-parametric\nprobes, in order to (i) query for acoustic and linguistic information content,\n(ii) characterize the evolution of information across model layers, and (iii)\nunderstand how fine-tuning the model for automatic speech recognition (ASR)\naffects these observations. Our findings motivate modifying the fine-tuning\nprotocol for ASR, which produces improved word error rates in a low-resource\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_J/0/1/0/all/0/1\">Ju-Chieh Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation. (arXiv:2107.09278v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09278","description":"<p>Transcripts generated by automatic speech recognition (ASR) systems for\nspoken documents lack structural annotations such as paragraphs, significantly\nreducing their readability. Automatically predicting paragraph segmentation for\nspoken documents may both improve readability and downstream NLP performance\nsuch as summarization and machine reading comprehension. We propose a sequence\nmodel with self-adaptive sliding window for accurate and efficient paragraph\nsegmentation. We also propose an approach to exploit phonetic information,\nwhich significantly improves robustness of spoken document segmentation to ASR\nerrors. Evaluations are conducted on the English Wiki-727K document\nsegmentation benchmark, a Chinese Wikipedia-based document segmentation dataset\nwe created, and an in-house Chinese spoken document dataset. Our proposed model\noutperforms the state-of-the-art (SOTA) model based on the same BERT-Base,\nincreasing segmentation F1 on the English benchmark by 4.2 points and on\nChinese datasets by 4.3-10.1 points, while reducing inference time to less than\n1/6 of inference time of the current SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01135","description":"<p>Sequence-to-sequence learning with neural networks has become the de facto\nstandard for sequence prediction tasks. This approach typically models the\nlocal distribution over the next word with a powerful neural network that can\ncondition on arbitrary context. While flexible and performant, these models\noften require large datasets for training and can fail spectacularly on\nbenchmarks designed to test for compositional generalization. This work\nexplores an alternative, hierarchical approach to sequence-to-sequence learning\nwith quasi-synchronous grammars, where each node in the target tree is\ntransduced by a node in the source tree. Both the source and target trees are\ntreated as latent and induced during training. We develop a neural\nparameterization of the grammar which enables parameter sharing over the\ncombinatorial space of derivation rules without the need for manual feature\nengineering. We apply this latent neural grammar to various domains -- a\ndiagnostic language navigation task designed to test for compositional\ngeneralization (SCAN), style transfer, and small-scale machine translation --\nand find that it performs respectably compared to standard baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02401","description":"<p>Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04947","description":"<p>Large-scale, pre-trained language models (LMs) have achieved human-level\nperformance on a breadth of language understanding tasks. However, evaluations\nonly based on end task performance shed little light on machines' true ability\nin language understanding and reasoning. In this paper, we highlight the\nimportance of evaluating the underlying reasoning process in addition to end\nperformance. Toward this goal, we introduce Tiered Reasoning for Intuitive\nPhysics (TRIP), a novel commonsense reasoning dataset with dense annotations\nthat enable multi-tiered evaluation of machines' reasoning process. Our\nempirical results show that while large LMs can achieve high end performance,\nthey struggle to support their predictions with valid supporting evidence. The\nTRIP dataset and our baseline results will motivate verifiable evaluation of\ncommonsense reasoning and facilitate future research toward developing better\nlanguage understanding and reasoning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pack Together: Entity and Relation Extraction with Levitated Marker. (arXiv:2109.06067v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06067","description":"<p>Named Entity Recognition (NER) and Relation Extraction (RE) are the core\nsub-tasks for information extraction. Many recent works formulate these two\ntasks as the span (pair) classification problem, and thus focus on\ninvestigating how to obtain a better span representation from the pre-trained\nencoder. However, a major limitation of existing works is that they ignore the\ndependencies between spans (pairs). In this work, we propose a novel span\nrepresentation approach, named Packed Levitated Markers, to consider the\ndependencies between the spans (pairs) by strategically packing the markers in\nthe encoder. In particular, we propose a group packing strategy to enable our\nmodel to process massive spans together to consider their dependencies with\nlimited resources. Furthermore, for those more complicated span pair\nclassification tasks, we design a subject-oriented packing strategy, which\npacks each subject and all its objects into an instance to model the\ndependencies between the same-subject span pairs. Our experiments show that our\nmodel with packed levitated markers outperforms the sequence labeling model by\n0.4%-1.9% F1 on three flat NER tasks, beats the token concat model on six NER\nbenchmarks, and obtains a 3.5%-3.6% strict relation F1 improvement with higher\nspeed over previous SOTA models on ACE04 and ACE05. Code and models are\npublicly available at https://github.com/thunlp/PL-Marker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1\">Deming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition. (arXiv:2109.09161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09161","description":"<p>Unifying acoustic and linguistic representation learning has become\nincreasingly crucial to transfer the knowledge learned on the abundance of\nhigh-resource language data for low-resource speech recognition. Existing\napproaches simply cascade pre-trained acoustic and language models to learn the\ntransfer from speech to text. However, how to solve the representation\ndiscrepancy of speech and text is unexplored, which hinders the utilization of\nacoustic and linguistic information. Moreover, previous works simply replace\nthe embedding layer of the pre-trained language model with the acoustic\nfeatures, which may cause the catastrophic forgetting problem. In this work, we\nintroduce Wav-BERT, a cooperative acoustic and linguistic representation\nlearning method to fuse and utilize the contextual information of speech and\ntext. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a\nlanguage model (BERT) into an end-to-end trainable framework. A Representation\nAggregation Module is designed to aggregate acoustic and linguistic\nrepresentation, and an Embedding Attention Module is introduced to incorporate\nacoustic information into BERT, which can effectively facilitate the\ncooperation of two pre-trained models and thus boost the representation\nlearning. Extensive experiments show that our Wav-BERT significantly\noutperforms the existing approaches and achieves state-of-the-art performance\non low-resource speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guolin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yubei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1\">Ke Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques. (arXiv:2109.09725v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09725","description":"<p>In this paper, a BERT based neural network model is applied to the JIGSAW\ndata set in order to create a model identifying hateful and toxic comments\n(strictly seperated from offensive language) in online social platforms\n(English language), in this case Twitter. Three other neural network\narchitectures and a GPT-2 model are also applied on the provided data set in\norder to compare these different models. The trained BERT model is then applied\non two different data sets to evaluate its generalisation power, namely on\nanother Twitter data set and the data set HASOC 2019 which includes Twitter and\nalso Facebook comments; we focus on the English HASOC 2019 data. In addition,\nit can be shown that by fine-tuning the trained BERT model on these two data\nsets by applying different transfer learning scenarios via retraining partial\nor all layers the predictive scores improve compared to simply applying the\nmodel pre-trained on the JIGSAW data set. With our results, we get precisions\nfrom 64% to around 90% while still achieving acceptable recall values of at\nleast lower 60s%, proving that BERT is suitable for real use cases in social\nplatforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zagidullina_A/0/1/0/all/0/1\">Aygul Zagidullina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patoulidis_G/0/1/0/all/0/1\">Georgios Patoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WRENCH: A Comprehensive Benchmark for Weak Supervision. (arXiv:2109.11377v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.11377","description":"<p>Recent Weak Supervision (WS) approaches have had widespread success in easing\nthe bottleneck of labeling training data for machine learning by synthesizing\nlabels from multiple potentially noisy supervision sources. However, proper\nmeasurement and analysis of these approaches remain a challenge. First,\ndatasets used in existing works are often private and/or custom, limiting\nstandardization. Second, WS datasets with the same name and base data often\nvary in terms of the labels and weak supervision sources used, a significant\n\"hidden\" source of evaluation variance. Finally, WS studies often diverge in\nterms of the evaluation protocol and ablations used. To address these problems,\nwe introduce a benchmark platform, WRENCH, for thorough and standardized\nevaluation of WS approaches. It consists of 22 varied real-world datasets for\nclassification and sequence tagging; a range of real, synthetic, and\nprocedurally-generated weak supervision sources; and a modular, extensible\nframework for WS evaluation, including implementations for popular WS methods.\nWe use WRENCH to conduct extensive comparisons over more than 120 method\nvariants to demonstrate its efficacy as a benchmark platform. The code is\navailable at https://github.com/JieyuZ2/wrench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12584","description":"<p>In recent times, there has been definitive progress in the field of NLP, with\nits applications growing as the utility of our language models increases with\nadvances in their performance. However, these models require a large amount of\ncomputational power and data to train, consequently leading to large carbon\nfootprints. Therefore, it is imperative that we study the carbon efficiency and\nlook for alternatives to reduce the overall environmental impact of training\nmodels, in particular large language models. In our work, we assess the\nperformance of models for machine translation, across multiple language pairs\nto assess the difference in computational power required to train these models\nfor each of these language pairs and examine the various components of these\nmodels to analyze aspects of our pipeline that can be optimized to reduce these\ncarbon emissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_M/0/1/0/all/0/1\">Mirza Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surana_P/0/1/0/all/0/1\">Praatibh Surana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications. (arXiv:2109.14776v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14776","description":"<p>Certainty and uncertainty are fundamental to science communication. Hedges\nhave widely been used as proxies for uncertainty. However, certainty is a\ncomplex construct, with authors expressing not only the degree but the type and\naspects of uncertainty in order to give the reader a certain impression of what\nis known. Here, we introduce a new study of certainty that models both the\nlevel and the aspects of certainty in scientific findings. Using a new dataset\nof 2167 annotated scientific findings, we demonstrate that hedges alone account\nfor only a partial explanation of certainty. We show that both the overall\ncertainty and individual aspects can be predicted with pre-trained language\nmodels, providing a more complete picture of the author's intended\ncommunication. Downstream analyses on 431K scientific findings from news and\nscientific abstracts demonstrate that modeling sentence-level and aspect-level\ncertainty is meaningful for areas like science communication. Both the model\nand datasets used in this paper are released at\nhttps://blablablab.si.umich.edu/projects/certainty/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#ContextMatters: Advantages and Limitations of Using Machine Learning to Support Women in Politics. (arXiv:2110.00116v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2110.00116","description":"<p>The United Nations identified gender equality as a Sustainable Development\nGoal in 2015, recognizing the underrepresentation of women in politics as a\nspecific barrier to achieving gender equality. Political systems around the\nworld experience gender inequality across all levels of elected government as\nfewer women run for office than men. This is due in part to online abuse,\nparticularly on social media platforms like Twitter, where women seeking or in\npower tend to be targeted with more toxic maltreatment than their male\ncounterparts. In this paper, we present reflections on ParityBOT - the first\nnatural language processing-based intervention designed to affect online\ndiscourse for women in politics for the better, at scale. Deployed across\nelections in Canada, the United States and New Zealand, ParityBOT was used to\nanalyse and classify more than 12 million tweets directed at women candidates\nand counter toxic tweets with supportive ones. From these elections we present\nthree case studies highlighting the current limitations of, and future research\nand application opportunities for, using a natural language processing-based\nsystem to detect online toxicity, specifically with regards to contextually\nimportant microaggressions. We examine the rate of false negatives, where\nParityBOT failed to pick up on insults directed at specific high profile women,\nwhich would be obvious to human users. We examine the unaddressed harms of\nmicroaggressions and the potential of yet unseen damage they cause for women in\nthese communities, and for progress towards gender equality overall, in light\nof these technological blindspots. This work concludes with a discussion on the\nbenefits of partnerships between nonprofit social groups and technology experts\nto develop responsible, socially impactful approaches to addressing online\nhate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Comer_J/0/1/0/all/0/1\">Jacqueline Comer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Work_S/0/1/0/all/0/1\">Sam Work</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathewson_K/0/1/0/all/0/1\">Kory W Mathewson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuthbertson_L/0/1/0/all/0/1\">Lana Cuthbertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machin_K/0/1/0/all/0/1\">Kasey Machin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles. (arXiv:2110.03179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03179","description":"<p>We present HowSumm, a novel large-scale dataset for the task of query-focused\nmulti-document summarization (qMDS), which targets the use-case of generating\nactionable instructions from a set of sources. This use-case is different from\nthe use-cases covered in existing multi-document summarization (MDS) datasets\nand is applicable to educational and industrial scenarios. We employed\nautomatic methods, and leveraged statistics from existing human-crafted qMDS\ndatasets, to create HowSumm from wikiHow website articles and the sources they\ncite. We describe the creation of the dataset and discuss the unique features\nthat distinguish it from other summarization corpora. Automatic and human\nevaluations of both extractive and abstractive summarization models on the\ndataset reveal that there is room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boni_O/0/1/0/all/0/1\">Odellia Boni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feigenblat_G/0/1/0/all/0/1\">Guy Feigenblat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lev_G/0/1/0/all/0/1\">Guy Lev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1\">Michal Shmueli-Scheuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sznajder_B/0/1/0/all/0/1\">Benjamin Sznajder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopnicki_D/0/1/0/all/0/1\">David Konopnicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over. (arXiv:2110.03342v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.03342","description":"<p>In this paper, we formulate a novel task to synthesize speech in sync with a\nsilent pre-recorded video, denoted as automatic voice over (AVO). Unlike\ntraditional speech synthesis, AVO seeks to generate not only human-sounding\nspeech, but also perfect lip-speech synchronization. A natural solution to AVO\nis to condition the speech rendering on the temporal progression of lip\nsequence in the video. We propose a novel text-to-speech model that is\nconditioned on visual input, named VisualTTS, for accurate lip-speech\nsynchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)\ntextual-visual attention, and 2) visual fusion strategy during acoustic\ndecoding, which both contribute to forming accurate alignment between the input\ntext content and lip motion in input lip sequence. Experimental results show\nthat VisualTTS achieves accurate lip-speech synchronization and outperforms all\nbaseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Junchen Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Phonological Features in Multilingual Text-To-Speech. (arXiv:2110.03609v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03609","description":"<p>This study investigates whether phonological features can be applied in\ntext-to-speech systems to generate native and non-native speech in English and\nMandarin. We present a mapping of ARPABET/pinyin to SAMPA/SAMPA-SC and then to\nphonological features. We tested whether this mapping could lead to the\nsuccessful generation of native, non-native, and code-switched speech in the\ntwo languages. We ran two experiments, one with a small dataset and one with a\nlarger dataset. The results proved that phonological features could be used as\na feasible input system, although further investigation is needed to improve\nmodel performance. The accented output generated by the TTS models also helps\nwith understanding human second language acquisition processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huinan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiewen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation of professions in entertainment media: Insights into frequency and sentiment trends through computational text analysis. (arXiv:2110.03873v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.03873","description":"<p>Societal ideas and trends dictate media narratives and cinematic depictions\nwhich in turn influences people's beliefs and perceptions of the real world.\nMedia portrayal of culture, education, government, religion, and family affect\ntheir function and evolution over time as people interpret and perceive these\nrepresentations and incorporate them into their beliefs and actions. It is\nimportant to study media depictions of these social structures so that they do\nnot propagate or reinforce negative stereotypes, or discriminate against any\ndemographic section. In this work, we examine media representation of\nprofessions and provide computational insights into their incidence, and\nsentiment expressed, in entertainment media content. We create a searchable\ntaxonomy of professional groups and titles to facilitate their retrieval from\nspeaker-agnostic text passages like movie and television (TV) show subtitles.\nWe leverage this taxonomy and relevant natural language processing (NLP) models\nto create a corpus of professional mentions in media content, spanning more\nthan 136,000 IMDb titles over seven decades (1950-2017). We analyze the\nfrequency and sentiment trends of different occupations, study the effect of\nmedia attributes like genre, country of production, and title type on these\ntrends, and investigate if the incidence of professions in media subtitles\ncorrelate with their real-world employment statistics. We observe increased\nmedia mentions of STEM, arts, sports, and entertainment occupations in the\nanalyzed subtitles, and a decreased frequency of manual labor jobs and military\noccupations. The sentiment expressed toward lawyers, police, and doctors is\nbecoming negative over time, whereas astronauts, musicians, singers, and\nengineers are mentioned favorably. Professions that employ more people have\nincreased media frequency, supporting our hypothesis that media acts as a\nmirror to society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baruah_S/0/1/0/all/0/1\">Sabyasachee Baruah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}