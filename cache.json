{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SRU++: Pioneering Fast Recurrence with Attention for Speech Recognition. (arXiv:2110.05571v1 [eess.AS])","link":"http://arxiv.org/abs/2110.05571","description":"<p>The Transformer architecture has been well adopted as a dominant architecture\nin most sequence transduction tasks including automatic speech recognition\n(ASR), since its attention mechanism excels in capturing long-range\ndependencies. While models built solely upon attention can be better\nparallelized than regular RNN, a novel network architecture, SRU++, was\nrecently proposed. By combining the fast recurrence and attention mechanism,\nSRU++ exhibits strong capability in sequence modeling and achieves\nnear-state-of-the-art results in various language modeling and machine\ntranslation tasks with improved compute efficiency. In this work, we present\nthe advantages of applying SRU++ in ASR tasks by comparing with Conformer\nacross multiple ASR benchmarks and study how the benefits can be generalized to\nlong-form speech inputs. On the popular LibriSpeech benchmark, our SRU++ model\nachieves 2.0% / 4.7% WER on test-clean / test-other, showing competitive\nperformances compared with the state-of-the-art Conformer encoder under the\nsame set-up. Specifically, SRU++ can surpass Conformer on long-form speech\ninput with a large margin, based on our analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jing Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_K/0/1/0/all/0/1\">Kyu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Data Mining of Public Transport Incidents reported in Social Media. (arXiv:2110.05573v1 [cs.SI])","link":"http://arxiv.org/abs/2110.05573","description":"<p>Public transport agencies use social media as an essential tool for\ncommunicating mobility incidents to passengers. However, while the short term,\nday-to-day information about transport phenomena is usually posted in social\nmedia with low latency, its availability is short term as the content is rarely\nmade an aggregated form. Social media communication of transport phenomena\nusually lacks GIS annotations as most social media platforms do not allow\nattaching non-POI GPS coordinates to posts. As a result, the analysis of\ntransport phenomena information is minimal. We collected three years of social\nmedia posts of a polish public transport company with user comments. Through\nexploration, we infer a six-class transport information typology. We\nsuccessfully build an information type classifier for social media posts,\ndetect stop names in posts, and relate them to GPS coordinates, obtaining a\nspatial understanding of long-term aggregated phenomena. We show that our\napproach enables citizen science and use it to analyze the impact of three\nyears of infrastructure incidents on passenger mobility, and the sentiment and\nreaction scale towards each of the events. All these results are achieved for\nPolish, an under-resourced language when it comes to spatial language\nunderstanding, especially in social media contexts. To improve the situation,\nwe released two of our annotated data sets: social media posts with incident\ntype labels and matched stop names and social media comments with the annotated\nsentiment. We also opensource the experimental codebase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raczycki_K/0/1/0/all/0/1\">Kamil Raczycki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szymanski_M/0/1/0/all/0/1\">Marcin Szyma&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeliseyenka_Y/0/1/0/all/0/1\">Yahor Yeliseyenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szymanski_P/0/1/0/all/0/1\">Piotr Szyma&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1\">Tomasz Kajdanowicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing to New Domains by Mapping Natural Language to Lifted LTL. (arXiv:2110.05603v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05603","description":"<p>Recent work on using natural language to specify commands to robots has\ngrounded that language to LTL. However, mapping natural language task\nspecifications to LTL task specifications using language models require\nprobability distributions over finite vocabulary. Existing state-of-the-art\nmethods have extended this finite vocabulary to include unseen terms from the\ninput sequence to improve output generalization. However, novel\nout-of-vocabulary atomic propositions cannot be generated using these methods.\nTo overcome this, we introduce an intermediate contextual query representation\nwhich can be learned from single positive task specification examples,\nassociating a contextual query with an LTL template. We demonstrate that this\nintermediate representation allows for generalization over unseen object\nreferences, assuming accurate groundings are available. We compare our method\nof mapping natural language task specifications to intermediate contextual\nqueries against state-of-the-art CopyNet models capable of translating natural\nlanguage to LTL, by evaluating whether correct LTL for manipulation and\nnavigation task specifications can be output, and show that our method\noutperforms the CopyNet model on unseen object references. We demonstrate that\nthe grounded LTL our method outputs can be used for planning in a simulated\nOO-MDP environment. Finally, we discuss some common failure modes encountered\nwhen translating natural language task specifications to grounded LTL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsiung_E/0/1/0/all/0/1\">Eric Hsiung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Hiloni Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1\">Junchi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Roma Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCube: Domain-Agnostic Neural Time-series Narration. (arXiv:2110.05633v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05633","description":"<p>The task of generating rich and fluent narratives that aptly describe the\ncharacteristics, trends, and anomalies of time-series data is invaluable to the\nsciences (geology, meteorology, epidemiology) or finance (trades, stocks, or\nsales and inventory). The efforts for time-series narration hitherto are\ndomain-specific and use predefined templates that offer consistency but lead to\nmechanical narratives. We present TCube (Time-series-to-text), a\ndomain-agnostic neural framework for time-series narration, that couples the\nrepresentation of essential time-series elements in the form of a dense\nknowledge graph and the translation of said knowledge graph into rich and\nfluent narratives through the transfer-learning capabilities of PLMs\n(Pre-trained Language Models). TCube's design primarily addresses the challenge\nthat lies in building a neural framework in the complete paucity of annotated\ntraining data for time-series. The design incorporates knowledge graphs as an\nintermediary for the representation of essential time-series elements which can\nbe linearized for textual translation. To the best of our knowledge, TCube is\nthe first investigation of the use of neural strategies for time-series\nnarration. Through extensive evaluations, we show that TCube can improve the\nlexical diversity of the generated narratives by up to 65.38% while still\nmaintaining grammatical integrity. The practicality and deployability of TCube\nis further validated through an expert review (n=21) where 76.2% of\nparticipating experts wary of auto-generated narratives favored TCube as a\ndeployable system for time-series narration due to its richer narratives. Our\ncode-base, models, and datasets, with detailed instructions for reproducibility\nis publicly hosted at https://github.com/Mandar-Sharma/TCube.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mandar Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brownstein_J/0/1/0/all/0/1\">John S. Brownstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Construction Grammars Converge Across Registers Given Increased Exposure. (arXiv:2110.05663v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05663","description":"<p>This paper measures the impact of increased exposure on whether learned\nconstruction grammars converge onto shared representations when trained on data\nfrom different registers. Register influences the frequency of constructions,\nwith some structures common in formal but not informal usage. We expect that a\ngrammar induction algorithm exposed to different registers will acquire\ndifferent constructions. To what degree does increased exposure lead to the\nconvergence of register-specific grammars? The experiments in this paper\nsimulate language learning in 12 languages (half Germanic and half Romance)\nwith corpora representing three registers (Twitter, Wikipedia, Web). These\nsimulations are repeated with increasing amounts of exposure, from 100k to 2\nmillion words, to measure the impact of exposure on the convergence of\ngrammars. The results show that increased exposure does lead to converging\ngrammars across all languages. In addition, a shared core of register-universal\nconstructions remains constant across increasing amounts of exposure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1\">Jonathan Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are you doing what I say? On modalities alignment in ALFRED. (arXiv:2110.05665v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05665","description":"<p>ALFRED is a recently proposed benchmark that requires a model to complete\ntasks in simulated house environments specified by instructions in natural\nlanguage. We hypothesize that key to success is accurately aligning the text\nmodality with visual inputs. Motivated by this, we inspect how well existing\nmodels can align these modalities using our proposed intrinsic metric, boundary\nadherence score (BAS). The results show the previous models are indeed failing\nto perform proper alignment. To address this issue, we introduce approaches\naimed at improving model alignment and demonstrate how improved alignment,\nimproves end task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_T/0/1/0/all/0/1\">Ta-Chung Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yau-Shian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models Can Be Strong Differentially Private Learners. (arXiv:2110.05679v1 [cs.LG])","link":"http://arxiv.org/abs/2110.05679","description":"<p>Differentially Private (DP) learning has seen limited success for building\nlarge deep learning models of text, and attempts at straightforwardly applying\nDifferentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have\nresulted in large performance drops and high computational overhead. We show\nthat this performance drop can be mitigated with (1) the use of large\npretrained models; (2) hyperparameters that suit DP optimization; and (3)\nfine-tuning objectives aligned with the pretraining procedure. With these\nfactors set right, we obtain private NLP models that outperform\nstate-of-the-art private training approaches and strong non-private baselines\n-- by directly fine-tuning pretrained models with DP optimization on\nmoderately-sized corpora. To address the computational challenge of running\nDP-SGD with large Transformers, we propose a memory saving technique that\nallows clipping in DP-SGD to run without instantiating per-example gradients\nfor any layer in the model. The technique enables privately training\nTransformers with almost the same memory cost as non-private training at a\nmodest run-time overhead. Contrary to conventional wisdom that DP optimization\nfails at learning high-dimensional models (due to noise that scales with\ndimension) empirical results reveal that private learning with pretrained\nmodels tends to not suffer from dimension-dependent performance degradation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1\">Tatsunori Hashimoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doubly-Trained Adversarial Data Augmentation for Neural Machine Translation. (arXiv:2110.05691v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05691","description":"<p>Neural Machine Translation (NMT) models are known to suffer from noisy\ninputs. To make models robust, we generate adversarial augmentation samples\nthat attack the model and preserve the source-side semantic meaning at the same\ntime. To generate such samples, we propose a doubly-trained architecture that\npairs two NMT models of opposite translation directions with a joint loss\nfunction, which combines the target-side attack and the source-side semantic\nsimilarity constraint. The results from our experiments across three different\nlanguage pairs and two evaluation metrics show that these adversarial samples\nimprove the model robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weiting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khayrallah_H/0/1/0/all/0/1\">Huda Khayrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Releasing Annotator-Level Labels and Information in Datasets. (arXiv:2110.05699v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05699","description":"<p>A common practice in building NLP datasets, especially using crowd-sourced\nannotations, involves obtaining multiple annotator judgements on the same data\ninstances, which are then flattened to produce a single \"ground truth\" label or\nscore, through majority voting, averaging, or adjudication. While these\napproaches may be appropriate in certain annotation tasks, such aggregations\noverlook the socially constructed nature of human perceptions that annotations\nfor relatively more subjective tasks are meant to capture. In particular,\nsystematic disagreements between annotators owing to their socio-cultural\nbackgrounds and/or lived experiences are often obfuscated through such\naggregations. In this paper, we empirically demonstrate that label aggregation\nmay introduce representational biases of individual and group perspectives.\nBased on this finding, we propose a set of recommendations for increased\nutility and transparency of datasets for downstream use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1\">Aida Mostafazadeh Davani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark D&#xed;az</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-aware Video Reading Comprehension for Temporal Language Grounding. (arXiv:2110.05717v1 [cs.CV])","link":"http://arxiv.org/abs/2110.05717","description":"<p>Temporal language grounding in videos aims to localize the temporal span\nrelevant to the given query sentence. Previous methods treat it either as a\nboundary regression task or a span extraction task. This paper will formulate\ntemporal language grounding into video reading comprehension and propose a\nRelation-aware Network (RaNet) to address it. This framework aims to select a\nvideo moment choice from the predefined answer set with the aid of\ncoarse-and-fine choice-query interaction and choice-choice relation\nconstruction. A choice-query interactor is proposed to match the visual and\ntextual information simultaneously in sentence-moment and token-moment levels,\nleading to a coarse-and-fine cross-modal interaction. Moreover, a novel\nmulti-choice relation constructor is introduced by leveraging graph convolution\nto capture the dependencies among video moment choices for the best choice\nselection. Extensive experiments on ActivityNet-Captions, TACoS, and\nCharades-STA demonstrate the effectiveness of our solution. Codes will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jialin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations. (arXiv:2110.05719v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05719","description":"<p>Majority voting and averaging are common approaches employed to resolve\nannotator disagreements and derive single ground truth labels from multiple\nannotations. However, annotators may systematically disagree with one another,\noften reflecting their individual biases and values, especially in the case of\nsubjective tasks such as detecting affect, aggression, and hate speech.\nAnnotator disagreements may capture important nuances in such tasks that are\noften ignored while aggregating annotations to a single ground truth. In order\nto address this, we investigate the efficacy of multi-annotator models. In\nparticular, our multi-task based approach treats predicting each annotators'\njudgements as separate subtasks, while sharing a common learned representation\nof the task. We show that this approach yields same or better performance than\naggregating labels in the data prior to training across seven different binary\nclassification tasks. Our approach also provides a way to estimate uncertainty\nin predictions, which we demonstrate better correlate with annotation\ndisagreements than traditional methods. Being able to model uncertainty is\nespecially useful in deployment scenarios where knowing when not to make a\nprediction is important.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1\">Aida Mostafazadeh Davani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mark D&#xed;az</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightSeq: Accelerated Training for Transformer-based Models on GPUs. (arXiv:2110.05722v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05722","description":"<p>Transformer-based models have proven to be powerful in many natural language,\ncomputer vision, and speech recognition applications. It is expensive to train\nthese types of models due to unfixed input length, complex computation, and\nlarge numbers of parameters. Existing systems either only focus on efficient\ninference or optimize only BERT-like encoder models. In this paper, we present\nLightSeq, a system for efficient training of Transformer-based models on GPUs.\nWe propose a series of GPU optimization techniques tailored to computation flow\nand memory access patterns of neural layers in Transformers. LightSeq supports\na variety of network architectures, including BERT (encoder-only), GPT\n(decoder-only), and Transformer (encoder-decoder). Our experiments on GPUs with\nvarying models and datasets show that LightSeq is 1.4-3.5x faster than previous\nsystems. In particular, it gains 308% training speedup compared with existing\nsystems on a large public machine translation benchmark (WMT14 English-German).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Ying Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xian Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of Political Leanings of Chinese Speaking Twitter Users. (arXiv:2110.05723v1 [cs.CY])","link":"http://arxiv.org/abs/2110.05723","description":"<p>This work presents a supervised method for generating a classifier model of\nthe stances held by Chinese-speaking politicians and other Twitter users. Many\nprevious works of political tweets prediction exist on English tweets, but to\nthe best of our knowledge, this is the first work that builds prediction model\non Chinese political tweets. It firstly collects data by scraping tweets of\nfamous political figure and their related users. It secondly defines the\npolitical spectrum in two groups: the group that shows approvals to the Chinese\nCommunist Party and the group that does not. Since there are not space between\nwords in Chinese to identify the independent words, it then completes\nsegmentation and vectorization by Jieba, a Chinese segmentation tool. Finally,\nit trains the data collected from political tweets and produce a classification\nmodel with high accuracy for understanding users' political stances from their\ntweets on Twitter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_F/0/1/0/all/0/1\">Fenglei Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Duoji Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomy of OntoGUM--Adapting GUM to the OntoNotes Scheme to Evaluate Robustness of SOTA Coreference Algorithms. (arXiv:2110.05727v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05727","description":"<p>SOTA coreference resolution produces increasingly impressive scores on the\nOntoNotes benchmark. However lack of comparable data following the same scheme\nfor more genres makes it difficult to evaluate generalizability to open domain\ndata. Zhu et al. (2021) introduced the creation of the OntoGUM corpus for\nevaluating geralizability of the latest neural LM-based end-to-end systems.\nThis paper covers details of the mapping process which is a set of\ndeterministic rules applied to the rich syntactic and discourse annotations\nmanually annotated in the GUM corpus. Out-of-domain evaluation across 12 genres\nshows nearly 15-20% degradation for both deterministic and deep learning\nsystems, indicating a lack of generalizability or covert overfitting in\nexisting coreference resolution models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yilun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1\">Sameer Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VarArray: Array-Geometry-Agnostic Continuous Speech Separation. (arXiv:2110.05745v1 [eess.AS])","link":"http://arxiv.org/abs/2110.05745","description":"<p>Continuous speech separation using a microphone array was shown to be\npromising in dealing with the speech overlap problem in natural conversation\ntranscription. This paper proposes VarArray, an array-geometry-agnostic speech\nseparation neural network model. The proposed model is applicable to any number\nof microphones without retraining while leveraging the nonlinear correlation\nbetween the input channels. The proposed method adapts different elements that\nwere proposed before separately, including transform-average-concatenate,\nconformer speech separation, and inter-channel phase differences, and combines\nthem in an efficient and cohesive way. Large-scale evaluation was performed\nwith two real meeting transcription tasks by using a fully developed\ntranscription system requiring no prior knowledge such as reference\nsegmentations, which allowed us to measure the impact that the continuous\nspeech separation system could have in realistic settings. The proposed model\noutperformed a previous approach to array-geometry-agnostic modeling for all of\nthe geometry configurations considered, achieving asclite-based\nspeaker-agnostic word error rates of 17.5% and 20.4% for the AMI development\nand evaluation sets, respectively, in the end-to-end setting using no\nground-truth segmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dongmei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_M/0/1/0/all/0/1\">Min Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zirun Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEPP: Similarity Estimation of Predicted Probabilities for Defending and Detecting Adversarial Text. (arXiv:2110.05748v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05748","description":"<p>There are two cases describing how a classifier processes input text, namely,\nmisclassification and correct classification. In terms of misclassified texts,\na classifier handles the texts with both incorrect predictions and adversarial\ntexts, which are generated to fool the classifier, which is called a victim.\nBoth types are misunderstood by the victim, but they can still be recognized by\nother classifiers. This induces large gaps in predicted probabilities between\nthe victim and the other classifiers. In contrast, text correctly classified by\nthe victim is often successfully predicted by the others and induces small\ngaps. In this paper, we propose an ensemble model based on similarity\nestimation of predicted probabilities (SEPP) to exploit the large gaps in the\nmisclassified predictions in contrast to small gaps in the correct\nclassification. SEPP then corrects the incorrect predictions of the\nmisclassified texts. We demonstrate the resilience of SEPP in defending and\ndetecting adversarial texts through different types of victim classifiers,\nclassification tasks, and adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Son_H/0/1/0/all/0/1\">Hoang-Quoc Nguyen-Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1\">Seira Hidano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukushima_K/0/1/0/all/0/1\">Kazuhide Fukushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyomoto_S/0/1/0/all/0/1\">Shinsaku Kiyomoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary. (arXiv:2110.05750v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05750","description":"<p>Sports game summarization aims to generate news articles from live text\ncommentaries. A recent state-of-the-art work, SportsSum, not only constructs a\nlarge benchmark dataset, but also proposes a two-step framework. Despite its\ngreat contributions, the work has three main drawbacks: 1) the noise existed in\nSportsSum dataset degrades the summarization performance; 2) the neglect of\nlexical overlap between news and commentaries results in low-quality\npseudo-labeling algorithm; 3) the usage of directly concatenating rewritten\nsentences to form news limits its practicability. In this paper, we publish a\nnew benchmark dataset SportsSum2.0, together with a modified summarization\nframework. In particular, to obtain a clean dataset, we employ crowd workers to\nmanually clean the original dataset. Moreover, the degree of lexical overlap is\nincorporated into the generation of pseudo labels. Further, we introduce a\nreranker-enhanced summarizer to take into account the fluency and\nexpressiveness of the summarized news. Extensive experiments show that our\nmodel outperforms the state-of-the-art baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianfeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingsheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoping Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware Pre-Training. (arXiv:2110.05752v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05752","description":"<p>Self-supervised learning (SSL) is a long-standing goal for speech processing,\nsince it utilizes large-scale unlabeled data and avoids extensive human\nlabeling. Recent years witness great successes in applying self-supervised\nlearning in speech recognition, while limited exploration was attempted in\napplying SSL for modeling speaker characteristics. In this paper, we aim to\nimprove the existing SSL framework for speaker representation learning. Two\nmethods are introduced for enhancing the unsupervised speaker information\nextraction. First, we apply the multi-task learning to the current SSL\nframework, where we integrate the utterance-wise contrastive loss with the SSL\nobjective function. Second, for better speaker discrimination, we propose an\nutterance mixing strategy for data augmentation, where additional overlapped\nutterances are created unsupervisely and incorporate during training. We\nintegrate the proposed methods into the HuBERT framework. Experiment results on\nSUPERB benchmark show that the proposed system achieves state-of-the-art\nperformance in universal representation learning, especially for speaker\nidentification oriented tasks. An ablation study is performed verifying the\nefficacy of each proposed method. Finally, we scale up training dataset to 94\nthousand hours public audio data and achieve further performance improvement in\nall SUPERB tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiangzhan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Cognitive Factors in Lexical Decline. (arXiv:2110.05775v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05775","description":"<p>We adopt an evolutionary view on language change in which cognitive factors\n(in addition to social ones) affect the fitness of words and their success in\nthe linguistic ecosystem. Specifically, we propose a variety of\npsycholinguistic factors -- semantic, distributional, and phonological -- that\nwe hypothesize are predictive of lexical decline, in which words greatly\ndecrease in frequency over time. Using historical data across three languages\n(English, French, and German), we find that most of our proposed factors show a\nsignificant difference in the expected direction between each curated set of\ndeclining words and their matched stable words. Moreover, logistic regression\nanalyses show that semantic and distributional factors are significant in\npredicting declining words. Further diachronic analysis reveals that declining\nwords tend to decrease in the diversity of their lexical contexts over time,\ngradually narrowing their 'ecological niches'.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_D/0/1/0/all/0/1\">David Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1\">Ella Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samir_F/0/1/0/all/0/1\">Farhan Samir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_S/0/1/0/all/0/1\">Suzanne Stevenson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"We've had this conversation before: A Novel Approach to Measuring Dialog Similarity. (arXiv:2110.05780v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05780","description":"<p>Dialog is a core building block of human natural language interactions. It\ncontains multi-party utterances used to convey information from one party to\nanother in a dynamic and evolving manner. The ability to compare dialogs is\nbeneficial in many real world use cases, such as conversation analytics for\ncontact center calls and virtual agent design.\n</p>\n<p>We propose a novel adaptation of the edit distance metric to the scenario of\ndialog similarity. Our approach takes into account various conversation aspects\nsuch as utterance semantics, conversation flow, and the participants. We\nevaluate this new approach and compare it to existing document similarity\nmeasures on two publicly available datasets. The results demonstrate that our\nmethod outperforms the other approaches in capturing dialog flow, and is better\naligned with the human perception of conversation similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavi_O/0/1/0/all/0/1\">Ofer Lavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_E/0/1/0/all/0/1\">Ella Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlomov_S/0/1/0/all/0/1\">Segev Shlomov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boaz_D/0/1/0/all/0/1\">David Boaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronen_I/0/1/0/all/0/1\">Inbal Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1\">Ateret Anaby-Tavor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTraffic: A Robust BERT-Based Approach for Speaker Change Detection and Role Identification of Air-Traffic Communications. (arXiv:2110.05781v1 [eess.AS])","link":"http://arxiv.org/abs/2110.05781","description":"<p>Automatic Speech Recognition (ASR) is gaining special interest in Air Traffic\nControl (ATC). ASR allows transcribing the communications between air traffic\ncontrollers (ATCOs) and pilots. These transcriptions are used to extract ATC\ncommand types and named entities such as aircraft callsigns. One common problem\nis when the Speech Activity Detection (SAD) or diarization system fails and\nthen two or more single speaker segments are in the same recording,\njeopardizing the overall system's performance. We developed a system that\ncombines the segmentation of a SAD module with a BERT-based model that performs\nSpeaker Change Detection (SCD) and Speaker Role Identification (SRI) based on\nASR transcripts (i.e., diarization + SRI). This research demonstrates on a\nreal-life ATC test set that performing diarization directly on textual data\nsurpass acoustic level diarization. The proposed model reaches up to\n~0.90/~0.95 F1-score on ATCO/pilot for SRI on several test sets. The text-based\ndiarization system brings a 27% relative improvement on Diarization Error Rate\n(DER) compared to standard acoustic-based diarization. These results were on\nASR transcripts of a challenging ATC test set with an estimated ~13% word error\nrate, validating the approach's robustness even on noisy ASR transcripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Seyyed Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting TTS models For New Speakers using Transfer Learning. (arXiv:2110.05798v1 [cs.SD])","link":"http://arxiv.org/abs/2110.05798","description":"<p>Training neural text-to-speech (TTS) models for a new speaker typically\nrequires several hours of high quality speech data. Prior works on voice\ncloning attempt to address this challenge by adapting pre-trained multi-speaker\nTTS models for a new voice, using a few minutes of speech data of the new\nspeaker. However, publicly available large multi-speaker datasets are often\nnoisy, thereby resulting in TTS models that are not suitable for use in\nproducts. We address this challenge by proposing transfer-learning guidelines\nfor adapting high quality single-speaker TTS models for a new speaker, using\nonly a few minutes of speech data. We conduct an extensive study using\ndifferent amounts of data for a new speaker and evaluate the synthesized speech\nin terms of naturalness and voice/style similarity to the target speaker. We\nfind that fine-tuning a single-speaker TTS model on just 30 minutes of data,\ncan yield comparable performance to a model trained from scratch on more than\n27 hours of data for both male and female target speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neekhara_P/0/1/0/all/0/1\">Paarth Neekhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jason Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Average and Worst-case Accuracy in Multitask Learning. (arXiv:2110.05838v1 [cs.LG])","link":"http://arxiv.org/abs/2110.05838","description":"<p>When training and evaluating machine learning models on a large number of\ntasks, it is important to not only look at average task accuracy -- which may\nbe biased by easy or redundant tasks -- but also worst-case accuracy (i.e. the\nperformance on the task with the lowest accuracy). In this work, we show how to\nuse techniques from the distributionally robust optimization (DRO) literature\nto improve worst-case performance in multitask learning. We highlight several\nfailure cases of DRO when applied off-the-shelf and present an improved method,\nLookahead-DRO (L-DRO), which mitigates these issues. The core idea of L-DRO is\nto anticipate the interaction between tasks during training in order to choose\na dynamic re-weighting of the various task losses, which will (i) lead to\nminimal worst-case loss and (ii) train on as many tasks as possible. After\ndemonstrating the efficacy of L-DRO on a small controlled synthetic setting, we\nevaluate it on two realistic benchmarks: a multitask version of the CIFAR-100\nimage classification dataset and a large-scale multilingual language modeling\nexperiment. Our empirical results show that L-DRO achieves a better trade-off\nbetween average and worst-case accuracy with little computational overhead\ncompared to several strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michel_P/0/1/0/all/0/1\">Paul Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Abstractive Summarisation Models with Machine Translation in Deliberative Processes. (arXiv:2110.05847v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05847","description":"<p>We present work on summarising deliberative processes for non-English\nlanguages. Unlike commonly studied datasets, such as news articles, this\ndeliberation dataset reflects difficulties of combining multiple narratives,\nmostly of poor grammatical quality, in a single text. We report an extensive\nevaluation of a wide range of abstractive summarisation models in combination\nwith an off-the-shelf machine translation model. Texts are translated into\nEnglish, summarised, and translated back to the original language. We obtain\npromising results regarding the fluency, consistency and relevance of the\nsummaries produced. Our approach is easy to implement for many languages for\nproduction purposes by simply changing the translation model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arana_Catania_M/0/1/0/all/0/1\">M. Arana-Catania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"text2sdg: An open-source solution to monitoring sustainable development goals from text. (arXiv:2110.05856v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05856","description":"<p>Monitoring progress on the United Nations Sustainable Development Goals\n(SDGs) is important for both academic and non-academic organizations. Existing\napproaches to monitoring SDGs have focused on specific data types, namely,\npublications listed in proprietary research databases. We present the text2sdg\nR package, a user-friendly, open-source package that detects SDGs in any kind\nof text data using several different query systems from any text source. The\ntext2sdg package thereby facilitates the monitoring of SDGs for a wide array of\ntext sources and provides a much-needed basis for validating and improving\nextant methods to detect SDGs from text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wulff_D/0/1/0/all/0/1\">Dirk U. Wulff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_R/0/1/0/all/0/1\">Rui Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_D/0/1/0/all/0/1\">Dominik S. Meier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetricGAN-U: Unsupervised speech enhancement/ dereverberation based only on noisy/ reverberated speech. (arXiv:2110.05866v1 [cs.SD])","link":"http://arxiv.org/abs/2110.05866","description":"<p>Most of the deep learning-based speech enhancement models are learned in a\nsupervised manner, which implies that pairs of noisy and clean speech are\nrequired during training. Consequently, several noisy speeches recorded in\ndaily life cannot be used to train the model. Although certain unsupervised\nlearning frameworks have also been proposed to solve the pair constraint, they\nstill require clean speech or noise for training. Therefore, in this paper, we\npropose MetricGAN-U, which stands for MetricGAN-unsupervised, to further\nrelease the constraint from conventional unsupervised learning. In MetricGAN-U,\nonly noisy speech is required to train the model by optimizing non-intrusive\nspeech quality metrics. The experimental results verified that MetricGAN-U\noutperforms baselines in both objective and subjective metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Szu-Wei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_K/0/1/0/all/0/1\">Kuo-Hsuan Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanelli_M/0/1/0/all/0/1\">Mirco Ravanelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages. (arXiv:2110.05877v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05877","description":"<p>AI technologies for Natural Languages have made tremendous progress recently.\nHowever, commensurate progress has not been made on Sign Languages, in\nparticular, in recognizing signs as individual words or as complete sentences.\nWe introduce OpenHands, a library where we take four key ideas from the NLP\ncommunity for low-resource languages and apply them to sign languages for\nword-level recognition. First, we propose using pose extracted through\npretrained models as the standard modality of data to reduce training time and\nenable efficient inference, and we release standardized pose datasets for 6\ndifferent sign languages - American, Argentinian, Chinese, Greek, Indian, and\nTurkish. Second, we train and release checkpoints of 4 pose-based isolated sign\nlanguage recognition models across all 6 languages, providing baselines and\nready checkpoints for deployment. Third, to address the lack of labelled data,\nwe propose self-supervised pretraining on unlabelled data. We curate and\nrelease the largest pose-based pretraining dataset on Indian Sign Language\n(Indian-SL). Fourth, we compare different pretraining strategies and for the\nfirst time establish that pretraining is effective for sign language\nrecognition by demonstrating (a) improved fine-tuning performance especially in\nlow-resource settings, and (b) high crosslingual transfer from Indian-SL to few\nother sign languages. We open-source all models and datasets in OpenHands with\na hope that it makes research in sign languages more accessible, available here\nat https://github.com/AI4Bharat/OpenHands .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selvaraj_P/0/1/0/all/0/1\">Prem Selvaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+NC_G/0/1/0/all/0/1\">Gokul NC</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigation on Data Adaptation Techniques for Neural Named Entity Recognition. (arXiv:2110.05892v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05892","description":"<p>Data processing is an important step in various natural language processing\ntasks. As the commonly used datasets in named entity recognition contain only a\nlimited number of samples, it is important to obtain additional labeled data in\nan efficient and reliable manner. A common practice is to utilize large\nmonolingual unlabeled corpora. Another popular technique is to create synthetic\ndata from the original labeled data (data augmentation). In this work, we\ninvestigate the impact of these two methods on the performance of three\ndifferent named entity recognition tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tokarchuk_E/0/1/0/all/0/1\">Evgeniia Tokarchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1\">David Thulke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugast_C/0/1/0/all/0/1\">Christian Dugast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05896","description":"<p>Trained on the large corpus, pre-trained language models (PLMs) can capture\ndifferent levels of concepts in context and hence generate universal language\nrepresentations. They can benefit multiple downstream natural language\nprocessing (NLP) tasks. Although PTMs have been widely used in most NLP\napplications, especially for high-resource languages such as English, it is\nunder-represented in Lao NLP research. Previous work on Lao has been hampered\nby the lack of annotated datasets and the sparsity of language resources. In\nthis work, we construct a text classification dataset to alleviate the\nresource-scare situation of the Lao language. We additionally present the first\ntransformer-based PTMs for Lao with four versions: BERT-small, BERT-base,\nELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:\npart-of-speech tagging and text classification. Experiments demonstrate the\neffectiveness of our Lao models. We will release our models and datasets to the\ncommunity, hoping to facilitate the future development of Lao NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Order Does Not Matter For Speech Recognition. (arXiv:2110.05994v1 [eess.AS])","link":"http://arxiv.org/abs/2110.05994","description":"<p>In this paper, we study training of automatic speech recognition system in a\nweakly supervised setting where the order of words in transcript labels of the\naudio training data is not known. We train a word-level acoustic model which\naggregates the distribution of all output frames using LogSumExp operation and\nuses a cross-entropy loss to match with the ground-truth words distribution.\nUsing the pseudo-labels generated from this model on the training set, we then\ntrain a letter-based acoustic model using Connectionist Temporal Classification\nloss. Our system achieves 2.4%/5.3% on test-clean/test-other subsets of\nLibriSpeech, which is competitive with the supervised baseline's performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer. (arXiv:2110.05999v1 [cs.CL])","link":"http://arxiv.org/abs/2110.05999","description":"<p>Despite the recent advances in applying pre-trained language models to\ngenerate high-quality texts, generating long passages that maintain long-range\ncoherence is yet challenging for these models. In this paper, we propose\nDiscoDVT, a discourse-aware discrete variational Transformer to tackle the\nincoherence issue. DiscoDVT learns a discrete variable sequence that summarizes\nthe global structure of the text and then applies it to guide the generation\nprocess at each decoding step. To further embed discourse-aware information\ninto the discrete latent representations, we introduce an auxiliary objective\nto model the discourse relations within the text. We conduct extensive\nexperiments on two open story generation datasets and demonstrate that the\nlatent codes learn meaningful correspondence to the discourse structures that\nguide the model to generate long texts with better long-range coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haozhe Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects. (arXiv:2110.06078v1 [q-bio.NC])","link":"http://arxiv.org/abs/2110.06078","description":"<p>A popular approach to decompose the neural bases of language consists in\ncorrelating, across individuals, the brain responses to different stimuli (e.g.\nregular speech versus scrambled words, sentences, or paragraphs). Although\nsuccessful, this `model-free' approach necessitates the acquisition of a large\nand costly set of neuroimaging data. Here, we show that a model-based approach\ncan reach equivalent results within subjects exposed to natural stimuli. We\ncapitalize on the recently-discovered similarities between deep language models\nand the human brain to compute the mapping between i) the brain responses to\nregular speech and ii) the activations of deep language models elicited by\nmodified stimuli (e.g. scrambled words, sentences, or paragraphs). Our\nmodel-based approach successfully replicates the seminal study of Lerner et al.\n(2011), which revealed the hierarchy of language areas by comparing the\nfunctional-magnetic resonance imaging (fMRI) of seven subjects listening to\n7min of both regular and scrambled narratives. We further extend and precise\nthese results to the brain signals of 305 individuals listening to 4.1 hours of\nnarrated stories. Overall, this study paves the way for efficient and flexible\nanalyses of the brain bases of language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Caucheteux_C/0/1/0/all/0/1\">Charlotte Caucheteux</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gramfort_A/0/1/0/all/0/1\">Alexandre Gramfort</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+King_J/0/1/0/all/0/1\">Jean-R&#xe9;mi King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A large scale lexical and semantic analysis of Spanish language variations in Twitter. (arXiv:2110.06128v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06128","description":"<p>Dialectometry is a discipline devoted to studying the variations of a\nlanguage around a geographical region. One of their goals is the creation of\nlinguistic atlases capturing the similarities and differences of the language\nunder study around the area in question. For instance, Spanish is one of the\nmost spoken languages across the world, but not necessarily Spanish is written\nand spoken in the same way in different countries. This manuscript presents a\nbroad analysis describing lexical and semantic relationships among 26\nSpanish-speaking countries around the globe. For this study, we analyze\nfour-year of the Twitter geotagged public stream to provide an extensive survey\nof the Spanish language vocabularies of different countries, its distributions,\nsemantic usage of terms, and emojis. We also offer open regional word-embedding\nresources for Spanish Twitter to help other researchers and practitioners take\nadvantage of regionalized models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tellez_E/0/1/0/all/0/1\">Eric S. Tellez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moctezuma_D/0/1/0/all/0/1\">Daniela Moctezuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_S/0/1/0/all/0/1\">Sabino Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_M/0/1/0/all/0/1\">Mario Graff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Feelings of People Regarding COVID-19 by Social Network Mining. (arXiv:2110.06151v1 [cs.SI])","link":"http://arxiv.org/abs/2110.06151","description":"<p>In 2020, COVID-19 became the chief concern of the world and is still\nreflected widely in all social networks. Each day, users post millions of\ntweets and comments on this subject, which contain significant implicit\ninformation about the public opinion. In this regard, a dataset of\nCOVID-related tweets in English language is collected, which consists of more\nthan two million tweets from March 23 to June 23 of 2020 to extract the\nfeelings of the people in various countries in the early stages of this\noutbreak. To this end, first, we use a lexicon-based approach in conjunction\nwith the GeoNames geographic database to label the tweets with their locations.\nNext, a method based on the recently introduced and widely cited RoBERTa model\nis proposed to analyze their sentimental content. After that, the trend graphs\nof the frequency of tweets as well as sentiments are produced for the world and\nthe nations that were more engaged with COVID-19. Graph analysis shows that the\nfrequency graphs of the tweets for the majority of nations are significantly\ncorrelated with the official statistics of the daily afflicted in them.\nMoreover, several implicit knowledge is extracted and discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_Nejad_H/0/1/0/all/0/1\">Hamed Vahdat-Nejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salmani_F/0/1/0/all/0/1\">Fatemeh Salmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiabadi_M/0/1/0/all/0/1\">Mahdi Hajiabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizi_F/0/1/0/all/0/1\">Faezeh Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1\">Sajedeh Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamalian_M/0/1/0/all/0/1\">Mohadese Jamalian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosafer_R/0/1/0/all/0/1\">Reyhane Mosafer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiabadi_H/0/1/0/all/0/1\">Hamideh Hajiabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mention Memory: incorporating textual knowledge into Transformers through entity mention attention. (arXiv:2110.06176v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06176","description":"<p>Natural language understanding tasks such as open-domain question answering\noften require retrieving and assimilating factual information from multiple\nsources. We propose to address this problem by integrating a semi-parametric\nrepresentation of a large text corpus into a Transformer model as a source of\nfactual knowledge. Specifically, our method represents knowledge with `mention\nmemory', a table of dense vector representations of every entity mention in a\ncorpus. The proposed model - TOME - is a Transformer that accesses the\ninformation through internal memory layers in which each entity mention in the\ninput passage attends to the mention memory. This approach enables synthesis of\nand reasoning over many disparate sources of information within a single\nTransformer model. In experiments using a memory of 150 million Wikipedia\nmentions, TOME achieves strong performance on several open-domain\nknowledge-intensive tasks, including the claim verification benchmarks HoVer\nand FEVER and several entity-based QA benchmarks. We also show that the model\nlearns to attend to informative mentions without any direct supervision.\nFinally we demonstrate that the model can generalize to new unseen entities by\nupdating the memory without retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1\">Michiel de Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemlyanskiy_Y/0/1/0/all/0/1\">Yury Zemlyanskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_N/0/1/0/all/0/1\">Nicholas FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Objectives of Extractive Question Answering. (arXiv:2008.12804v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.12804","description":"<p>This work demonstrates that using the objective with independence assumption\nfor modelling the span probability $P(a_s,a_e) = P(a_s)P(a_e)$ of span starting\nat position $a_s$ and ending at position $a_e$ has adverse effects. Therefore\nwe propose multiple approaches to modelling joint probability $P(a_s,a_e)$\ndirectly. Among those, we propose a compound objective, composed from the joint\nprobability while still keeping the objective with independence assumption as\nan auxiliary objective. We find that the compound objective is consistently\nsuperior or equal to other assumptions in exact match. Additionally, we\nidentified common errors caused by the assumption of independence and manually\nchecked the counterpart predictions, demonstrating the impact of the compound\nobjective on the real examples. Our findings are supported via experiments with\nthree extractive QA models (BIDAF, BERT, ALBERT) over six datasets and our\ncode, individual results and manual analysis are available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jon_J/0/1/0/all/0/1\">Josef Jon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimal Supervision for Morphological Inflection. (arXiv:2104.08512v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08512","description":"<p>Neural models for the various flavours of morphological inflection tasks have\nproven to be extremely accurate given ample labeled data -- data that may be\nslow and costly to obtain. In this work we aim to overcome this annotation\nbottleneck by bootstrapping labeled data from a seed as little as {\\em five}\nlabeled paradigms, accompanied by a large bulk of unlabeled text. Our approach\nexploits different kinds of regularities in morphological systems in a\ntwo-phased setup, where word tagging based on {\\em analogies} is followed by\nword pairing based on {\\em distances}. We experiment with the Paradigm Cell\nFilling Problem over eight typologically different languages, and find that, in\nlanguages with relatively simple morphology, orthographic regularities on their\nown allow inflection models to achieve respectable accuracy. Combined\northographic and semantic regularities alleviate difficulties with particularly\ncomplex morpho-phonological systems. Our results suggest that hand-crafting\nmany tagged examples might be an unnecessary effort. However, more work is\nneeded in order to address rarely used forms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goldman_O/0/1/0/all/0/1\">Omer Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding. (arXiv:2104.12763v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12763","description":"<p>Multi-modal reasoning systems rely on a pre-trained object detector to\nextract regions of interest from the image. However, this crucial module is\ntypically used as a black box, trained independently of the downstream task and\non a fixed vocabulary of objects and attributes. This makes it challenging for\nsuch systems to capture the long tail of visual concepts expressed in free form\ntext. In this paper we propose MDETR, an end-to-end modulated detector that\ndetects objects in an image conditioned on a raw text query, like a caption or\na question. We use a transformer-based architecture to reason jointly over text\nand image by fusing the two modalities at an early stage of the model. We\npre-train the network on 1.3M text-image pairs, mined from pre-existing\nmulti-modal datasets having explicit alignment between phrases in text and\nobjects in the image. We then fine-tune on several downstream tasks such as\nphrase grounding, referring expression comprehension and segmentation,\nachieving state-of-the-art results on popular benchmarks. We also investigate\nthe utility of our model as an object detector on a given label set when\nfine-tuned in a few-shot setting. We show that our pre-training approach\nprovides a way to handle the long tail of object categories which have very few\nlabelled instances. Our approach can be easily extended for visual question\nanswering, achieving competitive performance on GQA and CLEVR. The code and\nmodels are available at https://github.com/ashkamath/mdetr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamath_A/0/1/0/all/0/1\">Aishwarya Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carion_N/0/1/0/all/0/1\">Nicolas Carion</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prosodic segmentation for parsing spoken dialogue. (arXiv:2105.12667v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.12667","description":"<p>Parsing spoken dialogue poses unique difficulties, including disfluencies and\nunmarked boundaries between sentence-like units. Previous work has shown that\nprosody can help with parsing disfluent speech (Tran et al. 2018), but has\nassumed that the input to the parser is already segmented into sentence-like\nunits (SUs), which isn't true in existing speech applications. We investigate\nhow prosody affects a parser that receives an entire dialogue turn as input (a\nturn-based model), instead of gold standard pre-segmented SUs (an SU-based\nmodel). In experiments on the English Switchboard corpus, we find that when\nusing transcripts alone, the turn-based model has trouble segmenting SUs,\nleading to worse parse performance than the SU-based model. However, prosody\ncan effectively replace gold standard SU boundaries: with prosody, the\nturn-based model performs as well as the SU-based model (90.79 vs. 90.65 F1\nscore, respectively), despite performing two tasks (SU segmentation and\nparsing) rather than one (parsing alone). Analysis shows that pitch and\nintensity features are the most important for this corpus, since they allow the\nmodel to correctly distinguish an SU boundary from a speech disfluency -- a\ndistinction that the model otherwise struggles to make.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_E/0/1/0/all/0/1\">Elizabeth Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. (arXiv:2106.05707v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.05707","description":"<p>Fact verification has attracted a lot of attention in the machine learning\nand natural language processing communities, as it is one of the key methods\nfor detecting misinformation. Existing large-scale benchmarks for this task\nhave focused mostly on textual sources, i.e. unstructured information, and thus\nignored the wealth of information available in structured formats, such as\ntables. In this paper we introduce a novel dataset and benchmark, Fact\nExtraction and VERification Over Unstructured and Structured information\n(FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated\nwith evidence in the form of sentences and/or cells from tables in Wikipedia,\nas well as a label indicating whether this evidence supports, refutes, or does\nnot provide enough information to reach a verdict. Furthermore, we detail our\nefforts to track and minimize the biases present in the dataset and could be\nexploited by models, e.g. being able to predict the label without using\nevidence. Finally, we develop a baseline for verifying claims against text and\ntables which predicts both the correct evidence and verdict for 18% of the\nclaims.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aly_R/0/1/0/all/0/1\">Rami Aly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1\">James Thorne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1\">Christos Christodoulopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cocarascu_O/0/1/0/all/0/1\">Oana Cocarascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Arpit Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding. (arXiv:2106.07250v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07250","description":"<p>In knowledge graph embedding, the theoretical relationship between the\nsoftmax cross-entropy and negative sampling loss functions has not been\ninvestigated. This makes it difficult to fairly compare the results of the two\ndifferent loss functions. We attempted to solve this problem by using the\nBregman divergence to provide a unified interpretation of the softmax\ncross-entropy and negative sampling loss functions. Under this interpretation,\nwe can derive theoretical findings for fair comparison. Experimental results on\nthe FB15k-237 and WN18RR datasets show that the theoretical findings are valid\nin practical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1\">Hidetaka Kamigaito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1\">Katsuhiko Hayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07306","description":"<p>A major challenge in structured prediction is to represent the\ninterdependencies within output structures. When outputs are structured as\nsequences, linear-chain conditional random fields (CRFs) are a widely used\nmodel class which can learn \\textit{local} dependencies in the output. However,\nthe CRF's Markov assumption makes it impossible for CRFs to represent\ndistributions with \\textit{nonlocal} dependencies, and standard CRFs are unable\nto respect nonlocal constraints of the data (such as global arity constraints\non output labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show\nempirically that it can be substantially better in practice. Additionally, we\ndemonstrate a practical benefit on downstream tasks by incorporating a RegCCRF\ninto a deep neural model for semantic role labeling, exceeding state-of-the-art\nresults on a standard dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1\">Sean Papay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01951","description":"<p>The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (&lt; 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Voice Activated Framework using Self-supervised Learning. (arXiv:2110.01077v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.01077","description":"<p>Self-supervised learning methods such as wav2vec 2.0 have shown promising\nresults in learning speech representations from unlabelled and untranscribed\nspeech data that are useful for speech recognition. Since these representations\nare learned without any task-specific supervision, they can also be useful for\nother voice-activated tasks like speaker verification, keyword spotting,\nemotion classification etc. In our work, we propose a general purpose framework\nfor adapting a pre-trained wav2vec 2.0 model for different voice-activated\ntasks. We develop downstream network architectures that operate on the\ncontextualized speech representations of wav2vec 2.0 to adapt the\nrepresentations for solving a given task. Finally, we extend our framework to\nperform multi-task learning by jointly optimizing the network parameters on\nmultiple voice activated tasks using a shared transformer backbone. Both of our\nsingle and multi-task frameworks achieve state-of-the-art results in speaker\nverification and keyword spotting benchmarks. Our best performing models\nachieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and\nVoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0\nkeyword spotting dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Shehzeen Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Lexical Normalization with Multilingual Transformers. (arXiv:2110.02869v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02869","description":"<p>Current benchmark tasks for natural language processing contain text that is\nqualitatively different from the text used in informal day to day digital\ncommunication. This discrepancy has led to severe performance degradation of\nstate-of-the-art NLP models when fine-tuned on real-world data. One way to\nresolve this issue is through lexical normalization, which is the process of\ntransforming non-standard text, usually from social media, into a more\nstandardized form. In this work, we propose a sentence-level\nsequence-to-sequence model based on mBART, which frames the problem as a\nmachine translation problem. As the noisy text is a pervasive problem across\nlanguages, not just English, we leverage the multi-lingual pre-training of\nmBART to fine-tune it to our data. While current approaches mainly operate at\nthe word or subword level, we argue that this approach is straightforward from\na technical standpoint and builds upon existing pre-trained transformer\nnetworks. Our results show that while word-level, intrinsic, performance\nevaluation is behind other methods, our model improves performance on\nextrinsic, downstream tasks through normalization compared to models operating\non raw, unprocessed, social media text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.03370","description":"<p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1\">Qijie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1\">Hui Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chenchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taming Sparsely Activated Transformer with Stochastic Experts. (arXiv:2110.04260v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04260","description":"<p>Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can\neasily scale to have outrageously large amounts of parameters without\nsignificant increase in computational cost. However, SAMs are reported to be\nparameter inefficient such that larger models do not always lead to better\nperformance. While most on-going research focuses on improving SAMs models by\nexploring methods of routing inputs to experts, our analysis reveals that such\nresearch might not lead to the solution we expect, i.e., the commonly-used\nrouting methods based on gating mechanisms do not work better than randomly\nrouting inputs to experts. In this paper, we propose a new expert-based model,\nTHOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models,\nsuch as the Switch Transformer, experts in THOR are randomly activated for each\ninput during training and inference. THOR models are trained using a\nconsistency regularized loss, where experts learn not only from training data\nbut also from other experts as teachers, such that all the experts make\nconsistent predictions. We validate the effectiveness of THOR on machine\ntranslation tasks. Results show that THOR models are more parameter efficient\nin that they significantly outperform the Transformer and MoE models across\nvarious settings. For example, in multilingual translation, THOR outperforms\nthe Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as\nthat of a state-of-the-art MoE model that is 18 times larger. Our code is\npublicly available at:\nhttps://github.com/microsoft/Stochastic-Mixture-of-Experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1\">Simiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning. (arXiv:2110.04725v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04725","description":"<p>Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot\nand Few-Shot learning on many natural language processing (NLP) tasks by\nscaling up model size, dataset size and the amount of computation. However,\ntraining a model like GPT-3 requires huge amount of computational resources\nwhich makes it challengeable to researchers. In this work, we propose a method\nthat incorporates large-scale distributed training performance into model\narchitecture design. With this method, Yuan 1.0, the current largest singleton\nlanguage model with 245B parameters, achieves excellent performance on\nthousands GPUs during training, and the state-of-the-art results on NLP tasks.\nA data processing method is designed to efficiently filter massive amount of\nraw data. The current largest high-quality Chinese corpus with 5TB high quality\ntexts is built based on this method. In addition, a calibration and label\nexpansion method is proposed to improve the Zero-Shot and Few-Shot performance,\nand steady improvement is observed on the accuracy of various tasks. Yuan 1.0\npresents strong capacity of natural language generation, and the generated\narticles are difficult to distinguish from the human-written ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaohua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xudong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiangang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances in Multi-turn Dialogue Comprehension: A Survey. (arXiv:2110.04984v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04984","description":"<p>Training machines to understand natural language and interact with humans is\nan elusive and essential task of artificial intelligence. A diversity of\ndialogue systems has been designed with the rapid development of deep learning\ntechniques, especially the recent pre-trained language models (PrLMs). Among\nthese studies, the fundamental yet challenging type of task is dialogue\ncomprehension whose role is to teach the machines to read and comprehend the\ndialogue context before responding. In this paper, we review the previous\nmethods from the technical perspective of dialogue modeling for the dialogue\ncomprehension task. We summarize the characteristics and challenges of dialogue\ncomprehension in contrast to plain-text reading comprehension. Then, we discuss\nthree typical patterns of dialogue modeling. In addition, we categorize\ndialogue-related pre-training techniques which are employed to enhance PrLMs in\ndialogue scenarios. Finally, we highlight the technical advances in recent\nyears and point out the lessons from the empirical analysis and the prospects\ntowards a new frontier of researches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05006","description":"<p>Pre-trained language models (PLMs) have been the de facto paradigm for most\nnatural language processing (NLP) tasks. This also benefits biomedical domain:\nresearchers from informatics, medicine, and computer science (CS) communities\npropose various PLMs trained on biomedical datasets, e.g., biomedical text,\nelectronic health records, protein, and DNA sequences for various biomedical\ntasks. However, the cross-discipline characteristics of biomedical PLMs hinder\ntheir spreading among communities; some existing works are isolated from each\nother without comprehensive comparison and discussions. It expects a survey\nthat not only systematically reviews recent advances of biomedical PLMs and\ntheir applications but also standardizes terminology and benchmarks. In this\npaper, we summarize the recent progress of pre-trained language models in the\nbiomedical domain and their applications in biomedical downstream tasks.\nParticularly, we discuss the motivations and propose a taxonomy of existing\nbiomedical PLMs. Their applications in biomedical downstream tasks are\nexhaustively discussed. At last, we illustrate various limitations and future\ntrends, which we hope can provide inspiration for the future research of the\nresearch community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Benyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qianqian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1\">Prayag Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+fu_J/0/1/0/all/0/1\">Jie fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViSeRet: A simple yet effective approach to moment retrieval via fine-grained video segmentation. (arXiv:2110.05146v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05146","description":"<p>Video-text retrieval has many real-world applications such as media\nanalytics, surveillance, and robotics. This paper presents the 1st place\nsolution to the video retrieval track of the ICCV VALUE Challenge 2021. We\npresent a simple yet effective approach to jointly tackle two video-text\nretrieval tasks (video retrieval and video corpus moment retrieval) by\nleveraging the model trained only on the video retrieval task. In addition, we\ncreate an ensemble model that achieves the new state-of-the-art performance on\nall four datasets (TVr, How2r, YouCook2r, and VATEXr) presented in the VALUE\nChallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Aiden Seungjoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_H/0/1/0/all/0/1\">Hanseok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}