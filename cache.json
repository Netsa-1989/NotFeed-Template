{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Language Modelling via Learning to Rank. (arXiv:2110.06961v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06961","description":"<p>We consider language modelling (LM) as a multi-label structured prediction\ntask by re-framing training from solely predicting a single ground-truth word\nto ranking a set of words which could continue a given context. To avoid\nannotating top-$k$ ranks, we generate them using pre-trained LMs: GPT-2, BERT,\nand Born-Again models. This leads to a rank-based form of knowledge\ndistillation (KD). We also develop a method using $N$-grams to create a\nnon-probabilistic teacher which generates the ranks without the need of a\npre-trained LM.\n</p>\n<p>We confirm the hypotheses that we can treat LMing as a ranking task and that\nwe can do so without the use of a pre-trained LM. We show that rank-based KD\ngenerally improves perplexity (PPL), often with statistical significance, when\ncompared to Kullback-Leibler-based KD. Surprisingly, given the simplicity of\nthe method, $N$-grams act as competitive teachers and achieve similar\nperformance as using either BERT or a Born-Again model teachers. GPT-2 always\nacts as the best teacher, though, and using it and a Transformer-XL student on\nWiki-02, rank-based KD reduces a cross-entropy baseline from 65.27 to 55.94 and\nagainst a KL-based KD of 56.70.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frydenlund_A/0/1/0/all/0/1\">Arvid Frydenlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Domain Question-Answering for COVID-19 and Other Emergent Domains. (arXiv:2110.06962v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06962","description":"<p>Since late 2019, COVID-19 has quickly emerged as the newest biomedical\ndomain, resulting in a surge of new information. As with other emergent\ndomains, the discussion surrounding the topic has been rapidly changing,\nleading to the spread of misinformation. This has created the need for a public\nspace for users to ask questions and receive credible, scientific answers. To\nfulfill this need, we turn to the task of open-domain question-answering, which\nwe can use to efficiently find answers to free-text questions from a large set\nof documents. In this work, we present such a system for the emergent domain of\nCOVID-19. Despite the small data size available, we are able to successfully\ntrain the system to retrieve answers from a large-scale corpus of published\nCOVID-19 scientific papers. Furthermore, we incorporate effective re-ranking\nand question-answering techniques, such as document diversity and multiple\nanswer spans. Our open-domain question-answering system can further act as a\nmodel for the quick development of similar systems that can be adapted and\nmodified for other developing emergent domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kevin Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1\">Wenhan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexiTerm: A more efficient implementation of flexible multi-word term recognition. (arXiv:2110.06981v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06981","description":"<p>Terms are linguistic signifiers of domain-specific concepts. Automated\nrecognition of multi-word terms (MWT) in free text is a sequence labelling\nproblem, which is commonly addressed using supervised machine learning methods.\nTheir need for manual annotation of training data makes it difficult to port\nsuch methods across domains. FlexiTerm, on the other hand, is a fully\nunsupervised method for MWT recognition from domain-specific corpora.\nOriginally implemented in Java as a proof of concept, it did not scale well,\nthus offering little practical value in the context of big data. In this paper,\nwe describe its re-implementation in Python and compare the performance of\nthese two implementations. The results demonstrated major improvements in terms\nof efficiency, which allow FlexiTerm to transition from the proof of concept to\nthe production-grade application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spasic_I/0/1/0/all/0/1\">Irena Spasic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bandits Don't Follow Rules: Balancing Multi-Facet Machine Translation with Multi-Armed Bandits. (arXiv:2110.06997v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06997","description":"<p>Training data for machine translation (MT) is often sourced from a multitude\nof large corpora that are multi-faceted in nature, e.g. containing contents\nfrom multiple domains or different levels of quality or complexity. Naturally,\nthese facets do not occur with equal frequency, nor are they equally important\nfor the test scenario at hand. In this work, we propose to optimize this\nbalance jointly with MT model parameters to relieve system developers from\nmanual schedule design. A multi-armed bandit is trained to dynamically choose\nbetween facets in a way that is most beneficial for the MT system. We evaluate\nit on three different multi-facet applications: balancing translationese and\nnatural training data, or data from multiple domains or multiple language\npairs. We find that bandit learning leads to competitive MT systems across\ntasks, and our analysis provides insights into its learned strategies and the\nunderlying data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilar_D/0/1/0/all/0/1\">David Vilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation. (arXiv:2110.07002v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07002","description":"<p>Text autoencoders are often used for unsupervised conditional text generation\nby applying mappings in the latent space to change attributes to the desired\nvalues. Recently, Mai et al. (2020) proposed Emb2Emb, a method to learn these\nmappings in the embedding space of an autoencoder. However, their method is\nrestricted to autoencoders with a single-vector embedding, which limits how\nmuch information can be retained. We address this issue by extending their\nmethod to Bag-of-Vectors Autoencoders (BoV-AEs), which encode the text into a\nvariable-size bag of vectors that grows with the size of the text, as in\nattention-based models. This allows to encode and reconstruct much longer texts\nthan standard autoencoders. Analogous to conventional autoencoders, we propose\nregularization techniques that facilitate learning meaningful operations in the\nlatent space. Finally, we adapt for a training scheme that learns to map an\ninput bag to an output bag, including a novel loss function and neural\narchitecture. Our experimental evaluations on unsupervised sentiment transfer\nand sentence summarization show that our method performs substantially better\nthan a standard autoencoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mai_F/0/1/0/all/0/1\">Florian Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of SVD and factorized TDNN approaches for speech to text. (arXiv:2110.07027v1 [cs.SD])","link":"http://arxiv.org/abs/2110.07027","description":"<p>This work concentrates on reducing the RTF and word error rate of a hybrid\nHMM-DNN. Our baseline system uses an architecture with TDNN and LSTM layers. We\nfind this architecture particularly useful for lightly reverberated\nenvironments. However, these models tend to demand more computation than is\ndesirable. In this work, we explore alternate architectures employing singular\nvalue decomposition (SVD) is applied to the TDNN layers to reduce the RTF, as\nwell as to the affine transforms of every LSTM cell. We compare this approach\nwith specifying bottleneck layers similar to those introduced by SVD before\ntraining. Additionally, we reduced the search space of the decoding graph to\nmake it a better fit to operate in real-time applications. We report -61.57%\nrelative reduction in RTF and almost 1% relative decrease in WER for our\narchitecture trained on Fisher data along with reverberated versions of this\ndataset in order to match one of our target test distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Jeffrey Josanne Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1\">Nagendra Kumar Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_N/0/1/0/all/0/1\">Navneeth K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_J/0/1/0/all/0/1\">Jonas Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shravan Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness to Variations of Objects and Instructions with a Neuro-Symbolic Approach for Interactive Instruction Following. (arXiv:2110.07031v1 [cs.AI])","link":"http://arxiv.org/abs/2110.07031","description":"<p>An interactive instruction following task has been proposed as a benchmark\nfor learning to map natural language instructions and first-person vision into\nsequences of actions to interact with objects in a 3D simulated environment. We\nfind that an existing end-to-end neural model for this task is not robust to\nvariations of objects and language instructions. We assume that this problem is\ndue to the high sensitiveness of neural feature extraction to small changes in\nvision and language inputs. To mitigate this problem, we propose a\nneuro-symbolic approach that performs reasoning over high-level symbolic\nrepresentations that are robust to small changes in raw inputs. Our experiments\non the ALFRED dataset show that our approach significantly outperforms the\nexisting model by 18, 52, and 73 points in the success rate on the\nToggleObject, PickupObject, and SliceObject subtasks in unseen environments\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazutoshi Shinoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takezawa_Y/0/1/0/all/0/1\">Yuki Takezawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masahiro Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient NLP: A Standard Evaluation and A Strong Baseline. (arXiv:2110.07038v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07038","description":"<p>Supersized pre-trained language models have pushed the accuracy of various\nNLP tasks to a new state-of-the-art (SOTA). Rather than pursuing the reachless\nSOTA accuracy, most works are pursuing improvement on other dimensions such as\nefficiency, leading to \"Pareto SOTA\". Different from accuracy, the metric for\nefficiency varies across different studies, making them hard to be fairly\ncompared. To that end, this work presents ELUE (Efficient Language\nUnderstanding Evaluation), a standard evaluation, and a public leaderboard for\nefficient NLP models. ELUE is dedicated to depicting the Pareto Front for\nvarious language understanding tasks, such that it can tell whether and how\nmuch a method achieves Pareto improvement. Along with the benchmark, we also\npre-train and release a strong baseline, ElasticBERT, whose elasticity is both\nstatic and dynamic. ElasticBERT is static in that it allows reducing model\nlayers on demand. ElasticBERT is dynamic in that it selectively executes parts\nof model layers conditioned on the input. We demonstrate the ElasticBERT,\ndespite its simplicity, outperforms or performs on par with SOTA compressed and\nearly exiting models. The ELUE benchmark is publicly available at\n<a href=\"http://eluebenchmark.fastnlp.top/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingling Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual learning using lattice-free MMI for speech recognition. (arXiv:2110.07055v1 [eess.AS])","link":"http://arxiv.org/abs/2110.07055","description":"<p>Continual learning (CL), or domain expansion, recently became a popular topic\nfor automatic speech recognition (ASR) acoustic modeling because practical\nsystems have to be updated frequently in order to work robustly on types of\nspeech not observed during initial training. While sequential adaptation allows\ntuning a system to a new domain, it may result in performance degradation on\nthe old domains due to catastrophic forgetting. In this work we explore\nregularization-based CL for neural network acoustic models trained with the\nlattice-free maximum mutual information (LF-MMI) criterion. We simulate domain\nexpansion by incrementally adapting the acoustic model on different public\ndatasets that include several accents and speaking styles. We investigate two\nwell-known CL techniques, elastic weight consolidation (EWC) and learning\nwithout forgetting (LWF), which aim to reduce forgetting by preserving model\nweights or network outputs. We additionally introduce a sequence-level LWF\nregularization, which exploits posteriors from the denominator graph of LF-MMI\nto further reduce forgetting. Empirical results show that the proposed\nsequence-level LWF can improve the best average word error rate across all\ndomains by up to 9.4% relative compared with using regular LWF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hadian_H/0/1/0/all/0/1\">Hossein Hadian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorin_A/0/1/0/all/0/1\">Arseniy Gorin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIMICause : Defining, identifying and predicting types of causal relationships between biomedical concepts from clinical notes. (arXiv:2110.07090v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07090","description":"<p>Understanding of causal narratives communicated in clinical notes can help\nmake strides towards personalized healthcare. In this work, MIMICause, we\npropose annotation guidelines, develop an annotated corpus and provide baseline\nscores to identify types and direction of causal relations between a pair of\nbiomedical concepts in clinical notes; communicated implicitly or explicitly,\nidentified either in a single sentence or across multiple sentences.\n</p>\n<p>We annotate a total of 2714 de-identified examples sampled from the 2018 n2c2\nshared task dataset and train four different language model based\narchitectures. Annotation based on our guidelines achieved a high\ninter-annotator agreement i.e. Fleiss' kappa score of 0.72 and our model for\nidentification of causal relation achieved a macro F1 score of 0.56 on test\ndata. The high inter-annotator agreement for clinical text shows the quality of\nour annotation guidelines while the provided baseline F1 score sets the\ndirection for future research towards understanding narratives in clinical\ntexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khetan_V/0/1/0/all/0/1\">Vivek Khetan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizvi_M/0/1/0/all/0/1\">Md Imbesat Hassan Rizvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_J/0/1/0/all/0/1\">Jessica Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_P/0/1/0/all/0/1\">Paige Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sacaleanu_B/0/1/0/all/0/1\">Bogdan Sacaleanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fano_A/0/1/0/all/0/1\">Andrew Fano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Introductions in Podcast Episodes from Automatically Generated Transcripts. (arXiv:2110.07096v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07096","description":"<p>As the volume of long-form spoken-word content such as podcasts explodes,\nmany platforms desire to present short, meaningful, and logically coherent\nsegments extracted from the full content. Such segments can be consumed by\nusers to sample content before diving in, as well as used by the platform to\npromote and recommend content. However, little published work is focused on the\nsegmentation of spoken-word content, where the errors (noise) in transcripts\ngenerated by automatic speech recognition (ASR) services poses many challenges.\nHere we build a novel dataset of complete transcriptions of over 400 podcast\nepisodes, in which we label the position of introductions in each episode.\nThese introductions contain information about the episodes' topics, hosts, and\nguests, providing a valuable summary of the episode content, as it is created\nby the authors. We further augment our dataset with word substitutions to\nincrease the amount of available training data. We train three Transformer\nmodels based on the pre-trained BERT and different augmentation strategies,\nwhich achieve significantly better performance compared with a static embedding\nmodel, showing that it is possible to capture generalized, larger-scale\nstructural information from noisy, loosely-organized speech data. This is\nfurther demonstrated through an analysis of the models' inner architecture. Our\nmethods and dataset can be used to facilitate future work on the\nstructure-based segmentation of spoken-word content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_E/0/1/0/all/0/1\">Elise Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneck_K/0/1/0/all/0/1\">Kristiana Schneck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egan_D/0/1/0/all/0/1\">Dennis Egan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waterman_S/0/1/0/all/0/1\">Scott A. Waterman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A CLIP-Enhanced Method for Video-Language Understanding. (arXiv:2110.07137v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07137","description":"<p>This technical report summarizes our method for the Video-And-Language\nUnderstanding Evaluation (VALUE) challenge\n(https://value-benchmark.github.io/challenge\\_2021.html). We propose a\nCLIP-Enhanced method to incorporate the image-text pretrained knowledge into\ndownstream video-text tasks. Combined with several other improved designs, our\nmethod outperforms the state-of-the-art by $2.4\\%$ ($57.58$ to $60.00$)\nMeta-Ave score on VALUE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Feng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhifan Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer. (arXiv:2110.07139v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07139","description":"<p>Adversarial attacks and backdoor attacks are two common security threats that\nhang over deep learning. Both of them harness task-irrelevant features of data\nin their implementation. Text style is a feature that is naturally irrelevant\nto most NLP tasks, and thus suitable for adversarial and backdoor attacks. In\nthis paper, we make the first attempt to conduct adversarial and backdoor\nattacks based on text style transfer, which is aimed at altering the style of a\nsentence while preserving its meaning. We design an adversarial attack method\nand a backdoor attack method, and conduct extensive experiments to evaluate\nthem. Experimental results show that popular NLP models are vulnerable to both\nadversarial and backdoor attacks based on text style transfer -- the attack\nsuccess rates can exceed 90% without much effort. It reflects the limited\nability of NLP models to handle the feature of text style that has not been\nwidely realized. In addition, the style transfer-based adversarial and backdoor\nattack methods show superiority to baselines in many aspects. All the code and\ndata of this paper can be obtained at https://github.com/thunlp/StyleAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xurui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"bert2BERT: Towards Reusable Pretrained Language Models. (arXiv:2110.07143v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07143","description":"<p>In recent years, researchers tend to pre-train ever-larger language models to\nexplore the upper limit of deep models. However, large language model\npre-training costs intensive computational resources and most of the models are\ntrained from scratch without reusing the existing pre-trained models, which is\nwasteful. In this paper, we propose bert2BERT, which can effectively transfer\nthe knowledge of an existing smaller pre-trained model (e.g., BERT_BASE) to a\nlarge model (e.g., BERT_LARGE) through parameter initialization and\nsignificantly improve the pre-training efficiency of the large model.\nSpecifically, we extend the previous function-preserving on Transformer-based\nlanguage model, and further improve it by proposing advanced knowledge for\nlarge model's initialization. In addition, a two-stage pre-training method is\nproposed to further accelerate the training process. We did extensive\nexperiments on representative PLMs (e.g., BERT and GPT) and demonstrate that\n(1) our method can save a significant amount of training cost compared with\nbaselines including learning from scratch, StackBERT and MSLT; (2) our method\nis generic and applicable to different types of pre-trained models. In\nparticular, bert2BERT saves about 45% and 47% computational cost of\npre-training BERT_BASE and GPT_BASE by reusing the models of almost their half\nsizes. The source code will be publicly available upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual GenQA: A Language-Agnostic Generative Question Answering Approach for Open-Domain Question Answering. (arXiv:2110.07150v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07150","description":"<p>Open-Retrieval Generative Question Answering (GenQA) is proven to deliver\nhigh-quality, natural-sounding answers in English. In this paper, we present\nthe first generalization of the GenQA approach for the multilingual\nenvironment. To this end, we present the GenTyDiQA dataset, which extends the\nTyDiQA evaluation data (Clark et al., 2020) with natural-sounding, well-formed\nanswers in Arabic, Bengali, English, Japanese, and Russian. For all these\nlanguages, we show that a GenQA sequence-to-sequence-based model outperforms a\nstate-of-the-art Answer Sentence Selection model. We also show that a\nmultilingually-trained model competes with, and in some cases outperforms, its\nmonolingual counterparts. Finally, we show that our system can even compete\nwith strong baselines, even when fed with information from a variety of\nlanguages. Essentially, our system is able to answer a question in any language\nof our language set using information from many languages, making it the first\nLanguage-Agnostic GenQA system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1\">Benjamin Muller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1\">Luca Soldaini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lind_E/0/1/0/all/0/1\">Eric Lind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causally Estimating the Sensitivity of Neural NLP Models to Spurious Features. (arXiv:2110.07159v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07159","description":"<p>Recent work finds modern natural language processing (NLP) models relying on\nspurious features for prediction. Mitigating such effects is thus important.\nDespite this need, there is no quantitative measure to evaluate or compare the\neffects of different forms of spurious features in NLP. We address this gap in\nthe literature by quantifying model sensitivity to spurious features with a\ncausal estimand, dubbed CENT, which draws on the concept of average treatment\neffect from the causality literature. By conducting simulations with four\nprominent NLP models -- TextRNN, BERT, RoBERTa and XLNet -- we rank the models\nagainst their sensitivity to artificial injections of eight spurious features.\nWe further hypothesize and validate that models that are more sensitive to a\nspurious feature will be less robust against perturbations with this feature\nduring inference. Conversely, data augmentation with this feature improves\nrobustness to similar perturbations. We find statistically significant inverse\ncorrelations between sensitivity and robustness, providing empirical support\nfor our hypothesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence. (arXiv:2110.07160v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07160","description":"<p>This paper proposes a transformer over transformer framework, called\nTransformer$^2$, to perform neural text segmentation. It consists of two\ncomponents: bottom-level sentence encoders using pre-trained transformers, and\nan upper-level transformer-based segmentation model based on the sentence\nembeddings. The bottom-level component transfers the pre-trained knowledge\nlearned from large external corpora under both single and pair-wise supervised\nNLP tasks to model the sentence embeddings for the documents. Given the\nsentence embeddings, the upper-level transformer is trained to recover the\nsegmentation boundaries as well as the topic labels of each sentence. Equipped\nwith a multi-task loss and the pre-trained knowledge, Transformer$^2$ can\nbetter capture the semantic coherence within the same segments. Our experiments\nshow that (1) Transformer$^2$ manages to surpass state-of-the-art text\nsegmentation models in terms of a commonly-used semantic coherence measure; (2)\nin most cases, both single and pair-wise pre-trained knowledge contribute to\nthe model performance; (3) bottom-level sentence encoders pre-trained on\nspecific languages yield better performance than those pre-trained on specific\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kelvin Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Weicong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Attention-Aware Hierarchical Topic Model. (arXiv:2110.07161v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07161","description":"<p>Neural topic models (NTMs) apply deep neural networks to topic modelling.\nDespite their success, NTMs generally ignore two important aspects: (1) only\ndocument-level word count information is utilized for the training, while more\nfine-grained sentence-level information is ignored, and (2) external semantic\nknowledge regarding documents, sentences and words are not exploited for the\ntraining. To address these issues, we propose a variational autoencoder (VAE)\nNTM model that jointly reconstructs the sentence and document word counts using\ncombinations of bag-of-words (BoW) topical embeddings and pre-trained semantic\nembeddings. The pre-trained embeddings are first transformed into a common\nlatent topical space to align their semantics with the BoW embeddings. Our\nmodel also features hierarchical KL divergence to leverage embeddings of each\ndocument to regularize those of their sentences, thereby paying more attention\nto semantically relevant sentences. Both quantitative and qualitative\nexperiments have shown the efficacy of our model in 1) lowering the\nreconstruction errors at both the sentence and document levels, and 2)\ndiscovering more coherent topics from real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">He Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Distributed Robust Optimization for Vision-and-Language Inference. (arXiv:2110.07165v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07165","description":"<p>Analysis of vision-and-language models has revealed their brittleness under\nlinguistic phenomena such as paraphrasing, negation, textual entailment, and\nword substitutions with synonyms or antonyms. While data augmentation\ntechniques have been designed to mitigate against these failure modes, methods\nthat can integrate this knowledge into the training pipeline remain\nunder-explored. In this paper, we present \\textbf{SDRO}, a model-agnostic\nmethod that utilizes a set linguistic transformations in a distributed robust\noptimization setting, along with an ensembling technique to leverage these\ntransformations during inference. Experiments on benchmark datasets with images\n(NLVR$^2$) and video (VIOLIN) demonstrate performance improvements as well as\nrobustness to adversarial attacks. Experiments on binary VQA explore the\ngeneralizability of this method to other V\\&amp;L tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Abhishek Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization. (arXiv:2110.07166v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07166","description":"<p>Neural abstractive summarization models are susceptible to generating\nfactually inconsistent content, a phenomenon known as hallucination. This\nlimits the usability and adoption of these systems in real-world applications.\nTo reduce the presence of hallucination, we propose the Mixture of Factual\nExperts (MoFE) model, which combines multiple summarization experts that each\ntarget a specific type of error. We train our experts using reinforcement\nlearning (RL) to minimize the error defined by two factual consistency metrics:\nentity overlap and dependency arc entailment. We construct MoFE by combining\nthe experts using two ensembling strategies (weights and logits) and evaluate\nthem on two summarization datasets (XSUM and CNN/DM). Our experiments on BART\nmodels show that the MoFE improves performance according to both entity overlap\nand dependency arc entailment, without a significant performance drop on\nstandard ROUGE metrics. The performance improvement also transfers to unseen\nfactual consistency metrics, such as question answer-based factuality\nevaluation metric and BERTScore precision with respect to the source document.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-gloss Augmentation for Improving Word Sense Disambiguation. (arXiv:2110.07174v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07174","description":"<p>The goal of Word Sense Disambiguation (WSD) is to identify the sense of a\npolysemous word in a specific context. Deep-learning techniques using BERT have\nachieved very promising results in the field and different methods have been\nproposed to integrate structured knowledge to enhance performance. At the same\ntime, an increasing number of data augmentation techniques have been proven to\nbe useful for NLP tasks. Building upon previous works leveraging BERT and\nWordNet knowledge, we explore different data augmentation techniques on\ncontext-gloss pairs to improve the performance of WSD. In our experiment, we\nshow that both sentence-level and word-level augmentation methods are effective\nstrategies for WSD. Also, we find out that performance can be improved by\nadding hypernyms' glosses obtained from a lexical knowledge base. We compare\nand analyze different context-gloss augmentation techniques, and the results\nshow that applying back translation on gloss performs the best.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guan-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giambi_M/0/1/0/all/0/1\">Manuel Giambi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models. (arXiv:2110.07178v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07178","description":"<p>The common practice for training commonsense models has gone\nfrom-human-to-corpus-to-machine: humans author commonsense knowledge graphs in\norder to train commonsense models. In this work, we investigate an alternative,\nfrom-machine-to-corpus-to-machine: general language models author these\ncommonsense knowledge graphs to train commonsense models. Our study leads to a\nnew framework, Symbolic Knowledge Distillation. As with prior art in Knowledge\nDistillation (Hinton et al., 2015), our approach uses larger models to teach\nsmaller models. A key difference is that we distill knowledge symbolically-as\ntext-in addition to the neural model. We also distill only one aspect-the\ncommonsense of a general language model teacher, allowing the student to be a\ndifferent type, a commonsense model. Altogether, we show that careful prompt\nengineering and a separately trained critic model allow us to selectively\ndistill high-quality causal commonsense from GPT-3, a general language model.\nEmpirical results demonstrate that, for the first time, a human-authored\ncommonsense knowledge graph is surpassed by our automatically distilled variant\nin all three criteria: quantity, quality, and diversity. In addition, it\nresults in a neural commonsense model that surpasses the teacher model's\ncommonsense capabilities despite its 100x smaller size. We apply this to the\nATOMIC resource, and share our new symbolic knowledge graph and commonsense\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1\">Sean Welleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting IPA-based Cross-lingual Text-to-speech. (arXiv:2110.07187v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07187","description":"<p>International Phonetic Alphabet (IPA) has been widely used in cross-lingual\ntext-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However,\nIPA itself has been understudied in cross-lingual TTS. In this paper, we report\nsome empirical findings of building a cross-lingual TTS model using IPA as\ninputs. Experiments show that the way to process the IPA and suprasegmental\nsequence has a negligible impact on the CL VC performance. Furthermore, we find\nthat using a dataset including one speaker per language to build an IPA-based\nTTS system would fail CL VC since the language-unique IPA and tone/stress\nsymbols could leak the speaker information. In addition, we experiment with\ndifferent combinations of speakers in the training dataset to further\ninvestigate the effect of the number of speakers on the CL VC performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling. (arXiv:2110.07198v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07198","description":"<p>Although large-scale pre-trained neural models have shown impressive\nperformances in a variety of tasks, their ability to generate coherent text\nthat appropriately models discourse phenomena is harder to evaluate and less\nunderstood. Given the claims of improved text generation quality across various\nsystems, we consider the coherence evaluation of machine generated text to be\none of the principal applications of coherence models that needs to be\ninvestigated. We explore training data and self-supervision objectives that\nresult in a model that generalizes well across tasks and can be used\noff-the-shelf to perform such evaluations. Prior work in neural coherence\nmodeling has primarily focused on devising new architectures, and trained the\nmodel to distinguish coherent and incoherent text through pairwise\nself-supervision on the permuted documents task. We instead use a basic model\narchitecture and show significant improvements over state of the art within the\nsame training regime. We then design a harder self-supervision objective by\nincreasing the ratio of negative samples within a contrastive learning setup,\nand enhance the model further through automatic hard negative mining coupled\nwith a large global negative queue encoded by a momentum encoder. We show\nempirically that increasing the density of negative samples improves the basic\nmodel, and using a global negative queue further improves and stabilizes the\nmodel while training with hard negative samples. We evaluate the coherence\nmodel on task-independent test sets that resemble real-world use cases and show\nsignificant improvements in coherence evaluations of downstream applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jwalapuram_P/0/1/0/all/0/1\">Prathyusha Jwalapuram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SpeechT5: Unified-Modal Encoder-Decoder Pre-training for Spoken Language Processing. (arXiv:2110.07205v1 [eess.AS])","link":"http://arxiv.org/abs/2110.07205","description":"<p>Motivated by the success of T5 (Text-To-Text Transfer Transformer) in\npre-training natural language processing models, we propose a unified-modal\nSpeechT5 framework that explores the encoder-decoder pre-training for\nself-supervised speech/text representation learning. The SpeechT5 framework\nconsists of a shared encoder-decoder network and six modal-specific\n(speech/text) pre/post-nets. After preprocessing the speech/text input through\nthe pre-nets, the shared encoder-decoder network models the sequence to\nsequence transformation, and then the post-nets generate the output in the\nspeech/text modality based on the decoder output. Particularly, SpeechT5 can\npre-train on a large scale of unlabeled speech and text data to improve the\ncapability of the speech and textual modeling. To align the textual and speech\ninformation into a unified semantic space, we propose a cross-modal vector\nquantization method with random mixing-up to bridge speech and text. Extensive\nevaluations on a wide variety of spoken language processing tasks, including\nvoice conversion, automatic speech recognition, text to speech, and speaker\nidentification, show the superiority of the proposed SpeechT5 framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ao_J/0/1/0/all/0/1\">Junyi Ao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Z/0/1/0/all/0/1\">Zhihua Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Attention Neural Network for Pun Location and Using Pun-Gloss Pairs for Interpretation. (arXiv:2110.07209v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07209","description":"<p>Pun location is to identify the punning word (usually a word or a phrase that\nmakes the text ambiguous) in a given short text, and pun interpretation is to\nfind out two different meanings of the punning word. Most previous studies\nadopt limited word senses obtained by WSD(Word Sense Disambiguation) technique\nor pronunciation information in isolation to address pun location. For the task\nof pun interpretation, related work pays attention to various WSD algorithms.\nIn this paper, a model called DANN (Dual-Attentive Neural Network) is proposed\nfor pun location, effectively integrates word senses and pronunciation with\ncontext information to address two kinds of pun at the same time. Furthermore,\nwe treat pun interpretation as a classification task and construct pungloss\npairs as processing data to solve this task. Experiments on the two benchmark\ndatasets show that our proposed methods achieve new state-of-the-art results.\nOur source code is available in the public code repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Meirong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanbin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1\">Man Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve Cross-lingual Voice Cloning Using Low-quality Code-switched Data. (arXiv:2110.07210v1 [cs.SD])","link":"http://arxiv.org/abs/2110.07210","description":"<p>Recently, sequence-to-sequence (seq-to-seq) models have been successfully\napplied in text-to-speech (TTS) to synthesize speech for single-language text.\nTo synthesize speech for multiple languages usually requires multi-lingual\nspeech from the target speaker. However, it is both laborious and expensive to\ncollect high-quality multi-lingual TTS data for the target speakers. In this\npaper, we proposed to use low-quality code-switched found data from the\nnon-target speakers to achieve cross-lingual voice cloning for the target\nspeakers. Experiments show that our proposed method can generate high-quality\ncode-switched speech in the target voices in terms of both naturalness and\nspeaker consistency. More importantly, we find that our method can achieve a\ncomparable result to the state-of-the-art (SOTA) performance in cross-lingual\nvoice cloning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Transformers Perform Below Chance on Recursive Nested Constructions, Unlike Humans. (arXiv:2110.07240v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07240","description":"<p>Recursive processing is considered a hallmark of human linguistic abilities.\nA recent study evaluated recursive processing in recurrent neural language\nmodels (RNN-LMs) and showed that such models perform below chance level on\nembedded dependencies within nested constructions -- a prototypical example of\nrecursion in natural language. Here, we study if state-of-the-art Transformer\nLMs do any better. We test four different Transformer LMs on two different\ntypes of nested constructions, which differ in whether the embedded (inner)\ndependency is short or long range. We find that Transformers achieve\nnear-perfect performance on short-range embedded dependencies, significantly\nbetter than previous results reported for RNN-LMs and humans. However, on\nlong-range embedded dependencies, Transformers' performance sharply drops below\nchance level. Remarkably, the addition of only three words to the embedded\ndependency caused Transformers to fall from near-perfect to below-chance\nperformance. Taken together, our results reveal Transformers' shortcoming when\nit comes to recursive, structure-based, processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakretz_Y/0/1/0/all/0/1\">Yair Lakretz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desbordes_T/0/1/0/all/0/1\">Th&#xe9;o Desbordes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehaene_S/0/1/0/all/0/1\">Stanislas Dehaene</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Chinese Biomedical Language Models via Multi-Level Text Discrimination. (arXiv:2110.07244v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07244","description":"<p>Pre-trained language models (PLMs), such as BERT and GPT, have revolutionized\nthe field of NLP, not only in the general domain but also in the biomedical\ndomain. Most prior efforts in building biomedical PLMs have resorted simply to\ndomain adaptation and focused mainly on English. In this work we introduce\neHealth, a biomedical PLM in Chinese built with a new pre-training framework.\nThis new framework trains eHealth as a discriminator through both token-level\nand sequence-level discrimination. The former is to detect input tokens\ncorrupted by a generator and select their original signals from plausible\ncandidates, while the latter is to further distinguish corruptions of a same\noriginal sequence from those of the others. As such, eHealth can learn language\nsemantics at both the token and sequence levels. Extensive experiments on 11\nChinese biomedical language understanding tasks of various forms verify the\neffectiveness and superiority of our approach. The pre-trained model is\navailable to the public at\n\\url{https://github.com/PaddlePaddle/Research/tree/master/KG/eHealth} and the\ncode will also be released later.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Songtai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Benfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yajuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Approach to Mispronunciation Detection and Diagnosis with Acoustic, Phonetic and Linguistic (APL) Embeddings. (arXiv:2110.07274v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07274","description":"<p>Many mispronunciation detection and diagnosis (MD&amp;D) research approaches try\nto exploit both the acoustic and linguistic features as input. Yet the\nimprovement of the performance is limited, partially due to the shortage of\nlarge amount annotated training data at the phoneme level. Phonetic embeddings,\nextracted from ASR models trained with huge amount of word level annotations,\ncan serve as a good representation of the content of input speech, in a\nnoise-robust and speaker-independent manner. These embeddings, when used as\nimplicit phonetic supplementary information, can alleviate the data shortage of\nexplicit phoneme annotations. We propose to utilize Acoustic, Phonetic and\nLinguistic (APL) embedding features jointly for building a more powerful MD\\&amp;D\nsystem. Experimental results obtained on the L2-ARCTIC database show the\nproposed approach outperforms the baseline by 9.93%, 10.13% and 6.17% on the\ndetection accuracy, diagnosis error rate and the F-measure, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wenxuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaoguang Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenshan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yan Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tien_J/0/1/0/all/0/1\">Jonathan Tien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts. (arXiv:2110.07280v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07280","description":"<p>Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of\nthe factual information extracted from Large Language Models (LLMs) depends on\nthe prompts used to query them. This inconsistency is problematic because\ndifferent users will query LLMs for the same information using different\nwording, but should receive the same, accurate responses regardless. In this\nwork we aim to address this shortcoming by introducing P-Adapters: lightweight\nmodels that sit between the embedding layer and first attention layer of LLMs.\nThey take LLM embeddings as input and output continuous prompts that are used\nto query the LLM. Additionally, we investigate Mixture of Experts (MoE) models\nthat learn a set of continuous prompts (\"experts\") and select one to query the\nLLM. They require a separate classifier trained on human-annotated data to map\nnatural language prompts to the continuous ones. P-Adapters perform comparably\nto the more complex MoE models in extracting factual information from BERT and\nRoBERTa while eliminating the need for additional annotations. P-Adapters show\nbetween 12-26% absolute improvement in precision and 36-50% absolute\nimprovement in consistency over a baseline of only using natural language\nqueries. Finally, we investigate what makes a P-adapter successful and conclude\nthat access to the LLM's embeddings of the original natural language prompt,\nparticularly the subject of the entity pair being asked about, is a significant\nfactor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Newman_B/0/1/0/all/0/1\">Benjamin Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Rajani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. (arXiv:2110.07298v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07298","description":"<p>Existing approaches to lifelong language learning rely on plenty of labeled\ndata for learning a new task, which is hard to obtain in most real scenarios.\nConsidering that humans can continually learn new tasks from a handful of\nexamples, we expect the models also to be able to generalize well on new\nfew-shot tasks without forgetting the previous ones. In this work, we define\nthis more challenging yet practical problem as Lifelong Few-shot Language\nLearning (LFLL) and propose a unified framework for it based on prompt tuning\nof T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot\nlearning ability, and simultaneously trains the model as a task solver and a\ndata generator. Before learning a new domain of the same task type, LFPT5\ngenerates pseudo (labeled) samples of previously learned domains, and later\ngets trained on those samples to alleviate forgetting of previous knowledge as\nit learns the new domain. In addition, a KL divergence loss is minimized to\nachieve label consistency between the previous and the current model. While\nadapting to a new task type, LFPT5 includes and tunes additional prompt\nembeddings for the new task. With extensive experiments, we demonstrate that\nLFPT5 can be applied to various different types of tasks and significantly\noutperform previous methods in different LFLL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-Sentiment-Multiple-Opinion Triplet Extraction. (arXiv:2110.07303v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07303","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) aims to extract aspect term\n(aspect), sentiment and opinion term (opinion) triplets from sentences and can\ntell a complete story, i.e., the discussed aspect, the sentiment toward the\naspect, and the cause of the sentiment. ASTE is a charming task, however, one\ntriplet extracted by ASTE only includes one opinion of the aspect, but an\naspect in a sentence may have multiple corresponding opinions and one opinion\nonly provides part of the reason why the aspect has this sentiment, as a\nconsequence, some triplets extracted by ASTE are hard to understand, and\nprovide erroneous information for downstream tasks. In this paper, we introduce\na new task, named Aspect Sentiment Multiple Opinions Triplet Extraction\n(ASMOTE). ASMOTE aims to extract aspect, sentiment and multiple opinions\ntriplets. Specifically, one triplet extracted by ASMOTE contains all opinions\nabout the aspect and can tell the exact reason that the aspect has the\nsentiment. We propose an Aspect-Guided Framework (AGF) to address this task.\nAGF first extracts aspects, then predicts their opinions and sentiments.\nMoreover, with the help of the proposed Sequence Labeling Attention(SLA), AGF\nimproves the performance of the sentiment classification using the extracted\nopinions. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuncong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng-hua Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Cunxiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Investigation of Multi-bridge Multilingual NMT models. (arXiv:2110.07304v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07304","description":"<p>In this paper, we present an extensive investigation of multi-bridge,\nmany-to-many multilingual NMT models (MB-M2M) ie., models trained on\nnon-English language pairs in addition to English-centric language pairs. In\naddition to validating previous work which shows that MB-M2M models can\novercome zeroshot translation problems, our analysis reveals the following\nresults about multibridge models: (1) it is possible to extract a reasonable\namount of parallel corpora between non-English languages for low-resource\nlanguages (2) with limited non-English centric data, MB-M2M models are\ncompetitive with or outperform pivot models, (3) MB-M2M models can outperform\nEnglish-Any models and perform at par with Any-English models, so a single\nmultilingual NMT system can serve all translation directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Aspect Category Sentiment Analysis as a Text Generation Task. (arXiv:2110.07310v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07310","description":"<p>Aspect category sentiment analysis has attracted increasing research\nattention. The dominant methods make use of pre-trained language models by\nlearning effective aspect category-specific representations, and adding\nspecific output layers to its pre-trained representation. We consider a more\ndirect way of making use of pre-trained language models, by casting the ACSA\ntasks into natural language generation tasks, using natural language sentences\nto represent the output. Our method allows more direct use of pre-trained\nknowledge in seq2seq language models by directly following the task setting\nduring pre-training. Experiments on several benchmarks show that our method\ngives the best reported results, having large advantages in few-shot and\nzero-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhiyang Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanmeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WMDecompose: A Framework for Leveraging the Interpretable Properties of Word Mover's Distance in Sociocultural Analysis. (arXiv:2110.07330v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07330","description":"<p>Despite the increasing popularity of NLP in the humanities and social\nsciences, advances in model performance and complexity have been accompanied by\nconcerns about interpretability and explanatory power for sociocultural\nanalysis. One popular model that balances complexity and legibility is Word\nMover's Distance (WMD). Ostensibly adapted for its interpretability, WMD has\nnonetheless been used and further developed in ways which frequently discard\nits most interpretable aspect: namely, the word-level distances required for\ntranslating a set of words into another set of words. To address this apparent\ngap, we introduce WMDecompose: a model and Python library that 1) decomposes\ndocument-level distances into their constituent word-level distances, and 2)\nsubsequently clusters words to induce thematic elements, such that useful\nlexical information is retained and summarized for analysis. To illustrate its\npotential in a social scientific context, we apply it to a longitudinal social\nmedia corpus to explore the interrelationship between conspiracy theories and\nconservative American discourses. Finally, because of the full WMD model's high\ntime-complexity, we additionally suggest a method of sampling document pairs\nfrom large datasets in a reproducible way, with tight bounds that prevent\nextrapolation of unreliable results due to poor sampling practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brunila_M/0/1/0/all/0/1\">Mikael Brunila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaViolette_J/0/1/0/all/0/1\">Jack LaViolette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug-Tagger: A Pluggable Sequence Labeling Framework Using Language Models. (arXiv:2110.07331v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07331","description":"<p>Plug-and-play functionality allows deep learning models to adapt well to\ndifferent tasks without requiring any parameters modified. Recently,\nprefix-tuning was shown to be a plug-and-play method on various text generation\ntasks by simply inserting corresponding continuous vectors into the inputs.\nHowever, sequence labeling tasks invalidate existing plug-and-play methods\nsince different label sets demand changes to the architecture of the model\nclassifier. In this work, we propose the use of label word prediction instead\nof classification to totally reuse the architecture of pre-trained models for\nsequence labeling tasks. Specifically, for each task, a label word set is first\nconstructed by selecting a high-frequency word for each class respectively, and\nthen, task-specific vectors are inserted into the inputs and optimized to\nmanipulate the model predictions towards the corresponding label words. As a\nresult, by simply switching the plugin vectors on the input, a frozen\npre-trained language model is allowed to perform different tasks. Experimental\nresults on three sequence labeling tasks show that the performance of the\nproposed method can achieve comparable performance with standard fine-tuning\nwith only 0.1\\% task-specific parameters. In addition, our method is up to 70\ntimes faster than non-plug-and-play methods while switching different tasks\nunder the resource-constrained scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiding Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Legal Question Answering Systems. (arXiv:2110.07333v1 [cs.IR])","link":"http://arxiv.org/abs/2110.07333","description":"<p>Many legal professionals think that the explosion of information about local,\nregional, national, and international legislation makes their practice more\ncostly, time-consuming, and even error-prone. The two main reasons for this are\nthat most legislation is usually unstructured, and the tremendous amount and\npace with which laws are released causes information overload in their daily\ntasks. In the case of the legal domain, the research community agrees that a\nsystem allowing to generate automatic responses to legal questions could\nsubstantially impact many practical implications in daily activities. The\ndegree of usefulness is such that even a semi-automatic solution could\nsignificantly help to reduce the workload to be faced. This is mainly because a\nQuestion Answering system could be able to automatically process a massive\namount of legal resources to answer a question or doubt in seconds, which means\nthat it could save resources in the form of effort, money, and time to many\nprofessionals in the legal sector. In this work, we quantitatively and\nqualitatively survey the solutions that currently exist to meet this challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1\">Jorge Martinez-Gil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: Following Instructions in Language with Modular Methods. (arXiv:2110.07342v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07342","description":"<p>Recent methods for embodied instruction following are typically trained\nend-to-end using imitation learning. This requires the use of expert\ntrajectories and low-level language instructions. Such approaches assume\nlearned hidden states will simultaneously integrate semantics from the language\nand vision to perform state tracking, spatial memory, exploration, and\nlong-term planning. In contrast, we propose a modular method with structured\nrepresentations that (1) builds a semantic map of the scene, and (2) performs\nexploration with a semantic search policy, to achieve the natural language\ngoal. Our modular method achieves SOTA performance (24.46%) with a substantial\n(8.17 % absolute) gap from previous work while using less data by eschewing\nboth expert trajectories and low-level instructions. Leveraging low-level\nlanguage, however, can further increase our performance (26.49%). Our findings\nsuggest that an explicit spatial memory and a semantic search policy can\nprovide a stronger and more general representation for state-tracking and\nguidance, even in the absence of expert trajectories or low-level instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">So Yeon Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Singh Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Music Playlist Title Generation: A Machine-Translation Approach. (arXiv:2110.07354v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07354","description":"<p>We propose a machine-translation approach to automatically generate a\nplaylist title from a set of music tracks. We take a sequence of track IDs as\ninput and a sequence of words in a playlist title as output, adapting the\nsequence-to-sequence framework based on Recurrent Neural Network (RNN) and\nTransformer to the music data. Considering the orderless nature of music tracks\nin a playlist, we propose two techniques that remove the order of the input\nsequence. One is data augmentation by shuffling and the other is deleting the\npositional encoding. We also reorganize the existing music playlist datasets to\ngenerate phrase-level playlist titles. The result shows that the Transformer\nmodels generally outperform the RNN model. Also, removing the order of input\nsequence improves the performance further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doh_S/0/1/0/all/0/1\">SeungHeon Doh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Juhan Nam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization. (arXiv:2110.07356v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07356","description":"<p>In medical dialogue summarization, summaries must be coherent and must\ncapture all the medically relevant information in the dialogue. However,\nlearning effective models for summarization require large amounts of labeled\ndata which is especially hard to obtain. We present an algorithm to create\nsynthetic training data with an explicit focus on capturing medically relevant\ninformation. We utilize GPT-3 as the backbone of our algorithm and scale 210\nhuman labeled examples to yield results comparable to using 6400 human labeled\nexamples (~30x) leveraging low-shot learning and an ensemble method. In\ndetailed experiments, we show that this approach produces high quality training\ndata that can further be combined with human labeled data to get summaries that\nare strongly preferable to those produced by models trained on human data alone\nboth in terms of medical accuracy and coherency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chintagunta_B/0/1/0/all/0/1\">Bharath Chintagunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katariya_N/0/1/0/all/0/1\">Namit Katariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amatriain_X/0/1/0/all/0/1\">Xavier Amatriain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_A/0/1/0/all/0/1\">Anitha Kannan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Text Mining of COVID-19 Records. (arXiv:2110.07357v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07357","description":"<p>Since the beginning of coronavirus, the disease has spread worldwide and\ndrastically changed many aspects of the human's lifestyle. Twitter as a\npowerful tool can help researchers measure public health in response to\nCOVID-19. According to the high volume of data production on social networks,\nautomated text mining approaches can help search, read and summarize helpful\ninformation. This paper preprocessed the existing medical dataset regarding\nCOVID-19 named CORD-19 and annotated the dataset for supervised classification\ntasks. At this time of the COVID-19 pandemic, we made a preprocessed dataset\nfor the research community. This may contribute towards finding new solutions\nfor some social interventions that COVID-19 has made. The preprocessed version\nof the mentioned dataset is publicly available through Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zamini_M/0/1/0/all/0/1\">Mohamad Zamini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Based Semantic Parsing. (arXiv:2110.07358v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07358","description":"<p>We present a memory-based model for context-dependent semantic parsing.\nPrevious approaches focus on enabling the decoder to copy or modify the parse\nfrom the previous utterance, assuming there is a dependency between the current\nand previous parses. In this work, we propose to represent contextual\ninformation using an external memory. We learn a context memory controller that\nmanages the memory by maintaining the cumulative meaning of sequential user\nutterances. We evaluate our approach on three semantic parsing benchmarks.\nExperimental results show that our model can better process context-dependent\ninformation and demonstrates improved performance without using task-specific\ndecoders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Parag Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking. (arXiv:2110.07367v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07367","description":"<p>In various natural language processing tasks, passage retrieval and passage\nre-ranking are two key procedures in finding and ranking relevant information.\nSince both the two procedures contribute to the final performance, it is\nimportant to jointly optimize them in order to achieve mutual improvement. In\nthis paper, we propose a novel joint training approach for dense passage\nretrieval and passage re-ranking. A major contribution is that we introduce the\ndynamic listwise distillation, where we design a unified listwise training\napproach for both the retriever and the re-ranker. During the dynamic\ndistillation, the retriever and the re-ranker can be adaptively improved\naccording to each other's relevance information. We also propose a hybrid data\naugmentation strategy to construct diverse training instances for listwise\ntraining approach. Extensive experiments show the effectiveness of our approach\non both MSMARCO and Natural Questions datasets. Our code is available at\nhttps://github.com/PaddlePaddle/RocketQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qiaoqiao She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Semantic Knowledge Into Language Encoders. (arXiv:2110.07382v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07382","description":"<p>We introduce semantic form mid-tuning, an approach for transferring semantic\nknowledge from semantic meaning representations into transformer-based language\nencoders. In mid-tuning, we learn to align the text of general sentences -- not\ntied to any particular inference task -- and structured semantic\nrepresentations of those sentences. Our approach does not require gold\nannotated semantic representations. Instead, it makes use of automatically\ngenerated semantic representations, such as from off-the-shelf PropBank and\nFrameNet semantic parsers. We show that this alignment can be learned\nimplicitly via classification or directly via triplet loss. Our method yields\nlanguage encoders that demonstrate improved predictive performance across\ninference, reading comprehension, textual similarity, and other semantic tasks\ndrawn from the GLUE, SuperGLUE, and SentEval benchmarks. We evaluate our\napproach on three popular baseline models, where our experimental results and\nanalysis concludes that current pre-trained language models can further benefit\nfrom structured semantic frames with the proposed mid-tuning method, as they\ninject additional task-agnostic knowledge to the encoder, improving the\ngenerated embeddings as well as the linguistic properties of the given model,\nas evident from improvements on a popular sentence embedding toolkit and a\nvariety of probing tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Umair_M/0/1/0/all/0/1\">Mohammad Umair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Neglected Sibling: Isotropic Gaussian Posterior for VAE. (arXiv:2110.07383v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07383","description":"<p>Deep generative models have been widely used in several areas of NLP, and\nvarious techniques have been proposed to augment them or address their training\nchallenges. In this paper, we propose a simple modification to Variational\nAutoencoders (VAEs) by using an Isotropic Gaussian Posterior (IGP) that allows\nfor better utilisation of their latent representation space. This model avoids\nthe sub-optimal behavior of VAEs related to inactive dimensions in the\nrepresentation space. We provide both theoretical analysis, and empirical\nevidence on various datasets and tasks that show IGP leads to consistent\nimprovement on several quantitative and qualitative grounds, from downstream\ntask performance and sample efficiency to robustness. Additionally, we give\ninsights about the representational properties encouraged by IGP and also show\nthat its gain generalises to image domain as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Controllable Style Transfer for Low-Resource Settings: A Study in Indian Languages. (arXiv:2110.07385v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07385","description":"<p>Style transfer is the task of rewriting an input sentence into a target style\nwhile approximately preserving its content. While most prior literature assumes\naccess to large style-labelled corpora, recent work (Riley et al. 2021) has\nattempted \"few-shot\" style transfer using only 3-10 sentences at inference for\nextracting the target style. In this work we consider one such low resource\nsetting where no datasets are available: style transfer for Indian languages.\nWe find that existing few-shot methods perform this task poorly, with a strong\ntendency to copy inputs verbatim. We push the state-of-the-art for few-shot\nstyle transfer with a new method modeling the stylistic difference between\nparaphrases. When compared to prior work using automatic and human evaluations,\nour model achieves 2-3x better performance and output diversity in formality\ntransfer and code-mixing addition across five Indian languages. Moreover, our\nmethod is better able to control the amount of style transfer using an input\nscalar knob. We report promising qualitative results for several attribute\ntransfer directions, including sentiment transfer, text simplification, gender\nneutralization and text anonymization, all without retraining the model.\nFinally we found model evaluation to be difficult due to the lack of evaluation\ndatasets and metrics for Indian languages. To facilitate further research in\nformality transfer for Indic languages, we crowdsource annotations for 4000\nsentence pairs in four languages, and use this dataset to design our automatic\nevaluation suite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nathani_D/0/1/0/all/0/1\">Deepak Nathani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samanta_B/0/1/0/all/0/1\">Bidisha Samanta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1\">Partha Talukdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning. (arXiv:2110.07410v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07410","description":"<p>Automated audio captioning (AAC) is the task of automatically generating\ntextual descriptions for general audio signals. A captioning system has to\nidentify various information from the input signal and express it with natural\nlanguage. Existing works mainly focus on investigating new methods and try to\nimprove their performance measured on existing datasets. Having attracted\nattention only recently, very few works on AAC study the performance of\nexisting pre-trained audio and natural language processing resources. In this\npaper, we evaluate the performance of off-the-shelf models with a\nTransformer-based captioning approach. We utilize the freely available Clotho\ndataset to compare four different pre-trained machine listening models, four\nword embedding models, and their combinations in many different settings. Our\nevaluation suggests that YAMNet combined with BERT embeddings produces the best\ncaptions. Moreover, in general, fine-tuning pre-trained word embeddings can\nlead to better performance. Finally, we show that sequences of audio embeddings\ncan be processed using a Transformer encoder to produce higher-quality\ncaptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weck_B/0/1/0/all/0/1\">Benno Weck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favory_X/0/1/0/all/0/1\">Xavier Favory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drossos_K/0/1/0/all/0/1\">Konstantinos Drossos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1\">Xavier Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple, Strong and Robust Baseline for Distantly Supervised Relation Extraction. (arXiv:2110.07415v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07415","description":"<p>Distantly supervised relation extraction (DS-RE) is generally framed as a\nmulti-instance multi-label (MI-ML) task, where the optimal aggregation of\ninformation from multiple instances is of key importance. Intra-bag attention\n(Lin et al., 2016) is an example of a popularly used aggregation scheme for\nthis framework. Apart from this scheme, however, there is not much to choose\nfrom in the DS-RE literature as most of the advances in this field are focused\non improving the instance-encoding step rather than the instance-aggregation\nstep. With recent works leveraging large pre-trained language models as\nencoders, the increased capacity of models might allow for more flexibility in\nthe instance-aggregation step. In this work, we explore this hypothesis and\ncome up with a novel aggregation scheme which we call Passage-Att. Under this\naggregation scheme, we combine all instances mentioning an entity pair into a\n\"passage of instances\", which is summarized independently for each relation\nclass. These summaries are used to predict the validity of a potential triple.\nWe show that our Passage-Att with BERT as passage encoder achieves\nstate-of-the-art performance in three different settings (monolingual DS,\nmonolingual DS with manually-annotated test set, multilingual DS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rathore_V/0/1/0/all/0/1\">Vipul Rathore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badola_K/0/1/0/all/0/1\">Kartikeya Badola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Modeling of Social Concepts Evoked by Art Images as Multimodal Frames. (arXiv:2110.07420v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07420","description":"<p>Social concepts referring to non-physical objects--such as revolution,\nviolence, or friendship--are powerful tools to describe, index, and query the\ncontent of visual data, including ever-growing collections of art images from\nthe Cultural Heritage (CH) field. While much progress has been made towards\ncomplete image understanding in computer vision, automatic detection of social\nconcepts evoked by images is still a challenge. This is partly due to the\nwell-known semantic gap problem, worsened for social concepts given their lack\nof unique physical features, and reliance on more unspecific features than\nconcrete concepts. In this paper, we propose the translation of recent\ncognitive theories about social concept representation into a software approach\nto represent them as multimodal frames, by integrating multisensory data. Our\nmethod focuses on the extraction, analysis, and integration of multimodal\nfeatures from visual art material tagged with the concepts of interest. We\ndefine a conceptual model and present a novel ontology for formally\nrepresenting social concepts as multimodal frames. Taking the Tate Gallery's\ncollection as an empirical basis, we experiment our method on a corpus of art\nimages to provide a proof of concept of its potential. We discuss further\ndirections of research, and provide all software, data sources, and results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandiani_D/0/1/0/all/0/1\">Delfina Sol Martinez Pandiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Presutti_V/0/1/0/all/0/1\">Valentina Presutti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Model Robustness to User-generated Noisy Texts. (arXiv:2110.07428v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07428","description":"<p>Sensitivity of deep-neural models to input noise is known to be a challenging\nproblem. In NLP, model performance often deteriorates with naturally occurring\nnoise, such as spelling errors. To mitigate this issue, models may leverage\nartificially noised data. However, the amount and type of generated noise has\nso far been determined arbitrarily. We therefore propose to model the errors\nstatistically from grammatical-error-correction corpora. We present a thorough\nevaluation of several state-of-the-art NLP systems' robustness in multiple\nlanguages, with tasks including morpho-syntactic analysis, named entity\nrecognition, neural machine translation, a subset of the GLUE benchmark and\nreading comprehension. We also compare two approaches to address the\nperformance drop: a) training the NLP models with noised data generated by our\nframework; and b) reducing the input noise with external system for natural\nlanguage correction. The code is released at https://github.com/ufal/kazitext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1\">Jakub N&#xe1;plava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popel_M/0/1/0/all/0/1\">Martin Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1\">Jana Strakov&#xe1;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards More Effective and Economic Sparsely-Activated Model. (arXiv:2110.07431v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07431","description":"<p>The sparsely-activated models have achieved great success in natural language\nprocessing through large-scale parameters and relatively low computational\ncost, and gradually become a feasible technique for training and implementing\nextremely large models. Due to the limit of communication cost, activating\nmultiple experts is hardly affordable during training and inference. Therefore,\nprevious work usually activate just one expert at a time to alleviate\nadditional communication cost. Such routing mechanism limits the upper bound of\nmodel performance. In this paper, we first investigate a phenomenon that\nincreasing the number of activated experts can boost the model performance with\nhigher sparse ratio. To increase the number of activated experts without an\nincrease in computational cost, we propose SAM (Switch and Mixture) routing, an\nefficient hierarchical routing mechanism that activates multiple experts in a\nsame device (GPU). Our methods shed light on the training of extremely large\nsparse models and experiments prove that our models can achieve significant\nperformance gain with great efficiency improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1\">Ke Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1\">Jianwei Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongkang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhaoye Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zikai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Ruofei Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiawen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Enrui Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinxia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yantao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Designing Language Technologies for Social Good: The Road not Taken. (arXiv:2110.07444v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07444","description":"<p>Development of speech and language technology for social good (LT4SG),\nespecially those targeted at the welfare of marginalized communities and\nspeakers of low-resource and under-served languages, has been a prominent theme\nof research within NLP, Speech, and the AI communities. Researchers have mostly\nrelied on their individual expertise, experiences or ad hoc surveys for\nprioritization of language technologies that provide social good to the\nend-users. This has been criticized by several scholars who argue that work on\nLT4SG must include the target linguistic communities during the design and\ndevelopment process. However, none of the LT4SG work and their critiques\nsuggest principled techniques for prioritization of the technologies and\nmethods for inclusion of the end-user during the development cycle. Drawing\ninspiration from the fields of Economics, Ethics, Psychology, and Participatory\nDesign, here we chart out a set of methodologies for prioritizing LT4SG that\nare aligned with the end-user preferences. We then analyze several LT4SG\nefforts in light of the proposed methodologies and bring out their hidden\nassumptions and potential pitfalls. While the current study is limited to\nlanguage technologies, we believe that the principles and prioritization\ntechniques highlighted here are applicable more broadly to AI for Social Good.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukhija_N/0/1/0/all/0/1\">Namrata Mukhija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bali_K/0/1/0/all/0/1\">Kalika Bali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MReD: A Meta-Review Dataset for Controllable Text Generation. (arXiv:2110.07474v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07474","description":"<p>When directly using existing text generation datasets for controllable\ngeneration, we are facing the problem of not having the domain knowledge and\nthus the aspects that could be controlled are limited.A typical example is when\nusing CNN/Daily Mail dataset for controllable text summarization, there is no\nguided information on the emphasis of summary sentences. A more useful text\ngenerator should leverage both the input text and control variables to guide\nthe generation, which can only be built with deep understanding of the domain\nknowledge. Motivated by this vi-sion, our paper introduces a new text\ngeneration dataset, named MReD. Our new dataset consists of 7,089 meta-reviews\nand all its 45k meta-review sentences are manually annotated as one of the\ncarefully defined 9 categories, including abstract, strength, decision, etc. We\npresent experimental results on start-of-the-art summarization models, and\npropose methods for controlled generation on both extractive and abstractive\nmodels using our annotated data. By exploring various settings and analaysing\nthe model behavior with respect to the control inputs, we demonstrate the\nchallenges and values of our dataset. MReD allows us to have a better\nunderstanding of the meta-review corpora and enlarge the research room for\ncontrollable text generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chenhui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query and Extract: Refining Event Extraction as Type-oriented Binary Decoding. (arXiv:2110.07476v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07476","description":"<p>Event extraction is typically modeled as a multi-class classification problem\nwhere both event types and argument roles are treated as atomic symbols. These\napproaches are usually limited to a set of pre-defined types. We propose a\nnovel event extraction framework that takes event types and argument roles as\nnatural language queries to extract candidate triggers and arguments from the\ninput text. With the rich semantics in the queries, our framework benefits from\nthe attention mechanisms to better capture the semantic correlation between the\nevent types or argument roles and the input text. Furthermore, the\nquery-and-extract formulation allows our approach to leverage all available\nevent annotations from various ontologies as a unified model. Experiments on\ntwo public benchmarks, ACE and ERE, demonstrate that our approach achieves\nstate-of-the-art performance on each dataset and significantly outperforms\nexisting methods on zero-shot event extraction. We will make all the programs\npublicly available once the paper is accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sijia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning Large-Scale Pre-trained Language Models for Conversational Recommendation with Knowledge Graph. (arXiv:2110.07477v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07477","description":"<p>In this paper, we present a pre-trained language model (PLM) based framework\ncalled RID for conversational recommender system (CRS). RID finetunes the\nlarge-scale PLMs such as DialoGPT, together with a pre-trained Relational Graph\nConvolutional Network (RGCN) to encode the node representations of an\nitem-oriented knowledge graph. The former aims to generate fluent and diverse\ndialogue responses based on the strong language generation ability of PLMs,\nwhile the latter is to facilitate the item recommendation by learning better\nnode embeddings on the structural knowledge base. To unify two modules of\ndialogue generation and item recommendation into a PLMs-based framework, we\nexpand the generation vocabulary of PLMs to include an extra item vocabulary,\nand introduces a vocabulary pointer to control when to recommend target items\nin the generation process. Extensive experiments on the benchmark dataset\nReDial show RID significantly outperforms the state-of-the-art methods on both\nevaluations of dialogue and recommendation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lei Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Heterogeneous Factors with Triaffine Mechanism for Nested Named Entity Recognition. (arXiv:2110.07480v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07480","description":"<p>Nested entities are observed in many domains due to their compositionality,\nwhich cannot be easily recognized by the widely-used sequence labeling\nframework. A natural solution is to treat the task as a span classification\nproblem. To increase performance on span representation and classification, it\nis crucial to effectively integrate all useful information of different\nformats, which we refer to heterogeneous factors including tokens, labels,\nboundaries, and related spans. To fuse these heterogeneous factors, we propose\na novel triaffine mechanism including triaffine attention and scoring, which\ninteracts with multiple factors in both the stages of representation and\nclassification. Experiments results show that our proposed method achieves the\nstate-of-the-art F1 scores on four nested NER datasets: ACE2004, ACE2005,\nGENIA, and KBP2017.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Pitfalls of Analyzing Individual Neurons in Language Models. (arXiv:2110.07483v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07483","description":"<p>While many studies have shown that linguistic information is encoded in\nhidden word representations, few have studied individual neurons, to show how\nand in which neurons it is encoded. Among these, the common approach is to use\nan external probe to rank neurons according to their relevance to some\nlinguistic attribute, and to evaluate the obtained ranking using the same probe\nthat produced it. We show two pitfalls in this methodology: 1. It confounds\ndistinct factors: probe quality and ranking quality. We separate them and draw\nconclusions on each. 2. It focuses on encoded information, rather than\ninformation that is used by the model. We show that these are not the same. We\ncompare two recent ranking methods and a simple one we introduce, and evaluate\nthem with regard to both of these aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antverg_O/0/1/0/all/0/1\">Omer Antverg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Keyword Spotting using Xception-1d. (arXiv:2110.07498v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07498","description":"<p>The field of conversational agents is growing fast and there is an increasing\nneed for algorithms that enhance natural interaction. In this work we show how\nwe achieved state of the art results in the Keyword Spotting field by adapting\nand tweaking the Xception algorithm, which achieved outstanding results in\nseveral computer vision tasks. We obtained about 96\\% accuracy when classifying\naudio clips belonging to 35 different categories, beating human annotation at\nthe most complex tasks proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valles_Perez_I/0/1/0/all/0/1\">Iv&#xe1;n Vall&#xe9;s-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Sanchis_J/0/1/0/all/0/1\">Juan G&#xf3;mez-Sanchis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Sober_M/0/1/0/all/0/1\">Marcelino Mart&#xed;nez-Sober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vila_Frances_J/0/1/0/all/0/1\">Joan Vila-Franc&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_Lopez_A/0/1/0/all/0/1\">Antonio J. Serrano-L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soria_Olivas_E/0/1/0/all/0/1\">Emilio Soria-Olivas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Autoregressive Translation with Layer-Wise Prediction and Deep Supervision. (arXiv:2110.07515v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07515","description":"<p>How do we perform efficient inference while retaining high translation\nquality? Existing neural machine translation models, such as Transformer,\nachieve high performance, but they decode words one by one, which is\ninefficient. Recent non-autoregressive translation models speed up the\ninference, but their quality is still inferior. In this work, we propose DSLP,\na highly efficient and high-performance model for machine translation. The key\ninsight is to train a non-autoregressive Transformer with Deep Supervision and\nfeed additional Layer-wise Predictions. We conducted extensive experiments on\nfour translation tasks (both directions of WMT'14 EN-DE and WMT'16 EN-RO).\nResults show that our approach consistently improves the BLEU scores compared\nwith respective base models. Specifically, our best variant outperforms the\nautoregressive model on three translation tasks, while being 14.8 times more\nefficient in inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R. Za&#xef;ane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures. (arXiv:2110.07518v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07518","description":"<p>Current open-domain conversational models can easily be made to talk in\ninadequate ways. Online learning from conversational feedback given by the\nconversation partner is a promising avenue for a model to improve and adapt, so\nas to generate fewer of these safety failures. However, current\nstate-of-the-art models tend to react to feedback with defensive or oblivious\nresponses. This makes for an unpleasant experience and may discourage\nconversation partners from giving feedback in the future. This work proposes\nSaFeRDialogues, a task and dataset of graceful responses to conversational\nfeedback about safety failures. We collect a dataset of 10k dialogues\ndemonstrating safety failures, feedback signaling them, and a response\nacknowledging the feedback. We show how fine-tuning on this dataset results in\nconversations that human raters deem considerably more likely to lead to a\ncivil conversation, without sacrificing engagingness or general conversational\nability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ung_M/0/1/0/all/0/1\">Megan Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Opinion Summarization via Collaborative Decoding. (arXiv:2110.07520v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07520","description":"<p>Opinion summarization focuses on generating summaries that reflect popular\nopinions of multiple reviews for a single entity (e.g., a hotel or a product.)\nWhile generated summaries offer general and concise information about a\nparticular entity, the information may be insufficient to help the user compare\nmultiple entities. Thus, the user may still struggle with the question \"Which\none should I pick?\" In this paper, we propose a {\\em comparative opinion\nsummarization} task, which is to generate two contrastive summaries and one\ncommon summary from two given sets of reviews from different entities. We\ndevelop a comparative summarization framework CoCoSum, which consists of two\nfew-shot summarization models that are jointly used to generate contrastive and\ncommon summaries. Experimental results on a newly created benchmark CoCoTrip\nshow that CoCoSum can produce high-quality contrastive and common summaries\nthan state-of-the-art opinion summarization models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshihiko Suhara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Decoupling for Open-Domain Passage Retrieval. (arXiv:2110.07524v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07524","description":"<p>Training dense passage representations via contrastive learning (CL) has been\nshown effective for Open-Domain Passage Retrieval (ODPR). Recent studies mainly\nfocus on optimizing this CL framework by improving the sampling strategy or\nextra pretraining. Different from previous studies, this work devotes itself to\ninvestigating the influence of conflicts in the widely used CL strategy in\nODPR, motivated by our observation that a passage can be organized by multiple\nsemantically different sentences, thus modeling such a passage as a unified\ndense vector is not optimal. We call such conflicts Contrastive Conflicts. In\nthis work, we propose to solve it with a representation decoupling method, by\ndecoupling the passage representations into contextual sentence-level ones, and\ndesign specific CL strategies to mediate these conflicts. Experiments on widely\nused datasets including Natural Questions, Trivia QA, and SQuAD verify the\neffectiveness of our method, especially on the dataset where the conflicting\nproblem is severe. Our method also presents good transferability across the\ndatasets, which further supports our idea of mediating Contrastive Conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bohong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Irrationality of Neural Rationale Models. (arXiv:2110.07550v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07550","description":"<p>Neural rationale models are popular for interpretable predictions of NLP\ntasks. In these, a selector extracts segments of the input text, called\nrationales, and passes these segments to a classifier for prediction. Since the\nrationale is the only information accessible to the classifier, it is plausibly\ndefined as the explanation. Is such a characterization unconditionally correct?\nIn this paper, we argue to the contrary, with both philosophical perspectives\nand empirical evidence suggesting that rationale models are, perhaps, less\nrational and interpretable than expected. We call for more rigorous and\ncomprehensive evaluations of these models to ensure desired properties of\ninterpretability are indeed achieved. The code can be found at\nhttps://github.com/yimingz89/Neural-Rationale-Analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yiming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Booth_S/0/1/0/all/0/1\">Serena Booth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yilun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BI-RADS BERT & Using Section Tokenization to Understand Radiology Reports. (arXiv:2110.07552v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07552","description":"<p>Radiology reports are the main form of communication between radiologists and\nother clinicians, and contain important information for patient care. However\nin order to use this information for research it is necessary to convert the\nraw text into structured data suitable for analysis. Domain specific contextual\nword embeddings have been shown to achieve impressive accuracy at such natural\nlanguage processing tasks in medicine. In this work we pre-trained a contextual\nembedding BERT model using breast radiology reports and developed a classifier\nthat incorporated the embedding with auxiliary global textual features in order\nto perform a section tokenization task. This model achieved a 98% accuracy at\nsegregating free text reports into sections of information outlined in the\nBreast Imaging Reporting and Data System (BI-RADS) lexicon, a significant\nimprovement over the Classic BERT model without auxiliary information. We then\nevaluated whether using section tokenization improved the downstream extraction\nof the following fields: modality/procedure, previous cancer, menopausal\nstatus, purpose of exam, breast density and background parenchymal enhancement.\nUsing the BERT model pre-trained on breast radiology reports combined with\nsection tokenization resulted in an overall accuracy of 95.9% in field\nextraction. This is a 17% improvement compared to an overall accuracy of 78.9%\nfor field extraction for models without section tokenization and with Classic\nBERT embeddings. Our work shows the strength of using BERT in radiology report\nanalysis and the advantages of section tokenization in identifying key features\nof patient factors recorded in breast radiology reports.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuling_G/0/1/0/all/0/1\">Grey Kuling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curpen_D/0/1/0/all/0/1\">Dr. Belinda Curpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Composable Sparse Fine-Tuning for Cross-Lingual Transfer. (arXiv:2110.07560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07560","description":"<p>Fine-tuning all parameters of a pre-trained model has become the mainstream\napproach for transfer learning. To increase its efficiency and prevent\ncatastrophic forgetting and interference, techniques like adapters and sparse\nfine-tuning have been developed. Adapters are modular, as they can be combined\nto adapt a model towards different facets of knowledge (e.g., dedicated\nlanguage and/or task adapters). Sparse fine-tuning is expressive, as it\ncontrols the behavior of all model components. In this work, we introduce a new\nfine-tuning method with both these desirable properties. In particular, we\nlearn sparse, real-valued masks based on a simple variant of the Lottery Ticket\nHypothesis. Task-specific masks are obtained from annotated data in a source\nlanguage, and language-specific masks from masked language modeling in a target\nlanguage. Both these masks can then be composed with the pre-trained model.\nUnlike adapter-based fine-tuning, this method neither increases the number of\nparameters at inference time nor alters the original model architecture. Most\nimportantly, it outperforms adapters in zero-shot cross-lingual transfer by a\nlarge margin in a series of multilingual benchmarks, including Universal\nDependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we\nadditionally find that sparsity is crucial to prevent both 1) interference\nbetween the fine-tunings to be composed and 2) overfitting. We release the code\nand models at https://github.com/cambridgeltl/composable-sft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansell_A/0/1/0/all/0/1\">Alan Ansell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Benefits of Feature Feedback Under Distribution Shift. (arXiv:2110.07566v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07566","description":"<p>In attempts to develop sample-efficient algorithms, researcher have explored\nmyriad mechanisms for collecting and exploiting feature feedback, auxiliary\nannotations provided for training (but not test) instances that highlight\nsalient evidence. Examples include bounding boxes around objects and salient\nspans in text. Despite its intuitive appeal, feature feedback has not delivered\nsignificant gains in practical problems as assessed on iid holdout sets.\nHowever, recent works on counterfactually augmented data suggest an alternative\nbenefit of supplemental annotations: lessening sensitivity to spurious patterns\nand consequently delivering gains in out-of-domain evaluations. Inspired by\nthese findings, we hypothesize that while the numerous existing methods for\nincorporating feature feedback have delivered negligible in-sample gains, they\nmay nevertheless generalize better out-of-domain. In experiments addressing\nsentiment analysis, we show that feature feedback methods perform significantly\nbetter on various natural out-of-domain datasets even absent differences on\nin-domain evaluation. By contrast, on natural language inference tasks,\nperformance remains comparable. Finally, we compare those tasks where feature\nfeedback does (and does not) help.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Katakkar_A/0/1/0/all/0/1\">Anurag Katakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1\">Clay H. Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_D/0/1/0/all/0/1\">Divyansh Kaushik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAGr: Labeling Aligned Graphs for Improving Systematic Generalization in Semantic Parsing. (arXiv:2110.07572v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07572","description":"<p>Semantic parsing is the task of producing a structured meaning representation\nfor natural language utterances or questions. Recent research has pointed out\nthat the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle\nto generalize systematically, i.e. to handle examples that require recombining\nknown knowledge in novel settings. In this work, we show that better systematic\ngeneralization can be achieved by producing the meaning representation (MR)\ndirectly as a graph and not as a sequence. To this end we propose LAGr, the\nLabeling Aligned Graphs algorithm that produces semantic parses by predicting\nnode and edge labels for a complete multi-layer input-aligned graph. The\nstrongly-supervised LAGr algorithm requires aligned graphs as inputs, whereas\nweakly-supervised LAGr infers alignments for originally unaligned target graphs\nusing an approximate MAP inference procedure. On the COGS and CFQ compositional\ngeneralization benchmarks the strongly- and weakly- supervised LAGr algorithms\nachieve significant improvements upon the baseline seq2seq parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jambor_D/0/1/0/all/0/1\">Dora Jambor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delphi: Towards Machine Ethics and Norms. (arXiv:2110.07574v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07574","description":"<p>What would it take to teach a machine to behave ethically? While broad\nethical rules may seem straightforward to state (\"thou shalt not kill\"),\napplying such rules to real-world situations is far more complex. For example,\nwhile \"helping a friend\" is generally a good thing to do, \"helping a friend\nspread fake news\" is not. We identify four underlying challenges towards\nmachine ethics and norms: (1) an understanding of moral precepts and social\nnorms; (2) the ability to perceive real-world situations visually or by reading\nnatural language descriptions; (3) commonsense reasoning to anticipate the\noutcome of alternative actions in different contexts; (4) most importantly, the\nability to make ethical judgments given the interplay between competing values\nand their grounding in different contexts (e.g., the right to freedom of\nexpression vs. preventing the spread of fake news).\n</p>\n<p>Our paper begins to address these questions within the deep learning\nparadigm. Our prototype model, Delphi, demonstrates strong promise of\nlanguage-based commonsense moral reasoning, with up to 92.1% accuracy vetted by\nhumans. This is in stark contrast to the zero-shot performance of GPT-3 of\n52.3%, which suggests that massive scale alone does not endow pre-trained\nneural language models with human values. Thus, we present Commonsense Norm\nBank, a moral textbook customized for machines, which compiles 1.7M examples of\npeople's ethical judgments on a broad spectrum of everyday situations. In\naddition to the new resources and baseline performances for future research,\nour study provides new insights that lead to several important open research\nquestions: differentiating between universal human values and personal values,\nmodeling different moral frameworks, and explainable, consistent approaches to\nmachine ethics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Liwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_M/0/1/0/all/0/1\">Maxwell Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borchardt_J/0/1/0/all/0/1\">Jon Borchardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jenny Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etzioni_O/0/1/0/all/0/1\">Oren Etzioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset. (arXiv:2110.07575v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07575","description":"<p>Visually-grounded spoken language datasets can enable models to learn\ncross-modal correspondences with very weak supervision. However, modern\naudio-visual datasets contain biases that undermine the real-world performance\nof models trained on that data. We introduce Spoken ObjectNet, which is\ndesigned to remove some of these biases and provide a way to better evaluate\nhow effectively models will perform in real-world scenarios. This dataset\nexpands upon ObjectNet, which is a bias-controlled image dataset that features\nsimilar image classes to those present in ImageNet. We detail our data\ncollection pipeline, which features several methods to improve caption quality,\nincluding automated language model checks. Lastly, we show baseline results on\nimage retrieval and audio retrieval tasks. These results show that models\ntrained on other datasets and then evaluated on Spoken ObjectNet tend to\nperform poorly due to biases in other datasets that the models have learned. We\nalso show evidence that the performance decrease is due to the dataset\ncontrols, and not the transfer setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palmer_I/0/1/0/all/0/1\">Ian Palmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbu_A/0/1/0/all/0/1\">Andrei Barbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_B/0/1/0/all/0/1\">Boris Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. (arXiv:2110.07577v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07577","description":"<p>Conventional fine-tuning of pre-trained language models tunes all model\nparameters and stores a full model copy for each downstream task, which has\nbecome increasingly infeasible as the model size grows larger. Recent\nparameter-efficient language model tuning (PELT) methods manage to match the\nperformance of fine-tuning with much fewer trainable parameters and perform\nespecially well when the training data is limited. However, different PELT\nmethods may perform rather differently on the same task, making it nontrivial\nto select the most appropriate method for a specific task, especially\nconsidering the fast-growing number of new PELT methods and downstream tasks.\nIn light of model diversity and the difficulty of model selection, we propose a\nunified framework, UniPELT, which incorporates different PELT methods as\nsubmodules and learns to activate the ones that best suit the current data or\ntask setup. Remarkably, on the GLUE benchmark, UniPELT consistently achieves\n1~3pt gains compared to the best individual PELT method that it incorporates\nand even outperforms fine-tuning under different setups. Moreover, UniPELT\noften surpasses the upper bound when taking the best performance of all its\nsubmodules used individually on each task, indicating that a mixture of\nmultiple PELT methods may be inherently more effective than single methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_R/0/1/0/all/0/1\">Rui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almahairi_A/0/1/0/all/0/1\">Amjad Almahairi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations. (arXiv:2110.07581v1 [cs.IR])","link":"http://arxiv.org/abs/2110.07581","description":"<p>Dense retrieval (DR) methods conduct text retrieval by first encoding texts\nin the embedding space and then matching them by nearest neighbor search. This\nrequires strong locality properties from the representation space, i.e, the\nclose allocations of each small group of relevant texts, which are hard to\ngeneralize to domains without sufficient training data. In this paper, we aim\nto improve the generalization ability of DR models from source training domains\nwith rich supervision signals to target domains without any relevant labels, in\nthe zero-shot setting. To achieve that, we propose Momentum adversarial Domain\nInvariant Representation learning (MoDIR), which introduces a momentum method\nin the DR training process to train a domain classifier distinguishing source\nversus target, and then adversarially updates the DR encoder to learn domain\ninvariant representations. Our experiments show that MoDIR robustly outperforms\nits baselines on 10+ ranking datasets from the BEIR benchmark in the zero-shot\nsetup, with more than 10% relative gains on datasets with enough sensitivity\nfor DR models' evaluation. Source code of this paper will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Ji Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_A/0/1/0/all/0/1\">Ashwin Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ankita Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jose_D/0/1/0/all/0/1\">Damien Jose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Explanations Be Useful for Calibrating Black Box Models?. (arXiv:2110.07586v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07586","description":"<p>One often wants to take an existing, trained NLP model and use it on data\nfrom a new domain. While fine-tuning or few-shot learning can be used to adapt\nthe base model, there is no one simple recipe to getting these working;\nmoreover, one may not have access to the original model weights if it is\ndeployed as a black box. To this end, we study how to improve a black box\nmodel's performance on a new domain given examples from the new domain by\nleveraging explanations of the model's behavior. Our approach first extracts a\nset of features combining human intuition about the task with model\nattributions generated by black box interpretation techniques, and then uses a\nsimple model to calibrate or rerank the model's predictions based on the\nfeatures. We experiment with our method on two tasks, extractive question\nanswering and natural language inference, covering adaptation from several\npairs of domains. The experimental results across all the domain pairs show\nthat explanations are useful for calibrating these models. We show that the\ncalibration features transfer to some extent between tasks and shed light on\nhow to effectively use them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Toxicity Analysis: A New Spoken Language Processing Task. (arXiv:2110.07592v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07592","description":"<p>Toxic speech, also known as hate speech, is regarded as one of the crucial\nissues plaguing online social media today. Most recent work on toxic speech\ndetection is constrained to the modality of text with no existing work on\ntoxicity detection from spoken utterances. In this paper, we propose a new\nSpoken Language Processing task of detecting toxicity from spoken speech. We\nintroduce DeToxy, the first publicly available toxicity annotated dataset for\nEnglish speech, sourced from various openly available speech databases,\nconsisting of over 2 million utterances. Finally, we also provide analysis on\nhow a spoken speech corpus annotated for toxicity can help facilitate the\ndevelopment of E2E models which better capture various prosodic cues in speech,\nthereby boosting toxicity classification on spoken utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepcha_S/0/1/0/all/0/1\">Samden Lepcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakshi_S/0/1/0/all/0/1\">S Sakshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compressibility of Distributed Document Representations. (arXiv:2110.07595v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07595","description":"<p>Contemporary natural language processing (NLP) revolves around learning from\nlatent document representations, generated either implicitly by neural language\nmodels or explicitly by methods such as doc2vec or similar. One of the key\nproperties of the obtained representations is their dimension. Whilst the\ncommonly adopted dimensions of 256 and 768 offer sufficient performance on many\ntasks, it is many times unclear whether the default dimension is the most\nsuitable choice for the subsequent downstream learning tasks. Furthermore,\nrepresentation dimensions are seldom subject to hyperparameter tuning due to\ncomputational constraints. The purpose of this paper is to demonstrate that a\nsurprisingly simple and efficient recursive compression procedure can be\nsufficient to both significantly compress the initial representation, but also\npotentially improve its performance when considering the task of text\nclassification. Having smaller and less noisy representations is the desired\nproperty during deployment, as orders of magnitude smaller models can\nsignificantly reduce the computational overload and with it the deployment\ncosts. We propose CoRe, a straightforward, representation learner-agnostic\nframework suitable for representation compression. The CoRe's performance is\nshowcased and studied on a collection of 17 real-life corpora from biomedical,\nnews, social media, and literary domains. We explored CoRe's behavior when\nconsidering contextual and non-contextual document representations, different\ncompression levels, and 9 different compression algorithms. Current results\nbased on more than 100,000 compression experiments indicate that recursive\nSingular Value Decomposition offers a very good trade-off between the\ncompression efficiency and performance, making CoRe useful in many existing,\nrepresentation-dependent NLP pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_M/0/1/0/all/0/1\">Matej Petkovi&#x10d;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-guided Counterfactual Generation for QA. (arXiv:2110.07596v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07596","description":"<p>Deep NLP models have been shown to learn spurious correlations, leaving them\nbrittle to input perturbations. Recent work has shown that counterfactual or\ncontrastive data -- i.e. minimally perturbed inputs -- can reveal these\nweaknesses, and that data augmentation using counterfactuals can help\nameliorate them. Proposed techniques for generating counterfactuals rely on\nhuman annotations, perturbations based on simple heuristics, and meaning\nrepresentation frameworks. We focus on the task of creating counterfactuals for\nquestion answering, which presents unique challenges related to world\nknowledge, semantic diversity, and answerability. To address these challenges,\nwe develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual\nevaluation and training data with minimal human supervision. Using an\nopen-domain QA framework and question generation model trained on original task\ndata, we create counterfactuals that are fluent, semantically diverse, and\nautomatically labeled. Data augmentation with RGF counterfactuals improves\nperformance on out-of-domain and challenging evaluation sets over and above\nexisting methods, in both the reading comprehension and open-domain QA\nsettings. Moreover, we find that RGF data leads to significant improvements in\na model's robustness to local perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_B/0/1/0/all/0/1\">Bhargavi Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamm_M/0/1/0/all/0/1\">Matthew Lamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (arXiv:2110.07602v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07602","description":"<p>Prompt tuning, which only tunes continuous prompts with a frozen language\nmodel, substantially reduces per-task storage and memory usage at training.\nHowever, in the context of NLU, prior work and our results reveal that existing\nmethods of prompt tuning do not perform well for normal-sized pre-trained\nmodels and for hard sequence tasks, indicating lack of universality. We present\na novel empirical finding that properly-optimized prompt tuning can be\nuniversally effective across a wide range of model scales and NLU tasks, where\nit matches the performance of fine-tuning while having only 0.1\\%-3\\% tuned\nparameters. Our method P-Tuning v2 is not a new method but a version of\nprefix-tuning \\cite{li2021prefix} optimized and adapted for NLU. Given the\nuniversality and simplicity of P-Tuning v2, we believe it can serve as an\nalternative for fine-tuning and a strong baseline for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1\">Kaixuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yicheng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhengxiao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-word Level Lip Reading With Visual Attention. (arXiv:2110.07603v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07603","description":"<p>The goal of this paper is to learn strong lip reading models that can\nrecognise speech in silent videos. Most prior works deal with the open-set\nvisual speech recognition problem by adapting existing automatic speech\nrecognition techniques on top of trivially pooled visual features. Instead, in\nthis paper we focus on the unique challenges encountered in lip reading and\npropose tailored solutions. To that end we make the following contributions:\n(1) we propose an attention-based pooling mechanism to aggregate visual speech\nrepresentations; (2) we use sub-word units for lip reading for the first time\nand show that this allows us to better model the ambiguities of the task; (3)\nwe propose a training pipeline that balances the lip reading performance with\nother key factors such as data and compute efficiency. Following the above, we\nobtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks\nwhen training on public datasets, and even surpass models trained on\nlarge-scale industrial datasets by using an order of magnitude less data. Our\nbest model achieves 22.6% word error rate on the LRS2 dataset, a performance\nunprecedented for lip reading models, significantly reducing the performance\ngap between lip reading and automatic speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+R_P/0/1/0/all/0/1\">Prajwal K R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1\">Triantafyllos Afouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Political Text Scaling Meets Computational Semantics. (arXiv:1904.06217v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1904.06217","description":"<p>During the last fifteen years, automatic text scaling has become one of the\nkey tools of the Text as Data community in political science. Prominent text\nscaling algorithms, however, rely on the assumption that latent positions can\nbe captured just by leveraging the information about word frequencies in\ndocuments under study. We challenge this traditional view and present a new,\nsemantically aware text scaling algorithm, SemScale, which combines recent\ndevelopments in the area of computational linguistics with unsupervised\ngraph-based clustering. We conduct an extensive quantitative analysis over a\ncollection of speeches from the European Parliament in five different languages\nand from two different legislative terms, and show that a scaling approach\nrelying on semantic document representations is often better at capturing known\nunderlying political dimensions than the established frequency-based (i.e.,\nsymbolic) scaling method. We further validate our findings through a series of\nexperiments focused on text preprocessing and feature selection, document\nrepresentation, scaling of party manifestos, and a supervised extension of our\nalgorithm. To catalyze further research on this new branch of text scaling\nmethods, we release a Python implementation of SemScale with all included data\nsets and evaluation procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nanni_F/0/1/0/all/0/1\">Federico Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glavas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehbein_I/0/1/0/all/0/1\">Ines Rehbein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuckenschmidt_H/0/1/0/all/0/1\">Heiner Stuckenschmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAS: An Answer Selection Method Using BERT Language Model. (arXiv:1911.01528v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.01528","description":"<p>In recent years, Question Answering systems have become more popular and\nwidely used by users. Despite the increasing popularity of these systems, the\ntheir performance is not even sufficient for textual data and requires further\nresearch. These systems consist of several parts that one of them is the Answer\nSelection component. This component detects the most relevant answer from a\nlist of candidate answers. The methods presented in previous researches have\nattempted to provide an independent model to undertake the answer-selection\ntask. An independent model cannot comprehend the syntactic and semantic\nfeatures of questions and answers with a small training dataset. To fill this\ngap, language models can be employed in implementing the answer selection part.\nThis action enables the model to have a better understanding of the language in\norder to understand questions and answers better than previous works. In this\nresearch, we will present the \"BAS\" (BERT Answer Selection) that uses the BERT\nlanguage model to comprehend language. The empirical results of applying the\nmodel on the TrecQA Raw, TrecQA Clean, and WikiQA datasets demonstrate that\nusing a robust language model such as BERT can enhance the performance. Using a\nmore robust classifier also enhances the effect of the language model on the\nanswer selection component. The results demonstrate that language comprehension\nis an essential requirement in natural language processing tasks such as\nanswer-selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozafari_J/0/1/0/all/0/1\">Jamshid Mozafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatemi_A/0/1/0/all/0/1\">Afsaneh Fatemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematbakhsh_M/0/1/0/all/0/1\">Mohammad Ali Nematbakhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Deep Neural Networks. (arXiv:2010.01496v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01496","description":"<p>Deep neural networks are becoming more and more popular due to their\nrevolutionary success in diverse areas, such as computer vision, natural\nlanguage processing, and speech recognition. However, the decision-making\nprocesses of these models are generally not interpretable to users. In various\ndomains, such as healthcare, finance, or law, it is critical to know the\nreasons behind a decision made by an artificial intelligence system. Therefore,\nseveral directions for explaining neural models have recently been explored. In\nthis thesis, I investigate two major directions for explaining deep neural\nnetworks. The first direction consists of feature-based post-hoc explanatory\nmethods, that is, methods that aim to explain an already trained and fixed\nmodel (post-hoc), and that provide explanations in terms of input features,\nsuch as tokens for text and superpixels for images (feature-based). The second\ndirection consists of self-explanatory neural models that generate natural\nlanguage explanations, that is, models that have a built-in module that\ngenerates explanations for the predictions of the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering. (arXiv:2010.12643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12643","description":"<p>Coupled with the availability of large scale datasets, deep learning\narchitectures have enabled rapid progress on the Question Answering task.\nHowever, most of those datasets are in English, and the performances of\nstate-of-the-art multilingual models are significantly lower when evaluated on\nnon-English data. Due to high data collection costs, it is not realistic to\nobtain annotated data for each language one desires to support.\n</p>\n<p>We propose a method to improve the Cross-lingual Question Answering\nperformance without requiring additional annotated data, leveraging Question\nGeneration models to produce synthetic samples in a cross-lingual fashion. We\nshow that the proposed method allows to significantly outperform the baselines\ntrained on English data only. We report a new state-of-the-art on four\nmultilingual datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riabi_A/0/1/0/all/0/1\">Arij Riabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keraron_R/0/1/0/all/0/1\">Rachel Keraron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seddah_D/0/1/0/all/0/1\">Djam&#xe9; Seddah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-the-Fly Attention Modulation for Neural Generation. (arXiv:2101.00371v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00371","description":"<p>Despite considerable advancements with deep neural language models (LMs),\nneural text generation still suffers from degeneration: the generated text is\nrepetitive, generic, self-contradictory, and often lacks commonsense. Our\nanalyses on sentence-level attention patterns in LMs reveal that neural\ndegeneration may be associated with insufficient learning of task-specific\ncharacteristics by the attention mechanism. This finding motivates on-the-fly\nattention modulation -- a simple but effective method that enables the\ninjection of priors into attention computation during inference. Automatic and\nhuman evaluation results on three text generation benchmarks demonstrate that\nattention modulation helps LMs generate text with enhanced fluency, creativity,\nand commonsense reasoning, in addition to significantly reduce sentence-level\nrepetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Characterizing Partisan Political Narrative Frameworks about COVID-19 on Twitter. (arXiv:2103.06960v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.06960","description":"<p>The COVID-19 pandemic is a global crisis that has been testing every society\nand exposing the critical role of local politics in crisis response. In the\nUnited States, there has been a strong partisan divide between the Democratic\nand Republican party's narratives about the pandemic which resulted in\npolarization of individual behaviors and divergent policy adoption across\nregions. As shown in this case, as well as in most major social issues,\nstrongly polarized narrative frameworks facilitate such narratives. To\nunderstand polarization and other social chasms, it is critical to dissect\nthese diverging narratives. Here, taking the Democratic and Republican\npolitical social media posts about the pandemic as a case study, we demonstrate\nthat a combination of computational methods can provide useful insights into\nthe different contexts, framing, and characters and relationships that\nconstruct their narrative frameworks which individual posts source from.\nLeveraging a dataset of tweets from elite politicians in the U.S., we found\nthat the Democrats' narrative tends to be more concerned with the pandemic as\nwell as financial and social support, while the Republicans discuss more about\nother political entities such as China. We then perform an automatic framing\nanalysis to characterize the ways in which they frame their narratives, where\nwe found that the Democrats emphasize the government's role in responding to\nthe pandemic, and the Republicans emphasize the roles of individuals and\nsupport for small businesses. Finally, we present a semantic role analysis that\nuncovers the important characters and relationships in their narratives as well\nas how they facilitate a membership categorization process. Our findings\nconcretely expose the gaps in the \"elusive consensus\" between the two parties.\nOur methodologies may be applied to computationally study narratives in various\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_E/0/1/0/all/0/1\">Elise Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1\">Yong-Yeol Ahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Misinfo Reaction Frames: Reasoning about Readers' Reactions to News Headlines. (arXiv:2104.08790v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08790","description":"<p>Even to a simple and short news headline, readers react in a multitude of\nways: cognitively (e.g., inferring the writer's intent), emotionally (e.g.,\nfeeling distrust), and behaviorally (e.g., sharing the news with their\nfriends). Such reactions are instantaneous and yet complex, as they rely on\nfactors that go beyond interpreting the factual content the news headline.\nInstead, understanding reactions require pragmatic understanding of the news\nheadline, including broader background knowledge about contentious news topics\nas well as commonsense reasoning about people's intents and emotional\nreactions. We propose Misinfo Reaction Frames, a pragmatic formalism for\nmodeling how readers might react to a news headline cognitively, emotionally,\nand behaviorally. We also introduce a Misinfo Reaction Frames corpus, a dataset\nof over 200k news headline annotated with crowdsourced reactions focusing on\nglobal crises: the Covid-19 pandemic, climate change, and cancer. Empirical\nresults confirm that it is indeed possible to learn the prominent patterns of\nreaders' reactions to news headlines. We also find a potentially positive use\ncase of our model; When we present our model generated inferences to people, we\nfind that the machine inferences can increase readers' trust in real news while\ndecreasing their trust in misinformation. Our work demonstrates the feasibility\nand the importance of pragmatic inferences of news to help enhance AI-guided\nmisinformation detection and mitigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1\">Saadia Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallinan_S/0/1/0/all/0/1\">Skyler Hallinan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Pemi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roesner_F/0/1/0/all/0/1\">Franziska Roesner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Voice Activity Detection: Hybrid Audio Segmentation for Direct Speech Translation. (arXiv:2104.11710v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2104.11710","description":"<p>The audio segmentation mismatch between training data and those seen at\nrun-time is a major problem in direct speech translation. Indeed, while systems\nare usually trained on manually segmented corpora, in real use cases they are\noften presented with continuous audio requiring automatic (and sub-optimal)\nsegmentation. After comparing existing techniques (VAD-based, fixed-length and\nhybrid segmentation methods), in this paper we propose enhanced hybrid\nsolutions to produce better results without sacrificing latency. Through\nexperiments on different domains and language pairs, we show that our methods\noutperform all the other techniques, reducing by at least 30% the gap between\nthe traditional VAD-based approach and optimal manual segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cettolo_M/0/1/0/all/0/1\">Mauro Cettolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Emotions in Hindi-English Code Mixed Text Data. (arXiv:2105.09226v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.09226","description":"<p>In recent times, we have seen an increased use of text chat for communication\non social networks and smartphones. This particularly involves the use of\nHindi-English code-mixed text which contains words which are not recognized in\nEnglish vocabulary. We have worked on detecting emotions in these mixed data\nand classify the sentences in human emotions which are angry, fear, happy or\nsad. We have used state of the art natural language processing models and\ncompared their performance on the dataset comprising sentences in this mixed\ndata. The dataset was collected and annotated from sources and then used to\ntrain the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_D/0/1/0/all/0/1\">Divyansh Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RobeCzech: Czech RoBERTa, a monolingual contextualized language representation model. (arXiv:2105.11314v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11314","description":"<p>We present RobeCzech, a monolingual RoBERTa language representation model\ntrained on Czech data. RoBERTa is a robustly optimized Transformer-based\npretraining approach. We show that RobeCzech considerably outperforms\nequally-sized multilingual and Czech-trained contextualized language\nrepresentation models, surpasses current state of the art in all five evaluated\nNLP tasks and reaches state-of-the-art results in four of them. The RobeCzech\nmodel is released publicly at https://hdl.handle.net/11234/1-3691 and\nhttps://huggingface.co/ufal/robeczech-base.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Straka_M/0/1/0/all/0/1\">Milan Straka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naplava_J/0/1/0/all/0/1\">Jakub N&#xe1;plava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strakova_J/0/1/0/all/0/1\">Jana Strakov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Stable Classifiers by Transferring Unstable Features. (arXiv:2106.07847v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07847","description":"<p>While unbiased machine learning models are essential for many applications,\nbias is a human-defined concept that can vary across tasks. Given only\ninput-label pairs, algorithms may lack sufficient information to distinguish\nstable (causal) features from unstable (spurious) features. However, related\ntasks often share similar biases -- an observation we may leverage to develop\nstable classifiers in the transfer setting. In this work, we explicitly inform\nthe target classifier about unstable features in the source tasks.\nSpecifically, we derive a representation that encodes the unstable features by\ncontrasting different data environments in the source task. We achieve\nrobustness by clustering data of the target task according to this\nrepresentation and minimizing the worst-case risk across these clusters. We\nevaluate our method on both text and image classifications. Empirical results\ndemonstrate that our algorithm is able to maintain robustness on the target\ntask, outperforming the best baseline by 22.9% in absolute accuracy across 12\ntransfer settings. Our code is available at https://github.com/YujiaBao/Tofu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yujia Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConveRT for FAQ Answering. (arXiv:2108.00719v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00719","description":"<p>Knowledgeable FAQ chatbots are a valuable resource to any organization. While\npowerful and efficient retrieval-based models exist for English, it is rarely\nthe case for other languages for which the same amount of training data is not\navailable. In this paper, we propose a novel pre-training procedure to adapt\nConveRT, an English conversational retriever model, to other languages with\nless training data available. We apply it for the first time to the task of\nDutch FAQ answering related to the COVID-19 vaccine. We show it performs better\nthan an open-source alternative in both a low-data regime and a high-data\nregime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1\">Maxime De Bruyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1\">Ehsan Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1\">Jeska Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1\">Walter Daelemans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13741","description":"<p>Recent researches have demonstrated that BERT shows potential in a wide range\nof natural language processing tasks. It is adopted as an encoder for many\nstate-of-the-art automatic summarizing systems, which achieve excellent\nperformance. However, so far, there is not much work done for Vietnamese. In\nthis paper, we showcase how BERT can be implemented for extractive text\nsummarization in Vietnamese on multi-document. We introduce a novel comparison\nbetween different multilingual and monolingual BERT models. The experiment\nresults indicate that monolingual models produce promising results compared to\nother multilingual models and previous text summarizing models for Vietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+To_H/0/1/0/all/0/1\">Huy Quoc To</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VoicePrivacy 2020 Challenge: Results and findings. (arXiv:2109.00648v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00648","description":"<p>This paper presents the results and analyses stemming from the first\nVoicePrivacy 2020 Challenge which focuses on developing anonymization solutions\nfor speech technology. We provide a systematic overview of the challenge design\nwith an analysis of submitted systems and evaluation results. In particular, we\ndescribe the voice anonymization task and datasets used for system development\nand evaluation. Also, we present different attack models and the associated\nobjective and subjective evaluation metrics. We introduce two anonymization\nbaselines and provide a summary description of the anonymization systems\ndeveloped by the challenge participants. We report objective and subjective\nevaluation results for baseline and submitted systems. In addition, we present\nexperimental results for alternative privacy metrics and attack models\ndeveloped as a part of the post-evaluation analysis. Finally, we summarize our\ninsights and observations that will influence the design of the next\nVoicePrivacy challenge edition and some directions for future voice\nanonymization research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Brij Mohan Lal Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noe_P/0/1/0/all/0/1\">Paul-Gauthier No&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1\">Andreas Nautsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_B/0/1/0/all/0/1\">Benjamin O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanclu_A/0/1/0/all/0/1\">Ana&#xef;s Chanclu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonastre_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bonastre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maouche_M/0/1/0/all/0/1\">Mohamed Maouche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration. (arXiv:2109.06304v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06304","description":"<p>Phrase representations derived from BERT often do not exhibit complex phrasal\ncompositionality, as the model relies instead on lexical similarity to\ndetermine semantic relatedness. In this paper, we propose a contrastive\nfine-tuning objective that enables BERT to produce more powerful phrase\nembeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal\nparaphrases, which is automatically generated using a paraphrase generation\nmodel, as well as a large-scale dataset of phrases in context mined from the\nBooks3 corpus. Phrase-BERT outperforms baselines across a variety of\nphrase-level similarity tasks, while also demonstrating increased lexical\ndiversity between nearest neighbors in the vector space. Finally, as a case\nstudy, we show that Phrase-BERT embeddings can be easily integrated with a\nsimple autoencoder to build a phrase-based neural topic model that interprets\ntopics as mixtures of words and phrases by performing a nearest neighbor search\nin the embedding space. Crowdsourced evaluations demonstrate that this\nphrase-based topic model produces more coherent and meaningful topics than\nbaseline word and phrase-level topic models, further validating the utility of\nPhrase-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shufan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_L/0/1/0/all/0/1\">Laure Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. (arXiv:2109.10604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10604","description":"<p>While diverse question answering (QA) datasets have been proposed and\ncontributed significantly to the development of deep learning models for QA\ntasks, the existing datasets fall short in two aspects. First, we lack QA\ndatasets covering complex questions that involve answers as well as the\nreasoning processes to get the answers. As a result, the state-of-the-art QA\nresearch on numerical reasoning still focuses on simple calculations and does\nnot provide the mathematical expressions or evidences justifying the answers.\nSecond, the QA community has contributed much effort to improving the\ninterpretability of QA models. However, these models fail to explicitly show\nthe reasoning process, such as the evidence order for reasoning and the\ninteractions between different pieces of evidence. To address the above\nshortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset\nwith questions requiring numerical reasoning with compound mathematical\nexpressions. With NOAHQA, we develop an interpretable reasoning graph as well\nas the appropriate evaluation metric to measure the answer quality. We evaluate\nthe state-of-the-art QA models trained using existing QA datasets on NOAHQA and\nshow that the best among them can only achieve 55.5 exact match scores, while\nthe human performance is 89.7. We also present a new QA model for generating a\nreasoning graph where the reasoning graph metric still has a large gap compared\nwith that of humans, e.g., 28 scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sicheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking BERT: Understanding its Vulnerabilities for Biomedical Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11308","description":"<p>Biomedical named entity recognition (NER) is a key task in the extraction of\ninformation from biomedical literature and electronic health records. For this\ntask, both generic and biomedical BERT models are widely used. Robustness of\nthese models is vital for medical applications, such as automated medical\ndecision making. In this paper we investigate the vulnerability of BERT models\nto variation in input data for NER through adversarial attack. Since\nadversarial attack methods for NER are sparse, we propose two black-box methods\nfor NER based on existing methods for classification tasks. Experimental\nresults show that the original as well as the biomedical BERT models are highly\nvulnerable to entity replacement: They can be fooled in 89.2 to 99.4% of the\ncases to mislabel previously correct entities. BERT models are also vulnerable\nto variation in the entity context with 20.2 to 45.0% of entities predicted\ncompletely wrong and another 29.3 to 53.3% of entities predicted wrong\npartially. Often a single change is sufficient to fool the model. BERT models\nseem most vulnerable to changes in the local context of entities. Of the\nbiomedical BERT models, the vulnerability of BioBERT is comparable to the\noriginal BERT model whereas SciBERT is even more vulnerable. Our results chart\nthe vulnerabilities of BERT models for biomedical NER and emphasize the\nimportance of further research into uncovering and reducing these weaknesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dirkson_A/0/1/0/all/0/1\">Anne Dirkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraaij_W/0/1/0/all/0/1\">Wessel Kraaij</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11728","description":"<p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively\nused by states and language testing agencies alike to evaluate millions of\ncandidates for life-changing decisions ranging from college applications to\nvisa approvals. However, little research has been put to understand and\ninterpret the black-box nature of deep-learning based scoring algorithms.\nPrevious studies indicate that scoring models can be easily fooled. In this\npaper, we explore the reason behind their surprising adversarial brittleness.\nWe utilize recent advances in interpretability to find the extent to which\nfeatures such as coherence, content, vocabulary, and relevance are important\nfor automated scoring mechanisms. We use this to investigate the\noversensitivity i.e., large change in output score with a little change in\ninput essay content) and overstability i.e., little change in output scores\nwith large changes in input essay content) of AES. Our results indicate that\nautoscoring models, despite getting trained as \"end-to-end\" models with rich\ncontextual embeddings such as BERT, behave like bag-of-words models. A few\nwords determine the essay score without the requirement of any context making\nthe model largely overstable. This is in stark contrast to recent probing\nstudies on pre-trained representation learning models, which show that rich\nlinguistic features such as parts-of-speech and morphology are encoded by them.\nFurther, we also find that the models have learnt dataset biases, making them\noversensitive. To deal with these issues, we propose detection-based protection\nmodels that can detect oversensitivity and overstability causing samples with\nhigh accuracies. We find that our proposed models are able to detect unusual\nattribution patterns and flag adversarial samples successfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Somesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual AMR Parsing with Noisy Knowledge Distillation. (arXiv:2109.15196v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.15196","description":"<p>We study multilingual AMR parsing from the perspective of knowledge\ndistillation, where the aim is to learn and improve a multilingual AMR parser\nby using an existing English parser as its teacher. We constrain our\nexploration in a strict multilingual setting: there is but one model to parse\nall different languages including English. We identify that noisy input and\nprecise output are the key to successful distillation. Together with extensive\npre-training, we obtain an AMR parser whose performances surpass all previously\npublished results on four different foreign languages, including German,\nSpanish, Italian, and Chinese, by large margins (up to 18.8 \\textsc{Smatch}\npoints on Chinese and on average 11.3 \\textsc{Smatch} points). Our parser also\nachieves comparable performance on English to the latest state-of-the-art\nEnglish-only parser.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jackie Chun-Sing Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning. (arXiv:2110.00678v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.00678","description":"<p>To address the performance gap of English ASR models on L2 English speakers,\nwe evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;\nXu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,\n2018) under different training settings. We compare \\textbf{(a)} models trained\nwith a combination of diverse accents to ones trained with only specific\naccents and \\textbf{(b)} results from different single-accent models. Our\nexperiments demonstrate the promise of developing ASR models for non-native\nEnglish speakers, even with small amounts of L2 training data and even without\na language model. Our models also excel in the zero-shot setting where we train\non multiple L2 datasets and test on a blind L2 test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shibano_T/0/1/0/all/0/1\">Toshiko Shibano</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyi Zhang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mia Taige Li</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Haejin Cho</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Sullivan_P/0/1/0/all/0/1\">Peter Sullivan</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a> (1) ((1) University of British Columbia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05896","description":"<p>Trained on the large corpus, pre-trained language models (PLMs) can capture\ndifferent levels of concepts in context and hence generate universal language\nrepresentations. They can benefit multiple downstream natural language\nprocessing (NLP) tasks. Although PTMs have been widely used in most NLP\napplications, especially for high-resource languages such as English, it is\nunder-represented in Lao NLP research. Previous work on Lao has been hampered\nby the lack of annotated datasets and the sparsity of language resources. In\nthis work, we construct a text classification dataset to alleviate the\nresource-scare situation of the Lao language. We additionally present the first\ntransformer-based PTMs for Lao with four versions: BERT-small, BERT-base,\nELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:\npart-of-speech tagging and text classification. Experiments demonstrate the\neffectiveness of our Lao models. We will release our models and datasets to the\ncommunity, hoping to facilitate the future development of Lao NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Masking for Temporal Language Models. (arXiv:2110.06366v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06366","description":"<p>Our world is constantly evolving, and so is the content on the web.\nConsequently, our languages, often said to mirror the world, are dynamic in\nnature. However, most current contextual language models are static and cannot\nadapt to changes over time. In this work, we propose a temporal contextual\nlanguage model called TempoBERT, which uses time as an additional context of\ntexts. Our technique is based on modifying texts with temporal information and\nperforming time masking - specific masking for the supplementary time\ninformation. We leverage our approach for the tasks of semantic change\ndetection and sentence time prediction, experimenting on diverse datasets in\nterms of time, size, genre, and language. Our extensive evaluation shows that\nboth tasks benefit from exploiting time masking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_G/0/1/0/all/0/1\">Guy D. Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_I/0/1/0/all/0/1\">Ido Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese. (arXiv:2110.06696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06696","description":"<p>Although pre-trained models (PLMs) have achieved remarkable improvements in a\nwide range of NLP tasks, they are expensive in terms of time and resources.\nThis calls for the study of training more efficient models with less\ncomputation but still ensures impressive performance. Instead of pursuing a\nlarger scale, we are committed to developing lightweight yet more powerful\nmodels trained with equal or less computation and friendly to rapid deployment.\nThis technical report releases our pre-trained model called Mengzi, which\nstands for a family of discriminative, generative, domain-specific, and\nmultimodal pre-trained model variants, capable of a wide range of language and\nvision tasks. Compared with public Chinese PLMs, Mengzi is simple but more\npowerful. Our lightweight model has achieved new state-of-the-art results on\nthe widely-used CLUE benchmark with our optimized pre-training and fine-tuning\ntechniques. Without modifying the model architecture, our model can be easily\nemployed as an alternative to existing PLMs. Our sources are available at\nhttps://github.com/Langboat/Mengzi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1\">Jingyun Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}