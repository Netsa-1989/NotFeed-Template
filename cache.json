{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-08T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"NUS-IDS at FinCausal 2021: Dependency Tree in Graph Neural Network for Better Cause-Effect Span Detection. (arXiv:2110.02991v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02991","description":"<p>Automatic identification of cause-effect spans in financial documents is\nimportant for causality modelling and understanding reasons that lead to\nfinancial events. To exploit the observation that words are more connected to\nother words with the same cause-effect type in a dependency tree, we construct\nuseful graph embeddings by incorporating dependency relation features through a\ngraph neural network. Our model builds on a baseline BERT token classifier with\nViterbi decoding, and outperforms this baseline in cross-validation and during\nthe competition. In the official run of FinCausal 2021, we obtained Precision,\nRecall, and F1 scores of 95.56%, 95.56% and 95.57% that all ranked 1st place,\nand an Exact Match score of 86.05% which ranked 3rd place.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fiona Anting Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1\">See-Kiong Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Multimodal Language Representations using Convolutional Autoencoders. (arXiv:2110.03007v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03007","description":"<p>Multimodal Language Analysis is a demanding area of research, since it is\nassociated with two requirements: combining different modalities and capturing\ntemporal information. During the last years, several works have been proposed\nin the area, mostly centered around supervised learning in downstream tasks. In\nthis paper we propose extracting unsupervised Multimodal Language\nrepresentations that are universal and can be applied to different tasks.\nTowards this end, we map the word-level aligned multimodal sequences to 2-D\nmatrices and then use Convolutional Autoencoders to learn embeddings by\ncombining multiple datasets. Extensive experimentation on Sentiment Analysis\n(MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned\nrepresentations can achieve near-state-of-the-art performance with just the use\nof a Logistic Regression algorithm for downstream classification. It is also\nshown that our method is extremely lightweight and can be easily generalized to\nother tasks and unseen data with small performance drop and almost the same\nnumber of parameters. The proposed multimodal representation models are\nopen-sourced and will help grow the applicability of Multimodal Language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koromilas_P/0/1/0/all/0/1\">Panagiotis Koromilas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannakopoulos_T/0/1/0/all/0/1\">Theodoros Giannakopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emphasis control for parallel neural TTS. (arXiv:2110.03012v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03012","description":"<p>The semantic information conveyed by a speech signal is strongly influenced\nby local variations in prosody. Recent parallel neural text-to-speech (TTS)\nsynthesis methods are able to generate speech with high fidelity while\nmaintaining high performance. However, these systems often lack simple control\nover the output prosody, thus restricting the semantic information conveyable\nfor a given text. This paper proposes a hierarchical parallel neural TTS system\nfor prosodic emphasis control by learning a latent space that directly\ncorresponds to a change in emphasis. Three candidate features for the latent\nspace are compared: 1) Variance of pitch and duration within words in a\nsentence, 2) a wavelet based feature computed from pitch, energy, and duration\nand 3) a learned combination of the above features. Objective measures reveal\nthat the proposed methods are able to achieve a wide range of emphasis\nmodification, and subjective evaluations on the degree of emphasis and the\noverall quality indicate that they show promise for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Seshadri_S/0/1/0/all/0/1\">Shreyas Seshadri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raitio_T/0/1/0/all/0/1\">Tuomo Raitio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castellani_D/0/1/0/all/0/1\">Dan Castellani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangchuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation. (arXiv:2110.03036v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03036","description":"<p>A \"bigger is better\" explosion in the number of parameters in deep neural\nnetworks has made it increasingly challenging to make state-of-the-art networks\naccessible in compute-restricted environments. Compression techniques have\ntaken on renewed importance as a way to bridge the gap. However, evaluation of\nthe trade-offs incurred by popular compression techniques has been centered on\nhigh-resource datasets. In this work, we instead consider the impact of\ncompression in a data-limited regime. We introduce the term low-resource double\nbind to refer to the co-occurrence of data limitations and compute resource\nconstraints. This is a common setting for NLP for low-resource languages, yet\nthe trade-offs in performance are poorly studied. Our work offers surprising\ninsights into the relationship between capacity and generalization in\ndata-limited regimes for the task of machine translation. Our experiments on\nmagnitude pruning for translations from English into Yoruba, Hausa, Igbo and\nGerman show that in low-resource regimes, sparsity preserves performance on\nfrequent sentences but has a disparate impact on infrequent ones. However, it\nimproves robustness to out-of-distribution shifts, especially for datasets that\nare very distinct from the training distribution. Our findings suggest that\nsparsity can play a beneficial role at curbing memorization of low frequency\nattributes, and therefore offers a promising solution to the low-resource\ndouble bind.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahia_O/0/1/0/all/0/1\">Orevaoghene Ahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Categorical Features in End-to-End ASR. (arXiv:2110.03047v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03047","description":"<p>All-neural, end-to-end ASR systems gained rapid interest from the speech\nrecognition community. Such systems convert speech input to text units using a\nsingle trainable neural network model. E2E models require large amounts of\npaired speech text data that is expensive to obtain. The amount of data\navailable varies across different languages and dialects. It is critical to\nmake use of all these data so that both low resource languages and high\nresource languages can be improved. When we want to deploy an ASR system for a\nnew application domain, the amount of domain specific training data is very\nlimited. To be able to leverage data from existing domains is important for ASR\naccuracy in the new domain. In this paper, we treat all these aspects as\ncategorical information in an ASR system, and propose a simple yet effective\nway to integrate categorical features into E2E model. We perform detailed\nanalysis on various training strategies, and find that building a joint model\nthat includes categorical features can be more accurate than multiple\nindependently trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rongqing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Neurons Invariant to Sentence Structural Changes in Neural Machine Translation. (arXiv:2110.03067v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03067","description":"<p>To gain insight into the role neurons play, we study the activation patterns\ncorresponding to meaning-preserving paraphrases (e.g., active-passive). We\ncompile a dataset of controlled syntactic paraphrases in English with their\nreference German translations and demonstrate our model-agnostic approach with\nthe Transformer translation model. First, we identify neurons that correlate\nacross paraphrases and dissect the observed correlation into possible\nconfounds. Although lower-level components are found as the cause of similar\nactivations, no sentence-level semantics or syntax are detected locally. Later,\nwe manipulate neuron activations to influence translation towards a particular\nsyntactic form. We find that a simple value shift is effective, and more so\nwhen many neurons are modified. These suggest that complex syntactic\nconstructions are indeed encoded in the model. We conclude by discussing how to\nbetter manipulate it using the correlations we first obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_G/0/1/0/all/0/1\">Gal Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRAFT-What you always wanted to know but could not find about block-based environments. (arXiv:2110.03073v1 [cs.SE])","link":"http://arxiv.org/abs/2110.03073","description":"<p>Block-based environments are visual programming environments, which are\nbecoming more and more popular because of their ease of use. The ease of use\ncomes thanks to their intuitive graphical representation and structural\nmetaphors (jigsaw-like puzzles) to display valid combinations of language\nconstructs to the users. Part of the current popularity of block-based\nenvironments is thanks to Scratch. As a result they are often associated with\ntools for children or young learners. However, it is unclear how these types of\nprogramming environments are developed and used in general. So we conducted a\nsystematic literature review on block-based environments by studying 152 papers\npublished between 2014 and 2020, and a non-systematic tool review of 32\nblock-based environments. In particular, we provide a helpful inventory of\nblock-based editors for end-users on different topics and domains. Likewise, we\nfocused on identifying the main components of block-based environments, how\nthey are engineered, and how they are used. This survey should be equally\nhelpful for language engineering researchers and language engineers alike.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merino_M/0/1/0/all/0/1\">Mauricio Verano Merino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinju_J/0/1/0/all/0/1\">Jurgen Vinju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brand_M/0/1/0/all/0/1\">Mark van den Brand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTC Variations Through New WFST Topologies. (arXiv:2110.03098v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03098","description":"<p>This paper presents novel Weighted Finite-State Transducer (WFST) topologies\nto implement Connectionist Temporal Classification (CTC)-like algorithms for\nautomatic speech recognition. Three new CTC variants are proposed: (1) the\n\"compact-CTC\", in which direct transitions between units are replaced with\n&lt;epsilon&gt; back-off transitions; (2) the \"minimal-CTC\", that only adds &lt;blank&gt;\nself-loops when used in WFST-composition; and (3) \"selfless-CTC\", that\ndisallows self-loop for non-blank units. The new CTC variants have several\nbenefits, such as reducing decoding graph size and GPU memory required for\ntraining while keeping model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Laptev_A/0/1/0/all/0/1\">Aleksandr Laptev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Majumdar_S/0/1/0/all/0/1\">Somshubra Majumdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cut the CARP: Fishing for zero-shot story evaluation. (arXiv:2110.03111v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03111","description":"<p>Recent advances in large-scale language models (Raffel et al., 2019; Brown et\nal., 2020) have brought significant qualitative and quantitative improvements\nin machine-driven text generation. Despite this, generation and evaluation of\nmachine-generated narrative text remains a challenging problem. Objective\nevaluation of computationally-generated stories may be prohibitively expensive,\nrequire meticulously annotated datasets, or may not adequately measure the\nlogical coherence of a generated story's narratological structure.\n</p>\n<p>Informed by recent advances in contrastive learning (Radford et al., 2021),\nwe present Contrastive Authoring and Reviewing Pairing (CARP): a scalable,\nefficient method for performing qualitatively superior, zero-shot evaluation of\nstories. We show a strong correlation between human evaluation of stories and\nthose of CARP. Model outputs more significantly correlate with corresponding\nhuman input than those language-model based methods which utilize finetuning or\nprompt engineering approaches. We also present and analyze the Story-Critique\nDataset, a new corpora composed of 1.3 million aligned story-critique pairs\nderived from over 80,000 stories. We expect this corpus to be of interest to\nNLP researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matiana_S/0/1/0/all/0/1\">Shahbuland Matiana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">JR Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1\">Ryan Teehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frazier_S/0/1/0/all/0/1\">Spencer Frazier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Transformer-Based Language Models on Extractive Question Answering. (arXiv:2110.03142v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03142","description":"<p>Question Answering (QA) is a task in natural language processing that has\nseen considerable growth after the advent of transformers. There has been a\nsurge in QA datasets that have been proposed to challenge natural language\nprocessing models to improve human and existing model performance. Many\npre-trained language models have proven to be incredibly effective at the task\nof extractive question answering. However, generalizability remains as a\nchallenge for the majority of these models. That is, some datasets require\nmodels to reason more than others. In this paper, we train various pre-trained\nlanguage models and fine-tune them on multiple question answering datasets of\nvarying levels of difficulty to determine which of the models are capable of\ngeneralizing the most comprehensively across different datasets. Further, we\npropose a new architecture, BERT-BiLSTM, and compare it with other language\nmodels to determine if adding more bidirectionality can improve model\nperformance. Using the F1-score as our metric, we find that the RoBERTa and\nBART pre-trained models perform the best across all datasets and that our\nBERT-BiLSTM model outperforms the baseline BERT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pearce_K/0/1/0/all/0/1\">Kate Pearce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1\">Tiffany Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komanduri_A/0/1/0/all/0/1\">Aneesh Komanduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Justin Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers using End-to-End Speaker-Attributed ASR. (arXiv:2110.03151v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03151","description":"<p>This paper presents Transcribe-to-Diarize, a new approach for neural speaker\ndiarization that uses an end-to-end (E2E) speaker-attributed automatic speech\nrecognition (SA-ASR). The E2E SA-ASR is a joint model that was recently\nproposed for speaker counting, multi-talker speech recognition, and speaker\nidentification from monaural audio that contains overlapping speech. Although\nthe E2E SA-ASR model originally does not estimate any time-related information,\nwe show that the start and end times of each word can be estimated with\nsufficient accuracy from the internal state of the E2E SA-ASR by adding a small\nnumber of learnable parameters. Similar to the target-speaker voice activity\ndetection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to\nestimate speech activity of each speaker while it has the advantages of (i)\nhandling unlimited number of speakers, (ii) leveraging linguistic information\nfor speaker diarization, and (iii) simultaneously generating speaker-attributed\ntranscriptions. Experimental results on the LibriCSS and AMI corpora show that\nthe proposed method achieves significantly better diarization error rate than\nvarious existing speaker diarization methods when the number of speakers is\nunknown, and achieves a comparable performance to TS-VAD when the number of\nspeakers is given in advance. The proposed method simultaneously generates\nspeaker-attributed transcription with state-of-the-art accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transliteration of Foreign Words in Burmese. (arXiv:2110.03163v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03163","description":"<p>This manuscript provides general descriptions on transliteration of foreign\nwords in the Burmese language. Phenomena caused by phonetic and orthographic\nissues are discussed. Based on this work, we expect to gradually establish\nprescriptive guidelines to normalize the transliteration in Burmese in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chenchen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HowSumm: A Multi-Document Summarization Dataset Derived from WikiHow Articles. (arXiv:2110.03179v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03179","description":"<p>We present \\textsc{HowSumm}, a novel large-scale dataset for the task of\nquery-focused multi-document summarization (qMDS), which targets the use-case\nof generating actionable instructions from a set of sources. This use-case is\ndifferent from the use-cases covered in existing multi-document summarization\n(MDS) datasets and is applicable to educational and industrial scenarios. We\nemployed automatic methods, and leveraged statistics from existing\nhuman-crafted qMDS datasets, to create \\textsc{HowSumm} from wikiHow website\narticles and the sources they cite. We describe the creation of the dataset and\ndiscuss the unique features that distinguish it from other summarization\ncorpora. Automatic and human evaluations of both extractive and abstractive\nsummarization models on the dataset reveal that there is room for improvement.\n% in existing summarization models We propose that \\textsc{HowSumm} can be\nleveraged to advance summarization research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boni_O/0/1/0/all/0/1\">Odellia Boni</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Feigenblat_G/0/1/0/all/0/1\">Guy Feigenblat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lev_G/0/1/0/all/0/1\">Guy Lev</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1\">Michal Shmueli-Scheuer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sznajder_B/0/1/0/all/0/1\">Benjamin Sznajder</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Konopnicki_D/0/1/0/all/0/1\">David Konopnicki</a> ((1) IBM Research - AI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNN is a Counter? Revisiting GNN for Question Answering. (arXiv:2110.03192v1 [cs.AI])","link":"http://arxiv.org/abs/2110.03192","description":"<p>Question Answering (QA) has been a long-standing research topic in AI and NLP\nfields, and a wealth of studies have been conducted to attempt to equip QA\nsystems with human-level reasoning capability. To approximate the complicated\nhuman reasoning process, state-of-the-art QA systems commonly use pre-trained\nlanguage models (LMs) to access knowledge encoded in LMs together with\nelaborately designed modules based on Graph Neural Networks (GNNs) to perform\nreasoning over knowledge graphs (KGs). However, many problems remain open\nregarding the reasoning functionality of these GNN-based modules. Can these\nGNN-based modules really perform a complex reasoning process? Are they under-\nor over-complicated for QA? To open the black box of GNN and investigate these\nproblems, we dissect state-of-the-art GNN modules for QA and analyze their\nreasoning capability. We discover that even a very simple graph neural counter\ncan outperform all the existing GNN modules on CommonsenseQA and OpenBookQA,\ntwo popular QA benchmark datasets which heavily rely on knowledge-aware\nreasoning. Our work reveals that existing knowledge-aware GNN modules may only\ncarry out some simple reasoning such as counting. It remains a challenging open\nproblem to build comprehensive reasoning modules for knowledge-powered QA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influence Tuning: Demoting Spurious Correlations via Instance Attribution and Instance-Driven Updates. (arXiv:2110.03212v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03212","description":"<p>Among the most critical limitations of deep learning NLP models are their\nlack of interpretability, and their reliance on spurious correlations. Prior\nwork proposed various approaches to interpreting the black-box models to unveil\nthe spurious correlations, but the research was primarily used in\nhuman-computer interaction scenarios. It still remains underexplored whether or\nhow such model interpretations can be used to automatically \"unlearn\"\nconfounding features. In this work, we propose influence tuning--a procedure\nthat leverages model interpretations to update the model parameters towards a\nplausible interpretation (rather than an interpretation that relies on spurious\npatterns in the data) in addition to learning to predict the task labels. We\nshow that in a controlled setup, influence tuning can help deconfounding the\nmodel from spurious patterns in data, significantly outperforming baseline\nmethods that use adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaochuang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Continual Knowledge Learning of Language Models. (arXiv:2110.03215v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03215","description":"<p>Large Language Models (LMs) are known to encode world knowledge in their\nparameters as they pretrain on a vast amount of web corpus, which is often\nutilized for performing knowledge-dependent downstream tasks such as question\nanswering, fact-checking, and open dialogue. In real-world scenarios, the world\nknowledge stored in the LMs can quickly become outdated as the world changes,\nbut it is non-trivial to avoid catastrophic forgetting and reliably acquire new\nknowledge while preserving invariant knowledge. To push the community towards\nbetter maintenance of ever-changing LMs, we formulate a new continual learning\n(CL) problem called Continual Knowledge Learning (CKL). We construct a new\nbenchmark and metric to quantify the retention of time-invariant world\nknowledge, the update of outdated knowledge, and the acquisition of new\nknowledge. We adopt applicable recent methods from literature to create several\nstrong baselines. Through extensive experiments, we find that CKL exhibits\nunique challenges that are not addressed in previous CL setups, where parameter\nexpansion is necessary to reliably retain and learn knowledge simultaneously.\nBy highlighting the critical causes of knowledge forgetting, we show that CKL\nis a challenging and important problem that helps us better understand and\ntrain ever-changing LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1\">Joel Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Joongbo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Janghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeonghun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Stanley Jungkyu Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layer-wise Pruning of Transformer Attention Heads for Efficient Language Modeling. (arXiv:2110.03252v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03252","description":"<p>While Transformer-based models have shown impressive language modeling\nperformance, the large computation cost is often prohibitive for practical use.\nAttention head pruning, which removes unnecessary attention heads in the\nmultihead attention, is a promising technique to solve this problem. However,\nit does not evenly reduce the overall load because the heavy feedforward module\nis not affected by head pruning. In this paper, we apply layer-wise attention\nhead pruning on All-attention Transformer so that the entire computation and\nthe number of parameters can be reduced proportionally to the number of pruned\nheads. While the architecture has the potential to fully utilize head pruning,\nwe propose three training methods that are especially helpful to minimize\nperformance degradation and stabilize the pruning process. Our pruned model\nshows consistently lower perplexity within a comparable parameter size than\nTransformer-XL on WikiText-103 language modeling benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyuhong Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1\">Iksoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonyong Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jungwook Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situated Dialogue Learning through Procedural Environment Generation. (arXiv:2110.03262v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03262","description":"<p>We teach goal-driven agents to interactively act and speak in situated\nenvironments by training on generated curriculums. Our agents operate in LIGHT\n(Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text adventure\ngame wherein an agent perceives and interacts with the world through textual\nnatural language. Goals in this environment take the form of character-based\nquests, consisting of personas and motivations. We augment LIGHT by learning to\nprocedurally generate additional novel textual worlds and quests to create a\ncurriculum of steadily increasing difficulty for training agents to achieve\nsuch goals. In particular, we measure curriculum difficulty in terms of the\nrarity of the quest in the original training distribution -- an easier\nenvironment is one that is more likely to have been found in the unaugmented\ndataset. An ablation study shows that this method of learning from the tail of\na distribution results in significantly higher generalization abilities as\nmeasured by zero-shot performance on never-before-seen quests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Renee Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-tasking Dialogue Comprehension with Discourse Parsing. (arXiv:2110.03269v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03269","description":"<p>Multi-party dialogue machine reading comprehension (MRC) raises an even more\nchallenging understanding goal on dialogue with more than two involved\nspeakers, compared with the traditional plain passage style MRC. To accurately\nperform the question-answering (QA) task according to such multi-party\ndialogue, models have to handle fundamentally different discourse relationships\nfrom common non-dialogue plain text, where discourse relations are supposed to\nconnect two far apart utterances in a linguistics-motivated way.To further\nexplore the role of such unusual discourse structure on the correlated QA task\nin terms of MRC, we propose the first multi-task model for jointly performing\nQA and discourse parsing (DP) on the multi-party dialogue MRC task. Our\nproposed model is evaluated on the latest benchmark Molweni, whose results\nindicate that training with complementary tasks indeed benefits not only QA\ntask, but also DP task itself. We further find that the joint model is\ndistinctly stronger when handling longer dialogues which again verifies the\nnecessity of DP in the related MRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuchen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Autism Spectrum Disorders with Machine Learning Models Using Speech Transcripts. (arXiv:2110.03281v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03281","description":"<p>Autism spectrum disorder (ASD) can be defined as a neurodevelopmental\ndisorder that affects how children interact, communicate and socialize with\nothers. This disorder can occur in a broad spectrum of symptoms, with varying\neffects and severity. While there is no permanent cure for ASD, early detection\nand proactive treatment can substantially improve the lives of many children.\nCurrent methods to accurately diagnose ASD are invasive, time-consuming, and\ntedious. They can also be subjective perspectives of a number of clinicians\ninvolved, including pediatricians, speech pathologists, psychologists, and\npsychiatrists. New technologies are rapidly emerging that include machine\nlearning models using speech, computer vision from facial, retinal, and brain\nMRI images of patients to accurately and timely detect this disorder. Our\nresearch focuses on computational linguistics and machine learning using speech\ndata from TalkBank, the world's largest spoken language database. We used data\nof both ASD and Typical Development (TD) in children from TalkBank to develop\nmachine learning models to accurately predict ASD. More than 50 features were\nused from specifically two datasets in TalkBank to run our experiments using\nfive different classifiers. Logistic Regression and Random Forest models were\nfound to be the most effective for each of these two main datasets, with an\naccuracy of 0.75. These experiments confirm that while significant\nopportunities exist for improving the accuracy, machine learning models can\nreliably predict ASD status in children for effective diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1\">Vikram Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assaf_R/0/1/0/all/0/1\">Rida Assaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Supermask Pruning: Learning to Prune Image Captioning Models. (arXiv:2110.03298v1 [cs.CV])","link":"http://arxiv.org/abs/2110.03298","description":"<p>With the advancement of deep models, research work on image captioning has\nled to a remarkable gain in raw performance over the last decade, along with\nincreasing model complexity and computational cost. However, surprisingly works\non compression of deep networks for image captioning task has received little\nto no attention. For the first time in image captioning research, we provide an\nextensive comparison of various unstructured weight pruning methods on three\ndifferent popular image captioning architectures, namely Soft-Attention,\nUp-Down and Object Relation Transformer. Following this, we propose a novel\nend-to-end weight pruning method that performs gradual sparsification based on\nweight sensitivity to the training loss. The pruning schemes are then extended\nwith encoder pruning, where we show that conducting both decoder pruning and\ntraining simultaneously prior to the encoder pruning provides good overall\nperformance. Empirically, we show that an 80% to 95% sparse network (up to 75%\nreduction in model size) can either match or outperform its dense counterpart.\nThe code and pre-trained models for Up-Down and Object Relation Transformer\nthat are capable of achieving CIDEr scores &gt;120 on the MS-COCO dataset but with\nonly 8.7 MB and 14.5 MB in model size (size reduction of 96% and 94%\nrespectively against dense versions) are publicly available at\nhttps://github.com/jiahuei/sparse-image-captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jia Huei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chee Seng Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuah_J/0/1/0/all/0/1\">Joon Huang Chuah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Latent Holes of VAEs for Text Generation. (arXiv:2110.03318v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03318","description":"<p>In this paper, we provide the first focused study on the discontinuities\n(aka. holes) in the latent space of Variational Auto-Encoders (VAEs), a\nphenomenon which has been shown to have a detrimental effect on model capacity.\nWhen investigating latent holes, existing works are exclusively centred around\nthe encoder network and they merely explore the existence of holes. We tackle\nthese limitations by proposing a highly efficient Tree-based Decoder-Centric\n(TDC) algorithm for latent hole identification, with a focal point on the text\ndomain. In contrast to past studies, our approach pays attention to the decoder\nnetwork, as a decoder has a direct impact on the model's output quality.\nFurthermore, we provide, for the first time, in-depth empirical analysis of the\nlatent hole phenomenon, investigating several important aspects such as how the\nholes impact VAE algorithms' performance on text generation, and how the holes\nare distributed in the latent space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Logic-Based Framework for Natural Language Inference in Dutch. (arXiv:2110.03323v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03323","description":"<p>At its core, the system is powered by two ${\\lambda}$-calculi, used as\nsyntactic and semantic theories, respectively. Sentences are first converted to\nsyntactic proofs and terms of the linear ${\\lambda}$-calculus using a choice of\ntwo parsers: an Alpino-based pipeline, and Neural Proof Nets. The syntactic\nterms are then converted to semantic terms of the simply typed\n${\\lambda}$-calculus, via a set of hand designed type- and term-level\ntransformations. Pairs of semantic terms are then fed to an automated theorem\nprover for natural logic which reasons with them while using lexical relations\nfound in the Open Dutch WordNet. We evaluate the reasoning pipeline on the\nrecently created Dutch natural language inference dataset, and achieve\npromising results, remaining only within a $1.1-3.2{\\%}$ performance margin to\nstrong neural baselines. To the best of our knowledge, the reasoning pipeline\nis the first logic-based system for Dutch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abzianidze_L/0/1/0/all/0/1\">Lasha Abzianidze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kogkalidis_K/0/1/0/all/0/1\">Konstantinos Kogkalidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back from the future: bidirectional CTC decoding using future information in speech recognition. (arXiv:2110.03326v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03326","description":"<p>In this paper, we propose a simple but effective method to decode the output\nof Connectionist Temporal Classifier (CTC) model using a bi-directional neural\nlanguage model. The bidirectional language model uses the future as well as the\npast information in order to predict the next output in the sequence. The\nproposed method based on bi-directional beam search takes advantage of the CTC\ngreedy decoding output to represent the noisy future information. Experiments\non the Librispeechdataset demonstrate the superiority of our proposed method\ncompared to baselines using unidirectional decoding. In particular, the boost\ninaccuracy is most apparent at the start of a sequence which is the most\nerroneous part for existing systems based on unidirectional decoding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_N/0/1/0/all/0/1\">Namkyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geonmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Han-Gyu Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Language Learning for Entity Matching. (arXiv:2110.03338v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03338","description":"<p>Transformer-based matching methods have significantly moved the\nstate-of-the-art for less-structured matching tasks involving textual entity\ndescriptions. In order to excel on these tasks, Transformer-based matching\nmethods require a decent amount of training pairs. Providing enough training\ndata can be challenging, especially if a matcher for non-English entity\ndescriptions should be learned. This paper explores along the use case of\nmatching product offers from different e-shops to which extent it is possible\nto improve the performance of Transformer-based entity matchers by\ncomplementing a small set of training pairs in the target language, German in\nour case, with a larger set of English-language training pairs. Our experiments\nusing different Transformers show that extending the German set with English\npairs is always beneficial. The impact of adding the English pairs is\nespecially high in low-resource settings in which only a rather small number of\nnon-English pairs is available. As it is often possible to automatically gather\nEnglish training pairs from the Web by using schema.org annotations, our\nresults could proof relevant for many product matching scenarios targeting\nlow-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeters_R/0/1/0/all/0/1\">Ralph Peeters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizer_C/0/1/0/all/0/1\">Christian Bizer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualTTS: TTS with Accurate Lip-Speech Synchronization for Automatic Voice Over. (arXiv:2110.03342v1 [eess.AS])","link":"http://arxiv.org/abs/2110.03342","description":"<p>In this paper, we formulate a novel task to synthesize speech in sync with a\nsilent pre-recorded video, denoted as automatic voice over (AVO). Unlike\ntraditional speech synthesis, AVO seeks to generate not only human-sounding\nspeech, but also perfect lip-speech synchronization. A natural solution to AVO\nis to condition the speech rendering on the temporal progression of lip\nsequence in the video. We propose a novel text-to-speech model that is\nconditioned on visual input, named VisualTTS, for accurate lip-speech\nsynchronization. The proposed VisualTTS adopts two novel mechanisms that are 1)\ntextual-visual attention, and 2) visual fusion strategy during acoustic\ndecoding, which both contribute to forming accurate alignment between the input\ntext content and lip motion in input lip sequence. Experimental results show\nthat VisualTTS achieves accurate lip-speech synchronization and outperforms all\nbaseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Junchen Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sisman_B/0/1/0/all/0/1\">Berrak Sisman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noisy Text Data: Achilles' Heel of popular transformer based NLP models. (arXiv:2110.03353v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03353","description":"<p>In the last few years, the ML community has created a number of new NLP\nmodels based on transformer architecture. These models have shown great\nperformance for various NLP tasks on benchmark datasets, often surpassing SOTA\nresults. Buoyed with this success, one often finds industry practitioners\nactively experimenting with fine-tuning these models to build NLP applications\nfor industry use cases. However, for most datasets that are used by\npractitioners to build industrial NLP applications, it is hard to guarantee the\npresence of any noise in the data. While most transformer based NLP models have\nperformed exceedingly well in transferring the learnings from one dataset to\nanother, it remains unclear how these models perform when fine-tuned on noisy\ntext. We address the open question by Kumar et al. (2020) to explore the\nsensitivity of popular transformer based NLP models to noise in the text data.\nWe continue working with the noise as defined by them -- spelling mistakes &amp;\ntypos (which are the most commonly occurring noise). We show (via experimental\nresults) that these models perform badly on most common NLP tasks namely text\nclassification, textual similarity, NER, question answering, text summarization\non benchmark datasets. We further show that as the noise in data increases, the\nperformance degrades. Our findings suggest that one must be vary of the\npresence of noise in their datasets while fine-tuning popular transformer based\nNLP models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagla_K/0/1/0/all/0/1\">Kartikay Bagla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivam Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anuj Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. (arXiv:2110.03370v1 [cs.SD])","link":"http://arxiv.org/abs/2110.03370","description":"<p>In this paper, we present WenetSpeech, a multi-domain Mandarin corpus\nconsisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly\nlabeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in\ntotal. We collect the data from YouTube and Podcast, which covers a variety of\nspeaking styles, scenarios, domains, topics, and noisy conditions. An optical\ncharacter recognition (OCR) based method is introduced to generate the\naudio/text segmentation candidates for the YouTube data on its corresponding\nvideo captions, while a high-quality ASR transcription system is used to\ngenerate audio/text pair candidates for the Podcast data. Then we propose a\nnovel end-to-end label error detection approach to further validate and filter\nthe candidates. We also provide three manually labelled high-quality test sets\nalong with WenetSpeech for evaluation -- Dev for cross-validation purpose in\ntraining, Test_Net, collected from Internet for matched test, and\nTest\\_Meeting, recorded from real meetings for more challenging mismatched\ntest. Baseline systems trained with WenetSpeech are provided for three popular\nspeech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition\nresults on the three test sets are also provided as benchmarks. To the best of\nour knowledge, WenetSpeech is the current largest open-sourced Mandarin speech\ncorpus with transcriptions, which benefits research on production-level speech\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1\">Pengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1\">Qijie Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1\">Hui Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chenchen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled dimensionality reduction for noise-robust speaker diarisation. (arXiv:2110.03380v1 [cs.SD])","link":"http://arxiv.org/abs/2110.03380","description":"<p>The objective of this work is to train noise-robust speaker embeddings for\nspeaker diarisation. Speaker embeddings play a crucial role in the performance\nof diarisation systems, but they often capture spurious information such as\nnoise and reverberation, adversely affecting performance. Our previous work\nhave proposed an auto-encoder-based dimensionality reduction module to help\nremove the spurious information. However, they do not explicitly separate such\ninformation and have also been found to be sensitive to hyperparameter values.\nTo this end, we propose two contributions to overcome these issues: (i) a novel\ndimensionality reduction framework that can disentangle spurious information\nfrom the speaker embeddings; (ii) the use of a speech/non-speech indicator to\nprevent the speaker code from learning from the background noise. Through a\nrange of experiments conducted on four different datasets, our approach\nconsistently demonstrates the state-of-the-art performance among models that do\nnot adopt ensembles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">You Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_H/0/1/0/all/0/1\">Hee-Soo Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jee-weon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Youngki Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bong-Jin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beam Search with Bidirectional Strategies for Neural Response Generation. (arXiv:2110.03389v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03389","description":"<p>Sequence-to-sequence neural networks have been widely used in language-based\napplications as they have flexible capabilities to learn various language\nmodels. However, when seeking for the optimal language response through trained\nneural networks, current existing approaches such as beam-search decoder\nstrategies are still not able reaching to promising performances. Instead of\ndeveloping various decoder strategies based on a \"regular sentence order\"\nneural network (a trained model by outputting sentences from left-to-right\norder), we leveraged \"reverse\" order as additional language model (a trained\nmodel by outputting sentences from right-to-left order) which can provide\ndifferent perspectives for the path finding problems. In this paper, we propose\nbidirectional strategies in searching paths by combining two networks\n(left-to-right and right-to-left language models) making a bidirectional beam\nsearch possible. Besides, our solution allows us using any similarity measure\nin our sentence selection criterion. Our approaches demonstrate better\nperformance compared to the unidirectional beam search strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chouchang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varni_G/0/1/0/all/0/1\">Giovanna Varni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Attention always needed? A Case Study on Language Identification from Speech. (arXiv:2110.03427v1 [cs.LG])","link":"http://arxiv.org/abs/2110.03427","description":"<p>Language Identification (LID), a recommended initial step to Automatic Speech\nRecognition (ASR), is used to detect a spoken language from audio specimens. In\nstate-of-the-art systems capable of multilingual speech processing, however,\nusers have to explicitly set one or more languages before using them. LID,\ntherefore, plays a very important role in situations where ASR based systems\ncannot parse the uttered language in multilingual contexts causing failure in\nspeech recognition. We propose an attention based convolutional recurrent\nneural network (CRNN with Attention) that works on Mel-frequency Cepstral\nCoefficient (MFCC) features of audio specimens. Additionally, we reproduce some\nstate-of-the-art approaches, namely Convolutional Neural Network (CNN) and\nConvolutional Recurrent Neural Network (CRNN), and compare them to our proposed\nmethod. We performed extensive evaluation on thirteen different Indian\nlanguages and our model achieves classification accuracy over 98%. Our LID\nmodel is robust to noise and provides 91.2% accuracy in a noisy scenario. The\nproposed model is easily extensible to new languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandal_A/0/1/0/all/0/1\">Atanu Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Santanu Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_I/0/1/0/all/0/1\">Indranil Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_M/0/1/0/all/0/1\">Mahidas Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v1 [stat.ML])","link":"http://arxiv.org/abs/2110.03501","description":"<p>Solving symbolic mathematics has always been of in the arena of human\ningenuity that needs compositional reasoning and recurrence. However, recent\nstudies have shown that large-scale language models such as transformers are\nuniversal and surprisingly can be trained as a sequence-to-sequence task to\nsolve complex mathematical equations. These large transformer models need\nhumongous amounts of training data to generalize to unseen symbolic mathematics\nproblems. In this paper, we present a sample efficient way of solving the\nsymbolic tasks by first pretraining the transformer model with language\ntranslation and then fine-tuning the pretrained transformer model to solve the\ndownstream task of symbolic mathematics. We achieve comparable accuracy on the\nintegration task with our pretrained model while using around $1.5$ orders of\nmagnitude less number of training samples with respect to the state-of-the-art\ndeep learning for symbolic mathematics. The test accuracy on differential\nequation tasks is considerably lower comparing with integration as they need\nhigher order recursions that are not present in language translations. We\npretrain our model with different pairs of language translations. Our results\nshow language bias in solving symbolic mathematics tasks. Finally, we study the\nrobustness of the fine-tuned model on symbolic math tasks against distribution\nshift, and our approach generalizes better in distribution shift scenarios for\nthe function integration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Noorbakhsh_K/0/1/0/all/0/1\">Kimia Noorbakhsh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sulaiman_M/0/1/0/all/0/1\">Modar Sulaiman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sharifi_M/0/1/0/all/0/1\">Mahdi Sharifi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roy_K/0/1/0/all/0/1\">Kallol Roy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jamshidi_P/0/1/0/all/0/1\">Pooyan Jamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mandarin-English Code-switching Speech Recognition with Self-supervised Speech Representation Models. (arXiv:2110.03504v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03504","description":"<p>Code-switching (CS) is common in daily conversations where more than one\nlanguage is used within a sentence. The difficulties of CS speech recognition\nlie in alternating languages and the lack of transcribed data. Therefore, this\npaper uses the recently successful self-supervised learning (SSL) methods to\nleverage many unlabeled speech data without CS. We show that hidden\nrepresentations of SSL models offer frame-level language identity even if the\nmodels are trained with English speech only. Jointly training CTC and language\nidentification modules with self-supervised speech representations improves CS\nspeech recognition performance. Furthermore, using multilingual speech data for\npre-training obtains the best CS speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_L/0/1/0/all/0/1\">Liang-Hsuan Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yu-Kuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Single-Trial Representational Similarity Analysis with EEG to track semantic similarity in emotional word processing. (arXiv:2110.03529v1 [q-bio.NC])","link":"http://arxiv.org/abs/2110.03529","description":"<p>Electroencephalography (EEG) is a powerful non-invasive brain imaging\ntechnique with a high temporal resolution that has seen extensive use across\nmultiple areas of cognitive science research. This thesis adapts\nrepresentational similarity analysis (RSA) to single-trial EEG datasets and\nintroduces its principles to EEG researchers unfamiliar with multivariate\nanalyses. We have two separate aims: 1. we want to explore the effectiveness of\nsingle-trial RSA on EEG datasets; 2. we want to utilize single-trial RSA and\ncomputational semantic models to investigate the role of semantic meaning in\nemotional word processing. We report two primary findings: 1. single-trial RSA\non EEG datasets can produce meaningful and interpretable results given a high\nnumber of trials and subjects; 2. single-trial RSA reveals that emotional\nprocessing in the 500-800ms time window is associated with additional semantic\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Cheng_F/0/1/0/all/0/1\">Feng Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mRAT-SQL+GAP:A Portuguese Text-to-SQL Transformer. (arXiv:2110.03546v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03546","description":"<p>The translation of natural language questions to SQL queries has attracted\ngrowing attention, in particular in connection with transformers and similar\nlanguage models. A large number of techniques are geared towards the English\nlanguage; in this work, we thus investigated translation to SQL when input\nquestions are given in the Portuguese language. To do so, we properly adapted\nstate-of-the-art tools and resources. We changed the RAT-SQL+GAP system by\nrelying on a multilingual BART model (we report tests with other language\nmodels), and we produced a translated version of the Spider dataset. Our\nexperiments expose interesting phenomena that arise when non-English languages\nare targeted; in particular, it is better to train with original and translated\ntraining datasets together, even if a single target language is desired. This\nmultilingual BART model fine-tuned with a double-size training dataset (English\nand Portuguese) achieved 83% of the baseline, making inferences for the\nPortuguese test dataset. This investigation can help other researchers to\nproduce results in Machine Learning in a language different from English. Our\nmultilingual ready version of RAT-SQL+GAP and the data are available,\nopen-sourced as mRAT-SQL+GAP at: https://github.com/C4AI/gap-text2sql\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jose_M/0/1/0/all/0/1\">Marcelo Archanjo Jos&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cozman_F/0/1/0/all/0/1\">Fabio Gagliardi Cozman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Magic dust for cross-lingual adaptation of monolingual wav2vec-2.0. (arXiv:2110.03560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03560","description":"<p>We propose a simple and effective cross-lingual transfer learning method to\nadapt monolingual wav2vec-2.0 models for Automatic Speech Recognition (ASR) in\nresource-scarce languages. We show that a monolingual wav2vec-2.0 is a good\nfew-shot ASR learner in several languages. We improve its performance further\nvia several iterations of Dropout Uncertainty-Driven Self-Training (DUST) by\nusing a moderate-sized unlabeled speech dataset in the target language. A key\nfinding of this work is that the adapted monolingual wav2vec-2.0 achieves\nsimilar performance as the topline multilingual XLSR model, which is trained on\nfifty-three languages, on the target language ASR task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1\">Sameer Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1\">Antoine Laurent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GeSERA: General-domain Summary Evaluation by Relevance Analysis. (arXiv:2110.03567v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03567","description":"<p>We present GeSERA, an open-source improved version of SERA for evaluating\nautomatic extractive and abstractive summaries from the general domain. SERA is\nbased on a search engine that compares candidate and reference summaries\n(called queries) against an information retrieval document base (called index).\nSERA was originally designed for the biomedical domain only, where it showed a\nbetter correlation with manual methods than the widely used lexical-based ROUGE\nmethod. In this paper, we take out SERA from the biomedical domain to the\ngeneral one by adapting its content-based method to successfully evaluate\nsummaries from the general domain. First, we improve the query reformulation\nstrategy with POS Tags analysis of general-domain corpora. Second, we replace\nthe biomedical index used in SERA with two article collections from AQUAINT-2\nand Wikipedia. We conduct experiments with TAC2008, TAC2009, and CNNDM\ndatasets. Results show that, in most cases, GeSERA achieves higher correlations\nwith manual evaluation methods than SERA, while it reduces its gap with ROUGE\nfor general-domain summary evaluation. GeSERA even surpasses ROUGE in two cases\nof TAC2009. Finally, we conduct extensive experiments and provide a\ncomprehensive study of the impact of human annotators and the index size on\nsummary evaluation with SERA and GeSERA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Espejel_J/0/1/0/all/0/1\">Jessica L&#xf3;pez Espejel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalendar_G/0/1/0/all/0/1\">Ga&#xeb;l de Chalendar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flores_J/0/1/0/all/0/1\">Jorge Garcia Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charnois_T/0/1/0/all/0/1\">Thierry Charnois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_I/0/1/0/all/0/1\">Ivan Vladimir Meza Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling. (arXiv:2110.03572v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03572","description":"<p>Zero-shot cross-domain slot filling alleviates the data dependence in the\ncase of data scarcity in the target domain, which has aroused extensive\nresearch. However, as most of the existing methods do not achieve effective\nknowledge transfer to the target domain, they just fit the distribution of the\nseen slot and show poor performance on unseen slot in the target domain. To\nsolve this, we propose a novel approach based on prototypical contrastive\nlearning with a dynamic label confusion strategy for zero-shot slot filling.\nThe prototypical contrastive learning aims to reconstruct the semantic\nconstraints of labels, and we introduce the label confusion strategy to\nestablish the label dependence between the source domains and the target domain\non-the-fly. Experimental results show that our model achieves significant\nimprovement on the unseen slots, while also set new state-of-the-arts on slot\nfilling task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Keqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuanmeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Phonological Features in Multilingual Text-To-Speech. (arXiv:2110.03609v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03609","description":"<p>This study investigates whether phonological features can be applied in\ntext-to-speech systems to generate native and non-native speech. We present a\nmapping between ARPABET/pinyin-&gt;SAMPA/SAMPA-SC-&gt;phonological features in this\npaper, and tested whether native, non-native, and code-switched speech could be\nsuccessfully generated using this mapping. We ran two experiments, one with a\nsmall dataset and one with a larger dataset. The results proved that\nphonological features can be a feasible input system, although it needs further\ninvestigation to improve model performance. The accented output generated by\nthe TTS models also helps with understanding human second language acquisition\nprocesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Huinan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiewen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Retriever-Ranker for dense text retrieval. (arXiv:2110.03611v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03611","description":"<p>Current dense text retrieval models face two typical challenges. First, it\nadopts a siamese dual-encoder architecture to encode query and document\nindependently for fast indexing and searching, whereas neglecting the\nfiner-grained term-wise interactions. This results in a sub-optimal recall\nperformance. Second, it highly relies on a negative sampling technique to build\nup the negative documents in its contrastive loss. To address these challenges,\nwe present Adversarial Retriever-Ranker (AR2), which consists of a dual-encoder\nretriever plus a cross-encoder ranker. The two models are jointly optimized\naccording to a minimax adversarial objective: the retriever learns to retrieve\nnegative documents to cheat the ranker, while the ranker learns to rank a\ncollection of candidates including both the ground-truth and the retrieved\nones, as well as providing progressive direct feedback to the dual-encoder\nretriever. Through this adversarial game, the retriever gradually produces\nharder negative documents to train a better ranker, whereas the cross-encoder\nranker provides progressive feedback to improve retriever. We evaluate AR2 on\nthree benchmarks. Experimental results show that AR2 consistently and\nsignificantly outperforms existing dense retriever methods and achieves new\nstate-of-the-art results on all of them. This includes the improvements on\nNatural Questions R@5 to 77.9%(+2.1%), TriviaQA R@5 to 78.2%(+1.4), and\nMS-MARCO MRR@10 to 39.5%(+1.3%). We will make our code, models, and data\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning in NLP. (arXiv:2110.03618v1 [cs.CL])","link":"http://arxiv.org/abs/2110.03618","description":"<p>The principle of independent causal mechanisms (ICM) states that generative\nprocesses of real world data consist of independent modules which do not\ninfluence or inform each other. While this idea has led to fruitful\ndevelopments in the field of causal inference, it is not widely-known in the\nNLP community. In this work, we argue that the causal direction of the data\ncollection process bears nontrivial implications that can explain a number of\npublished NLP findings, such as differences in semi-supervised learning (SSL)\nand domain adaptation (DA) performance across different settings. We categorize\ncommon NLP tasks according to their causal direction and empirically assay the\nvalidity of the ICM principle for text data using minimum description length.\nWe conduct an extensive meta-analysis of over 100 published SSL and 30 DA\nstudies, and find that the results are consistent with our expectations based\non causal insights. This work presents the first attempt to analyze the ICM\nprinciple in NLP, and provides constructive suggestions for future modeling\nchoices. Code available at https://github.com/zhijing-jin/icm4nlp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidhya_T/0/1/0/all/0/1\">Tejas Vaidhya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushal_A/0/1/0/all/0/1\">Ayush Kaushal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoelkopf_B/0/1/0/all/0/1\">Bernhard Schoelkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying the Suicidal Tendency on Social Media: A Survey. (arXiv:2110.03663v1 [cs.SI])","link":"http://arxiv.org/abs/2110.03663","description":"<p>Amid lockdown period more people express their feelings over social media\nplatforms due to closed third-place and academic researchers have witnessed\nstrong associations between the mental healthcare and social media posts. The\nstress for a brief period may lead to clinical depressions and the long-lasting\ntraits of prevailing depressions can be life threatening with suicidal ideation\nas the possible outcome. The increasing concern towards the rise in number of\nsuicide cases is because it is one of the leading cause of premature but\npreventable death. Recent studies have shown that mining social media data has\nhelped in quantifying the suicidal tendency of users at risk. This potential\nmanuscript elucidates the taxonomy of mental healthcare and highlights some\nrecent attempts in examining the potential of quantifying suicidal tendency on\nsocial media data. This manuscript presents the classification of heterogeneous\nfeatures from social media data and handling feature vector representation.\nAiming to identify the new research directions and advances in the development\nof Machine Learning (ML) and Deep Learning (DL) based models, a quantitative\nsynthesis and a qualitative review was carried out with corpus of over 77\npotential research articles related to stress, depression and suicide risk from\n2013 to 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TBCOV: Two Billion Multilingual COVID-19 Tweets with Sentiment, Entity, Geo, and Gender Labels. (arXiv:2110.03664v1 [cs.SI])","link":"http://arxiv.org/abs/2110.03664","description":"<p>The widespread usage of social networks during mass convergence events, such\nas health emergencies and disease outbreaks, provides instant access to\ncitizen-generated data that carry rich information about public opinions,\nsentiments, urgent needs, and situational reports. Such information can help\nauthorities understand the emergent situation and react accordingly. Moreover,\nsocial media plays a vital role in tackling misinformation and disinformation.\nThis work presents TBCOV, a large-scale Twitter dataset comprising more than\ntwo billion multilingual tweets related to the COVID-19 pandemic collected\nworldwide over a continuous period of more than one year. More importantly,\nseveral state-of-the-art deep learning models are used to enrich the data with\nimportant attributes, including sentiment labels, named-entities (e.g.,\nmentions of persons, organizations, locations), user types, and gender\ninformation. Last but not least, a geotagging method is proposed to assign\ncountry, state, county, and city information to tweets, enabling a myriad of\ndata analysis tasks to understand real-world issues. Our sentiment and trend\nanalyses reveal interesting insights and confirm TBCOV's broad coverage of\nimportant topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qazi_U/0/1/0/all/0/1\">Umair Qazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention. (arXiv:2006.03654v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.03654","description":"<p>Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic aspect based sentiment analysis using bidirectional GRU based models. (arXiv:2101.10539v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.10539","description":"<p>Aspect-based Sentiment analysis (ABSA) accomplishes a fine-grained analysis\nthat defines the aspects of a given document or sentence and the sentiments\nconveyed regarding each aspect. This level of analysis is the most detailed\nversion that is capable of exploring the nuanced viewpoints of the reviews. The\nbulk of study in ABSA focuses on English with very little work available in\nArabic. Most previous work in Arabic has been based on regular methods of\nmachine learning that mainly depends on a group of rare resources and tools for\nanalyzing and processing Arabic content such as lexicons, but the lack of those\nresources presents another challenge. In order to address these challenges,\nDeep Learning (DL)-based methods are proposed using two models based on Gated\nRecurrent Units (GRU) neural networks for ABSA. The first is a DL model that\ntakes advantage of word and character representations by combining\nbidirectional GRU, Convolutional Neural Network (CNN), and Conditional Random\nField (CRF) making up the (BGRU-CNN-CRF) model to extract the main opinionated\naspects (OTE). The second is an interactive attention network based on\nbidirectional GRU (IAN-BGRU) to identify sentiment polarity toward extracted\naspects. We evaluated our models using the benchmarked Arabic hotel reviews\ndataset. The results indicate that the proposed methods are better than\nbaseline research on both tasks having 39.7% enhancement in F1-score for\nopinion target extraction (T2) and 7.58% in accuracy for aspect-based sentiment\npolarity classification (T3). Achieving F1 score of 70.67% for T2, and accuracy\nof 83.98% for T3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1\">Mohammed M.Abdelgwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soliman_T/0/1/0/all/0/1\">Taysir Hassan A Soliman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taloba_A/0/1/0/all/0/1\">Ahmed I.Taloba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farghaly_M/0/1/0/all/0/1\">Mohamed Fawzy Farghaly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation. (arXiv:2104.07412v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07412","description":"<p>Machine learning has brought striking advances in multilingual natural\nlanguage processing capabilities over the past year. For example, the latest\ntechniques have improved the state-of-the-art performance on the XTREME\nmultilingual benchmark by more than 13 points. While a sizeable gap to\nhuman-level performance remains, improvements have been easier to achieve in\nsome tasks than in others. This paper analyzes the current state of\ncross-lingual transfer learning and summarizes some lessons learned. In order\nto catalyze meaningful progress, we extend XTREME to XTREME-R, which consists\nof an improved set of ten natural language understanding tasks, including\nchallenging language-agnostic retrieval tasks, and covers 50 typologically\ndiverse languages. In addition, we provide a massively multilingual diagnostic\nsuite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities\nthrough an interactive public leaderboard to gain a better understanding of\nsuch models. The leaderboard and code for XTREME-R will be made available at\nhttps://sites.research.google/xtreme and\nhttps://github.com/google-research/xtreme respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botha_J/0/1/0/all/0/1\">Jan Botha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1\">Aditya Siddhant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Adaptive Document-Level Neural Machine Translation. (arXiv:2104.08259v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08259","description":"<p>Most existing document-level neural machine translation (NMT) models leverage\na fixed number of the previous or all global source sentences to handle the\ncontext-independent problem in standard NMT. However, the translating of each\nsource sentence benefits from various sizes of context, and inappropriate\ncontext may harm the translation performance. In this work, we introduce a\ndata-adaptive method that enables the model to adopt the necessary and useful\ncontext. Specifically, we introduce a light predictor into two document-level\ntranslation models to select the explicit context. Experiments demonstrate the\nproposed approach can significantly improve the performance over the previous\nmethods with a gain up to 1.99 BLEU points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linlin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching a Model's Notion of Belief using a Persistent Memory. (arXiv:2104.08401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08401","description":"<p>Although pretrained language models (PTLMs) have been shown to contain\nsignificant amounts of world knowledge, they can still produce inconsistent\nanswers to questions when probed, even after using specialized training\ntechniques to reduce inconsistency. As a result, it can be hard to identify\nwhat the model actually \"believes\" about the world. Our goal is to reduce this\nproblem, so systems are more globally consistent and accurate in their answers.\nOur approach is to add a memory component -- a BeliefBank -- that records a\nmodel's answers, and two mechanisms that use it to improve consistency among\nbeliefs. First, a reasoning component -- a weighted SAT solver -- improves\nconsistency by flipping answers that significantly clash with others. Second, a\nfeedback component re-queries the model but using known beliefs as context. We\nshow that, in a controlled experimental setting, these two mechanisms improve\nboth accuracy and consistency. This is significant as it is a first step\ntowards endowing models with an evolving memory, allowing them to construct a\nmore coherent picture of the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kassner_N/0/1/0/all/0/1\">Nora Kassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Schutze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Speech Recognition. (arXiv:2105.11084v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11084","description":"<p>Despite rapid progress in the recent past, current speech recognition systems\nstill require labeled training data which limits this technology to a small\nfraction of the languages spoken around the globe. This paper describes\nwav2vec-U, short for wav2vec Unsupervised, a method to train speech recognition\nmodels without any labeled data. We leverage self-supervised speech\nrepresentations to segment unlabeled audio and learn a mapping from these\nrepresentations to phonemes via adversarial training. The right representations\nare key to the success of our method. Compared to the best previous\nunsupervised work, wav2vec-U reduces the phoneme error rate on the TIMIT\nbenchmark from 26.1 to 11.3. On the larger English Librispeech benchmark,\nwav2vec-U achieves a word error rate of 5.9 on test-other, rivaling some of the\nbest published systems trained on 960 hours of labeled data from only two years\nago. We also experiment on nine other languages, including low-resource\nlanguages such as Kyrgyz, Swahili and Tatar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1\">Alexis Conneau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Language Model for Efficient Linguistic Steganalysis. (arXiv:2107.12168v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12168","description":"<p>Recent advances in linguistic steganalysis have successively applied CNN,\nRNN, GNN and other efficient deep models for detecting secret information in\ngenerative texts. These methods tend to seek stronger feature extractors to\nachieve higher steganalysis effects. However, we have found through experiments\nthat there actually exists significant difference between automatically\ngenerated stego texts and carrier texts in terms of the conditional probability\ndistribution of individual words. Such kind of difference can be naturally\ncaptured by the language model used for generating stego texts. Through further\nexperiments, we conclude that this ability can be transplanted to a text\nclassifier by pre-training and fine-tuning to improve the detection\nperformance. Motivated by this insight, we propose two methods for efficient\nlinguistic steganalysis. One is to pre-train a language model based on RNN, and\nthe other is to pre-train a sequence autoencoder. The results indicate that the\ntwo methods have different degrees of performance gain compared to the randomly\ninitialized RNN, and the convergence speed is significantly accelerated.\nMoreover, our methods have achieved the state-of-the-art detection results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1\">Biao Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hanzhou Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1\">Guorui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Similar Language Translation With Transfer Learning. (arXiv:2108.03533v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.03533","description":"<p>We investigate transfer learning based on pre-trained neural machine\ntranslation models to translate between (low-resource) similar languages. This\nwork is part of our contribution to the WMT 2021 Similar Languages Translation\nShared Task where we submitted models for different language pairs, including\nFrench-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our\nmodels for Catalan-Spanish ($82.79$ BLEU) and Portuguese-Spanish ($87.11$ BLEU)\nrank top 1 in the official shared task evaluation, and we are the only team to\nsubmit models for the French-Bambara pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Exploration in Quality Filtering of Text Data. (arXiv:2109.00698v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00698","description":"<p>While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01500","description":"<p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)\nsystems have achieved great success due to their simplicity and promising\nperformance. Neural Transducer based models are increasingly popular in\nstreaming E2E based ASR systems and have been reported to outperform the\ntraditional hybrid system in some scenarios. However, the joint optimization of\nacoustic model, lexicon and language model in neural Transducer also brings\nabout challenges to utilize pure text for language model adaptation. This\ndrawback might prevent their potential applications in practice. In order to\naddress this issue, in this paper, we propose a novel model, factorized neural\nTransducer, by factorizing the blank and vocabulary prediction, and adopting a\nstandalone language model for the vocabulary prediction. It is expected that\nthis factorization can transfer the improvement of the standalone language\nmodel to the Transducer for speech recognition, which allows various language\nmodel adaptation techniques to be applied. We demonstrate that the proposed\nfactorized neural Transducer yields 15% to 20% WER improvements when\nout-of-domain text data is used for language model adaptation, at the cost of a\nminor degradation in WER on a general test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Sarangarajan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01661","description":"<p>Iterating with new and improved OCR solutions enforces decisions to be taken\nwhen it comes to targeting the right reprocessing candidates. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those exact decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. As an extension of this technique, another contribution\ncomes in the form of a regression model that takes the enhancement potential of\na new OCR engine into account. They both mark promising approaches, especially\nfor cultural institutions dealing with historic data of lower quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactively Generating Explanations for Transformer Language Models. (arXiv:2110.02058v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02058","description":"<p>Transformer language models are state-of-the-art in a multitude of NLP tasks.\nDespite these successes, their opaqueness remains problematic. Recent methods\naiming to provide interpretability and explainability to black-box models\nprimarily focus on post-hoc explanations of (sometimes spurious) input-output\ncorrelations. Instead, we emphasize using prototype networks directly\nincorporated into the model architecture and hence explain the reasoning\nprocess behind the network's decisions. Moreover, while our architecture\nperforms on par with several language models, it enables one to learn from user\ninteractions. This not only offers a better understanding of language models\nbut uses human capabilities to incorporate knowledge outside of the rigid range\nof purely data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tauchmann_C/0/1/0/all/0/1\">Christopher Tauchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPAD: An Optimized Policy-based Active Learning Framework for Document Content Analysis. (arXiv:2110.02069v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2110.02069","description":"<p>Documents are central to many business systems, and include forms, reports,\ncontracts, invoices or purchase orders. The information in documents is\ntypically in natural language, but can be organized in various layouts and\nformats. There have been recent spurt of interest in understanding document\ncontent with novel deep learning architectures. However, document understanding\ntasks need dense information annotations, which are costly to scale and\ngeneralize. Several active learning techniques have been proposed to reduce the\noverall budget of annotation while maintaining the performance of the\nunderlying deep learning model. However, most of these techniques work only for\nclassification problems. But content detection is a more complex task, and has\nbeen scarcely explored in active learning literature. In this paper, we propose\n\\textit{OPAD}, a novel framework using reinforcement policy for active learning\nin content detection tasks for documents. The proposed framework learns the\nacquisition function to decide the samples to be selected while optimizing\nperformance metrics that the tasks typically have. Furthermore, we extend to\nweak labelling scenarios to further reduce the cost of annotation\nsignificantly. We propose novel rewards to account for class imbalance and user\nfeedback in the annotation interface, to improve the active learning method. We\nshow superior performance of the proposed \\textit{OPAD} framework for active\nlearning for various tasks related to document understanding like layout\nparsing, object detection and named entity recognition. Ablation studies for\nhuman feedback and class imbalance rewards are presented, along with a\ncomparison of annotation times for different approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Sumit Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guda_B/0/1/0/all/0/1\">Bhanu Prakash Reddy Guda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaubey_A/0/1/0/all/0/1\">Ashutosh Chaubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_I/0/1/0/all/0/1\">Ishan Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Avneet Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Contextual Adaptation with Neural Associative Memory for On-Device Personalized Speech Recognition. (arXiv:2110.02220v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.02220","description":"<p>Fast contextual adaptation has shown to be effective in improving Automatic\nSpeech Recognition (ASR) of rare words and when combined with an on-device\npersonalized training, it can yield an even better recognition result. However,\nthe traditional re-scoring approaches based on an external language model is\nprone to diverge during the personalized training. In this work, we introduce a\nmodel-based end-to-end contextual adaptation approach that is decoder-agnostic\nand amenable to on-device personalization. Our on-device simulation experiments\ndemonstrate that the proposed approach outperforms the traditional re-scoring\ntechnique by 12% relative WER and 15.7% entity mention specific F1-score in a\ncontinues personalization scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Munkhdalai_T/0/1/0/all/0/1\">Tsendsuren Munkhdalai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandorkar_A/0/1/0/all/0/1\">Angad Chandorkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Fan Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chua_M/0/1/0/all/0/1\">Mason Chua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02442","description":"<p>Transformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 96.0% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chao-Hong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSG@HASOC-Dravidian CodeMixFIRE2021: Pretrained Transformers for Offensive Language Identification in Tanglish. (arXiv:2110.02852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02852","description":"<p>This paper describes the system submitted to Dravidian-Codemix-HASOC2021:\nHate Speech and Offensive Language Identification in Dravidian Languages\n(Tamil-English and Malayalam-English). This task aims to identify offensive\ncontent in code-mixed comments/posts in Dravidian Languages collected from\nsocial media. Our approach utilizes pooling the last layers of pretrained\ntransformer multilingual BERT for this task which helped us achieve rank nine\non the leaderboard with a weighted average score of 0.61 for the Tamil-English\ndataset in subtask B. After the task deadline, we sampled the dataset uniformly\nand used the MuRIL pretrained model, which helped us achieve a weighted average\nscore of 0.67, the top score in the leaderboard. Furthermore, our approach to\nutilizing the pretrained models helps reuse our models for the same task with a\ndifferent dataset. Our code and models are available in\nhttps://github.com/seanbenhur/tanglish-offensive-language-identification\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benhur_S/0/1/0/all/0/1\">Sean Benhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivanraju_K/0/1/0/all/0/1\">Kanchana Sivanraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Lexical Normalization with Multilingual Transformers. (arXiv:2110.02869v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02869","description":"<p>Current benchmark tasks for natural language processing contain text that is\nqualitatively different from the text used in informal day to day digital\ncommunication. This discrepancy has led to severe performance degradation of\nstate-of-the-art NLP models when fine-tuned on real-world data. One way to\nresolve this issue is through lexical normalization, which is the process of\ntransforming non-standard text, usually from social media, into a more\nstandardized form. In this work, we propose a sentence-level\nsequence-to-sequence model based on mBART, which frames the problem as a\nmachine translation problem. As the noisy text is a pervasive problem across\nlanguages, not just English, we leverage the multi-lingual pre-training of\nmBART to fine-tune it to our data. While current approaches mainly operate at\nthe word or subword level, we argue that this approach is straightforward from\na technical standpoint and builds upon existing pre-trained transformer\nnetworks. Our results show that while word-level, intrinsic, performance\nevaluation is behind other methods, our model improves performance on\nextrinsic, downstream tasks through normalization compared to models operating\non raw, unprocessed, social media text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-07T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}