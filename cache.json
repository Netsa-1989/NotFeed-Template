{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Grammatical Profiling for Semantic Change Detection. (arXiv:2109.10397v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10397","description":"<p>Semantics, morphology and syntax are strongly interdependent. However, the\nmajority of computational methods for semantic change detection use\ndistributional word representations which encode mostly semantics. We\ninvestigate an alternative method, grammatical profiling, based entirely on\nchanges in the morphosyntactic behaviour of words. We demonstrate that it can\nbe used for semantic change detection and even outperforms some distributional\nsemantic methods. We present an in-depth qualitative and quantitative analysis\nof the predictions made by our grammatical profiling system, showing that they\nare plausible and interpretable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giulianelli_M/0/1/0/all/0/1\">Mario Giulianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutuzov_A/0/1/0/all/0/1\">Andrey Kutuzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pivovarova_L/0/1/0/all/0/1\">Lidia Pivovarova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RETRONLU: Retrieval Augmented Task-Oriented Semantic Parsing. (arXiv:2109.10410v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10410","description":"<p>While large pre-trained language models accumulate a lot of knowledge in\ntheir parameters, it has been demonstrated that augmenting it with\nnon-parametric retrieval-based memory has a number of benefits from accuracy\nimprovements to data efficiency for knowledge-focused tasks, such as question\nanswering. In this paper, we are applying retrieval-based modeling ideas to the\nproblem of multi-domain task-oriented semantic parsing for conversational\nassistants. Our approach, RetroNLU, extends a sequence-to-sequence model\narchitecture with a retrieval component, used to fetch existing similar\nexamples and provide them as an additional input to the model. In particular,\nwe analyze two settings, where we augment an input with (a) retrieved nearest\nneighbor utterances (utterance-nn), and (b) ground-truth semantic parses of\nnearest neighbor utterances (semparse-nn). Our technique outperforms the\nbaseline method by 1.5% absolute macro-F1, especially at the low resource\nsetting, matching the baseline model accuracy with only 40% of the data.\nFurthermore, we analyze the nearest neighbor retrieval component's quality,\nmodel sensitivity and break down the performance for semantic parses of\ndifferent utterance complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Akshat Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Adithya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savenkov_D/0/1/0/all/0/1\">Denis Savenkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Would it Take to get Biomedical QA Systems into Practice?. (arXiv:2109.10415v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10415","description":"<p>Medical question answering (QA) systems have the potential to answer\nclinicians uncertainties about treatment and diagnosis on demand, informed by\nthe latest evidence. However, despite the significant progress in general QA\nmade by the NLP community, medical QA systems are still not widely used in\nclinical environments. One likely reason for this is that clinicians may not\nreadily trust QA system outputs, in part because transparency, trustworthiness,\nand provenance have not been key considerations in the design of such models.\nIn this paper we discuss a set of criteria that, if met, we argue would likely\nincrease the utility of biomedical QA systems, which may in turn lead to\nadoption of such systems in practice. We assess existing models, tasks, and\ndatasets with respect to these criteria, highlighting shortcomings of\npreviously proposed approaches and pointing toward what might be more usable QA\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kell_G/0/1/0/all/0/1\">Gregory Kell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_I/0/1/0/all/0/1\">Iain J. Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaun_A/0/1/0/all/0/1\">Andre Jaun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Debiasing Techniques for Intersectional Biases. (arXiv:2109.10441v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10441","description":"<p>Bias is pervasive in NLP models, motivating the development of automatic\ndebiasing techniques. Evaluation of NLP debiasing methods has largely been\nlimited to binary attributes in isolation, e.g., debiasing with respect to\nbinary gender or race, however many corpora involve multiple such attributes,\npossibly with higher cardinality. In this paper we argue that a truly fair\nmodel must consider `gerrymandering' groups which comprise not only single\nattributes, but also intersectional groups. We evaluate a form of\nbias-constrained model which is new to NLP, as well an extension of the\niterative nullspace projection technique which can handle multiple protected\nattributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Shivashankar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness-aware Class Imbalanced Learning. (arXiv:2109.10444v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10444","description":"<p>Class imbalance is a common challenge in many NLP tasks, and has clear\nconnections to bias, in that bias in training data often leads to higher\naccuracy for majority groups at the expense of minority groups. However there\nhas traditionally been a disconnect between research on class-imbalanced\nlearning and mitigating bias, and only recently have the two been looked at\nthrough a common lens. In this work we evaluate long-tail learning methods for\ntweet sentiment and occupation classification, and extend a margin-loss based\napproach with methods to enforce fairness. We empirically show through\ncontrolled experiments that the proposed approaches help mitigate both class\nimbalance and demographic biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Shivashankar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Afshin Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results. (arXiv:2109.10453v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10453","description":"<p>Recent transformer-based approaches demonstrate promising results on\nrelational scientific information extraction. Existing datasets focus on\nhigh-level description of how research is carried out. Instead we focus on the\nsubtleties of how experimental associations are presented by building SciClaim,\na dataset of scientific claims drawn from Social and Behavior Science (SBS),\nPubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not\nonly coarse-grained entity spans as nodes and relations as edges between them,\nbut also fine-grained attributes that modify entities and their relations, for\na total of 12,738 labels in the corpus. By including more label types and more\nthan twice the label density of previous datasets, SciClaim captures causal,\ncomparative, predictive, statistical, and proportional associations over\nexperimental variables along with their qualifications, subtypes, and evidence.\nWe extend work in transformer-based joint entity and relation extraction to\neffectively infer our schema, showing the promise of fine-grained knowledge\ngraphs in scientific claims and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magnusson_I/0/1/0/all/0/1\">Ian H. Magnusson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_S/0/1/0/all/0/1\">Scott E. Friedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable and Efficient MoE Training for Multitask Multilingual Models. (arXiv:2109.10465v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10465","description":"<p>The Mixture of Experts (MoE) models are an emerging class of sparsely\nactivated deep learning models that have sublinear compute costs with respect\nto their parameters. In contrast with dense models, the sparse architecture of\nMoE offers opportunities for drastically growing model size with significant\naccuracy gain while consuming much lower compute budget. However, supporting\nlarge scale MoE training also has its own set of system and modeling\nchallenges. To overcome the challenges and embrace the opportunities of MoE, we\nfirst develop a system capable of scaling MoE models efficiently to trillions\nof parameters. It combines multi-dimensional parallelism and heterogeneous\nmemory technologies harmoniously with MoE to empower 8x larger models on the\nsame hardware compared with existing work. Besides boosting system efficiency,\nwe also present new training methods to improve MoE sample efficiency and\nleverage expert pruning strategy to improve inference time efficiency. By\ncombining the efficient system and training methods, we are able to\nsignificantly scale up large multitask multilingual models for language\ngeneration which results in a great improvement in model accuracy. A model\ntrained with 10 billion parameters on 50 languages can achieve state-of-the-art\nperformance in Machine Translation (MT) and multilingual natural language\ngeneration tasks. The system support of efficient MoE training has been\nimplemented and open-sourced with the DeepSpeed library.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1\">Ammar Ahmad Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salinas_A/0/1/0/all/0/1\">Andres Felipe Cruz Salinas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Liyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendy_A/0/1/0/all/0/1\">Amr Hendy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1\">Hany Hassan Awadalla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salience-Aware Event Chain Modeling for Narrative Understanding. (arXiv:2109.10475v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10475","description":"<p>Storytelling, whether via fables, news reports, documentaries, or memoirs,\ncan be thought of as the communication of interesting and related events that,\ntaken together, form a concrete process. It is desirable to extract the event\nchains that represent such processes. However, this extraction remains a\nchallenging problem. We posit that this is due to the nature of the texts from\nwhich chains are discovered. Natural language text interleaves a narrative of\nconcrete, salient events with background information, contextualization,\nopinion, and other elements that are important for a variety of necessary\ndiscourse and pragmatics acts but are not part of the principal chain of events\nbeing communicated. We introduce methods for extracting this principal chain\nfrom natural language text, by filtering away non-salient events and supportive\nsentences. We demonstrate the effectiveness of our methods at isolating\ncritical event chains by comparing their effect on downstream tasks. We show\nthat by pre-training large language models on our extracted chains, we obtain\nimprovements in two tasks that benefit from a clear understanding of event\nchains: narrative prediction and event-based temporal question answering. The\ndemonstrated improvements and ablative studies confirm that our extraction\nmethod isolates critical event chains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogueBERT: A Self-Supervised Learning based Dialogue Pre-training Encoder. (arXiv:2109.10480v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10480","description":"<p>With the rapid development of artificial intelligence, conversational bots\nhave became prevalent in mainstream E-commerce platforms, which can provide\nconvenient customer service timely. To satisfy the user, the conversational\nbots need to understand the user's intention, detect the user's emotion, and\nextract the key entities from the conversational utterances. However,\nunderstanding dialogues is regarded as a very challenging task. Different from\ncommon language understanding, utterances in dialogues appear alternately from\ndifferent roles and are usually organized as hierarchical structures. To\nfacilitate the understanding of dialogues, in this paper, we propose a novel\ncontextual dialogue encoder (i.e. DialogueBERT) based on the popular\npre-trained language model BERT. Five self-supervised learning pre-training\ntasks are devised for learning the particularity of dialouge utterances. Four\ndifferent input embeddings are integrated to catch the relationship between\nutterances, including turn embedding, role embedding, token embedding and\nposition embedding. DialogueBERT was pre-trained with 70 million dialogues in\nreal scenario, and then fine-tuned in three different downstream dialogue\nunderstanding tasks. Experimental results show that DialogueBERT achieves\nexciting results with 88.63% accuracy for intent recognition, 94.25% accuracy\nfor emotion recognition and 97.04% F1 score for named entity recognition, which\noutperforms several strong baselines by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The NiuTrans Machine Translation Systems for WMT21. (arXiv:2109.10485v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10485","description":"<p>This paper describes NiuTrans neural machine translation systems of the WMT\n2021 news translation tasks. We made submissions to 9 language directions,\nincluding English$\\leftrightarrow$$\\{$Chinese, Japanese, Russian, Icelandic$\\}$\nand English$\\rightarrow$Hausa tasks. Our primary systems are built on several\neffective variants of Transformer, e.g., Transformer-DLCL, ODE-Transformer. We\nalso utilize back-translation, knowledge distillation, post-ensemble, and\niterative fine-tuning techniques to enhance the model performance further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuhan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Binghao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yingfeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1\">Yongyu Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zefan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuanjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chuanhao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1\">Yi Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Laohu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingnan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Canan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhongxiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the OBQA Context. (arXiv:2109.10497v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10497","description":"<p>In the open question answering (OBQA) task, how to select the relevant\ninformation from a large corpus is a crucial problem for reasoning and\ninference. Some datasets (e.g, HotpotQA) mainly focus on testing the model's\nreasoning ability at the sentence level. To overcome this challenge, many\nexisting frameworks use a deep learning model to select relevant passages and\nthen answer each question by matching a sentence in the corresponding passage.\nHowever, such frameworks require long inference time and fail to take advantage\nof the relationship between passages and sentences. In this work, we present a\nsimple yet effective framework to address these problems by jointly ranking\npassages and selecting sentences. We propose consistency and similarity\nconstraints to promote the correlation and interaction between passage ranking\nand sentence selection. In our experiments, we demonstrate that our framework\ncan achieve competitive results and outperform the baseline by 28\\% in terms of\nexact matching of relevant sentences on the HotpotQA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperExpan: Taxonomy Expansion with Hyperbolic Representation Learning. (arXiv:2109.10500v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10500","description":"<p>Taxonomies are valuable resources for many applications, but the limited\ncoverage due to the expensive manual curation process hinders their general\napplicability. Prior works attempt to automatically expand existing taxonomies\nto improve their coverage by learning concept embeddings in Euclidean space,\nwhile taxonomies, inherently hierarchical, more naturally align with the\ngeometric properties of a hyperbolic space. In this paper, we present\nHyperExpan, a taxonomy expansion algorithm that seeks to preserve the structure\nof a taxonomy in a more expressive hyperbolic embedding space and learn to\nrepresent concepts and their relations with a Hyperbolic Graph Neural Network\n(HGNN). Specifically, HyperExpan leverages position embeddings to exploit the\nstructure of the existing taxonomies, and characterizes the concept profile\ninformation to support the inference on unseen concepts during training.\nExperiments show that our proposed HyperExpan outperforms baseline models with\nrepresentation learning in a Euclidean feature space and achieves\nstate-of-the-art performance on the taxonomy expansion benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingyu Derek Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Te-Lin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tecnologica cosa: Modeling Storyteller Personalities in Boccaccio's Decameron. (arXiv:2109.10506v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10506","description":"<p>We explore Boccaccio's Decameron to see how digital humanities tools can be\nused for tasks that have limited data in a language no longer in contemporary\nuse: medieval Italian. We focus our analysis on the question: Do the different\nstorytellers in the text exhibit distinct personalities? To answer this\nquestion, we curate and release a dataset based on the authoritative edition of\nthe text. We use supervised classification methods to predict storytellers\nbased on the stories they tell, confirming the difficulty of the task, and\ndemonstrate that topic modeling can extract thematic storyteller \"profiles.\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Feder Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniak_M/0/1/0/all/0/1\">Maria Antoniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Christopher De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migiel_M/0/1/0/all/0/1\">Marilyn Migiel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimno_D/0/1/0/all/0/1\">David Mimno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Contextualized Document Representation. (arXiv:2109.10509v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10509","description":"<p>Several NLP tasks need the effective representation of text documents. Arora\net. al., 2017 demonstrate that simple weighted averaging of word vectors\nfrequently outperforms neural models. SCDV (Mekala et. al., 2017) further\nextends this from sentences to documents by employing soft and sparse\nclustering over pre-computed word vectors. However, both techniques ignore the\npolysemy and contextual character of words. In this paper, we address this\nissue by proposing SCDV+BERT(ctxd), a simple and effective unsupervised\nrepresentation that combines contextualized BERT (Devlin et al., 2019) based\nword embedding for word sense disambiguation with SCDV soft clustering\napproach. We show that our embeddings outperform original SCDV, pre-train BERT,\nand several other baselines on many classification datasets. We also\ndemonstrate our embeddings effectiveness on other tasks, such as concept\nmatching and sentence similarity. In addition, we show that SCDV+BERT(ctxd)\noutperforms fine-tune BERT and different embedding approaches in scenarios with\nlimited data and only few shots examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ankur Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCM: A Fine-grained Comparison Model forMulti-turn Dialogue Reasoning. (arXiv:2109.10510v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10510","description":"<p>Despite the success of neural dialogue systems in achieving high performance\non the leader-board, they cannot meet users' requirements in practice, due to\ntheir poor reasoning skills. The underlying reason is that most neural dialogue\nmodels only capture the syntactic and semantic information, but fail to model\nthe logical consistency between the dialogue history and the generated\nresponse. Recently, a new multi-turn dialogue reasoning task has been proposed,\nto facilitate dialogue reasoning research. However, this task is challenging,\nbecause there are only slight differences between the illogical response and\nthe dialogue history. How to effectively solve this challenge is still worth\nexploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle\nthis problem. Inspired by human's behavior in reading comprehension, a\ncomparison mechanism is proposed to focus on the fine-grained differences in\nthe representation of each response candidate. Specifically, each candidate\nrepresentation is compared with the whole history to obtain a history\nconsistency representation. Furthermore, the consistency signals between each\ncandidate and the speaker's own history are considered to drive a model to\nprefer a candidate that is logically consistent with the speaker's history\nlogic. Finally, the above consistency representations are employed to output a\nranking list of the candidate responses for multi-turn dialogue reasoning.\nExperimental results on two public dialogue datasets show that our method\nobtains higher ranking scores than the baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards The Automatic Coding of Medical Transcripts to Improve Patient-Centered Communication. (arXiv:2109.10514v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10514","description":"<p>This paper aims to provide an approach for automatic coding of\nphysician-patient communication transcripts to improve patient-centered\ncommunication (PCC). PCC is a central part of high-quality health care. To\nimprove PCC, dialogues between physicians and patients have been recorded and\ntagged with predefined codes. Trained human coders have manually coded the\ntranscripts. Since it entails huge labor costs and poses possible human errors,\nautomatic coding methods should be considered for efficiency and effectiveness.\nWe adopted three machine learning algorithms (Na\\\"ive Bayes, Random Forest, and\nSupport Vector Machine) to categorize lines in transcripts into corresponding\ncodes. The result showed that there is evidence to distinguish the codes, and\nthis is considered to be sufficient for training of human annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Gilchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1\">Julia Taylor Rayz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shields_C/0/1/0/all/0/1\">Cleveland G. Shields</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Role of Language Relatedness in Multilingual Fine-tuning of Language Models: A Case Study in Indo-Aryan Languages. (arXiv:2109.10534v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10534","description":"<p>We explore the impact of leveraging the relatedness of languages that belong\nto the same family in NLP models using multilingual fine-tuning. We hypothesize\nand validate that multilingual fine-tuning of pre-trained language models can\nyield better performance on downstream NLP applications, compared to models\nfine-tuned on individual languages. A first of its kind detailed study is\npresented to track performance change as languages are added to a base language\nin a graded and greedy (in the sense of best boost of performance) manner;\nwhich reveals that careful selection of subset of related languages can\nsignificantly improve performance than utilizing all related languages. The\nIndo-Aryan (IA) language family is chosen for the study, the exact languages\nbeing Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script\nbarrier is crossed by simple rule-based transliteration of the text of all\nlanguages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL\nand two RoBERTa-based LMs, the last two being pre-trained by us. Low resource\nlanguages, such as Oriya and Punjabi, are found to be the largest beneficiaries\nof multilingual fine-tuning. Textual Entailment, Entity Classification, Section\nTitle Prediction, tasks of IndicGLUE and POS tagging form our test bed.\nCompared to monolingual fine tuning we get relative performance improvement of\nup to 150% in the downstream tasks. The surprise take-away is that for any\nlanguage there is a particular combination of other languages which yields the\nbest performance, and any additional language is in fact detrimental.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhamecha_T/0/1/0/all/0/1\">Tejas Indulal Dhamecha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+V_R/0/1/0/all/0/1\">Rudra Murthy V</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1\">Karthik Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing. (arXiv:2109.10540v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10540","description":"<p>Recent years pretrained language models (PLMs) hit a success on several\ndownstream tasks, showing their power on modeling language. To better\nunderstand and leverage what PLMs have learned, several techniques have emerged\nto explore syntactic structures entailed by PLMs. However, few efforts have\nbeen made to explore grounding capabilities of PLMs, which are also essential.\nIn this paper, we highlight the ability of PLMs to discover which token should\nbe grounded to which concept, if combined with our proposed\nerasing-then-awakening approach. Empirical studies on four datasets demonstrate\nthat our approach can awaken latent grounding which is understandable to human\nexperts, even if it is not exposed to such labels during training. More\nimportantly, our approach shows great potential to benefit downstream semantic\nparsing models. Taking text-to-SQL as a case study, we successfully couple our\napproach with two off-the-shelf parsers, obtaining an absolute improvement of\nup to 9.8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dejian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaqi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diarisation using Location tracking with agglomerative clustering. (arXiv:2109.10598v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10598","description":"<p>Previous works have shown that spatial location information can be\ncomplementary to speaker embeddings for a speaker diarisation task. However,\nthe models used often assume that speakers are fairly stationary throughout a\nmeeting. This paper proposes to relax this assumption, by explicitly modelling\nthe movements of speakers within an Agglomerative Hierarchical Clustering (AHC)\ndiarisation framework. Kalman filters, which track the locations of speakers,\nare used to compute log-likelihood ratios that contribute to the cluster\naffinity computations for the AHC merging and stopping decisions. Experiments\nshow that the proposed approach is able to yield improvements on a Microsoft\nrich meeting transcription task, compared to methods that do not use location\ninformation or that make stationarity assumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jeremy H. M. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramovski_I/0/1/0/all/0/1\">Igor Abramovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset. (arXiv:2109.10604v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10604","description":"<p>While diverse question answering (QA) datasets have been proposed and\ncontributed significantly to the development of deep learning models for QA\ntasks, the existing datasets fall short in two aspects. First, we lack QA\ndatasets covering complex questions that involve answers as well as the\nreasoning processes to get the answers. As a result, the state-of-the-art QA\nresearch on numerical reasoning still focuses on simple calculations and does\nnot provide the mathematical expressions or evidences justifying the answers.\nSecond, the QA community has contributed much effort to improving the\ninterpretability of QA models. However, these models fail to explicitly show\nthe reasoning process, such as the evidence order for reasoning and the\ninteractions between different pieces of evidence. To address the above\nshortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset\nwith questions requiring numerical reasoning with compound mathematical\nexpressions. With NOAHQA, we develop an interpretable reasoning graph as well\nas the appropriate evaluation metric to measure the answer quality. We evaluate\nthe state-of-the-art QA models trained using existing QA datasets on NOAHQA and\nshow that the best among them can only achieve 55.5 exact match scores, while\nthe human performance is 89.7. We also present a new QA model for generating a\nreasoning graph where the reasoning graph metric still has a large gap compared\nwith that of humans, e.g., 28 scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sicheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVR: A test-bed for Visually Grounded Compositional Generalization with real images. (arXiv:2109.10613v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10613","description":"<p>While interest in models that generalize at test time to new compositions has\nrisen in recent years, benchmarks in the visually-grounded domain have thus far\nbeen restricted to synthetic images. In this work, we propose COVR, a new\ntest-bed for visually-grounded compositional generalization with real images.\nTo create COVR, we use real images annotated with scene graphs, and propose an\nalmost fully automatic procedure for generating question-answer pairs along\nwith a set of context images. COVR focuses on questions that require complex\nreasoning, including higher-order operations such as quantification and\naggregation. Due to the automatic generation process, COVR facilitates the\ncreation of compositional splits, where models at test time need to generalize\nto new concepts and compositions in a zero- or few-shot setting. We construct\ncompositional splits using COVR and demonstrate a myriad of cases where\nstate-of-the-art pre-trained language-and-vision models struggle to\ncompositionally generalize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogin_B/0/1/0/all/0/1\">Ben Bogin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shivanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching and Controlling Global Semantics for Text Summarization. (arXiv:2109.10616v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10616","description":"<p>Recently, Transformer-based models have been proven effective in the\nabstractive summarization task by creating fluent and informative summaries.\nNevertheless, these models still suffer from the short-range dependency\nproblem, causing them to produce summaries that miss the key points of\ndocument. In this paper, we attempt to address this issue by introducing a\nneural topic model empowered with normalizing flow to capture the global\nsemantics of the document, which are then integrated into the summarization\nmodel. In addition, to avoid the overwhelming effect of global semantics on\ncontextualized representation, we introduce a mechanism to control the amount\nof global semantics supplied to the text generation module. Our method\noutperforms state-of-the-art summarization models on five common text\nsummarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and\nPubMed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_A/0/1/0/all/0/1\">Anh Tuan Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Truc Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_T/0/1/0/all/0/1\">Tho Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning for Fair Representations. (arXiv:2109.10645v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10645","description":"<p>Trained classification models can unintentionally lead to biased\nrepresentations and predictions, which can reinforce societal preconceptions\nand stereotypes. Existing debiasing methods for classification models, such as\nadversarial training, are often expensive to train and difficult to optimise.\nIn this paper, we propose a method for mitigating bias in classifier training\nby incorporating contrastive learning, in which instances sharing the same\nclass label are encouraged to have similar representations, while instances\nsharing a protected attribute are forced further apart. In such a way our\nmethod learns representations which capture the task label in focused regions,\nwhile ensuring the protected attribute has diverse spread, and thus has limited\nimpact on prediction and thereby results in fairer models. Extensive\nexperimental results across four tasks in NLP and computer vision show (a) that\nour proposed method can achieve fairer representations and realises bias\nreductions compared with competitive baselines; and (b) that it can do so\nwithout sacrificing main task performance; (c) that it sets a new\nstate-of-the-art performance in one task despite reducing the bias. Finally,\nour method is conceptually simple and agnostic to network architectures, and\nincurs minimal additional compute cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_A/0/1/0/all/0/1\">Aili Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frermann_L/0/1/0/all/0/1\">Lea Frermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiRANews: Dataset and Benchmarks for Multi-Resource-Assisted News Summarization. (arXiv:2109.10650v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10650","description":"<p>One of the most challenging aspects of current single-document news\nsummarization is that the summary often contains 'extrinsic hallucinations',\ni.e., facts that are not present in the source document, which are often\nderived via world knowledge. This causes summarization systems to act more like\nopen-ended language models tending to hallucinate facts that are erroneous. In\nthis paper, we mitigate this problem with the help of multiple supplementary\nresource documents assisting the task. We present a new dataset MiRANews and\nbenchmark existing summarization models. In contrast to multi-document\nsummarization, which addresses multiple events from several source documents,\nwe still aim at generating a summary for a single document. We show via data\nanalysis that it's not only the models which are to blame: more than 27% of\nfacts mentioned in the gold summaries of MiRANews are better grounded on\nassisting documents than in the main source articles. An error analysis of\ngenerated summaries from pretrained models fine-tuned on MiRANews reveals that\nthis has an even bigger effects on models: assisted summarization reduces 55%\nof hallucinations when compared to single-document summarization models trained\non the main article only. Our code and data are available at\nhttps://github.com/XinnuoXu/MiRANews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinnuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konstas_I/0/1/0/all/0/1\">Ioannis Konstas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers. (arXiv:2109.10686v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10686","description":"<p>There remain many open questions pertaining to the scaling behaviour of\nTransformer architectures. These scaling decisions and findings can be\ncritical, as training runs often come with an associated computational cost\nwhich have both financial and/or environmental impact. The goal of this paper\nis to present scaling insights from pretraining and finetuning Transformers.\nWhile Kaplan et al. presents a comprehensive study of the scaling behaviour of\nTransformer language models, the scope is only on the upstream (pretraining)\nloss. Therefore, it is still unclear if these set of findings transfer to\ndownstream task within the context of the pretrain-finetune paradigm. The key\nfindings of this paper are as follows: (1) we show that aside from only the\nmodel size, model shape matters for downstream fine-tuning, (2) scaling\nprotocols operate differently at different compute regions, (3) widely adopted\nT5-base and T5-large sizes are Pareto-inefficient. To this end, we present\nimproved scaling protocols whereby our redesigned models achieve similar\ndownstream fine-tuning quality while having 50\\% fewer parameters and training\n40\\% faster compared to the widely adopted T5-base model. We publicly release\nover 100 pretrained checkpoints of different T5 configurations to facilitate\nfuture research and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mostafa Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1\">Jinfeng Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abnar_S/0/1/0/all/0/1\">Samira Abnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1\">Ashish Vaswani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulated Annealing for Emotional Dialogue Systems. (arXiv:2109.10715v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10715","description":"<p>Explicitly modeling emotions in dialogue generation has important\napplications, such as building empathetic personal companions. In this study,\nwe consider the task of expressing a specific emotion for dialogue generation.\nPrevious approaches take the emotion as an input signal, which may be ignored\nduring inference. We instead propose a search-based emotional dialogue system\nby simulated annealing (SA). Specifically, we first define a scoring function\nthat combines contextual coherence and emotional correctness. Then, SA\niteratively edits a general response and searches for a sentence with a higher\nscore, enforcing the presence of the desired emotion. We evaluate our system on\nthe NLPCC2017 dataset. Our proposed method shows 12% improvements in emotion\naccuracy compared with the previous state-of-the-art method, without hurting\nthe generation quality (measured by BLEU).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengzhang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Za&#xef;ane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Latency Incremental Text-to-Speech Synthesis with Distilled Context Prediction Network. (arXiv:2109.10724v1 [cs.SD])","link":"http://arxiv.org/abs/2109.10724","description":"<p>Incremental text-to-speech (TTS) synthesis generates utterances in small\nlinguistic units for the sake of real-time and low-latency applications. We\npreviously proposed an incremental TTS method that leverages a large\npre-trained language model to take unobserved future context into account\nwithout waiting for the subsequent segment. Although this method achieves\ncomparable speech quality to that of a method that waits for the future\ncontext, it entails a huge amount of processing for sampling from the language\nmodel at each time step. In this paper, we propose an incremental TTS method\nthat directly predicts the unobserved future context with a lightweight model,\ninstead of sampling words from the large-scale language model. We perform\nknowledge distillation from a GPT2-based context prediction network into a\nsimple recurrent model by minimizing a teacher-student loss defined between the\ncontext embedding vectors of those models. Experimental results show that the\nproposed method requires about ten times less inference time to achieve\ncomparable synthetic speech quality to that of our previous method, and it can\nperform incremental synthesis much faster than the average speaking speed of\nhuman English speakers, demonstrating the availability of our method to\nreal-time applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeki_T/0/1/0/all/0/1\">Takaaki Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamichi_S/0/1/0/all/0/1\">Shinnosuke Takamichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saruwatari_H/0/1/0/all/0/1\">Hiroshi Saruwatari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing. (arXiv:2109.10847v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10847","description":"<p>Recent progress in the Natural Language Processing domain has given us\nseveral State-of-the-Art (SOTA) pretrained models which can be finetuned for\nspecific tasks. These large models with billions of parameters trained on\nnumerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In\nthis paper, we discuss the need for a benchmark for cost and time effective\nsmaller models trained on a single GPU. This will enable researchers with\nresource constraints experiment with novel and innovative ideas on\ntokenization, pretraining tasks, architecture, fine tuning methods etc. We set\nup Small-Bench NLP, a benchmark for small efficient neural language models\ntrained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks\non the publicly available GLUE datasets and a leaderboard to track the progress\nof the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture\nachieves an average score of 81.53 which is comparable to that of BERT-Base's\n82.20 (110M parameters). Our models, code and leaderboard are available at\nhttps://github.com/smallbenchnlp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanakarajan_K/0/1/0/all/0/1\">Kamal Raj Kanakarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundumani_B/0/1/0/all/0/1\">Bhuvana Kundumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankarasubbu_M/0/1/0/all/0/1\">Malaikannan Sankarasubbu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pix2seq: A Language Modeling Framework for Object Detection. (arXiv:2109.10852v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10852","description":"<p>This paper presents Pix2Seq, a simple and generic framework for object\ndetection. Unlike existing approaches that explicitly integrate prior knowledge\nabout the task, we simply cast object detection as a language modeling task\nconditioned on the observed pixel inputs. Object descriptions (e.g., bounding\nboxes and class labels) are expressed as sequences of discrete tokens, and we\ntrain a neural net to perceive the image and generate the desired sequence. Our\napproach is based mainly on the intuition that if a neural net knows about\nwhere and what the objects are, we just need to teach it how to read them out.\nBeyond the use of task-specific data augmentations, our approach makes minimal\nassumptions about the task, yet it achieves competitive results on the\nchallenging COCO dataset, compared to highly specialized and well optimized\ndetection algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Saurabh Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hinton_G/0/1/0/all/0/1\">Geoffrey Hinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BFClass: A Backdoor-free Text Classification Framework. (arXiv:2109.10855v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10855","description":"<p>Backdoor attack introduces artificial vulnerabilities into the model by\npoisoning a subset of the training data via injecting triggers and modifying\nlabels. Various trigger design strategies have been explored to attack text\nclassifiers, however, defending such attacks remains an open problem. In this\nwork, we propose BFClass, a novel efficient backdoor-free training framework\nfor text classification. The backbone of BFClass is a pre-trained discriminator\nthat predicts whether each token in the corrupted input was replaced by a\nmasked language model. To identify triggers, we utilize this discriminator to\nlocate the most suspicious token from each training sample and then distill a\nconcise set by considering their association strengths with particular labels.\nTo recognize the poisoned subset, we examine the training samples with these\nidentified triggers as the most suspicious token, and check if removing the\ntrigger will change the poisoned model's prediction. Extensive experiments\ndemonstrate that BFClass can identify all the triggers, remove 95% poisoned\ntraining samples with very limited false alarms, and achieve almost the same\nperformance as the models trained on the benign training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chengyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data. (arXiv:2109.10856v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10856","description":"<p>Existing text classification methods mainly focus on a fixed label set,\nwhereas many real-world applications require extending to new fine-grained\nclasses as the number of samples per label increases. To accommodate such\nrequirements, we introduce a new problem called coarse-to-fine grained\nclassification, which aims to perform fine-grained classification on coarsely\nannotated data. Instead of asking for new fine-grained human annotations, we\nopt to leverage label surface names as the only human guidance and weave in\nrich pre-trained generative language models into the iterative weak supervision\nstrategy. Specifically, we first propose a label-conditioned finetuning\nformulation to attune these generators for our task. Furthermore, we devise a\nregularization objective based on the coarse-fine label constraints derived\nfrom our problem setting, giving us even further improvements over the prior\nformulation. Our framework uses the fine-tuned generative models to sample\npseudo-training data for training the classifier, and bootstraps on real\nunlabeled data for model refinement. Extensive experiments and case studies on\ntwo real-world datasets demonstrate superior performance over SOTA zero-shot\nclassification baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekala_D/0/1/0/all/0/1\">Dheeraj Mekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Right Buttons: Adversarial Evaluation of Quality Estimation. (arXiv:2109.10859v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10859","description":"<p>Current Machine Translation (MT) systems achieve very good results on a\ngrowing variety of language pairs and datasets. However, they are known to\nproduce fluent translation outputs that can contain important meaning errors,\nthus undermining their reliability in practice. Quality Estimation (QE) is the\ntask of automatically assessing the performance of MT systems at test time.\nThus, in order to be useful, QE systems should be able to detect such errors.\nHowever, this ability is yet to be tested in the current evaluation practices,\nwhere QE systems are assessed only in terms of their correlation with human\njudgements. In this work, we bridge this gap by proposing a general methodology\nfor adversarial testing of QE for MT. First, we show that despite a high\ncorrelation with human judgements achieved by the recent SOTA, certain types of\nmeaning errors are still problematic for QE to detect. Second, we show that on\naverage, the ability of a given model to discriminate between\nmeaning-preserving and meaning-altering perturbations is predictive of its\noverall performance, thus potentially allowing for comparing QE systems without\nrelying on manual quality annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Or&#x103;san</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursively Summarizing Books with Human Feedback. (arXiv:2109.10862v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10862","description":"<p>A major challenge for scaling machine learning is training models to perform\ntasks that are very difficult or time-consuming for humans to evaluate. We\npresent progress on this problem on the task of abstractive summarization of\nentire fiction novels. Our method combines learning from human feedback with\nrecursive task decomposition: we use models trained on smaller parts of the\ntask to assist humans in giving feedback on the broader task. We collect a\nlarge volume of demonstrations and comparisons from human labelers, and\nfine-tune GPT-3 using behavioral cloning and reward modeling to do\nsummarization recursively. At inference time, the model first summarizes small\nsections of the book and then recursively summarizes these summaries to produce\na summary of the entire book. Our human labelers are able to supervise and\nevaluate the models quickly, despite not having read the entire books\nthemselves. Our resulting model generates sensible summaries of entire books,\neven matching the quality of human-written summaries in a few cases ($\\sim5\\%$\nof books). We achieve state-of-the-art results on the recent BookSum dataset\nfor book-length summarization. A zero-shot question-answering model using these\nsummaries achieves state-of-the-art results on the challenging NarrativeQA\nbenchmark for answering questions about books and movie scripts. We release\ndatasets of samples from our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiennon_N/0/1/0/all/0/1\">Nissan Stiennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_R/0/1/0/all/0/1\">Ryan Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1\">Paul Christiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OTEANN: Estimating the Transparency of Orthographies with an Artificial Neural Network. (arXiv:1912.13321v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1912.13321","description":"<p>To transcribe spoken language to written medium, most alphabets enable an\nunambiguous sound-to-letter rule. However, some writing systems have distanced\nthemselves from this simple concept and little work exists in Natural Language\nProcessing (NLP) on measuring such distance. In this study, we use an\nArtificial Neural Network (ANN) model to evaluate the transparency between\nwritten words and their pronunciation, hence its name Orthographic Transparency\nEstimation with an ANN (OTEANN). Based on datasets derived from Wikimedia\ndictionaries, we trained and tested this model to score the percentage of\ncorrect predictions in phoneme-to-grapheme and grapheme-to-phoneme translation\ntasks. The scores obtained on 17 orthographies were in line with the\nestimations of other studies. Interestingly, the model also provided insight\ninto typical mistakes made by learners who only consider the phonemic rule in\nreading and writing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjou_X/0/1/0/all/0/1\">Xavier Marjou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Wait, I'm Still Talking!\" Predicting the Dialogue Interaction Behavior Using Imagine-Then-Arbitrate Model. (arXiv:2002.09616v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2002.09616","description":"<p>Producing natural and accurate responses like human beings is the ultimate\ngoal of intelligent dialogue agents. So far, most of the past works concentrate\non selecting or generating one pertinent and fluent response according to\ncurrent query and its context. These models work on a one-to-one environment,\nmaking one response to one utterance each round. However, in real human-human\nconversations, human often sequentially sends several short messages for\nreadability instead of a long message in one turn. Thus messages will not end\nwith an explicit ending signal, which is crucial for agents to decide when to\nreply. So the first step for an intelligent dialogue agent is not replying but\ndeciding if it should reply at the moment. To address this issue, in this\npaper, we propose a novel Imagine-then-Arbitrate (ITA) neural dialogue model to\nhelp the agent decide whether to wait or to make a response directly. Our\nmethod has two imaginator modules and an arbitrator module. The two imaginators\nwill learn the agent's and user's speaking style respectively, generate\npossible utterances as the input of the arbitrator, combining with dialogue\nhistory. And the arbitrator decides whether to wait or to make a response to\nthe user directly. To verify the performance and effectiveness of our method,\nwe prepared two dialogue datasets and compared our approach with several\npopular models. Experimental results show that our model performs well on\naddressing ending prediction issue and outperforms baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiaoming Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1\">Feng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00033","description":"<p>With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Addressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus confirming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1\">Alex Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Homaid_A/0/1/0/all/0/1\">Abdulaziz Al-Homaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1\">Wajdi Zaghouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselli_T/0/1/0/all/0/1\">Tommaso Caselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danoe_G/0/1/0/all/0/1\">Gijs Danoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolk_F/0/1/0/all/0/1\">Friso Stolk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruntink_B/0/1/0/all/0/1\">Britt Bruntink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predict-then-Decide: A Predictive Approach for Wait or Answer Task in Dialogue Systems. (arXiv:2005.13119v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.13119","description":"<p>Different people have different habits of describing their intents in\nconversations. Some people tend to deliberate their intents in several\nsuccessive utterances, i.e., they use several consistent messages for\nreadability instead of a long sentence to express their question. This creates\na predicament faced by the application of dialogue systems, especially in\nreal-world industry scenarios, in which the dialogue system is unsure whether\nit should answer the query of user immediately or wait for further\nsupplementary input. Motivated by such an interesting predicament, we define a\nnovel Wait-or-Answer task for dialogue systems. We shed light on a new research\ntopic about how the dialogue system can be more intelligent to behave in this\nWait-or-Answer quandary. Further, we propose a predictive approach named\nPredict-then-Decide (PTD) to tackle this Wait-or-Answer task. More\nspecifically, we take advantage of a decision model to help the dialogue system\ndecide whether to wait or answer. The decision of decision model is made with\nthe assistance of two ancillary prediction models: a user prediction and an\nagent prediction. The user prediction model tries to predict what the user\nwould supplement and uses its prediction to persuade the decision model that\nthe user has some information to add, so the dialogue system should wait. The\nagent prediction model tries to predict the answer of the dialogue system and\nconvince the decision model that it is a superior choice to answer the query of\nuser immediately since the input of user has come to an end. We conduct our\nexperiments on two real-life scenarios and three public datasets. Experimental\nresults on five datasets show our proposed PTD approach significantly\noutperforms the existing models in solving this Wait-or-Answer problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shaobo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xiaoming Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_F/0/1/0/all/0/1\">Feng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongzhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haiqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIPFA: Generating IPA Pronunciation from Audio. (arXiv:2006.07573v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.07573","description":"<p>Transcribing spoken audio samples into the International Phonetic Alphabet\n(IPA) has long been reserved for experts. In this study, we examine the use of\nan Artificial Neural Network (ANN) model to automatically extract the IPA\nphonemic pronunciation of a word based on its audio pronunciation, hence its\nname Generating IPA Pronunciation From Audio (GIPFA). Based on the French\nWikimedia dictionary, we trained our model which then correctly predicted 75%\nof the IPA pronunciations tested. Interestingly, by studying inference errors,\nthe model made it possible to highlight possible errors in the dataset as well\nas to identify the closest phonemes in French.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjou_X/0/1/0/all/0/1\">Xavier Marjou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures. (arXiv:2007.08970v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.08970","description":"<p>While mainstream machine learning methods are known to have limited ability\nto compositionally generalize, new architectures and techniques continue to be\nproposed to address this limitation. We investigate state-of-the-art techniques\nand architectures in order to assess their effectiveness in improving\ncompositional generalization in semantic parsing tasks based on the SCAN and\nCFQ datasets. We show that masked language model (MLM) pre-training rivals\nSCAN-inspired architectures on primitive holdout splits. On a more complex\ncompositional task, we show that pre-training leads to significant improvements\nin performance vs. comparable non-pre-trained models, whereas architectures\nproposed to encourage compositional generalization on SCAN or in the area of\nalgorithm learning fail to lead to significant improvements. We establish a new\nstate of the art on the CFQ compositional generalization benchmark using MLM\npre-training together with an intermediate representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Furrer_D/0/1/0/all/0/1\">Daniel Furrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zee_M/0/1/0/all/0/1\">Marc van Zee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scales_N/0/1/0/all/0/1\">Nathan Scales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharli_N/0/1/0/all/0/1\">Nathanael Sch&#xe4;rli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Sound Change: Deep and Iterative Learning, Convolutional Neural Networks, and Language Change. (arXiv:2011.05463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.05463","description":"<p>This paper proposes a framework for modeling sound change that combines deep\nlearning and iterative learning. Acquisition and transmission of speech is\nmodeled by training generations of Generative Adversarial Networks (GANs) on\nunannotated raw speech data. The paper argues that several properties of sound\nchange emerge from the proposed architecture. GANs (Goodfellow et al. 2014\n<a href=\"/abs/1406.2661\">arXiv:1406.2661</a>, Donahue et al. 2019 <a href=\"/abs/1705.07904\">arXiv:1705.07904</a>) are uniquely appropriate\nfor modeling language change because the networks are trained on raw\nunsupervised acoustic data, contain no language-specific features and, as\nargued in Begu\\v{s} (2020 <a href=\"/abs/2006.03965\">arXiv:2006.03965</a>), encode phonetic and phonological\nrepresentations in their latent space and generate linguistically informative\ninnovative data. The first generation of networks is trained on the relevant\nsequences in human speech from TIMIT. The subsequent generations are not\ntrained on TIMIT, but on generated outputs from the previous generation and\nthus start learning from each other in an iterative learning task. The initial\nallophonic distribution is progressively being lost with each generation,\nlikely due to pressures from the global distribution of aspiration in the\ntraining data. The networks show signs of a gradual shift in phonetic targets\ncharacteristic of a gradual phonetic sound change. At endpoints, the outputs\nsuperficially resemble a phonological change -- rule loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1\">Ga&#x161;per Begu&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Transferability of Adversarial Attacksagainst Neural Text Classifier. (arXiv:2011.08558v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.08558","description":"<p>Deep neural networks are vulnerable to adversarial attacks, where a small\nperturbation to an input alters the model prediction. In many cases, malicious\ninputs intentionally crafted for one model can fool another model. In this\npaper, we present the first study to systematically investigate the\ntransferability of adversarial examples for text classification models and\nexplore how various factors, including network architecture, tokenization\nscheme, word embedding, and model capacity, affect the transferability of\nadversarial examples. Based on these studies, we propose a genetic algorithm to\nfind an ensemble of models that can be used to induce adversarial examples to\nfool almost all existing models. Such adversarial examples reflect the defects\nof the learning process and the data bias in the training set. Finally, we\nderive word replacement rules that can be used for model diagnostics from these\nadversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liping Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach. (arXiv:2102.10242v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.10242","description":"<p>Reliable automatic evaluation of dialogue systems under an interactive\nenvironment has long been overdue. An ideal environment for evaluating dialog\nsystems, also known as the Turing test, needs to involve human interaction,\nwhich is usually not affordable for large-scale experiments. Though researchers\nhave attempted to use metrics (e.g., perplexity, BLEU) in language generation\ntasks or some model-based reinforcement learning methods (e.g., self-play\nevaluation) for automatic evaluation, these methods only show a very weak\ncorrelation with the actual human evaluation in practice. To bridge such a gap,\nwe propose a new framework named ENIGMA for estimating human evaluation scores\nbased on recent advances of off-policy evaluation in reinforcement learning.\nENIGMA only requires a handful of pre-collected experience data, and therefore\ndoes not involve human interaction with the target policy during the\nevaluation, making automatic evaluations feasible. More importantly, ENIGMA is\nmodel-free and agnostic to the behavior policies for collecting the experience\ndata (see details in Section 2), which significantly alleviates the technical\ndifficulties of modeling complex dialogue environments and human behaviors. Our\nexperiments show that ENIGMA significantly outperforms existing methods in\nterms of correlation with human evaluation scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Classifiers: Promises, Shortcomings, and Advances. (arXiv:2102.12452v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.12452","description":"<p>Probing classifiers have emerged as one of the prominent methodologies for\ninterpreting and analyzing deep neural network models of natural language\nprocessing. The basic idea is simple -- a classifier is trained to predict some\nlinguistic property from a model's representations -- and has been used to\nexamine a wide variety of models and properties. However, recent studies have\ndemonstrated various methodological limitations of this approach. This article\ncritically reviews the probing classifiers framework, highlighting their\npromises, shortcomings, and advances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media. (arXiv:2104.05893v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05893","description":"<p>Online misinformation is a prevalent societal issue, with adversaries relying\non tools ranging from cheap fakes to sophisticated deep fakes. We are motivated\nby the threat scenario where an image is used out of context to support a\ncertain narrative. While some prior datasets for detecting image-text\ninconsistency generate samples via text manipulation, we propose a dataset\nwhere both image and text are unmanipulated but mismatched. We introduce\nseveral strategies for automatically retrieving convincing images for a given\ncaption, capturing cases with inconsistent entities or semantic context. Our\nlarge-scale automatically generated NewsCLIPpings Dataset: (1) demonstrates\nthat machine-driven image repurposing is now a realistic threat, and (2)\nprovides samples that represent challenging instances of mismatch between text\nand image in news that are able to mislead humans. We benchmark several\nstate-of-the-art multimodal models on our dataset and analyze their performance\nacross different pretraining domains and visual backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Grace Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Attention Free Transformer. (arXiv:2105.14103v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14103","description":"<p>We introduce Attention Free Transformer (AFT), an efficient variant of\nTransformers that eliminates the need for dot product self attention. In an AFT\nlayer, the key and value are first combined with a set of learned position\nbiases, the result of which is multiplied with the query in an element-wise\nfashion. This new operation has a memory complexity linear w.r.t. both the\ncontext size and the dimension of features, making it compatible to both large\ninput and model sizes. We also introduce AFT-local and AFT-conv, two model\nvariants that take advantage of the idea of locality and spatial weight sharing\nwhile maintaining global connectivity. We conduct extensive experiments on two\nautoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image\nrecognition task (ImageNet-1K classification). We show that AFT demonstrates\ncompetitive performance on all the benchmarks, while providing excellent\nefficiency at the same time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_N/0/1/0/all/0/1\">Nitish Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hanlin Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Learning with Cross Attention for Keyword Spotting. (arXiv:2107.07634v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.07634","description":"<p>Keyword spotting (KWS) is an important technique for speech applications,\nwhich enables users to activate devices by speaking a keyword phrase. Although\na phoneme classifier can be used for KWS, exploiting a large amount of\ntranscribed data for automatic speech recognition (ASR), there is a mismatch\nbetween the training criterion (phoneme recognition) and the target task (KWS).\nRecently, multi-task learning has been applied to KWS to exploit both ASR and\nKWS training data. In this approach, an output of an acoustic model is split\ninto two branches for the two tasks, one for phoneme transcription trained with\nthe ASR data and one for keyword classification trained with the KWS data. In\nthis paper, we introduce a cross attention decoder in the multi-task learning\nframework. Unlike the conventional multi-task learning approach with the simple\nsplit of the output layer, the cross attention decoder summarizes information\nfrom a phonetic encoder by performing cross attention between the encoder\noutputs and a trainable query sequence to predict a confidence score for the\nKWS task. Experimental results on KWS tasks show that the proposed approach\nachieves a 12% relative reduction in the false reject ratios compared to the\nconventional multi-task learning with split branches and a bi-directional long\nshort-team memory decoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_T/0/1/0/all/0/1\">Takuya Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Anmol Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dhir_C/0/1/0/all/0/1\">Chandra Dhir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04699","description":"<p>While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2109.05184","description":"<p>Internet memes have become powerful means to transmit political,\npsychological, and socio-cultural ideas. Although memes are typically humorous,\nrecent days have witnessed an escalation of harmful memes used for trolling,\ncyberbullying, and abuse. Detecting such memes is challenging as they can be\nhighly satirical and cryptic. Moreover, while previous work has focused on\nspecific aspects of memes such as hate speech and propaganda, there has been\nlittle work on harm in general. Here, we aim to bridge this gap. We focus on\ntwo tasks: (i)detecting harmful memes, and (ii)identifying the social entities\nthey target. We further extend a recently released HarMeme dataset, which\ncovered COVID-19, with additional memes and a new topic: US politics. To solve\nthese tasks, we propose MOMENTA (MultimOdal framework for detecting harmful\nMemEs aNd Their tArgets), a novel multimodal deep neural network that uses\nglobal and local perspectives to detect harmful memes. MOMENTA systematically\nanalyzes the local and the global perspective of the input meme (in both\nmodalities) and relates it to the background context. MOMENTA is interpretable\nand generalizable, and our experiments show that it outperforms several strong\nrivaling approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyword Extraction for Improved Document Retrieval in Conversational Search. (arXiv:2109.05979v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05979","description":"<p>Recent research has shown that mixed-initiative conversational search, based\non the interaction between users and computers to clarify and improve a query,\nprovides enormous advantages. Nonetheless, incorporating additional information\nprovided by the user from the conversation poses some challenges. In fact,\nfurther interactions could confuse the system as a user might use words\nirrelevant to the information need but crucial for correct sentence\nconstruction in the context of multi-turn conversations. To this aim, in this\npaper, we have collected two conversational keyword extraction datasets and\npropose an end-to-end document retrieval pipeline incorporating them.\nFurthermore, we study the performance of two neural keyword extraction models,\nnamely, BERT and sequence to sequence, in terms of extraction accuracy and\nhuman annotation. Finally, we study the effect of keyword extraction on the\nend-to-end neural IR performance and show that our approach beats\nstate-of-the-art IR models. We make the two datasets publicly available to\nfoster research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borisov_O/0/1/0/all/0/1\">Oleg Borisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliannejadi_M/0/1/0/all/0/1\">Mohammad Aliannejadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crestani_F/0/1/0/all/0/1\">Fabio Crestani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense-Focused Dialogues for Response Generation: An Empirical Study. (arXiv:2109.06427v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06427","description":"<p>Smooth and effective communication requires the ability to perform latent or\nexplicit commonsense inference. Prior commonsense reasoning benchmarks (such as\nSocialIQA and CommonsenseQA) mainly focus on the discriminative task of\nchoosing the right answer from a set of candidates, and do not involve\ninteractive language generation as in dialogue. Moreover, existing dialogue\ndatasets do not explicitly focus on exhibiting commonsense as a facet. In this\npaper, we present an empirical study of commonsense in dialogue response\ngeneration. We first auto-extract commonsensical dialogues from existing\ndialogue datasets by leveraging ConceptNet, a commonsense knowledge graph.\nFurthermore, building on social contexts/situations in SocialIQA, we collect a\nnew dialogue dataset with 25K dialogues aimed at exhibiting social commonsense\nin an interactive setting. We evaluate response generation models trained using\nthese datasets and find that models trained on both extracted and our collected\ndata produce responses that consistently exhibit more commonsense than\nbaselines. Finally we propose an approach for automatic evaluation of\ncommonsense that relies on features derived from ConceptNet and pre-trained\nlanguage and dialog models, and show reasonable correlation with human\nevaluation of responses' commonsense quality. We are releasing a subset of our\ncollected data, Commonsense-Dialogues, containing about 11K dialogs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Clinical Information Extraction with Transferred Contextual Embeddings. (arXiv:2109.07243v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07243","description":"<p>The Bidirectional Encoder Representations from Transformers (BERT) model has\nachieved the state-of-the-art performance for many natural language processing\n(NLP) tasks. Yet, limited research has been contributed to studying its\neffectiveness when the target domain is shifted from the pre-training corpora,\nfor example, for biomedical or clinical NLP applications. In this paper, we\napplied it to a widely studied a hospital information extraction (IE) task and\nanalyzed its performance under the transfer learning setting. Our application\nbecame the new state-of-the-art result by a clear margin, compared with a range\nof existing IE models. Specifically, on this nursing handover data set, the\nmacro-average F1 score from our model was 0.438, whilst the previous best deep\nlearning models had 0.416. In conclusion, we showed that BERT based\npre-training models can be transferred to health-related documents under mild\nconditions and with a proper fine-tuning process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zimin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenchen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suominen_H/0/1/0/all/0/1\">Hanna Suominen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DyLex: Incorporating Dynamic Lexicons into BERT for Sequence Labeling. (arXiv:2109.08818v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08818","description":"<p>Incorporating lexical knowledge into deep learning models has been proved to\nbe very effective for sequence labeling tasks. However, previous works commonly\nhave difficulty dealing with large-scale dynamic lexicons which often cause\nexcessive matching noise and problems of frequent updates. In this paper, we\npropose DyLex, a plug-in lexicon incorporation approach for BERT based sequence\nlabeling tasks. Instead of leveraging embeddings of words in the lexicon as in\nconventional methods, we adopt word-agnostic tag embeddings to avoid\nre-training the representation while updating the lexicon. Moreover, we employ\nan effective supervised lexical knowledge denoising method to smooth out\nmatching noise. Finally, we introduce a col-wise attention based knowledge\nfusion mechanism to guarantee the pluggability of the proposed framework.\nExperiments on ten datasets of three tasks show that the proposed framework\nachieves new SOTA, even with very large scale lexicons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1\">Guang-Yuan Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Joint Intent Detection and Slot Filling via Higher-order Attention. (arXiv:2109.08890v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08890","description":"<p>Intent detection (ID) and Slot filling (SF) are two major tasks in spoken\nlanguage understanding (SLU). Recently, attention mechanism has been shown to\nbe effective in jointly optimizing these two tasks in an interactive manner.\nHowever, latest attention-based works concentrated only on the first-order\nattention design, while ignoring the exploration of higher-order attention\nmechanisms. In this paper, we propose a BiLinear attention block, which\nleverages bilinear pooling to simultaneously exploit both the contextual and\nchannel-wise bilinear attention distributions to capture the second-order\ninteractions between the input intent or slot features. Higher and even\ninfinity order interactions are built by stacking numerous blocks and assigning\nExponential Linear Unit (ELU) to blocks. Before the decoding stage, we\nintroduce the Dynamic Feature Fusion Layer to implicitly fuse intent and slot\ninformation in a more effective way. Technically, instead of simply\nconcatenating intent and slot features, we first compute two correlation\nmatrices to weight on two features. Furthermore, we present Higher-order\nAttention Network for the SLU tasks. Experiments on two benchmark datasets show\nthat our approach yields improvements compared with the state-of-the-art\napproach. We also provide discussion to demonstrate the effectiveness of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongsheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}