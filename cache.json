{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Cross-linguistically Consistent Semantic and Syntactic Annotation of Child-directed Speech. (arXiv:2109.10952v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10952","description":"<p>While corpora of child speech and child-directed speech (CDS) have enabled\nmajor contributions to the study of child language acquisition, semantic\nannotation for such corpora is still scarce and lacks a uniform standard. We\ncompile two CDS corpora with sentential logical forms, one in English and the\nother in Hebrew. In compiling the corpora we employ a methodology that enforces\na cross-linguistically consistent representation, building on recent advances\nin dependency representation and semantic parsing. The corpora are based on a\nsizable portion of Brown's Adam corpus from CHILDES (about 80% of its\nchild-directed utterances), and to all child-directed utterances from Berman's\nHebrew CHILDES corpus Hagar.\n</p>\n<p>We begin by annotating the corpora with the Universal Dependencies (UD)\nscheme for syntactic annotation, motivated by its applicability to a wide\nvariety of domains and languages. We then proceed by applying an automatic\nmethod for transducing sentential logical forms (LFs) from UD structures. The\ntwo representations have complementary strengths: UD structures are\nlanguage-neutral and support direct annotation, whereas LFs are neutral as to\nthe interface between syntax and semantics, and transparently encode semantic\ndistinctions. We verify the quality of the annotated UD annotation using an\ninter-annotator agreement study. We then demonstrate the utility of the\ncompiled corpora through a longitudinal corpus study of the prevalence of\ndifferent syntactic and semantic phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szubert_I/0/1/0/all/0/1\">Ida Szubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibbon_S/0/1/0/all/0/1\">Samuel Gibbon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Fact-checking with Human-in-the-Loop. (arXiv:2109.10992v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10992","description":"<p>Researchers have been investigating automated solutions for fact-checking in\na variety of fronts. However, current approaches often overlook the fact that\nthe amount of information released every day is escalating, and a large amount\nof them overlap. Intending to accelerate fact-checking, we bridge this gap by\ngrouping similar messages and summarizing them into aggregated claims.\nSpecifically, we first clean a set of social media posts (e.g., tweets) and\nbuild a graph of all posts based on their semantics; Then, we perform two\nclustering methods to group the messages for further claim summarization. We\nevaluate the summaries both quantitatively with ROUGE scores and qualitatively\nwith human evaluation. We also generate a graph of summaries to verify that\nthere is no significant overlap among them. The results reduced 28,818 original\nmessages to 700 summary claims, showing the potential to speed up the\nfact-checking process by organizing and selecting representative claims from\nmassive disorganized and redundant messages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vega_Oliveros_D/0/1/0/all/0/1\">Didier Vega-Oliveros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibt_T/0/1/0/all/0/1\">Tais Seibt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT. (arXiv:2109.11010v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11010","description":"<p>Alzheimers disease is a fatal progressive brain disorder that worsens with\ntime. It is high time we have inexpensive and quick clinical diagnostic\ntechniques for early detection and care. In previous studies, various Machine\nLearning techniques and Pre-trained Deep Learning models have been used in\nconjunction with the extraction of various acoustic and linguistic features.\nOur study focuses on three models for the classification task in the ADReSS\n(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021\nChallenge. We use the well-balanced dataset provided by the ADReSS Challenge\nfor training and validating our models. Model 1 uses various acoustic features\nfrom the eGeMAPs feature-set, Model 2 uses various linguistic features that we\ngenerated from auto-generated transcripts and Model 3 uses the auto-generated\ntranscripts directly to extract features using a Pre-trained BERT and TF-IDF.\nThese models are described in detail in the models section.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valsaraj_A/0/1/0/all/0/1\">Akshay Valsaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madala_I/0/1/0/all/0/1\">Ithihas Madala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1\">Nikhil Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1\">Veeky Baths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Decomposition for Table-based Fact Verification. (arXiv:2109.11020v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11020","description":"<p>Fact verification based on structured data is challenging as it requires\nmodels to understand both natural language and symbolic operations performed\nover tables. Although pre-trained language models have demonstrated a strong\ncapability in verifying simple statements, they struggle with complex\nstatements that involve multiple operations. In this paper, we improve fact\nverification by decomposing complex statements into simpler subproblems.\nLeveraging the programs synthesized by a weakly supervised semantic parser, we\npropose a program-guided approach to constructing a pseudo dataset for\ndecomposition model training. The subproblems, together with their predicted\nanswers, serve as the intermediate evidence to enhance our fact verification\nmodel. Experiments show that our proposed approach achieves the new\nstate-of-the-art performance, an 82.7\\% accuracy, on the TabFact benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Poisson Stochastic Beam Search. (arXiv:2109.11034v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11034","description":"<p>Beam search is the default decoding strategy for many sequence generation\ntasks in NLP. The set of approximate K-best items returned by the algorithm is\na useful summary of the distribution for many applications; however, the\ncandidates typically exhibit high overlap and may give a highly biased estimate\nfor expectations under our model. These problems can be addressed by instead\nusing stochastic decoding strategies. In this work, we propose a new method for\nturning beam search into a stochastic process: Conditional Poisson stochastic\nbeam search. Rather than taking the maximizing set at each iteration, we sample\nK candidates without replacement according to the conditional Poisson sampling\ndesign. We view this as a more natural alternative to Kool et. al. 2019's\nstochastic beam search (SBS). Furthermore, we show how samples generated under\nthe CPSBS design can be used to build consistent estimators and sample diverse\nsets from sequence models. In our experiments, we observe CPSBS produces lower\nvariance and more efficient estimators than SBS, even showing improvements in\nhigh entropy settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viera_T/0/1/0/all/0/1\">Tim Viera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models. (arXiv:2109.11058v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11058","description":"<p>Prior work has shown that structural supervision helps English language\nmodels learn generalizations about syntactic phenomena such as subject-verb\nagreement. However, it remains unclear if such an inductive bias would also\nimprove language models' ability to learn grammatical dependencies in\ntypologically different languages. Here we investigate this question in\nMandarin Chinese, which has a logographic, largely syllable-based writing\nsystem; different word order; and sparser morphology than English. We train\nLSTMs, Recurrent Neural Network Grammars, Transformer language models, and\nTransformer-parameterized generative parsing models on two Mandarin Chinese\ndatasets of different sizes. We evaluate the models' ability to learn different\naspects of Mandarin grammar that assess syntactic and semantic relationships.\nWe find suggestive evidence that structural supervision helps with representing\nsyntactic state across intervening content and improves performance in low-data\nsettings, suggesting that the benefits of hierarchical inductive biases in\nacquiring dependency relationships may extend beyond English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jennifer Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender. (arXiv:2109.11061v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11061","description":"<p>Individuals signal aspects of their identity and beliefs through linguistic\nchoices. Studying these choices in aggregate allows us to examine large-scale\nattitude shifts within a population. Here, we develop computational methods to\nstudy word choice within a sociolinguistic lexical variable -- alternate words\nused to express the same concept -- in order to test for change in the United\nStates towards sexuality and gender. We examine two variables: i) referents to\nsignificant others, such as the word \"partner\" and ii) referents to an\nindefinite person, both of which could optionally be marked with gender. The\nlinguistic choices in each variable allow us to study increased rates of\nacceptances of gay marriage and gender equality, respectively. In longitudinal\nanalyses across Twitter and Reddit over 87M messages, we demonstrate that\nattitudes are changing but that these changes are driven by specific\ndemographics within the United States. Further, in a quasi-causal analysis, we\nshow that passages of Marriage Equality Acts in different states are drivers of\nlinguistic change.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+CH_Wang_S/0/1/0/all/0/1\">Sky CH-Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actionable Conversational Quality Indicators for Improving Task-Oriented Dialog Systems. (arXiv:2109.11064v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11064","description":"<p>Automatic dialog systems have become a mainstream part of online customer\nservice. Many such systems are built, maintained, and improved by customer\nservice specialists, rather than dialog systems engineers and computer\nprogrammers. As conversations between people and machines become commonplace,\nit is critical to understand what is working, what is not, and what actions can\nbe taken to reduce the frequency of inappropriate system responses. These\nanalyses and recommendations need to be presented in terms that directly\nreflect the user experience rather than the internal dialog processing.\n</p>\n<p>This paper introduces and explains the use of Actionable Conversational\nQuality Indicators (ACQIs), which are used both to recognize parts of dialogs\nthat can be improved, and to recommend how to improve them. This combines\nbenefits of previous approaches, some of which have focused on producing dialog\nquality scoring while others have sought to categorize the types of errors the\ndialog system is making.\n</p>\n<p>We demonstrate the effectiveness of using ACQIs on LivePerson internal dialog\nsystems used in commercial customer service applications, and on the publicly\navailable CMU LEGOv2 conversational dataset (Raux et al. 2005). We report on\nthe annotation and analysis of conversational datasets showing which ACQIs are\nimportant to fix in various situations.\n</p>\n<p>The annotated datasets are then used to build a predictive model which uses a\nturn-based vector embedding of the message texts and achieves an 79% weighted\naverage f1-measure at the task of finding the correct ACQI for a given\nconversation. We predict that if such a model worked perfectly, the range of\npotential improvement actions a bot-builder must consider at each turn could be\nreduced by an average of 81%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Higgins_M/0/1/0/all/0/1\">Michael Higgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widdows_D/0/1/0/all/0/1\">Dominic Widdows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brew_C/0/1/0/all/0/1\">Chris Brew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christian_G/0/1/0/all/0/1\">Gwen Christian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1\">Andrew Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_M/0/1/0/all/0/1\">Matthew Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathi_S/0/1/0/all/0/1\">Sujit Mathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazare_A/0/1/0/all/0/1\">Akshay Hazare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonev_G/0/1/0/all/0/1\">George Bonev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hockey_B/0/1/0/all/0/1\">Beth Ann Hockey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howell_K/0/1/0/all/0/1\">Kristen Howell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_J/0/1/0/all/0/1\">Joe Bradley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Universal Dense Retrieval for Open-domain Question Answering. (arXiv:2109.11085v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11085","description":"<p>In open-domain question answering, a model receives a text question as input\nand searches for the correct answer using a large evidence corpus. The\nretrieval step is especially difficult as typical evidence corpora have\n\\textit{millions} of documents, each of which may or may not have the correct\nanswer to the question. Very recently, dense models have replaced sparse\nmethods as the de facto retrieval method. Rather than focusing on lexical\noverlap to determine similarity, dense methods build an encoding function that\ncaptures semantic similarity by learning from a small collection of\nquestion-answer or question-context pairs. In this paper, we investigate dense\nretrieval models in the context of open-domain question answering across\ndifferent input distributions. To do this, first we introduce an entity-rich\nquestion answering dataset constructed from Wikidata facts and demonstrate\ndense models are unable to generalize to unseen input question distributions.\nSecond, we perform analyses aimed at better understanding the source of the\nproblem and propose new training techniques to improve out-of-domain\nperformance on a wide variety of datasets. We encourage the field to further\ninvestigate the creation of a single, universal dense retrieval model that\ngeneralizes well across all input distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sciavolino_C/0/1/0/all/0/1\">Christopher Sciavolino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles. (arXiv:2109.11087v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11087","description":"<p>A riddle is a question or statement with double or veiled meanings, followed\nby an unexpected answer. Solving riddle is a challenging task for both machine\nand human, testing the capability of understanding figurative, creative natural\nlanguage and reasoning with commonsense knowledge. We introduce BiRdQA, a\nbilingual multiple-choice question answering dataset with 6614 English riddles\nand 8751 Chinese riddles. For each riddle-answer pair, we provide four\ndistractors with additional information from Wikipedia. The distractors are\nautomatically generated at scale with minimal bias. Existing monolingual and\nmultilingual QA models fail to perform well on our dataset, indicating that\nthere is a long way to go before machine can beat human on solving tricky\nriddles. The dataset has been released to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing. (arXiv:2109.11105v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11105","description":"<p>We aim to identify how different components in the KD pipeline affect the\nresulting performance and how much the optimal KD pipeline varies across\ndifferent datasets/tasks, such as the data augmentation policy, the loss\nfunction, and the intermediate representation for transferring the knowledge\nbetween teacher and student. To tease apart their effects, we propose\nDistiller, a meta KD framework that systematically combines a broad range of\ntechniques across different stages of the KD pipeline, which enables us to\nquantify each component's contribution. Within Distiller, we unify commonly\nused objectives for distillation of intermediate representations under a\nuniversal mutual information (MI) objective and propose a class of MI-$\\alpha$\nobjective functions with better bias/variance trade-off for estimating the MI\nbetween the teacher and the student. On a diverse set of NLP datasets, the best\nDistiller configurations are identified via large-scale hyperparameter\noptimization. Our experiments reveal the following: 1) the approach used to\ndistill the intermediate representations is the most important factor in KD\nperformance, 2) among different objectives for intermediate distillation,\nMI-$\\alpha$ performs the best, and 3) data augmentation provides a large boost\nfor small training datasets or small student networks. Moreover, we find that\ndifferent datasets/tasks prefer different KD algorithms, and thus propose a\nsimple AutoDistiller algorithm that can recommend a good KD pipeline for a new\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zha Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Language Model Meta-Pretraining. (arXiv:2109.11129v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11129","description":"<p>The success of pretrained cross-lingual language models relies on two\nessential abilities, i.e., generalization ability for learning downstream tasks\nin a source language, and cross-lingual transferability for transferring the\ntask knowledge to other languages. However, current methods jointly learn the\ntwo abilities in a single-phase cross-lingual pretraining process, resulting in\na trade-off between generalization and cross-lingual transfer. In this paper,\nwe propose cross-lingual language model meta-pretraining, which learns the two\nabilities in different training phases. Our method introduces an additional\nmeta-pretraining phase before cross-lingual pretraining, where the model learns\ngeneralization ability on a large-scale monolingual corpus. Then, the model\nfocuses on learning cross-lingual transfer on a multilingual corpus.\nExperimental results show that our method improves both generalization and\ncross-lingual transfer, and produces better-aligned representations across\ndifferent languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Online Learning from Human Feedback for Neural Machine Translation. (arXiv:2109.11136v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11136","description":"<p>We study the problem of online learning with human feedback in the\nhuman-in-the-loop machine translation, in which the human translators revise\nthe machine-generated translations and then the corrected translations are used\nto improve the neural machine translation (NMT) system. However, previous\nmethods require online model updating or additional translation memory networks\nto achieve high-quality performance, making them inflexible and inefficient in\npractice. In this paper, we propose a novel non-parametric online learning\nmethod without changing the model structure. This approach introduces two\nk-nearest-neighbor (KNN) modules: one module memorizes the human feedback,\nwhich is the correct sentences provided by human translators, while the other\nbalances the usage of the history human feedback and original NMT models\nadaptively. Experiments conducted on EMEA and JRC-Acquis benchmarks demonstrate\nthat our proposed method obtains substantial improvements on translation\naccuracy and achieves better adaptation performance with less repeating human\ncorrection operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Haoran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint speaker diarisation and tracking in switching state-space model. (arXiv:2109.11140v1 [cs.SD])","link":"http://arxiv.org/abs/2109.11140","description":"<p>Speakers may move around while diarisation is being performed. When a\nmicrophone array is used, the instantaneous locations of where the sounds\noriginated from can be estimated, and previous investigations have shown that\nsuch information can be complementary to speaker embeddings in the diarisation\ntask. However, these approaches often assume that speakers are fairly\nstationary throughout a meeting. This paper relaxes this assumption, by\nproposing to explicitly track the movements of speakers while jointly\nperforming diarisation within a unified model. A state-space model is proposed,\nwhere the hidden state expresses the identity of the current active speaker and\nthe predicted locations of all speakers. The model is implemented as a particle\nfilter. Experiments on a Microsoft rich meeting transcription task show that\nthe proposed joint location tracking and diarisation approach is able to\nperform comparably with other methods that use location information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jeremy H. M. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Information Extraction as a Unified Text-to-Triple Translation. (arXiv:2109.11171v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11171","description":"<p>We cast a suite of information extraction tasks into a text-to-triple\ntranslation framework. Instead of solving each task relying on task-specific\ndatasets and models, we formalize the task as a translation between\ntask-specific input text and output triples. By taking the task-specific input,\nwe enable a task-agnostic translation by leveraging the latent knowledge that a\npre-trained language model has about the task. We further demonstrate that a\nsimple pre-training task of predicting which relational information corresponds\nto which input text is an effective way to produce task-specific outputs. This\nenables the zero-shot transfer of our framework to downstream tasks. We study\nthe zero-shot performance of this framework on open information extraction\n(OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and\nfactual probe (Google-RE and T-REx). The model transfers non-trivially to most\ntasks and is often competitive with a fully supervised method without the need\nfor any task-specific training. For instance, we significantly outperform the\nF1 score of the supervised open information extraction without needing to use\nits training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Curriculum Learning in Unsupervised Neural Machine Translation. (arXiv:2109.11177v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11177","description":"<p>Back-translation (BT) has become one of the de facto components in\nunsupervised neural machine translation (UNMT), and it explicitly makes UNMT\nhave translation ability. However, all the pseudo bi-texts generated by BT are\ntreated equally as clean data during optimization without considering the\nquality diversity, leading to slow convergence and limited translation\nperformance. To address this problem, we propose a curriculum learning method\nto gradually utilize pseudo bi-texts based on their quality from multiple\ngranularities. Specifically, we first apply cross-lingual word embedding to\ncalculate the potential translation difficulty (quality) for the monolingual\nsentences. Then, the sentences are fed into UNMT from easy to hard batch by\nbatch. Furthermore, considering the quality of sentences/tokens in a particular\nbatch are also diverse, we further adopt the model itself to calculate the\nfine-grained quality scores, which are served as learning factors to balance\nthe contributions of different parts when computing loss and encourage the UNMT\nmodel to focus on pseudo data with higher quality. Experimental results on WMT\n14 En-Fr, WMT 16 En-De, WMT 16 En-Ro, and LDC En-Zh translation tasks\ndemonstrate that the proposed method achieves consistent improvements with\nfaster convergence speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinliang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Linguistic Knowledge for Abstractive Multi-document Summarization. (arXiv:2109.11199v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11199","description":"<p>Within natural language processing tasks, linguistic knowledge can always\nserve an important role in assisting the model to learn excel representations\nand better guide the natural language generation. In this work, we develop a\nneural network based abstractive multi-document summarization (MDS) model which\nleverages dependency parsing to capture cross-positional dependencies and\ngrammatical structures. More concretely, we process the dependency information\ninto the linguistic-guided attention mechanism and further fuse it with the\nmulti-head attention for better feature representation. With the help of\nlinguistic signals, sentence-level relations can be correctly captured, thus\nimproving MDS performance. Our model has two versions based on Flat-Transformer\nand Hierarchical Transformer respectively. Empirical studies on both versions\ndemonstrate that this simple but effective method outperforms existing works on\nthe benchmark dataset. Extensive analyses examine different settings and\nconfigurations of the proposed model which provide a good reference to the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Congbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shubham Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingyu Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fuzzy Generalised Quantifiers for Natural Language in Categorical Compositional Distributional Semantics. (arXiv:2109.11227v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11227","description":"<p>Recent work on compositional distributional models shows that bialgebras over\nfinite dimensional vector spaces can be applied to treat generalised\nquantifiers for natural language. That technique requires one to construct the\nvector space over powersets, and therefore is computationally costly. In this\npaper, we overcome this problem by considering fuzzy versions of quantifiers\nalong the lines of Zadeh, within the category of many valued relations. We show\nthat this category is a concrete instantiation of the compositional\ndistributional model. We show that the semantics obtained in this model is\nequivalent to the semantics of the fuzzy quantifiers of Zadeh. As a result, we\nare now able to treat fuzzy quantification without requiring a powerset\nconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dostal_M/0/1/0/all/0/1\">Matej Dostal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijnholds_G/0/1/0/all/0/1\">Gijs Wijnholds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pregroup Grammars, their Syntax and Semantics. (arXiv:2109.11237v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11237","description":"<p>Pregroup grammars were developed in 1999 and stayed Lambek's preferred\nalgebraic model of grammar. The set-theoretic semantics of pregroups, however,\nfaces an ambiguity problem. In his latest book, Lambek suggests that this\nproblem might be overcome using finite dimensional vector spaces rather than\nsets. What is the right notion of composition in this setting, direct sum or\ntensor product of spaces?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21. (arXiv:2109.11247v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11247","description":"<p>This paper describes the Volctrans' submission to the WMT21 news translation\nshared task for German-&gt;English translation. We build a parallel (i.e.,\nnon-autoregressive) translation system using the Glancing Transformer, which\nenables fast and accurate parallel decoding in contrast to the currently\nprevailing autoregressive models. To the best of our knowledge, this is the\nfirst parallel translation system that can be scaled to such a practical\nscenario like WMT competition. More importantly, our parallel translation\nsystem achieves the best BLEU score (35.0) on German-&gt;English translation task,\noutperforming all strong autoregressive counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lihua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Question Generation Debias Question Answering Models? A Case Study on Question-Context Lexical Overlap. (arXiv:2109.11256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11256","description":"<p>Question answering (QA) models for reading comprehension have been\ndemonstrated to exploit unintended dataset biases such as question-context\nlexical overlap. This hinders QA models from generalizing to under-represented\nsamples such as questions with low lexical overlap. Question generation (QG), a\nmethod for augmenting QA datasets, can be a solution for such performance\ndegradation if QG can properly debias QA datasets. However, we discover that\nrecent neural QG models are biased towards generating questions with high\nlexical overlap, which can amplify the dataset bias. Moreover, our analysis\nreveals that data augmentation with these QG models frequently impairs the\nperformance on questions with low lexical overlap, while improving that on\nquestions with high lexical overlap. To address this problem, we use a synonym\nreplacement-based approach to augment questions with low lexical overlap. We\ndemonstrate that the proposed data augmentation approach is simple yet\neffective to mitigate the degradation problem with only 70k synthetic examples.\nOur data is publicly available at\nhttps://github.com/KazutoshiShinoda/Synonym-Replacement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazutoshi Shinoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System. (arXiv:2109.11292v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11292","description":"<p>Consistency Identification has obtained remarkable success on open-domain\ndialogue, which can be used for preventing inconsistent response generation.\nHowever, in contrast to the rapid development in open-domain dialogue, few\nefforts have been made to the task-oriented dialogue direction. In this paper,\nwe argue that consistency problem is more urgent in task-oriented domain. To\nfacilitate the research, we introduce CI-ToD, a novel dataset for Consistency\nIdentification in Task-oriented Dialog system. In addition, we not only\nannotate the single label to enable the model to judge whether the system\nresponse is contradictory, but also provide more fine-grained labels (i.e.,\nDialogue History Inconsistency, User Query Inconsistency and Knowledge Base\nInconsistency) to encourage model to know what inconsistent sources lead to it.\nEmpirical results show that state-of-the-art methods only achieve 51.3%, which\nis far behind the human performance of 93.2%, indicating that there is ample\nroom for improving consistency identification ability. Finally, we conduct\nexhaustive experiments and qualitative analysis to comprehend key challenges\nand provide guidance for future directions. All datasets and models are\npublicly available at \\url{https://github.com/yizhen20133868/CI-ToD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Knowledge Distillation for Pre-trained Language Models. (arXiv:2109.11295v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11295","description":"<p>Knowledge distillation~(KD) has been proved effective for compressing\nlarge-scale pre-trained language models. However, existing methods conduct KD\nstatically, e.g., the student model aligns its output distribution to that of a\nselected teacher model on the pre-defined training dataset. In this paper, we\nexplore whether a dynamic knowledge distillation that empowers the student to\nadjust the learning procedure according to its competency, regarding the\nstudent performance and learning efficiency. We explore the dynamical\nadjustments on three aspects: teacher model adoption, data selection, and KD\nobjective adaptation. Experimental results show that (1) proper selection of\nteacher model can boost the performance of student model; (2) conducting KD\nwith 10% informative instances achieves comparable performance while greatly\naccelerates the training; (3) the student performance can be boosted by\nadjusting the supervision contribution of different alignment objective. We\nfind dynamic knowledge distillation is promising and provide discussions on\npotential future directions towards more efficient KD methods. Our code is\navailable at https://github.com/lancopku/DynamicKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11308","description":"<p>Both generic and domain-specific BERT models are widely used for natural\nlanguage processing (NLP) tasks. In this paper we investigate the vulnerability\nof BERT models to variation in input data for Named Entity Recognition (NER)\nthrough adversarial attack. Experimental results show that the original as well\nas the domain-specific BERT models are highly vulnerable to entity replacement:\nThey can be fooled in 89.2 to 99.4% of the cases to mislabel previously correct\nentities. BERT models are also vulnerable to variation in the entity context\nwith 20.2 to 45.0% of entities predicted completely wrong and another 29.3 to\n53.3% of entities predicted wrong partially. Often a single change is\nsufficient to fool the model. BERT models seem most vulnerable to changes in\nthe local context of entities. Of the two domain-specific BERT models, the\nvulnerability of BioBERT is comparable to the original BERT model whereas\nSciBERT is even more vulnerable. Our results chart the vulnerabilities of BERT\nmodels for NER and emphasize the importance of further research into uncovering\nand reducing these weaknesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dirkson_A/0/1/0/all/0/1\">Anne Dirkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraaij_W/0/1/0/all/0/1\">Wessel Kraaij</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaShoot: A Hebrew Question Answering Dataset. (arXiv:2109.11314v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11314","description":"<p>NLP research in Hebrew has largely focused on morphology and syntax, where\nrich annotated datasets in the spirit of Universal Dependencies are available.\nSemantic datasets, however, are in short supply, hindering crucial advances in\nthe development of NLP technology in Hebrew. In this work, we present\nParaShoot, the first question answering dataset in modern Hebrew. The dataset\nfollows the format and crowdsourcing methodology of SQuAD, and contains\napproximately 3000 annotated examples, similar to other question-answering\ndatasets in low-resource languages. We provide the first baseline results using\nrecently-released BERT-style models for Hebrew, showing that there is\nsignificant room for improvement on this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keren_O/0/1/0/all/0/1\">Omri Keren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Argument Strength Estimation. (arXiv:2109.11319v1 [cs.LG])","link":"http://arxiv.org/abs/2109.11319","description":"<p>High-quality arguments are an essential part of decision-making.\nAutomatically predicting the quality of an argument is a complex task that\nrecently got much attention in argument mining. However, the annotation effort\nfor this task is exceptionally high. Therefore, we test uncertainty-based\nactive learning (AL) methods on two popular argument-strength data sets to\nestimate whether sample-efficient learning can be enabled. Our extensive\nempirical evaluation shows that uncertainty-based acquisition functions can not\nsurpass the accuracy reached with the random acquisition on these data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kees_N/0/1/0/all/0/1\">Nataliia Kees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fromm_M/0/1/0/all/0/1\">Michael Fromm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faerman_E/0/1/0/all/0/1\">Evgeniy Faerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidl_T/0/1/0/all/0/1\">Thomas Seidl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?. (arXiv:2109.11321v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11321","description":"<p>Large language models are known to suffer from the hallucination problem in\nthat they are prone to output statements that are false or inconsistent,\nindicating a lack of knowledge. A proposed solution to this is to provide the\nmodel with additional data modalities that complements the knowledge obtained\nthrough text. We investigate the use of visual data to complement the knowledge\nof large language models by proposing a method for evaluating visual knowledge\ntransfer to text for uni- or multimodal language models. The method is based on\ntwo steps, 1) a novel task querying for knowledge of memory colors, i.e.\ntypical colors of well-known objects, and 2) filtering of model training data\nto clearly separate knowledge contributions. Additionally, we introduce a model\narchitecture that involves a visual imagination step and evaluate it with our\nproposed method. We find that our method can successfully be used to measure\nvisual knowledge transfer capabilities in models and that our novel model\narchitecture shows promising results for leveraging multimodal knowledge in a\nunimodal setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1\">Tobias Norlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_L/0/1/0/all/0/1\">Lovisa Hagstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johanssom_R/0/1/0/all/0/1\">Richard Johanssom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Current State of Finnish NLP. (arXiv:2109.11326v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11326","description":"<p>There are a lot of tools and resources available for processing Finnish. In\nthis paper, we survey recent papers focusing on Finnish NLP related to many\ndifferent subcategories of NLP such as parsing, generation, semantics and\nspeech. NLP research is conducted in many different research groups in Finland,\nand it is frequently the case that NLP tools and models resulting from academic\nresearch are made available for others to use on platforms such as Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Pattern- and Fact-based Fake News Detection via Model Preference Learning. (arXiv:2109.11333v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11333","description":"<p>To defend against fake news, researchers have developed various methods based\non texts. These methods can be grouped as 1) pattern-based methods, which focus\non shared patterns among fake news posts rather than the claim itself; and 2)\nfact-based methods, which retrieve from external sources to verify the claim's\nveracity without considering patterns. The two groups of methods, which have\ndifferent preferences of textual clues, actually play complementary roles in\ndetecting fake news. However, few works consider their integration. In this\npaper, we study the problem of integrating pattern- and fact-based models into\none framework via modeling their preference differences, i.e., making the\npattern- and fact-based models focus on respective preferred parts in a post\nand mitigate interference from non-preferred parts as possible. To this end, we\nbuild a Preference-aware Fake News Detection Framework (Pref-FEND), which\nlearns the respective preferences of pattern- and fact-based models for joint\ndetection. We first design a heterogeneous dynamic graph convolutional network\nto generate the respective preference maps, and then use these maps to guide\nthe joint learning of pattern- and fact-based models for final prediction.\nExperiments on two real-world datasets show that Pref-FEND effectively captures\nmodel preferences and improves the performance of models based on patterns,\nfacts, or both.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Lei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Second Pandemic? Analysis of Fake News About COVID-19 Vaccines in Qatar. (arXiv:2109.11372v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11372","description":"<p>While COVID-19 vaccines are finally becoming widely available, a second\npandemic that revolves around the circulation of anti-vaxxer fake news may\nhinder efforts to recover from the first one. With this in mind, we performed\nan extensive analysis of Arabic and English tweets about COVID-19 vaccines,\nwith focus on messages originating from Qatar. We found that Arabic tweets\ncontain a lot of false information and rumors, while English tweets are mostly\nfactual. However, English tweets are much more propagandistic than Arabic ones.\nIn terms of propaganda techniques, about half of the Arabic tweets express\ndoubt, and 1/5 use loaded language, while English tweets are abundant in loaded\nlanguage, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in\nterms of framing, Arabic tweets adopt a health and safety perspective, while in\nEnglish economic concerns dominate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WRENCH: A Comprehensive Benchmark for Weak Supervision. (arXiv:2109.11377v1 [cs.LG])","link":"http://arxiv.org/abs/2109.11377","description":"<p>Recent \\emph{Weak Supervision (WS)} approaches have had widespread success in\neasing the bottleneck of labeling training data for machine learning by\nsynthesizing labels from multiple potentially noisy supervision sources.\nHowever, proper measurement and analysis of these approaches remain a\nchallenge. First, datasets used in existing works are often private and/or\ncustom, limiting standardization. Second, WS datasets with the same name and\nbase data often vary in terms of the labels and weak supervision sources used,\na significant \"hidden\" source of evaluation variance. Finally, WS studies often\ndiverge in terms of the evaluation protocol and ablations used. To address\nthese problems, we introduce a benchmark platform, \\benchmark, for a thorough\nand standardized evaluation of WS approaches. It consists of 22 varied\nreal-world datasets for classification and sequence tagging; a range of real,\nsynthetic, and procedurally-generated weak supervision sources; and a modular,\nextensible framework for WS evaluation, including implementations for popular\nWS methods. We use \\benchmark to conduct extensive comparisons over more than\n100 method variants to demonstrate its efficacy as a benchmark platform. The\ncode is available at \\url{https://github.com/JieyuZ2/wrench}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-based Mention Typing for Named Entity Disambiguation. (arXiv:2109.11389v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11389","description":"<p>An entity mention in text such as \"Washington\" may correspond to many\ndifferent named entities such as the city \"Washington D.C.\" or the newspaper\n\"Washington Post.\" The goal of named entity disambiguation is to identify the\nmentioned named entity correctly among all possible candidates. If the type\n(e.g. location or person) of a mentioned entity can be correctly predicted from\nthe context, it may increase the chance of selecting the right candidate by\nassigning low probability to the unlikely ones. This paper proposes\ncluster-based mention typing for named entity disambiguation. The aim of\nmention typing is to predict the type of a given mention based on its context.\nGenerally, manually curated type taxonomies such as Wikipedia categories are\nused. We introduce cluster-based mention typing, where named entities are\nclustered based on their contextual similarities and the cluster ids are\nassigned as types. The hyperlinked mentions and their context in Wikipedia are\nused in order to obtain these cluster-based types. Then, mention typing models\nare trained on these mentions, which have been labeled with their cluster-based\ntypes through distant supervision. At the named entity disambiguation phase,\nfirst the cluster-based types of a given mention are predicted and then, these\ntypes are used as features in a ranking model to select the best entity among\nthe candidates. We represent entities at multiple contextual levels and obtain\ndifferent clusterings (and thus typing models) based on each level. As each\nclustering breaks the entity space differently, mention typing based on each\nclustering discriminates the mention differently. When predictions from all\ntyping models are used together, our system achieves better or comparable\nresults based on randomization tests with respect to the state-of-the-art\nlevels on four defacto test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Celebi_A/0/1/0/all/0/1\">Arda &#xc7;elebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Recognition and Classification on Historical Documents: A Survey. (arXiv:2109.11406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11406","description":"<p>After decades of massive digitisation, an unprecedented amount of historical\ndocuments is available in digital format, along with their machine-readable\ntexts. While this represents a major step forward with respect to preservation\nand accessibility, it also opens up new opportunities in terms of content\nmining and the next fundamental challenge is to develop appropriate\ntechnologies to efficiently search, retrieve and explore information from this\n'big data of the past'. Among semantic indexing opportunities, the recognition\nand classification of named entities are in great demand among humanities\nscholars. Yet, named entity recognition (NER) systems are heavily challenged\nwith diverse, historical and noisy inputs. In this survey, we present the array\nof challenges posed by historical documents to NER, inventory existing\nresources, describe the main approaches deployed so far, and identify key\npriorities for future developments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ehrmann_M/0/1/0/all/0/1\">Maud Ehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Ahmed Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pontes_E/0/1/0/all/0/1\">Elvys Linhares Pontes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanello_M/0/1/0/all/0/1\">Matteo Romanello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Antoine Doucet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Algorithm for Generating Gap-Fill Multiple Choice Questions of an Expert System. (arXiv:2109.11421v1 [cs.AI])","link":"http://arxiv.org/abs/2109.11421","description":"<p>This research is aimed to propose an artificial intelligence algorithm\ncomprising an ontology-based design, text mining, and natural language\nprocessing for automatically generating gap-fill multiple choice questions\n(MCQs). The simulation of this research demonstrated an application of the\nalgorithm in generating gap-fill MCQs about software testing. The simulation\nresults revealed that by using 103 online documents as inputs, the algorithm\ncould automatically produce more than 16 thousand valid gap-fill MCQs covering\na variety of topics in the software testing domain. Finally, in the discussion\nsection of this paper we suggest how the proposed algorithm should be applied\nto produce gap-fill MCQs being collected in a question pool used by a knowledge\nexpert system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirithumgul_P/0/1/0/all/0/1\">Pornpat Sirithumgul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasertsilp_P/0/1/0/all/0/1\">Pimpaka Prasertsilp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olfman_L/0/1/0/all/0/1\">Lorne Olfman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Fact-Checking: A Survey. (arXiv:2109.11427v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11427","description":"<p>As online false information continues to grow, automated fact-checking has\ngained an increasing amount of attention in recent years. Researchers in the\nfield of Natural Language Processing (NLP) have contributed to the task by\nbuilding fact-checking datasets, devising automated fact-checking pipelines and\nproposing NLP methods to further research in the development of different\ncomponents. This paper reviews relevant research on automated fact-checking\ncovering both the claim detection and claim validation components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xia Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abumansour_A/0/1/0/all/0/1\">Amani S. Abumansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corpus and Models for Lemmatisation and POS-tagging of Old French. (arXiv:2109.11442v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11442","description":"<p>Old French is a typical example of an under-resourced historic languages,\nthat furtherly displays animportant amount of linguistic variation. In this\npaper, we present the current results of a long going project (2015-...) and\ndescribe how we broached the difficult question of providing lemmatisation\nandPOS models for Old French with the help of neural taggers and the\nprogressive constitution of dedicated corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camps_J/0/1/0/all/0/1\">Jean-Baptiste Camps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clerice_T/0/1/0/all/0/1\">Thibault Cl&#xe9;rice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duval_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Duval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ing_L/0/1/0/all/0/1\">Lucence Ing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanaoka_N/0/1/0/all/0/1\">Naomi Kanaoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinche_A/0/1/0/all/0/1\">Ariane Pinche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. (arXiv:2109.11491v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11491","description":"<p>We present a method for exploring regions around individual points in a\ncontextualized vector space (particularly, BERT space), as a way to investigate\nhow these regions correspond to word senses. By inducing a contextualized\n\"pseudoword\" as a stand-in for a static embedding in the input layer, and then\nperforming masked prediction of a word in the sentence, we are able to\ninvestigate the geometry of the BERT-space in a controlled manner around\nindividual instances. Using our method on a set of carefully constructed\nsentences targeting ambiguous English words, we find substantial regularity in\nthe contextualized space, with regions that correspond to distinct word senses;\nbut between these regions there are occasionally \"sense voids\" -- regions that\ndo not correspond to any intelligible sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1\">Taelin Karidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding a Balanced Degree of Automation for Summary Evaluation. (arXiv:2109.11503v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11503","description":"<p>Human evaluation for summarization tasks is reliable but brings in issues of\nreproducibility and high costs. Automatic metrics are cheap and reproducible\nbut sometimes poorly correlated with human judgment. In this work, we propose\nflexible semiautomatic to automatic summary evaluation metrics, following the\nPyramid human evaluation method. Semi-automatic Lite2Pyramid retains the\nreusable human-labeled Summary Content Units (SCUs) for reference(s) but\nreplaces the manual work of judging SCUs' presence in system summaries with a\nnatural language inference (NLI) model. Fully automatic Lite3Pyramid further\nsubstitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via\na semantic role labeling (SRL) model. Finally, we propose in-between metrics,\nLite2.xPyramid, where we use a simple regressor to predict how well the STUs\ncan simulate SCUs and retain SCUs that are more difficult to simulate, which\nprovides a smooth transition and balance between automation and manual\nevaluation. Comparing to 15 existing metrics, we evaluate human-metric\ncorrelations on 3 existing meta-evaluation datasets and our newly-collected\nPyrXSum (with 100/10 XSum examples/systems). It shows that Lite2Pyramid\nconsistently has the best summary-level correlations; Lite3Pyramid works better\nthan or comparable to other automatic metrics; Lite2.xPyramid trades off small\ncorrelation drops for larger manual effort reduction, which can reduce costs\nfor future data collection. Our code and data are publicly available at:\nhttps://github.com/ZhangShiyue/Lite2-3Pyramid\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MARMOT: A Deep Learning Framework for Constructing Multimodal Representations for Vision-and-Language Tasks. (arXiv:2109.11526v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11526","description":"<p>Political activity on social media presents a data-rich window into political\nbehavior, but the vast amount of data means that almost all content analyses of\nsocial media require a data labeling step. However, most automated machine\nclassification methods ignore the multimodality of posted content, focusing\neither on text or images. State-of-the-art vision-and-language models are\nunusable for most political science research: they require all observations to\nhave both image and text and require computationally expensive pretraining.\nThis paper proposes a novel vision-and-language framework called multimodal\nrepresentations using modality translation (MARMOT). MARMOT presents two\nmethodological contributions: it can construct representations for observations\nmissing image or text, and it replaces the computationally expensive\npretraining with modality translation. MARMOT outperforms an ensemble text-only\nclassifier in 19 of 20 categories in multilabel classifications of tweets\nreporting election incidents during the 2016 U.S. general election. Moreover,\nMARMOT shows significant improvements over the results of benchmark multimodal\nmodels on the Hateful Memes dataset, improving the best result set by\nVisualBERT in terms of accuracy from 0.6473 to 0.6760 and area under the\nreceiver operating characteristic curve (AUC) from 0.7141 to 0.7530.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mebane_W/0/1/0/all/0/1\">Walter R. Mebane Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.06438","description":"<p>Online advertising is an important revenue source for many IT companies. In\nthe search advertising scenario, advertisement text that meets the need of the\nsearch query would be more attractive to the user. However, the manual creation\nof query-variant advertisement texts for massive items is expensive.\nTraditional text generation methods tend to focus on the general searching\nneeds with high frequency while ignoring the diverse personalized searching\nneeds with low frequency. In this paper, we propose the query-variant\nadvertisement text generation task that aims to generate candidate\nadvertisement texts for different web search queries with various needs based\non queries and item keywords. To solve the problem of ignoring low-frequency\nneeds, we propose a dynamic association mechanism to expand the receptive field\nbased on external knowledge, which can obtain associated words to be added to\nthe input. These associated words can serve as bridges to transfer the ability\nof the model from the familiar high-frequency words to the unfamiliar\nlow-frequency words. With association, the model can make use of various\npersonalized needs in queries and generate query-variant advertisement texts.\nBoth automatic and human evaluations show that our model can generate more\nattractive advertisement text than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Siyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_C/0/1/0/all/0/1\">Cai Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary-Source Proposition-level Alignment: Task, Datasets and Supervised Baseline. (arXiv:2009.00590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.00590","description":"<p>Aligning sentences in a reference summary with their counterparts in source\ndocuments was shown as a useful auxiliary summarization task, notably for\ngenerating training data for salience detection. Despite its assessed utility,\nthe alignment step was mostly approached with heuristic unsupervised methods,\ntypically ROUGE-based, and was never independently optimized or evaluated. In\nthis paper, we propose establishing summary-source alignment as an explicit\ntask, while introducing two major novelties: (1) applying it at the more\naccurate proposition span level, and (2) approaching it as a supervised\nclassification task. To that end, we created a novel training dataset for\nproposition-level alignment, derived automatically from available summarization\nevaluation data. In addition, we crowdsourced dev and test datasets, enabling\nmodel development and proper evaluation. Utilizing these data, we present a\nsupervised proposition alignment baseline model, showing improved\nalignment-quality over the unsupervised approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Ori Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_O/0/1/0/all/0/1\">Ori Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepioshkin_M/0/1/0/all/0/1\">Michael Lepioshkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1\">Jacob Goldberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-LTP: An Open-source Neural Language Technology Platform for Chinese. (arXiv:2009.11616v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.11616","description":"<p>We introduce \\texttt{N-LTP}, an open-source neural language technology\nplatform supporting six fundamental Chinese NLP tasks: {lexical analysis}\n(Chinese word segmentation, part-of-speech tagging, and named entity\nrecognition), {syntactic parsing} (dependency parsing), and {semantic parsing}\n(semantic dependency parsing and semantic role labeling). Unlike the existing\nstate-of-the-art toolkits, such as \\texttt{Stanza}, that adopt an independent\nmodel for each task, \\texttt{N-LTP} adopts the multi-task framework by using a\nshared pre-trained model, which has the advantage of capturing the shared\nknowledge across relevant Chinese tasks. In addition, a knowledge distillation\nmethod \\cite{DBLP:journals/corr/abs-1907-04829} where the single-task model\nteaches the multi-task model is further introduced to encourage the multi-task\nmodel to surpass its single-task teacher. Finally, we provide a collection of\neasy-to-use APIs and a visualization tool to make users to use and view the\nprocessing results more easily and directly. To the best of our knowledge, this\nis the first toolkit to support six Chinese NLP fundamental tasks. Source code,\ndocumentation, and pre-trained models are available at\n\\url{https://github.com/HIT-SCIR/ltp}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yunlong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings. (arXiv:2010.11247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11247","description":"<p>Simultaneous translation is vastly different from full-sentence translation,\nin the sense that it starts translation before the source sentence ends, with\nonly a few words delay. However, due to the lack of large-scale, high-quality\nsimultaneous translation datasets, most such systems are still trained on\nconventional full-sentence bitexts. This is far from ideal for the simultaneous\nscenario due to the abundance of unnecessary long-distance reorderings in those\nbitexts. We propose a novel method that rewrites the target side of existing\nfull-sentence corpora into simultaneous-style translation. Experiments on\nZh-&gt;En and Ja-&gt;En simultaneous translation show substantial improvements (up to\n+2.7 BLEU) with the addition of these generated pseudo-references.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kita_A/0/1/0/all/0/1\">Atsuhito Kita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Validating Label Consistency in NER Data Annotation. (arXiv:2101.08698v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.08698","description":"<p>Data annotation plays a crucial role in ensuring your named entity\nrecognition (NER) projects are trained with the right information to learn\nfrom. Producing the most accurate labels is a challenge due to the complexity\ninvolved with annotation. Label inconsistency between multiple subsets of data\nannotation (e.g., training set and test set, or multiple training subsets) is\nan indicator of label mistakes. In this work, we present an empirical method to\nexplore the relationship between label (in-)consistency and NER model\nperformance. It can be used to validate the label consistency (or catches the\ninconsistency) in multiple sets of NER data annotation. In experiments, our\nmethod identified the label inconsistency of test data in SCIERC and CoNLL03\ndatasets (with 26.7% and 5.4% label mistakes). It validated the consistency in\nthe corrected version of both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qingkai Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mengxia Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking. (arXiv:2104.04466v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04466","description":"<p>Dialogue State Tracking is central to multi-domain task-oriented dialogue\nsystems, responsible for extracting information from user utterances. We\npresent a novel hybrid architecture that augments GPT-2 with representations\nderived from Graph Attention Networks in such a way to allow causal, sequential\nprediction of slot values. The model architecture captures inter-slot\nrelationships and dependencies across domains that otherwise can be lost in\nsequential prediction. We report improvements in state tracking performance in\nMultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified\nsparse training scenario in which DST models are trained only on session-level\nannotations but evaluated at the turn level. We further report detailed\nanalyses to demonstrate the effectiveness of graph models in DST by showing\nthat the proposed graph modules capture inter-slot dependencies and improve the\npredictions of values that are common to multiple domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weizhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">Bo-Hsiang Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models. (arXiv:2104.08066v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08066","description":"<p>A method for creating a vision-and-language (V&amp;L) model is to extend a\nlanguage model through structural modifications and V&amp;L pre-training. Such an\nextension aims to make a V&amp;L model inherit the capability of natural language\nunderstanding (NLU) from the original language model. To see how well this is\nachieved, we propose to evaluate V&amp;L models using an NLU benchmark (GLUE). We\ncompare five V&amp;L models, including single-stream and dual-stream models,\ntrained with the same pre-training. Dual-stream models, with their higher\nmodality independence achieved by approximately doubling the number of\nparameters, are expected to preserve the NLU capability better. Our main\nfinding is that the dual-stream scores are not much different than the\nsingle-stream scores, contrary to expectation. Further analysis shows that\npre-training causes the performance drop in NLU tasks with few exceptions.\nThese results suggest that adopting a single-stream structure and devising the\npre-training could be an effective method for improving the maintenance of\nlanguage knowledge in V&amp;L extensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iki_T/0/1/0/all/0/1\">Taichi Iki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers: \"The End of History\" for NLP?. (arXiv:2105.00813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00813","description":"<p>Recent advances in neural architectures, such as the Transformer, coupled\nwith the emergence of large-scale pre-trained models such as BERT, have\nrevolutionized the field of Natural Language Processing (NLP), pushing the\nstate of the art for a number of NLP tasks. A rich family of variations of\nthese models has been proposed, such as RoBERTa, ALBERT, and XLNet, but\nfundamentally, they all remain limited in their ability to model certain kinds\nof information, and they cannot cope with certain information sources, which\nwas easy for pre-existing models. Thus, here we aim to shed light on some\nimportant theoretical limitations of pre-trained BERT-style models that are\ninherent in the general Transformer architecture. First, we demonstrate in\npractice on two general types of tasks -- segmentation and segment labeling --\nand on four datasets that these limitations are indeed harmful and that\naddressing them, even in some very simple and naive ways, can yield sizable\nimprovements over vanilla RoBERTa and XLNet models. Then, we offer a more\ngeneral discussion on desiderata for future additions to the Transformer\narchitecture that would increase its expressiveness, which we hope could help\nin the design of the next generation of deep NLP architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Anton Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilvovsky_D/0/1/0/all/0/1\">Dmitry Ilvovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors. (arXiv:2107.01545v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.01545","description":"<p>Attractor-based end-to-end diarization is achieving comparable accuracy to\nthe carefully tuned conventional clustering-based methods on challenging\ndatasets. However, the main drawback is that it cannot deal with the case where\nthe number of speakers is larger than the one observed during training. This is\nbecause its speaker counting relies on supervised learning. In this work, we\nintroduce an unsupervised clustering process embedded in the attractor-based\nend-to-end diarization. We first split a sequence of frame-wise embeddings into\nshort subsequences and then perform attractor-based diarization for each\nsubsequence. Given subsequence-wise diarization results, inter-subsequence\nspeaker correspondence is obtained by unsupervised clustering of the vectors\ncomputed from the attractors from all the subsequences. This makes it possible\nto produce diarization results of a large number of speakers for the whole\nrecording even if the number of output speakers for each subsequence is\nlimited. Experimental results showed that our method could produce accurate\ndiarization results of an unseen number of speakers. Our method achieved 11.84\n%, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets,\nrespectively, each of which is better than the conventional end-to-end\ndiarization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1\">Yawen Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_Y/0/1/0/all/0/1\">Yuki Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1\">Yohei Kawaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09099","description":"<p>Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">H Lilian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning Pretrained Transformers into Variational Autoencoders. (arXiv:2108.02446v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02446","description":"<p>Text variational autoencoders (VAEs) are notorious for posterior collapse, a\nphenomenon where the model's decoder learns to ignore signals from the encoder.\nBecause posterior collapse is known to be exacerbated by expressive decoders,\nTransformers have seen limited adoption as components of text VAEs. Existing\nstudies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et\nal., 2021) mitigate posterior collapse using massive pretraining, a technique\nunavailable to most of the research community without extensive computing\nresources. We present a simple two-phase training scheme to convert a\nsequence-to-sequence Transformer into a VAE with just finetuning. The resulting\nlanguage model is competitive with massively pretrained Transformer-based VAEs\nin some internal metrics while falling short on others. To facilitate training\nwe comprehensively explore the impact of common posterior collapse alleviation\ntechniques in the literature. We release our code for reproducability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihwa Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues. (arXiv:2109.00185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00185","description":"<p>We present an effective system adapted from the end-to-end neural coreference\nresolution model, targeting on the task of anaphora resolution in dialogues.\nThree aspects are specifically addressed in our approach, including the support\nof singletons, encoding speakers and turns throughout dialogue interactions,\nand knowledge transfer utilizing existing resources. Despite the simplicity of\nour adaptation strategies, they are shown to bring significant impact to the\nfinal performance, with up to 27 F1 improvement over the baseline. Our final\nsystem ranks the 1st place on the leaderboard of the anaphora resolution track\nin the CRAC 2021 shared task, and achieves the best evaluation results on all\nfour datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation. (arXiv:2109.00194v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00194","description":"<p>Recent multilingual pre-trained language models have achieved remarkable\nzero-shot performance, where the model is only finetuned on one source language\nand directly evaluated on target languages. In this work, we propose a\nself-learning framework that further utilizes unlabeled data of target\nlanguages, combined with uncertainty estimation in the process to select\nhigh-quality silver labels. Three different uncertainties are adapted and\nanalyzed specifically for the cross lingual transfer: Language\nHeteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty\n(EVI). We evaluate our framework with uncertainties on two cross-lingual tasks\nincluding Named Entity Recognition (NER) and Natural Language Inference (NLI)\ncovering 40 languages in total, which outperforms the baselines significantly\nby 10 F1 on average for NER and 2.5 accuracy score for NLI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Bias in NLP -- Application to Hate Speech Classification. (arXiv:2109.09725v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09725","description":"<p>This document sums up our results forthe NLP lecture at ETH in the spring\nsemester 2021. In this work, a BERT based neural network model (Devlin et\nal.,2018) is applied to the JIGSAW dataset (Jigsaw/Conversation AI, 2019) in\norder to create a model identifying hateful and toxic comments (strictly\nseperated from offensive language) in online social platforms (English\nlanguage), inthis case Twitter. Three other neural network architectures and a\nGPT-2 (Radfordet al., 2019) model are also applied on the provided data set in\norder to compare these different models. The trained BERT model is then applied\non two different data sets to evaluate its generalisation power, namely on\nanother Twitter data set (Tom Davidson, 2017) (Davidsonet al., 2017) and the\ndata set HASOC 2019 (Thomas Mandl, 2019) (Mandl et al.,2019) which includes\nTwitter and also Facebook comments; we focus on the English HASOC 2019 data. In\naddition, it can be shown that by fine-tuning the trained BERT model on these\ntwo datasets by applying different transfer learning scenarios via retraining\npartial or all layers the predictive scores improve compared to simply applying\nthe model pre-trained on the JIGSAW data set. Withour results, we get\nprecisions from 64% to around 90% while still achieving acceptable recall\nvalues of at least lower 60s%, proving that BERT is suitable for real usecases\nin social platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patoulidis_G/0/1/0/all/0/1\">Georgios Patoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zagidullina_A/0/1/0/all/0/1\">Aygul Zagidullina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning. (arXiv:2109.10510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10510","description":"<p>Despite the success of neural dialogue systems in achieving high performance\non the leader-board, they cannot meet users' requirements in practice, due to\ntheir poor reasoning skills. The underlying reason is that most neural dialogue\nmodels only capture the syntactic and semantic information, but fail to model\nthe logical consistency between the dialogue history and the generated\nresponse. Recently, a new multi-turn dialogue reasoning task has been proposed,\nto facilitate dialogue reasoning research. However, this task is challenging,\nbecause there are only slight differences between the illogical response and\nthe dialogue history. How to effectively solve this challenge is still worth\nexploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle\nthis problem. Inspired by human's behavior in reading comprehension, a\ncomparison mechanism is proposed to focus on the fine-grained differences in\nthe representation of each response candidate. Specifically, each candidate\nrepresentation is compared with the whole history to obtain a history\nconsistency representation. Furthermore, the consistency signals between each\ncandidate and the speaker's own history are considered to drive a model to\nprefer a candidate that is logically consistent with the speaker's history\nlogic. Finally, the above consistency representations are employed to output a\nranking list of the candidate responses for multi-turn dialogue reasoning.\nExperimental results on two public dialogue datasets show that our method\nobtains higher ranking scores than the baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing. (arXiv:2109.10847v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.10847","description":"<p>Recent progress in the Natural Language Processing domain has given us\nseveral State-of-the-Art (SOTA) pretrained models which can be finetuned for\nspecific tasks. These large models with billions of parameters trained on\nnumerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In\nthis paper, we discuss the need for a benchmark for cost and time effective\nsmaller models trained on a single GPU. This will enable researchers with\nresource constraints experiment with novel and innovative ideas on\ntokenization, pretraining tasks, architecture, fine tuning methods etc. We set\nup Small-Bench NLP, a benchmark for small efficient neural language models\ntrained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks\non the publicly available GLUE datasets and a leaderboard to track the progress\nof the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture\nachieves an average score of 81.53 which is comparable to that of BERT-Base's\n82.20 (110M parameters). Our models, code and leaderboard are available at\nhttps://github.com/smallbenchnlp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanakarajan_K/0/1/0/all/0/1\">Kamal Raj Kanakarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundumani_B/0/1/0/all/0/1\">Bhuvana Kundumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankarasubbu_M/0/1/0/all/0/1\">Malaikannan Sankarasubbu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}