{"site_title":"ArxivDaily","days":[{"date":"2021-08-11","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"FairyTailor: A Multimodal Generative Framework for Storytelling. (arXiv:2108.04324v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04324","description":"<p>Storytelling is an open-ended task that entails creative thinking and\nrequires a constant flow of ideas. Natural language generation (NLG) for\nstorytelling is especially challenging because it requires the generated text\nto follow an overall theme while remaining creative and diverse to engage the\nreader. In this work, we introduce a system and a web-based demo, FairyTailor,\nfor human-in-the-loop visual story co-creation. Users can create a cohesive\nchildren's fairytale by weaving generated texts and retrieved images with their\ninput. FairyTailor adds another modality and modifies the text generation\nprocess to produce a coherent and creative sequence of text and images. To our\nknowledge, this is the first dynamic tool for multimodal story generation that\nallows interactive co-formation of both texts and images. It allows users to\ngive feedback on co-created stories and share their results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1\">Eden Bensaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1\">Mauro Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMPARE: A Taxonomy and Dataset of Comparison Discussions in Peer Reviews. (arXiv:2108.04366v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04366","description":"<p>Comparing research papers is a conventional method to demonstrate progress in\nexperimental research. We present COMPARE, a taxonomy and a dataset of\ncomparison discussions in peer reviews of research papers in the domain of\nexperimental deep learning. From a thorough observation of a large set of\nreview sentences, we build a taxonomy of categories in comparison discussions\nand present a detailed annotation scheme to analyze this. Overall, we annotate\n117 reviews covering 1,800 sentences. We experiment with various methods to\nidentify comparison sentences in peer reviews and report a maximum F1 Score of\n0.49. We also pretrain two language models specifically on ML, NLP, and CV\npaper abstracts and reviews to learn informative representations of peer\nreviews. The annotated dataset and the pretrained models are available at\nhttps://github.com/shruti-singh/COMPARE .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shruti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Transformers Solve Compositional Tasks. (arXiv:2108.04378v1 [cs.AI])","link":"http://arxiv.org/abs/2108.04378","description":"<p>Several studies have reported the inability of Transformer models to\ngeneralize compositionally, a key type of generalization in many NLP tasks such\nas semantic parsing. In this paper we explore the design space of Transformer\nmodels showing that the inductive biases given to the model by several design\ndecisions significantly impact compositional generalization. Through this\nexploration, we identified Transformer configurations that generalize\ncompositionally significantly better than previously reported in the literature\nin a diverse set of compositional tasks, and that achieve state-of-the-art\nresults in a semantic parsing compositional generalization benchmark (COGS),\nand a string edit operation composition benchmark (PCFG).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onta%7Bn%7D%7Bo%7Dn_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cvicek_V/0/1/0/all/0/1\">Vaclav Cvicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_Z/0/1/0/all/0/1\">Zachary Fisher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Intent Detection via Multi-Strategy Rebalancing. (arXiv:2108.04445v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04445","description":"<p>Conventional Intent Detection (ID) models are usually trained offline, which\nrelies on a fixed dataset and a predefined set of intent classes. However, in\nreal-world applications, online systems usually involve continually emerging\nnew user intents, which pose a great challenge to the offline training\nparadigm. Recently, lifelong learning has received increasing attention and is\nconsidered to be the most promising solution to this challenge. In this paper,\nwe propose Lifelong Intent Detection (LID), which continually trains an ID\nmodel on new data to learn newly emerging intents while avoiding\ncatastrophically forgetting old data. Nevertheless, we find that existing\nlifelong learning methods usually suffer from a serious imbalance between old\nand new data in the LID task. Therefore, we propose a novel lifelong learning\nmethod, Multi-Strategy Rebalancing (MSR), which consists of cosine\nnormalization, hierarchical knowledge distillation, and inter-class margin loss\nto alleviate the multiple negative effects of the imbalance problem.\nExperimental results demonstrate the effectiveness of our method, which\nsignificantly outperforms previous state-of-the-art lifelong learning methods\non the ATIS, SNIPS, HWU64, and CLINC150 benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaoyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shizhu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04539","description":"<p>Understanding documents from their visual snapshots is an emerging problem\nthat requires both advanced computer vision and NLP methods. The recent advance\nin OCR enables the accurate recognition of text blocks, yet it is still\nchallenging to extract key information from documents due to the diversity of\ntheir layouts. Although recent studies on pre-trained language models show the\nimportance of incorporating layout information on this task, the conjugation of\ntexts and their layouts still follows the style of BERT optimized for\nunderstanding the 1D text. This implies there is room for further improvement\nconsidering the 2D nature of text layouts. This paper introduces a pre-trained\nlanguage model, BERT Relying On Spatiality (BROS), which effectively utilizes\nthe information included in individual text blocks and their layouts.\nSpecifically, BROS encodes spatial information by utilizing relative positions\nand learns spatial dependencies between OCR blocks with a novel area-masking\nstrategy. These two novel approaches lead to an efficient encoding of spatial\nlayout information highlighted by the robust performance of BROS under\nlow-resource environments. We also introduce a general-purpose parser that can\nbe combined with BROS to extract key information even when there is no order\ninformation between text blocks. BROS shows its superiority on four public\nbenchmarks---FUNSD, SROIE*, CORD, and SciTSR---and its robustness in practical\ncases where order information of text blocks is not available. Further\nexperiments with a varying number of training examples demonstrate the high\ntraining efficiency of our approach. Our code will be open to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model. (arXiv:2108.04556v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04556","description":"<p>Pre-trained models for programming languages have proven their significant\nvalues in various code-related tasks, such as code search, code clone\ndetection, and code translation. Currently, most pre-trained models treat a\ncode snippet as a sequence of tokens or only focus on the data flow between\ncode identifiers. However, rich code syntax and hierarchy are ignored which can\nprovide important structure information and semantic rules of codes to help\nenhance code representations. In addition, although the BERT-based code\npre-trained models achieve high performance on many downstream tasks, the\nnative derived sequence representations of BERT are proven to be of\nlow-quality, it performs poorly on code matching and similarity tasks. To\naddress these problems, we propose CLSEBERT, a Constrastive Learning Framework\nfor Syntax Enhanced Code Pre-Trained Model, to deal with various code\nintelligence tasks. In the pre-training stage, we consider the code syntax and\nhierarchy contained in the Abstract Syntax Tree (AST) and leverage the\nconstrastive learning to learn noise-invariant code representations. Besides\nthe masked language modeling (MLM), we also introduce two novel pre-training\nobjectives. One is to predict the edges between nodes in the abstract syntax\ntree, and the other is to predict the types of code tokens. Through extensive\nexperiments on four code intelligence tasks, we successfully show the\neffectiveness of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yadao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hope Speech detection in under-resourced Kannada language. (arXiv:2108.04616v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04616","description":"<p>Numerous methods have been developed to monitor the spread of negativity in\nmodern years by eliminating vulgar, offensive, and fierce comments from social\nmedia platforms. However, there are relatively lesser amounts of study that\nconverges on embracing positivity, reinforcing supportive and reassuring\ncontent in online forums. Consequently, we propose creating an English-Kannada\nHope speech dataset, KanHope and comparing several experiments to benchmark the\ndataset. The dataset consists of 6,176 user-generated comments in code mixed\nKannada scraped from YouTube and manually annotated as bearing hope speech or\nNot-hope speech. In addition, we introduce DC-BERT4HOPE, a dual-channel model\nthat uses the English translation of KanHope for additional training to promote\nhope speech detection. The approach achieves a weighted F1-score of 0.756,\nbettering other models. Henceforth, KanHope aims to instigate research in\nKannada while broadly promoting researchers to take a pragmatic approach\ntowards online content that encourages, positive, and supportive.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1\">Kingston Pal Thamburaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1\">Prabakaran Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04657","description":"<p>Multi-head attention, a collection of several attention mechanisms that\nindependently attend to different parts of the input, is the key ingredient in\nthe Transformer (Vaswaniet al., 2017). Recent work has shown, however, that a\nlarge proportion of the heads in a Transformer's multi-head attention mechanism\ncan be safely pruned away without significantly harming the performance of the\nmodel; such pruning leads to models that are noticeably smaller and faster in\npractice. Our work introduces a new head pruning technique that we term\ndifferentiable subset pruning. Intuitively, our method learns per-head\nimportance variables and then enforces a user-specified hard constraint on the\nnumber of unpruned heads. The importance variables are learned via stochastic\ngradient descent. We conduct experiments on natural language inference and\nmachine translation; we show that differentiable subset pruning performs\ncomparably or better than Voita et al. (2019) while offering the same exact\ncontrol over the number of heads as Michel et al. (2019).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Commonsense Knowledge Helps with Natural Language Tasks: A Survey of Recent Resources and Methodologies. (arXiv:2108.04674v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04674","description":"<p>In this paper, we give an overview of commonsense reasoning in natural\nlanguage processing, which requires a deeper understanding of the contexts and\nusually involves inference over implicit external knowledge. We first review\nsome popular commonsense knowledge bases and commonsense reasoning benchmarks,\nbut give more emphasis on the methodologies, including recent approaches that\naim at solving some general natural language problems that take advantage of\nexternal knowledge bases. Finally, we discuss some future directions in pushing\nthe boundary of commonsense reasoning in natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Audio Captioning using Transfer Learning and Reconstruction Latent Space Similarity Regularization. (arXiv:2108.04692v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04692","description":"<p>In this paper, we examine the use of Transfer Learning using Pretrained Audio\nNeural Networks (PANNs), and propose an architecture that is able to better\nleverage the acoustic features provided by PANNs for the Automated Audio\nCaptioning Task. We also introduce a novel self-supervised objective,\nReconstruction Latent Space Similarity Regularization (RLSSR). The RLSSR module\nsupplements the training of the model by minimizing the similarity between the\nencoder and decoder embedding. The combination of both methods allows us to\nsurpass state of the art results by a significant margin on the Clotho dataset\nacross several metrics and benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koh_A/0/1/0/all/0/1\">Andrew Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation. (arXiv:2108.04718v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04718","description":"<p>In neural machine translation (NMT), we search for the mode of the model\ndistribution to form predictions. The mode as well as other high probability\ntranslations found by beam search have been shown to often be inadequate in a\nnumber of ways. This prevents practitioners from improving translation quality\nthrough better search, as these idiosyncratic translations end up being\nselected by the decoding algorithm, a problem known as the beam search curse.\nRecently, a sampling-based approximation to minimum Bayes risk (MBR) decoding\nhas been proposed as an alternative decision rule for NMT that would likely not\nsuffer from the same problems. We analyse this approximation and establish that\nit has no equivalent to the beam search curse, i.e. better search always leads\nto better translations. We also design different approximations aimed at\ndecoupling the cost of exploration from the cost of robust estimation of\nexpected utility. This allows for exploration of much larger hypothesis spaces,\nwhich we show to be beneficial. We also show that it can be beneficial to make\nuse of strategies like beam search and nucleus sampling to construct hypothesis\nspaces efficiently. We show on three language pairs (English into and from\nGerman, Romanian, and Nepali) that MBR can improve upon beam search with\nmoderate computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eikema_B/0/1/0/all/0/1\">Bryan Eikema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Headed Span-Based Projective Dependency Parsing. (arXiv:2108.04750v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04750","description":"<p>We propose a headed span-based method for projective dependency parsing. In a\nprojective tree, the subtree rooted at each word occurs in a contiguous\nsequence (i.e., span) in the surface order, we call the span-headword pair\n\\textit{headed span}. In this view, a projective tree can be regarded as a\ncollection of headed spans. It is similar to the case in constituency parsing\nsince a constituency tree can be regarded as a collection of constituent spans.\nSpan-based methods decompose the score of a constituency tree sorely into the\nscore of constituent spans and use the CYK algorithm for global training and\nexact inference, obtaining state-of-the-art results in constituency parsing.\nInspired by them, we decompose the score of a dependency tree into the score of\nheaded spans. We use neural networks to score headed spans and design a novel\n$O(n^3)$ dynamic programming algorithm to enable global training and exact\ninference. We evaluate our method on PTB, CTB, and UD, achieving\nstate-of-the-art or comparable results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Songlin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1\">Kewei Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. (arXiv:2108.04812v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04812","description":"<p>We study continual learning for natural language instruction generation, by\nobserving human users' instruction execution. We focus on a collaborative\nscenario, where the system both acts and delegates tasks to human users using\nnatural language. We compare user execution of generated instructions to the\noriginal system intent as an indication to the system's success communicating\nits intent. We show how to use this signal to improve the system's ability to\ngenerate instructions via contextual bandit learning. In interaction with real\nusers, our system demonstrates dramatic improvements in its ability to generate\nlanguage over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Vaccine and Social Media: Exploring Emotions and Discussions on Twitter. (arXiv:2108.04816v1 [cs.SI])","link":"http://arxiv.org/abs/2108.04816","description":"<p>Public response to COVID-19 vaccines is the key success factor to control the\nCOVID-19 pandemic. To understand the public response, there is a need to\nexplore public opinion. Traditional surveys are expensive and time-consuming,\naddress limited health topics, and obtain small-scale data. Twitter can provide\na great opportunity to understand public opinion regarding COVID-19 vaccines.\nThe current study proposes an approach using computational and human coding\nmethods to collect and analyze a large number of tweets to provide a wider\nperspective on the COVID-19 vaccine. This study identifies the sentiment of\ntweets and their temporal trend, discovers major topics, compares topics of\nnegative and non-negative tweets, and discloses top topics of negative and\nnon-negative tweets. Our findings show that the negative sentiment regarding\nthe COVID-19 vaccine had a decreasing trend between November 2020 and February\n2021. We found Twitter users have discussed a wide range of topics from\nvaccination sites to the 2020 U.S. election between November 2020 and February\n2021. The findings show that there was a significant difference between\nnegative and non-negative tweets regarding the weight of most topics. Our\nresults also indicate that the negative and non-negative tweets had different\ntopic priorities and focuses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karami_A/0/1/0/all/0/1\">Amir Karami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Michael Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldschmidt_B/0/1/0/all/0/1\">Bailey Goldschmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajieff_H/0/1/0/all/0/1\">Hannah R. Boyajieff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafabadi_M/0/1/0/all/0/1\">Mahdi M. Najafabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise Robust Named Entity Understanding for Voice Assistants. (arXiv:2005.14408v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.14408","description":"<p>Named Entity Recognition (NER) and Entity Linking (EL) play an essential role\nin voice assistant interaction, but are challenging due to the special\ndifficulties associated with spoken user queries. In this paper, we propose a\nnovel architecture that jointly solves the NER and EL tasks by combining them\nin a joint reranking module. We show that our proposed framework improves NER\naccuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features\nused also lead to better accuracies in other natural language understanding\ntasks, such as domain classification and semantic parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sida Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Justine Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Atish Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ray Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinying Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1\">Vivek Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1\">Mubarak Seyed Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1\">Gang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1\">Nan Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yidan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1\">Andy O</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1\">Kushal Tayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Roger Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1\">Peter Grasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMUSED: An Annotation Framework of Multi-modal Social Media Data. (arXiv:2010.00502v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2010.00502","description":"<p>In this paper, we present a semi-automated framework called AMUSED for\ngathering multi-modal annotated data from the multiple social media platforms.\nThe framework is designed to mitigate the issues of collecting and annotating\nsocial media data by cohesively combining machine and human in the data\ncollection process. From a given list of the articles from professional news\nmedia or blog, AMUSED detects links to the social media posts from news\narticles and then downloads contents of the same post from the respective\nsocial media platform to gather details about that specific post. The framework\nis capable of fetching the annotated data from multiple platforms like Twitter,\nYouTube, Reddit. The framework aims to reduce the workload and problems behind\nthe data annotation from the social media platforms. AMUSED can be applied in\nmultiple application domains, as a use case, we have implemented the framework\nfor collecting COVID-19 misinformation data from different social media\nplatforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty and Surprisal Jointly Deliver the Punchline: Exploiting Incongruity-Based Features for Humor Recognition. (arXiv:2012.12007v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.12007","description":"<p>Humor recognition has been widely studied as a text classification problem\nusing data-driven approaches. However, most existing work does not examine the\nactual joke mechanism to understand humor. We break down any joke into two\ndistinct components: the set-up and the punchline, and further explore the\nspecial relationship between them. Inspired by the incongruity theory of humor,\nwe model the set-up as the part developing semantic uncertainty, and the\npunchline disrupting audience expectations. With increasingly powerful language\nmodels, we were able to feed the set-up along with the punchline into the GPT-2\nlanguage model, and calculate the uncertainty and surprisal values of the\njokes. By conducting experiments on the SemEval 2021 Task 7 dataset, we found\nthat these two features have better capabilities of telling jokes from\nnon-jokes, compared with existing baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Zero Resource Speech Challenge 2021: Spoken language modelling. (arXiv:2104.14700v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.14700","description":"<p>We present the Zero Resource Speech Challenge 2021, which asks participants\nto learn a language model directly from audio, without any text or labels. The\nchallenge is based on the Libri-light dataset, which provides up to 60k hours\nof audio from English audio books without any associated text. We provide a\npipeline baseline system consisting on an encoder based on contrastive\npredictive coding (CPC), a quantizer ($k$-means) and a standard language model\n(BERT or LSTM). The metrics evaluate the learned representations at the\nacoustic (ABX discrimination), lexical (spot-the-word), syntactic\n(acceptability judgment) and semantic levels (similarity judgment). We present\nan overview of the eight submitted systems from four groups and discuss the\nmain results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunbar_E/0/1/0/all/0/1\">Ewan Dunbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_M/0/1/0/all/0/1\">Mathieu Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilakis_N/0/1/0/all/0/1\">Nicolas Hamilakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seyssel_M/0/1/0/all/0/1\">Maureen de Seyssel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roze_P/0/1/0/all/0/1\">Patricia Roz&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's not what you said, it's how you said it: discriminative perception of speech as a multichannel communication system. (arXiv:2105.00260v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00260","description":"<p>People convey information extremely effectively through spoken interaction\nusing multiple channels of information transmission: the lexical channel of\nwhat is said, and the non-lexical channel of how it is said. We propose\nstudying human perception of spoken communication as a means to better\nunderstand how information is encoded across these channels, focusing on the\nquestion 'What characteristics of communicative context affect listener's\nexpectations of speech?'. To investigate this, we present a novel behavioural\ntask testing whether listeners can discriminate between the true utterance in a\ndialogue and utterances sampled from other contexts with the same lexical\ncontent. We characterize how perception - and subsequent discriminative\ncapability - is affected by different degrees of additional contextual\ninformation across both the lexical and non-lexical channel of speech. Results\ndemonstrate that people can effectively discriminate between different prosodic\nrealisations, that non-lexical context is informative, and that this channel\nprovides more salient information than the lexical channel, highlighting the\nimportance of the non-lexical channel in spoken interaction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallbridge_S/0/1/0/all/0/1\">Sarenne Wallbridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01139","description":"<p>EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02923","description":"<p>Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"GANmapper: geographical content filling. (arXiv:2108.04232v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04232","description":"<p>We present a new method to create spatial data using a generative adversarial\nnetwork (GAN). Our contribution uses coarse and widely available geospatial\ndata to create maps of less available features at the finer scale in the built\nenvironment, bypassing their traditional acquisition techniques (e.g. satellite\nimagery or land surveying). In the work, we employ land use data and road\nnetworks as input to generate building footprints, and conduct experiments in 9\ncities around the world. The method, which we implement in a tool we release\nopenly, enables generating approximate maps of the urban form, and it is\ngeneralisable to augment other types of geoinformation, enhancing the\ncompleteness and quality of spatial data infrastructure. It may be especially\nuseful in locations missing detailed and high-resolution data and those that\nare mapped with uncertain or heterogeneous quality, such as much of\nOpenStreetMap. The quality of the results is influenced by the urban form and\nscale. In most cases, experiments suggest promising performance as the method\ntends to truthfully indicate the locations, amount, and shape of buildings. The\nwork has the potential to support several applications, such as energy,\nclimate, and urban morphology studies in areas previously lacking required\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Abraham Noah Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biljecki_F/0/1/0/all/0/1\">Filip Biljecki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Transfer Learning for Identifications of Slope Surface Cracks. (arXiv:2108.04235v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04235","description":"<p>Geohazards such as landslides have caused great losses to the safety of\npeople's lives and property, which is often accompanied with surface cracks. If\nsuch surface cracks could be identified in time, it is of great significance\nfor the monitoring and early warning of geohazards. Currently, the most common\nmethod for crack identification is manual detection, which is with low\nefficiency and accuracy. In this paper, a deep transfer learning framework is\nproposed to effectively and efficiently identify slope surface cracks for the\nsake of fast monitoring and early warning of geohazards such as landslides. The\nessential idea is to employ transfer learning by training (a) the large sample\ndataset of concrete cracks and (b) the small sample dataset of soil and rock\nmasses cracks. In the proposed framework, (1) pretrained cracks identification\nmodels are constructed based on the large sample dataset of concrete cracks;\n(2) refined cracks identification models are further constructed based on the\nsmall sample dataset of soil and rock masses cracks. The proposed framework\ncould be applied to conduct UAV surveys on high-steep slopes to realize the\nmonitoring and early warning of landslides to ensure the safety of people's\nlives and property.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Gang Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An optical biomimetic eyes with interested object imaging. (arXiv:2108.04236v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04236","description":"<p>We presented an optical system to perform imaging interested objects in\ncomplex scenes, like the creature easy see the interested prey in the hunt for\ncomplex environments. It utilized Deep-learning network to learn the interested\nobjects's vision features and designed the corresponding \"imaging matrices\",\nfurthermore the learned matrixes act as the measurement matrix to complete\ncompressive imaging with a single-pixel camera, finally we can using the\ncompressed image data to only image the interested objects without the rest\nobjects and backgrounds of the scenes with the previous Deep-learning network.\nOur results demonstrate that no matter interested object is single feature or\nrich details, the interference can be successfully filtered out and this idea\ncan be applied in some common applications that effectively improve the\nperformance. This bio-inspired optical system can act as the creature eye to\nachieve success on interested-based object imaging, object detection, object\nrecognition and object tracking, etc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shimei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shangyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_M/0/1/0/all/0/1\">Miao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiaofang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chuangxue Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kunyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuxin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yuer Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Ting Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04238","description":"<p>Explanation of AI, as well as fairness of algorithms' decisions and the\ntransparency of the decision model, are becoming more and more important. And\nit is crucial to design effective and human-friendly techniques when opening\nthe black-box model. Counterfactual conforms to the human way of thinking and\nprovides a human-friendly explanation, and its corresponding explanation\nalgorithm refers to a strategic alternation of a given data point so that its\nmodel output is \"counter-facted\", i.e. the prediction is reverted. In this\npaper, we adapt counterfactual explanation over fine-grained image\nclassification problem. We demonstrated an adaptive method that could give a\ncounterfactual explanation by showing the composed counterfactual feature map\nusing top-down layer searching algorithm (TDLS). We have proved that our TDLS\nalgorithm could provide more flexible counterfactual visual explanation in an\nefficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,\nwe discussed several applicable scenarios of counterfactual visual\nexplanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haocheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI. (arXiv:2108.04267v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04267","description":"<p>The neuroimage analysis community has neglected the automated segmentation of\nthe olfactory bulb (OB) despite its crucial role in olfactory function. The\nlack of an automatic processing method for the OB can be explained by its\nchallenging properties. Nonetheless, recent advances in MRI acquisition\ntechniques and resolution have allowed raters to generate more reliable manual\nannotations. Furthermore, the high accuracy of deep learning methods for\nsolving semantic segmentation problems provides us with an option to reliably\nassess even small structures. In this work, we introduce a novel, fast, and\nfully automated deep learning pipeline to accurately segment OB tissue on\nsub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we\ndesigned a three-stage pipeline: (1) Localization of a region containing both\nOBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized\nregion through four independent AttFastSurferCNN - a novel deep learning\narchitecture with a self-attention mechanism to improve modeling of contextual\ninformation, and (3) Ensemble of the predicted label maps. The OB pipeline\nexhibits high performance in terms of boundary delineation, OB localization,\nand volume estimation across a wide range of ages in 203 participants of the\nRhineland Study. Moreover, it also generalizes to scans of an independent\ndataset never encountered during training, the Human Connectome Project (HCP),\nwith different acquisition parameters and demographics, evaluated in 30 cases\nat the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.\nWe extensively validated our pipeline not only with respect to segmentation\naccuracy but also to known OB volume effects, where it can sensitively\nreplicate age effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1\">Santiago Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1\">Kersten Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Weiyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1\">Philipp Ehses</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1\">Tony St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1\">Monique M.B Breteler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual SLAM with Graph-Cut Optimized Multi-Plane Reconstruction. (arXiv:2108.04281v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04281","description":"<p>This paper presents a semantic planar SLAM system that improves pose\nestimation and mapping using cues from an instance planar segmentation network.\nWhile the mainstream approaches are using RGB-D sensors, employing a monocular\ncamera with such a system still faces challenges such as robust data\nassociation and precise geometric model fitting. In the majority of existing\nwork, geometric model estimation problems such as homography estimation and\npiece-wise planar reconstruction (PPR) are usually solved by standard (greedy)\nRANSAC separately and sequentially. However, setting the inlier-outlier\nthreshold is difficult in absence of information about the scene (i.e. the\nscale). In this work, we revisit these problems and argue that two mentioned\ngeometric models (homographies/3D planes) can be solved by minimizing an energy\nfunction that exploits the spatial coherence, i.e. with graph-cut optimization,\nwhich also tackles the practical issue when the output of a trained CNN is\ninaccurate. Moreover, we propose an adaptive parameter setting strategy based\non our experiments, and report a comprehensive evaluation on various\nopen-source datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_F/0/1/0/all/0/1\">Fangwen Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yaxu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1\">Jason Rambach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagani_A/0/1/0/all/0/1\">Alain Pagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Cut by Watching Movies. (arXiv:2108.04294v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04294","description":"<p>Video content creation keeps growing at an incredible pace; yet, creating\nengaging stories remains challenging and requires non-trivial video editing\nexpertise. Many video editing components are astonishingly hard to automate\nprimarily due to the lack of raw video materials. This paper focuses on a new\ntask for computational video editing, namely the task of raking cut\nplausibility. Our key idea is to leverage content that has already been edited\nto learn fine-grained audiovisual patterns that trigger cuts. To do this, we\nfirst collected a data source of more than 10K videos, from which we extract\nmore than 255K cuts. We devise a model that learns to discriminate between real\nand artificial cuts via contrastive learning. We set up a new task and a set of\nbaselines to benchmark video cut generation. We observe that our proposed model\noutperforms the baselines by large margins. To demonstrate our model in\nreal-world applications, we conduct human studies in a collection of unedited\nvideos. The results show that our model does a better job at cutting than\nrandom and alternative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. (arXiv:2108.04308v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04308","description":"<p>Data is a crucial component of machine learning. The field is reliant on data\nto train, validate, and test models. With increased technical capabilities,\nmachine learning research has boomed in both academic and industry settings,\nand one major focus has been on computer vision. Computer vision is a popular\ndomain of machine learning increasingly pertinent to real-world applications,\nfrom facial recognition in policing to object detection for autonomous\nvehicles. Given computer vision's propensity to shape machine learning research\nand impact human life, we seek to understand disciplinary practices around\ndataset documentation - how data is collected, curated, annotated, and packaged\ninto datasets for computer vision researchers and practitioners to use for\nmodel tuning and development. Specifically, we examine what dataset\ndocumentation communicates about the underlying values of vision data and the\nlarger practices and goals of computer vision as a field. To conduct this\nstudy, we collected a corpus of about 500 computer vision datasets, from which\nwe sampled 114 dataset publications across different vision tasks. Through both\na structured and thematic content analysis, we document a number of values\naround accepted data practices, what makes desirable data, and the treatment of\nhumans in the dataset construction process. We discuss how computer vision\ndatasets authors value efficiency at the expense of care; universality at the\nexpense of contextuality; impartiality at the expense of positionality; and\nmodel work at the expense of data work. Many of the silenced values we identify\nsit in opposition with social computing practices. We conclude with suggestions\non how to better incorporate silenced values into the dataset creation and\ncuration process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheuerman_M/0/1/0/all/0/1\">Morgan Klaus Scheuerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denton_E/0/1/0/all/0/1\">Emily Denton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanna_A/0/1/0/all/0/1\">Alex Hanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FairyTailor: A Multimodal Generative Framework for Storytelling. (arXiv:2108.04324v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04324","description":"<p>Storytelling is an open-ended task that entails creative thinking and\nrequires a constant flow of ideas. Natural language generation (NLG) for\nstorytelling is especially challenging because it requires the generated text\nto follow an overall theme while remaining creative and diverse to engage the\nreader. In this work, we introduce a system and a web-based demo, FairyTailor,\nfor human-in-the-loop visual story co-creation. Users can create a cohesive\nchildren's fairytale by weaving generated texts and retrieved images with their\ninput. FairyTailor adds another modality and modifies the text generation\nprocess to produce a coherent and creative sequence of text and images. To our\nknowledge, this is the first dynamic tool for multimodal story generation that\nallows interactive co-formation of both texts and images. It allows users to\ngive feedback on co-created stories and share their results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bensaid_E/0/1/0/all/0/1\">Eden Bensaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_M/0/1/0/all/0/1\">Mauro Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1\">Benjamin Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strobelt_H/0/1/0/all/0/1\">Hendrik Strobelt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnyoneNet: Synchronized Speech and Talking Head Generation for arbitrary person. (arXiv:2108.04325v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04325","description":"<p>Automatically generating videos in which synthesized speech is synchronized\nwith lip movements in a talking head has great potential in many human-computer\ninteraction scenarios. In this paper, we present an automatic method to\ngenerate synchronized speech and talking-head videos on the basis of text and a\nsingle face image of an arbitrary person as input. In contrast to previous\ntext-driven talking head generation methods, which can only synthesize the\nvoice of a specific person, the proposed method is capable of synthesizing\nspeech for any person that is inaccessible in the training stage. Specifically,\nthe proposed method decomposes the generation of synchronized speech and\ntalking head videos into two stages, i.e., a text-to-speech (TTS) stage and a\nspeech-driven talking head generation stage. The proposed TTS module is a\nface-conditioned multi-speaker TTS model that gets the speaker identity\ninformation from face images instead of speech, which allows us to synthesize a\npersonalized voice on the basis of the input face image. To generate the\ntalking head videos from the face images, a facial landmark-based method that\ncan predict both lip movements and head rotations is proposed. Extensive\nexperiments demonstrate that the proposed method is able to generate\nsynchronized speech and talking head videos for arbitrary persons and\nnon-persons. Synthesized speech shows consistency with the given face regarding\nto the synthesized voice's timbre and one's appearance in the image, and the\nproposed landmark-based talking head method outperforms the state-of-the-art\nlandmark-based method on generating natural talking head videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinsheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qicong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg/0/1/0/all/0/1\">Scharenborg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v1 [math.NA])","link":"http://arxiv.org/abs/2108.04327","description":"<p>Natural numerical networks are introduced as a new classification algorithm\nbased on the numerical solution of nonlinear partial differential equations of\nforward-backward diffusion type on complete graphs. The proposed natural\nnumerical network is applied to open important environmental and nature\nconservation task, the automated identification of protected habitats by using\nsatellite images. In the natural numerical network, the forward diffusion\ncauses the movement of points in a feature space toward each other. The\nopposite effect, keeping the points away from each other, is caused by backward\ndiffusion. This yields the desired classification. The natural numerical\nnetwork contains a few parameters that are optimized in the learning phase of\nthe method. After learning parameters and optimizing the topology of the\nnetwork graph, classification necessary for habitat identification is\nperformed. A relevancy map for each habitat is introduced as a tool for\nvalidating the classification and finding new Natura 2000 habitat appearances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1\">Karol Mikula</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1\">Michal Kollar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1\">Aneta A. Ozvat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1\">Martin Ambroz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1\">Lucia Cahojova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1\">Ivan Jarolimek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1\">Jozef Sibik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1\">Maria Sibikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Neural Cellular Automata. (arXiv:2108.04328v1 [cs.NE])","link":"http://arxiv.org/abs/2108.04328","description":"<p>Motivated by the interaction between cells, the recently introduced concept\nof Neural Cellular Automata shows promising results in a variety of tasks. So\nfar, this concept was mostly used to generate images for a single scenario. As\neach scenario requires a new model, this type of generation seems contradictory\nto the adaptability of cells in nature. To address this contradiction, we\nintroduce a concept using different initial environments as input while using a\nsingle Neural Cellular Automata to produce several outputs. Additionally, we\nintroduce GANCA, a novel algorithm that combines Neural Cellular Automata with\nGenerative Adversarial Networks, allowing for more generalization through\nadversarial training. The experiments show that a single model is capable of\nlearning several images when presented with different inputs, and that the\nadversarially trained model improves drastically on out-of-distribution data\ncompared to a supervised trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Otte_M/0/1/0/all/0/1\">Maximilian Otte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delfosse_Q/0/1/0/all/0/1\">Quentin Delfosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czech_J/0/1/0/all/0/1\">Johannes Czech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class dependency based learning using Bi-LSTM coupled with the transfer learning of VGG16 for the diagnosis of Tuberculosis from chest x-rays. (arXiv:2108.04329v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04329","description":"<p>Tuberculosis is an infectious disease that is leading to the death of\nmillions of people across the world. The mortality rate of this disease is high\nin patients suffering from immuno-compromised disorders. The early diagnosis of\nthis disease can save lives and can avoid further complications. But the\ndiagnosis of TB is a very complex task. The standard diagnostic tests still\nrely on traditional procedures developed in the last century. These procedures\nare slow and expensive. So this paper presents an automatic approach for the\ndiagnosis of TB from posteroanterior chest x-rays. This is a two-step approach,\nwhere in the first step the lung regions are segmented from the chest x-rays\nusing the graph cut method, and then in the second step the transfer learning\nof VGG16 combined with Bi-directional LSTM is used for extracting high-level\ndiscriminative features from the segmented lung regions and then classification\nis performed using a fully connected layer. The proposed model is evaluated\nusing data from two publicly available databases namely Montgomery Country set\nand Schezien set. The proposed model achieved accuracy and sensitivity of\n97.76%, 97.01% and 96.42%, 94.11% on Schezien and Montgomery county datasets.\nThis model enhanced the diagnostic accuracy of TB by 0.7% and 11.68% on\nSchezien and Montgomery county datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdary_G/0/1/0/all/0/1\">G Jignesh Chowdary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1\">Suganya G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_P/0/1/0/all/0/1\">Premalatha M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_K/0/1/0/all/0/1\">Karunamurthy K</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating synthetic meteorology satellite visible light images during night based on GAN method. (arXiv:2108.04330v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04330","description":"<p>Meteorology satellite visible light images is critical for meteorology\nsupport and forecast. However, there is no such kind of data during night time.\nTo overcome this, we propose a method based on deep learning to create\nsynthetic satellite visible light images during night. Specifically, to produce\nmore realistic products, we train a Generative Adversarial Networks (GAN) model\nto generate visible light images given the corresponding satellite infrared\nimages and numerical weather prediction(NWP) products. To better model the\nnonlinear relationship from infrared data and NWP products to visible light\nimages, we propose to use the channel-wise attention mechanics, e.g., SEBlock\nto quantitative weight the input channels. The experiments based on the ECMWF\nNWP products and FY-4A meteorology satellite visible light and infrared\nchannels date show that the proposed methods can be effective to create\nrealistic synthetic satellite visible light images during night.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wencong_C/0/1/0/all/0/1\">CHENG Wencong</a> (1) ((1) Beijing Aviation Meteorological Institute)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging. (arXiv:2108.04344v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04344","description":"<p>Due to the limited availability and high cost of the reverse\ntranscription-polymerase chain reaction (RT-PCR) test, many studies have\nproposed machine learning techniques for detecting COVID-19 from medical\nimaging. The purpose of this study is to systematically review, assess, and\nsynthesize research articles that have used different machine learning\ntechniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.\nA structured literature search was conducted in the relevant bibliographic\ndatabases to ensure that the survey solely centered on reproducible and\nhigh-quality research. We selected papers based on our inclusion criteria. In\nthis survey, we reviewed $98$ articles that fulfilled our inclusion criteria.\nWe have surveyed a complete pipeline of chest imaging analysis techniques\nrelated to COVID-19, including data collection, pre-processing, feature\nextraction, classification, and visualization. We have considered CT scans and\nX-rays as both are widely used to describe the latest developments in medical\nimaging to detect COVID-19. This survey provides researchers with valuable\ninsights into different machine learning techniques and their performance in\nthe detection and diagnosis of COVID-19 from chest imaging. At the end, the\nchallenges and limitations in detecting COVID-19 using machine learning\ntechniques and the future direction of research are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1\">Aishwarza Panday</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Muhammad Ashad Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Nihad Karim Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (arXiv:2108.04345v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04345","description":"<p>Ultrasound is a non-invasive imaging modality that can be conveniently used\nto classify suspicious breast nodules and potentially detect the onset of\nbreast cancer. Recently, Convolutional Neural Networks (CNN) techniques have\nshown promising results in classifying ultrasound images of the breast into\nbenign or malignant. However, CNN inference acts as a black-box model, and as\nsuch, its decision-making is not interpretable. Therefore, increasing effort\nhas been dedicated to explaining this process, most notably through GRAD-CAM\nand other techniques that provide visual explanations into inner workings of\nCNNs. In addition to interpretation, these methods provide clinically important\ninformation, such as identifying the location for biopsy or treatment. In this\nwork, we analyze how adversarial assaults that are practically undetectable may\nbe devised to alter these importance maps dramatically. Furthermore, we will\nshow that this change in the importance maps can come with or without altering\nthe classification result, rendering them even harder to detect. As such, care\nmust be taken when using these importance maps to shed light on the inner\nworkings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and\npropose a new network based on ResNet-50 to improve the classification\naccuracies. Our sensitivity and specificity is comparable to the state of the\nart results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1\">Hamza Rasaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04349","description":"<p>In this paper, we present a new network named Attention Aware Network (AASeg)\nfor real time semantic image segmentation. Our network incorporates spatial and\nchannel information using Spatial Attention (SA) and Channel Attention (CA)\nmodules respectively. It also uses dense local multi-scale context information\nusing Multi Scale Context (MSC) module. The feature maps are concatenated\nindividually to produce the final segmentation map. We demonstrate the\neffectiveness of our method using a comprehensive analysis, quantitative\nexperimental results and ablation study using Cityscapes, ADE20K and Camvid\ndatasets. Our network performs better than most previous architectures with a\n74.4\\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VirtualConductor: Music-driven Conducting Video Generation System. (arXiv:2108.04350v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04350","description":"<p>In this demo, we present VirtualConductor, a system that can generate\nconducting video from any given music and a single user's image. First, a\nlarge-scale conductor motion dataset is collected and constructed. Then, we\npropose Audio Motion Correspondence Network (AMCNet) and adversarial-perceptual\nlearning to learn the cross-modal relationship and generate diverse, plausible,\nmusic-synchronized motion. Finally, we combine 3D animation rendering and a\npose transfer model to synthesize conducting video from a single given user's\nimage. Therefore, any user can become a virtual conductor through the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Delong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Feng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04351","description":"<p>This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attribute Guided Sparse Tensor-Based Model for Person Re-Identification. (arXiv:2108.04352v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04352","description":"<p>Visual perception of a person is easily influenced by many factors such as\ncamera parameters, pose and viewpoint variations. These variations make person\nRe-Identification (ReID) a challenging problem. Nevertheless, human attributes\nusually stand as robust visual properties to such variations. In this paper, we\npropose a new method to leverage features from human attributes for person\nReID. Our model uses a tensor to non-linearly fuse identity and attribute\nfeatures, and then forces the parameters of the tensor in the loss function to\ngenerate discriminative fused features for ReID. Since tensor-based methods\nusually contain a large number of parameters, training all of these parameters\nbecomes very slow, and the chance of overfitting increases as well. To address\nthis issue, we propose two new techniques based on Structural Sparsity Learning\n(SSL) and Tensor Decomposition (TD) methods to create an accurate and stable\nlearning problem. We conducted experiments on several standard pedestrian\ndatasets, and experimental results indicate that our tensor-based approach\nsignificantly improves person ReID baselines and also outperforms state of the\nart methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tasks Structure Regularization in Multi-Task Learning for Improving Facial Attribute Prediction. (arXiv:2108.04353v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04353","description":"<p>The great success of Convolutional Neural Networks (CNN) for facial attribute\nprediction relies on a large amount of labeled images. Facial image datasets\nare usually annotated by some commonly used attributes (e.g., gender), while\nlabels for the other attributes (e.g., big nose) are limited which causes their\nprediction challenging. To address this problem, we use a new Multi-Task\nLearning (MTL) paradigm in which a facial attribute predictor uses the\nknowledge of other related attributes to obtain a better generalization\nperformance. Here, we leverage MLT paradigm in two problem settings. First, it\nis assumed that the structure of the tasks (e.g., grouping pattern of facial\nattributes) is known as a prior knowledge, and parameters of the tasks (i.e.,\npredictors) within the same group are represented by a linear combination of a\nlimited number of underlying basis tasks. Here, a sparsity constraint on the\ncoefficients of this linear combination is also considered such that each task\nis represented in a more structured and simpler manner. Second, it is assumed\nthat the structure of the tasks is unknown, and then structure and parameters\nof the tasks are learned jointly by using a Laplacian regularization framework.\nOur MTL methods are compared with competing methods for facial attribute\nprediction to show its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taherkhani_F/0/1/0/all/0/1\">Fariborz Taherkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Morphometry of Closed, Implicit Surfaces. (arXiv:2108.04354v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04354","description":"<p>Anatomical structures such as the hippocampus, liver, and bones can be\nanalyzed as orientable, closed surfaces. This permits the computation of\nvolume, surface area, mean curvature, Gaussian curvature, and the\nEuler-Poincar\\'e characteristic as well as comparison of these morphometrics\nbetween structures of different topology. The structures are commonly\nrepresented implicitly in curve evolution problems as the zero level set of an\nembedding. Practically, binary images of anatomical structures are embedded\nusing a signed distance transform. However, quantization prevents the accurate\ncomputation of curvatures, leading to considerable errors in morphometry. This\npaper presents a fast, simple embedding procedure for accurate local\nmorphometry as the zero crossing of the Gaussian blurred binary image. The\nproposed method was validated based on the femur and fourth lumbar vertebrae of\n50 clinical computed tomography datasets. The results show that the signed\ndistance transform leads to large quantization errors in the computed local\ncurvature. Global validation of morphometry using regression and Bland-Altman\nanalysis revealed that the coefficient of determination for the average mean\ncurvature is improved from 93.8% with the signed distance transform to 100%\nwith the proposed method. For the surface area, the proportional bias is\nimproved from -5.0% for the signed distance transform to +0.6% for the proposed\nmethod. The Euler-Poincar\\'e characteristic is improved from unusable in the\nsigned distance transform to 98% accuracy for the proposed method. The proposed\nmethod enables an improved local and global evaluation of curvature for\npurposes of morphometry on closed, implicit surfaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Besler_B/0/1/0/all/0/1\">Bryce A Besler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_T/0/1/0/all/0/1\">Tannis D. Kemp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalski_A/0/1/0/all/0/1\">Andrew S. Michalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forkert_N/0/1/0/all/0/1\">Nils D. Forkert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_S/0/1/0/all/0/1\">Steven K. Boyd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperparameter Analysis for Derivative Compressive Sampling. (arXiv:2108.04355v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04355","description":"<p>Derivative compressive sampling (DCS) is a signal reconstruction method from\nmeasurements of the spatial gradient with sub-Nyquist sampling rate.\nApplications of DCS include optical image reconstruction, photometric stereo,\nand shape-from-shading. In this work, we study the sensitivity of DCS with\nrespect to algorithmic hyperparameters using a brute-force search algorithm. We\nperform experiments on a dataset of surface images and deduce guidelines for\nthe user to setup values for the hyperparameters for improved signal recovery\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabbi_M/0/1/0/all/0/1\">Md Fazle Rabbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Lane Detection Associated with Quaternion Hardy Filter. (arXiv:2108.04356v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04356","description":"<p>In this article, a robust color-edge feature extraction method based on the\nQuaternion Hardy filter is proposed. The Quaternion Hardy filter is an emerging\nedge detection theory. It is along with the Poisson and conjugate Poisson\nsmoothing kernels to handle various types of noise. Combining with the\nQuaternion Hardy filter, Jin's color gradient operator and Hough transform, the\ncolor-edge feature detection algorithm is proposed and applied to the lane\nmarking detection. Experiments are presented to demonstrate the validity of the\nproposed algorithm. The results are accurate and robust with respect to the\ncomplex environment lane markings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wenshan Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">Dong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1\">Kit Ian Kou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam. (arXiv:2108.04357v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04357","description":"<p>Touchless computer interaction has become an important consideration during\nthe COVID-19 pandemic period. Despite progress in machine learning and computer\nvision that allows for advanced gesture recognition, an integrated collection\nof such open-source methods and a user-customisable approach to utilising them\nin a low-cost solution for touchless interaction in existing software is still\nmissing. In this paper, we introduce the MotionInput v2.0 application. This\napplication utilises published open-source libraries and additional gesture\ndefinitions developed to take the video stream from a standard RGB webcam as\ninput. It then maps human motion gestures to input operations for existing\napplications and games. The user can choose their own preferred way of\ninteracting from a series of motion types, including single and bi-modal hand\ngesturing, full-body repetitive or extremities-based exercises, head and facial\nmovements, eye tracking, and combinations of the above. We also introduce a\nseries of bespoke gesture recognition classifications as DirectInput triggers,\nincluding gestures for idle states, auto calibration, depth capture from a 2D\nRGB webcam stream and tracking of facial motions such as mouth motions,\nwinking, and head direction with rotation. Three use case areas assisted the\ndevelopment of the modules: creativity software, office and clinical software,\nand gaming software. A collection of open-source libraries has been integrated\nand provide a layer of modular gesture mapping on top of existing mouse and\nkeyboard controls in Windows via DirectX. With ease of access to webcams\nintegrated into most laptops and desktop computers, touchless computing becomes\nmore available with MotionInput v2.0, in a federated and locally processed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1\">Ashild Kummen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1\">Ali Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1\">Teodora Ganeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qianying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1\">Robert Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1\">Chenuka Ratwatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1\">Emil Almazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1\">Sheena Visram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Andrew Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1\">Neil J Sebire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1\">Lee Stott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1\">Yvonne Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1\">Graham Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1\">Dean Mohamedally</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients. (arXiv:2108.04358v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04358","description":"<p>Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as\na result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye\nillness caused by diabetes, can lead to blindness if it is not identified and\ntreated in its early stages. Unfortunately, diagnosis of DR requires medically\ntrained professionals, but Bangladesh has limited specialists in comparison to\nits population. Moreover, the screening process is often expensive, prohibiting\nmany from receiving timely and proper diagnosis. To address the problem, we\nintroduce a deep learning algorithm which screens for different stages of DR.\nWe use a state-of-the-art CNN architecture to diagnose patients based on\nretinal fundus imagery. This paper is an experimental evaluation of the\nalgorithm we developed for DR diagnosis and screening specifically for\nBangladeshi patients. We perform this validation study using separate pools of\nretinal image data of real patients from a hospital and field studies in\nBangladesh. Our results show that the algorithm is effective at screening\nBangladeshi eyes even when trained on a public dataset which is out of domain,\nand can accurately determine the stage of DR as well, achieving an overall\naccuracy of 92.27\\% and 93.02\\% on two validation sets of Bangladeshi eyes. The\nresults confirm the ability of the algorithm to be used in real clinical\nsettings and applications due to its high accuracy and classwise metrics. Our\nalgorithm is implemented in the application Drishti, which is used to screen\nfor DR in patients living in rural areas in Bangladesh, where access to\nprofessional screening is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1\">Ipsita Sutradhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mahziba Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1\">Malabika Sarker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptable image quality assessment using meta-reinforcement learning of task amenability. (arXiv:2108.04359v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04359","description":"<p>The performance of many medical image analysis tasks are strongly associated\nwith image data quality. When developing modern deep learning algorithms,\nrather than relying on subjective (human-based) image quality assessment (IQA),\ntask amenability potentially provides an objective measure of task-specific\nimage quality. To predict task amenability, an IQA agent is trained using\nreinforcement learning (RL) with a simultaneously optimised task predictor,\nsuch as a classification or segmentation neural network. In this work, we\ndevelop transfer learning or adaptation strategies to increase the adaptability\nof both the IQA agent and the task predictor so that they are less dependent on\nhigh-quality, expert-labelled training data. The proposed transfer learning\nstrategy re-formulates the original RL problem for task amenability in a\nmeta-reinforcement learning (meta-RL) framework. The resulting algorithm\nfacilitates efficient adaptation of the agent to different definitions of image\nquality, each with its own Markov decision process environment including\ndifferent images, labels and an adaptable task predictor. Our work demonstrates\nthat the IQA agents pre-trained on non-expert task labels can be adapted to\npredict task amenability as defined by expert task labels, using only a small\nset of expert labels. Using 6644 clinical ultrasound images from 249 prostate\ncancer patients, our results for image classification and segmentation tasks\nshow that the proposed IQA method can be adapted using data with as few as\nrespective 19.7% and 29.6% expert-reviewed consensus labels and still achieve\ncomparable IQA and task performance, which would otherwise require a training\ndataset with 100% expert labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1\">Shaheer U. Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1\">Vasilis Stavrinides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M. C. Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Richard E. Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey A. Sonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. (arXiv:2108.04384v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04384","description":"<p>For the past ten years, CNN has reigned supreme in the world of computer\nvision, but recently, Transformer is on the rise. However, the quadratic\ncomputational cost of self-attention has become a severe problem of practice.\nThere has been much research on architectures without CNN and self-attention in\nthis context. In particular, MLP-Mixer is a simple idea designed using MLPs and\nhit an accuracy comparable to the Vision Transformer. However, the only\ninductive bias in this architecture is the embedding of tokens. Thus, there is\nstill a possibility to build a non-convolutional inductive bias into the\narchitecture itself, and we built in an inductive bias using two simple ideas.\nA way is to divide the token-mixing block vertically and horizontally. Another\nway is to make spatial correlations denser among some channels of token-mixing.\nWith this approach, we were able to improve the accuracy of the MLP-Mixer while\nreducing its parameters and computational complexity. Compared to other\nMLP-based models, the proposed model, named RaftMLP has a good balance of\ncomputational complexity, the number of parameters, and actual memory usage. In\naddition, our work indicates that MLP-based models have the potential to\nreplace CNNs by adopting inductive bias. The source code in PyTorch version is\navailable at \\url{https://github.com/okojoalg/raft-mlp}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1\">Yuki Tatsunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Architecture Selection in Differentiable NAS. (arXiv:2108.04392v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04392","description":"<p>Differentiable Neural Architecture Search is one of the most popular Neural\nArchitecture Search (NAS) methods for its search efficiency and simplicity,\naccomplished by jointly optimizing the model weight and architecture parameters\nin a weight-sharing supernet via gradient-based algorithms. At the end of the\nsearch phase, the operations with the largest architecture parameters will be\nselected to form the final architecture, with the implicit assumption that the\nvalues of architecture parameters reflect the operation strength. While much\nhas been discussed about the supernet's optimization, the architecture\nselection process has received little attention. We provide empirical and\ntheoretical analysis to show that the magnitude of architecture parameters does\nnot necessarily indicate how much the operation contributes to the supernet's\nperformance. We propose an alternative perturbation-based architecture\nselection that directly measures each operation's influence on the supernet. We\nre-evaluate several differentiable NAS methods with the proposed architecture\nselection and find that it is able to extract significantly improved\narchitectures from the underlying supernets consistently. Furthermore, we find\nthat several failure modes of DARTS can be greatly alleviated with the proposed\nselection method, indicating that much of the poor generalization observed in\nDARTS can be attributed to the failure of magnitude-based architecture\nselection rather than entirely the optimization of its supernet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stroke Correspondence by Labeling Closed Areas. (arXiv:2108.04393v1 [cs.GR])","link":"http://arxiv.org/abs/2108.04393","description":"<p>Constructing stroke correspondences between keyframes is one of the most\nimportant processes in the production pipeline of hand-drawn inbetweening\nframes. This process requires time-consuming manual work imposing a tremendous\nburden on the animators. We propose a method to estimate stroke correspondences\nbetween raster character images (keyframes) without vectorization processes.\nFirst, the proposed system separates the closed areas in each keyframe and\nestimates the correspondences between closed areas by using the characteristics\nof shape, depth, and closed area connection. Second, the proposed system\nestimates stroke correspondences from the estimated closed area\ncorrespondences. We demonstrate the effectiveness of our method by performing a\nuser study and comparing the proposed system with conventional approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miyauchi_R/0/1/0/all/0/1\">Ryoma Miyauchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukusato_T/0/1/0/all/0/1\">Tsukasa Fukusato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyata_K/0/1/0/all/0/1\">Kazunori Miyata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04409","description":"<p>Deep Neural Networks (DNNs) are vulnerable to adversarial examples which\nwould inveigle neural networks to make prediction errors with small per-\nturbations on the input images. Researchers have been devoted to promoting the\nresearch on the universal adversarial perturbations (UAPs) which are\ngradient-free and have little prior knowledge on data distributions. Procedural\nadversarial noise at- tack is a data-free universal perturbation generation\nmethod. In this paper, we propose two universal adversarial perturbation (UAP)\ngeneration methods based on procedural noise functions: Simplex noise and\nWorley noise. In our framework, the shading which disturbs visual\nclassification is generated with rendering technology. Without changing the\nsemantic representations, the adversarial examples generated via our methods\nshow superior performance on the attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean. (arXiv:2108.04423v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04423","description":"<p>Deep learning models achieve strong performance for radiology image\nclassification, but their practical application is bottlenecked by the need for\nlarge labeled training datasets. Semi-supervised learning (SSL) approaches\nleverage small labeled datasets alongside larger unlabeled datasets and offer\npotential for reducing labeling cost. In this work, we introduce NoTeacher, a\nnovel consistency-based SSL framework which incorporates probabilistic\ngraphical models. Unlike Mean Teacher which maintains a teacher network updated\nvia a temporal ensemble, NoTeacher employs two independent networks, thereby\neliminating the need for a teacher network. We demonstrate how NoTeacher can be\ncustomized to handle a range of challenges in radiology image classification.\nSpecifically, we describe adaptations for scenarios with 2D and 3D inputs, uni\nand multi-label classification, and class distribution mismatch between labeled\nand unlabeled portions of the training data. In realistic empirical evaluations\non three public benchmark datasets spanning the workhorse modalities of\nradiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the\nfully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher\noutperforms established SSL methods with minimal hyperparameter tuning, and has\nimplications as a principled and practical option for semisupervised learning\nin radiology applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1\">Balagopal Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1\">Shafa Balaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan Sheng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1\">Pavitra Krishnaswamy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face Inpainting. (arXiv:2108.04424v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04424","description":"<p>Blind face inpainting refers to the task of reconstructing visual contents\nwithout explicitly indicating the corrupted regions in a face image.\nInherently, this task faces two challenges: (1) how to detect various mask\npatterns of different shapes and contents; (2) how to restore visually\nplausible and pleasing contents in the masked regions. In this paper, we\npropose a novel two-stage blind face inpainting method named Frequency-guided\nTransformer and Top-Down Refinement Network (FT-TDR) to tackle these\nchallenges. Specifically, we first use a transformer-based network to detect\nthe corrupted regions to be inpainted as masks by modeling the relation among\ndifferent patches. We also exploit the frequency modality as complementary\ninformation for improved detection results and capture the local contextual\nincoherence to enhance boundary consistency. Then a top-down refinement network\nis proposed to hierarchically restore features at different levels and generate\ncontents that are semantically consistent with the unmasked face regions.\nExtensive experiments demonstrate that our method outperforms current\nstate-of-the-art blind and non-blind face inpainting methods qualitatively and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Aware Universal Style Transfer. (arXiv:2108.04441v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04441","description":"<p>Style transfer aims to reproduce content images with the styles from\nreference images. Existing universal style transfer methods successfully\ndeliver arbitrary styles to original images either in an artistic or a\nphoto-realistic way. However, the range of 'arbitrary style' defined by\nexisting works is bounded in the particular domain due to their structural\nlimitation. Specifically, the degrees of content preservation and stylization\nare established according to a predefined target domain. As a result, both\nphoto-realistic and artistic models have difficulty in performing the desired\nstyle transfer for the other domain. To overcome this limitation, we propose a\nunified architecture, Domain-aware Style Transfer Networks (DSTN) that transfer\nnot only the style but also the property of domain (i.e., domainness) from a\ngiven reference image. To this end, we design a novel domainness indicator that\ncaptures the domainness value from the texture and structural features of\nreference images. Moreover, we introduce a unified framework with domain-aware\nskip connection to adaptively transfer the stroke and palette to the input\ncontents guided by the domainness indicator. Our extensive experiments validate\nthat our model produces better qualitative results and outperforms previous\nmethods in terms of proxy metrics on both artistic and photo-realistic\nstylizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kibeom Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1\">Seogkyu Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1\">Hyeran Byun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer. (arXiv:2108.04444v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04444","description":"<p>Point cloud completion aims to predict a complete shape in high accuracy from\nits partial observation. However, previous methods usually suffered from\ndiscrete nature of point cloud and unstructured prediction of points in local\nregions, which makes it hard to reveal fine local geometric details on the\ncomplete shape. To resolve this issue, we propose SnowflakeNet with Snowflake\nPoint Deconvolution (SPD) to generate the complete point clouds. The\nSnowflakeNet models the generation of complete point clouds as the\nsnowflake-like growth of points in 3D space, where the child points are\nprogressively generated by splitting their parent points after each SPD. Our\ninsight of revealing detailed geometry is to introduce skip-transformer in SPD\nto learn point splitting patterns which can fit local regions the best.\nSkip-transformer leverages attention mechanism to summarize the splitting\npatterns used in the previous SPD layer to produce the splitting in the current\nSPD layer. The locally compact and structured point cloud generated by SPD is\nable to precisely capture the structure characteristic of 3D shape in local\npatches, which enables the network to predict highly detailed geometries, such\nas smooth regions, sharp edges and corners. Our experimental results outperform\nthe state-of-the-art point cloud completion methods under widely used\nbenchmarks. Code will be available at\nhttps://github.com/AllenXiangX/SnowflakeNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_P/0/1/0/all/0/1\">Peng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan-Pei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1\">Pengfei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Method Towards CVPR 2021 Image Matching Challenge. (arXiv:2108.04453v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04453","description":"<p>This report describes Megvii-3D team's approach towards CVPR 2021 Image\nMatching Workshop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiaopeng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dehao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ran Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPNet: Cross-Parallel Network for Efficient Anomaly Detection. (arXiv:2108.04454v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04454","description":"<p>Anomaly detection in video streams is a challengingproblem because of the\nscarcity of abnormal events andthe difficulty of accurately annotating them.To\nallevi-ate these issues, unsupervised learning-based predictionmethods have\nbeen previously applied. These approachestrain the model with only normal\nevents and predict a fu-ture frame from a sequence of preceding frames by use\nofencoder-decoder architectures so that they result in smallprediction errors\non normal events but large errors on ab-normal events. The architecture,\nhowever, comes with thecomputational burden as some anomaly detection tasks\nre-quire low computational cost without sacrificing perfor-mance. In this\npaper, Cross-Parallel Network (CPNet) forefficient anomaly detection is\nproposed here to minimizecomputations without performance drops. It consists\nofNsmaller parallel U-Net, each of which is designed to handlea single input\nframe, to make the calculations significantlymore efficient. Additionally, an\ninter-network shift moduleis incorporated to capture temporal relationships\namong se-quential frames to enable more accurate future predictions.The\nquantitative results show that our model requires lesscomputational cost than\nthe baseline U-Net while deliver-ing equivalent performance in anomaly\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngsaeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">David Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1\">Hanseok Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reference-based Defect Detection Network. (arXiv:2108.04456v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04456","description":"<p>The defect detection task can be regarded as a realistic scenario of object\ndetection in the computer vision field and it is widely used in the industrial\nfield. Directly applying vanilla object detector to defect detection task can\nachieve promising results, while there still exists challenging issues that\nhave not been solved. The first issue is the texture shift which means a\ntrained defect detector model will be easily affected by unseen texture, and\nthe second issue is partial visual confusion which indicates that a partial\ndefect box is visually similar with a complete box. To tackle these two\nproblems, we propose a Reference-based Defect Detection Network (RDDN).\nSpecifically, we introduce template reference and context reference to against\nthose two problems, respectively. Template reference can reduce the texture\nshift from image, feature or region levels, and encourage the detectors to\nfocus more on the defective area as a result. We can use either well-aligned\ntemplate images or the outputs of a pseudo template generator as template\nreferences in this work, and they are jointly trained with detectors by the\nsupervision of normal samples. To solve the partial visual confusion issue, we\npropose to leverage the carried context information of context reference, which\nis the concentric bigger box of each region proposal, to perform more accurate\nregion classification and regression. Experiments on two defect detection\ndatasets demonstrate the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zhaoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Method Towards CVPR 2021 SimLocMatch Challenge. (arXiv:2108.04466v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04466","description":"<p>This report describes Megvii-3D team's approach to-wards SimLocMatch\nChallenge @ CVPR 2021 Image Matching Workshop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_X/0/1/0/all/0/1\">Xiaopeng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ran Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zheng Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SP-GAN: Sphere-Guided 3D Shape Generation and Manipulation. (arXiv:2108.04476v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04476","description":"<p>We present SP-GAN, a new unsupervised sphere-guided generative model for\ndirect synthesis of 3D shapes in the form of point clouds. Compared with\nexisting models, SP-GAN is able to synthesize diverse and high-quality shapes\nwith fine details and promote controllability for part-aware shape generation\nand manipulation, yet trainable without any parts annotations. In SP-GAN, we\nincorporate a global prior (uniform points on a sphere) to spatially guide the\ngenerative process and attach a local prior (a random latent code) to each\nsphere point to provide local details. The key insight in our design is to\ndisentangle the complex 3D shape generation task into a global shape modeling\nand a local structure adjustment, to ease the learning process and enhance the\nshape generation quality. Also, our model forms an implicit dense\ncorrespondence between the sphere points and points in every generated shape,\nenabling various forms of structure-aware shape manipulations such as part\nediting, part-wise shape interpolation, and multi-shape part composition, etc.,\nbeyond the existing generative models. Experimental results, which include both\nvisual and quantitative evaluations, demonstrate that our model is able to\nsynthesize diverse point clouds with fine details and less noise, as compared\nwith the state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1\">Ka-Hei Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Reverse Image Search Engine for NASAWorldview. (arXiv:2108.04479v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04479","description":"<p>Researchers often spend weeks sifting through decades of unlabeled satellite\nimagery(on NASA Worldview) in order to develop datasets on which they can start\nconducting research. We developed an interactive, scalable and fast image\nsimilarity search engine (which can take one or more images as the query image)\nthat automatically sifts through the unlabeled dataset reducing dataset\ngeneration time from weeks to minutes. In this work, we describe key components\nof the end to end pipeline. Our similarity search system was created to be able\nto identify similar images from a potentially petabyte scale database that are\nsimilar to an input image, and for this we had to break down each query image\ninto its features, which were generated by a classification layer stripped CNN\ntrained in a supervised manner. To store and search these features efficiently,\nwe had to make several scalability improvements. To improve the speed, reduce\nthe storage, and shrink memory requirements for embedding search, we add a\nfully connected layer to our CNN make all images into a 128 length vector\nbefore entering the classification layers. This helped us compress the size of\nour image features from 2048 (for ResNet, which was initially tried as our\nfeaturizer) to 128 for our new custom model. Additionally, we utilize existing\napproximate nearest neighbor search libraries to significantly speed up\nembedding search. Our system currently searches over our entire database of\nimages at 5 seconds per query on a single virtual machine in the cloud. In the\nfuture, we would like to incorporate a SimCLR based featurizing model which\ncould be trained without any labelling by a human (since the classification\naspect of the model is irrelevant to this use case).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sodani_A/0/1/0/all/0/1\">Abhigya Sodani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1\">Michael Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1\">Meher Anand Kasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Featureswith Split-and-Share Module. (arXiv:2108.04500v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04500","description":"<p>Deep convolutional neural networks (CNNs) have shown state-of-the-art\nperformances in various computer vision tasks. Advances on CNN architectures\nhave focused mainly on designing convolutional blocks of the feature\nextractors, but less on the classifiers that exploit extracted features. In\nthis work, we propose Split-and-Share Module (SSM),a classifier that splits a\ngiven feature into parts, which are partially shared by multiple\nsub-classifiers. Our intuition is that the more the features are shared, the\nmore common they will become, and SSM can encourage such structural\ncharacteristics in the split features. SSM can be easily integrated into any\narchitecture without bells and whistles. We have extensively validated the\nefficacy of SSM on ImageNet-1K classification task, andSSM has shown consistent\nand significant improvements over baseline architectures. In addition, we\nanalyze the effect of SSM using the Grad-CAM visualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaemin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongchan Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dong-Geol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TBNet:Two-Stream Boundary-aware Network for Generic Image Manipulation Localization. (arXiv:2108.04508v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04508","description":"<p>Finding tampered regions in images is a hot research topic in machine\nlearning and computer vision. Although many image manipulation location\nalgorithms have been proposed, most of them only focus on the RGB images with\ndifferent color spaces, and the frequency information that contains the\npotential tampering clues is often ignored. In this work, a novel end-to-end\ntwo-stream boundary-aware network (abbreviated as TBNet) is proposed for\ngeneric image manipulation localization in which the RGB stream, the frequency\nstream, and the boundary artifact location are explored in a unified framework.\nSpecifically, we first design an adaptive frequency selection module (AFS) to\nadaptively select the appropriate frequency to mine inconsistent statistics and\neliminate the interference of redundant statistics. Then, an adaptive\ncross-attention fusion module (ACF) is proposed to adaptively fuse the RGB\nfeature and the frequency feature. Finally, the boundary artifact location\nnetwork (BAL) is designed to locate the boundary artifacts for which the\nparameters are jointly updated by the outputs of the ACF, and its results are\nfurther fed into the decoder. Thus, the parameters of the RGB stream, the\nfrequency stream, and the boundary artifact location network are jointly\noptimized, and their latent complementary relationships are fully mined. The\nresults of extensive experiments performed on four public benchmarks of the\nimage manipulation localization task, namely, CASIA1.0, COVER, Carvalho, and\nIn-The-Wild, demonstrate that the proposed TBNet can significantly outperform\nstate-of-the-art generic image manipulation localization methods in terms of\nboth MCC and F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Weili Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Self-consistent Parallel Magnetic Resonance Imaging Reconstruction based on Nonlocal Low-Rank Regularization. (arXiv:2108.04517v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04517","description":"<p>Iterative self-consistent parallel imaging reconstruction (SPIRiT) is an\neffective self-calibrated reconstruction model for parallel magnetic resonance\nimaging (PMRI). The joint L1 norm of wavelet coefficients and joint total\nvariation (TV) regularization terms are incorporated into the SPIRiT model to\nimprove the reconstruction performance. The simultaneous two-directional\nlow-rankness (STDLR) in k-space data is incorporated into SPIRiT to realize\nimproved reconstruction. Recent methods have exploited the nonlocal\nself-similarity (NSS) of images by imposing nonlocal low-rankness of similar\npatches to achieve a superior performance. To fully utilize both the NSS in\nMagnetic resonance (MR) images and calibration consistency in the k-space\ndomain, we propose a nonlocal low-rank (NLR)-SPIRiT model by incorporating NLR\nregularization into the SPIRiT model. We apply the weighted nuclear norm (WNN)\nas a surrogate of the rank and employ the Nash equilibrium (NE) formulation and\nalternating direction method of multipliers (ADMM) to efficiently solve the\nNLR-SPIRiT model. The experimental results demonstrate the superior performance\nof NLR-SPIRiT over the state-of-the-art methods via three objective metrics and\nvisual comparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Ting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jizhong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking. (arXiv:2108.04521v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04521","description":"<p>Jointly exploiting multiple different yet complementary domain information\nhas been proven to be an effective way to perform robust object tracking. This\npaper focuses on effectively representing and utilizing complementary features\nfrom the frame domain and event domain for boosting object tracking performance\nin challenge scenarios. Specifically, we propose Common Features Extractor\n(CFE) to learn potential common representations from the RGB domain and event\ndomain. For learning the unique features of the two domains, we utilize a\nUnique Extractor for Event (UEE) based on Spiking Neural Networks to extract\nedge cues in the event domain which may be missed in RGB in some challenging\nconditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional\nNeural Networks to extract texture and semantic information in RGB domain.\nExtensive experiments on standard RGB benchmark and real event tracking dataset\ndemonstrate the effectiveness of the proposed approach. We show our approach\noutperforms all compared state-of-the-art tracking algorithms and verify\nevent-based data is a powerful cue for tracking in challenging scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingkai Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multigranular Visual-Semantic Embedding for Cloth-Changing Person Re-identification. (arXiv:2108.04527v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04527","description":"<p>Person reidentification (ReID) is a very hot research topic in machine\nlearning and computer vision, and many person ReID approaches have been\nproposed; however, most of these methods assume that the same person has the\nsame clothes within a short time interval, and thus their visual appearance\nmust be similar. However, in an actual surveillance environment, a given person\nhas a great probability of changing clothes after a long time span, and they\nalso often take different personal belongings with them. When the existing\nperson ReID methods are applied in this type of case, almost all of them fail.\nTo date, only a few works have focused on the cloth-changing person ReID task,\nbut since it is very difficult to extract generalized and robust features for\nrepresenting people with different clothes, their performances need to be\nimproved. Moreover, visual-semantic information is often ignored. To solve\nthese issues, in this work, a novel multigranular visual-semantic embedding\nalgorithm (MVSE) is proposed for cloth-changing person ReID, where visual\nsemantic information and human attributes are embedded into the network, and\nthe generalized features of human appearance can be well learned to effectively\nsolve the problem of clothing changes. Specifically, to fully represent a\nperson with clothing changes, a multigranular feature representation scheme\n(MGR) is employed to focus on the unchanged part of the human, and then a cloth\ndesensitization network (CDN) is designed to improve the feature robustness of\nthe approach for the person with different clothing, where different high-level\nhuman attributes are fully utilized. Moreover, to further solve the issue of\npose changes and occlusion under different camera perspectives, a partially\nsemantically aligned network (PSA) is proposed to obtain the visual-semantic\ninformation that is used to align the human attributes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Hongwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1\">Weili Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weizhi Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand Pose Classification Based on Neural Networks. (arXiv:2108.04529v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04529","description":"<p>In this work, deep learning models are applied to a segment of a robust\nhand-washing dataset that has been created with the help of 30 volunteers. This\nwork demonstrates the classification of presence of one hand, two hands and no\nhand in the scene based on transfer learning. The pre-trained model; simplest\nNN from Keras library is utilized to train the network with 704 images of hand\ngestures and the predictions are carried out for the input image. Due to the\ncontrolled and restricted dataset, 100% accuracy is achieved during the\ntraining with correct predictions for the input image. Complete handwashing\ndataset with dense models such as AlexNet for video classification for hand\nhygiene stages will be used in the future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1\">Rashmi Bakshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer. (arXiv:2108.04533v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04533","description":"<p>Attribute-based person search is the task of finding person images that are\nbest matched with a set of text attributes given as query. The main challenge\nof this task is the large modality gap between attributes and images. To reduce\nthe gap, we present a new loss for learning cross-modal embeddings in the\ncontext of attribute-based person search. We regard a set of attributes as a\ncategory of people sharing the same traits. In a joint embedding space of the\ntwo modalities, our loss pulls images close to their person categories for\nmodality alignment. More importantly, it pushes apart a pair of person\ncategories by a margin determined adaptively by their semantic distance, where\nthe distance metric is learned end-to-end so that the loss considers importance\nof each attribute when relating person categories. Our loss guided by the\nadaptive semantic margin leads to more discriminative and semantically\nwell-arranged distributions of person images. As a consequence, it enables a\nsimple embedding model to achieve state-of-the-art records on public benchmarks\nwithout bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_B/0/1/0/all/0/1\">Boseung Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jicheol Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1\">Suha Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition. (arXiv:2108.04536v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04536","description":"<p>The task of skeleton-based action recognition remains a core challenge in\nhuman-centred scene understanding due to the multiple granularities and large\nvariation in human motion. Existing approaches typically employ a single neural\nrepresentation for different motion patterns, which has difficulty in capturing\nfine-grained action classes given limited training data. To address the\naforementioned problems, we propose a novel multi-granular spatio-temporal\ngraph network for skeleton-based action classification that jointly models the\ncoarse- and fine-grained skeleton motion patterns. To this end, we develop a\ndual-head graph network consisting of two interleaved branches, which enables\nus to extract features at two spatio-temporal resolutions in an effective and\nefficient manner. Moreover, our network utilises a cross-head communication\nstrategy to mutually enhance the representations of both heads. We conducted\nextensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU\nRGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance\non all the benchmarks, which validates the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrUMAn: Trope Understanding in Movies and Animations. (arXiv:2108.04542v1 [cs.AI])","link":"http://arxiv.org/abs/2108.04542","description":"<p>Understanding and comprehending video content is crucial for many real-world\napplications such as search and recommendation systems. While recent progress\nof deep learning has boosted performance on various tasks using visual cues,\ndeep cognition to reason intentions, motivation, or causality remains\nchallenging. Existing datasets that aim to examine video reasoning capability\nfocus on visual signals such as actions, objects, relations, or could be\nanswered utilizing text bias. Observing this, we propose a novel task, along\nwith a new dataset: Trope Understanding in Movies and Animations (TrUMAn),\nintending to evaluate and develop learning systems beyond visual signals.\nTropes are frequently used storytelling devices for creative works. By coping\nwith the trope understanding task and enabling the deep cognition skills of\nmachines, we are optimistic that data mining applications and algorithms could\nbe taken to the next level. To tackle the challenging TrUMAn dataset, we\npresent a Trope Understanding and Storytelling (TrUSt) with a new Conceptual\nStoryteller module, which guides the video encoder by performing video\nstorytelling on a latent space. The generated story embedding is then fed into\nthe trope understanding model to provide further signals. Experimental results\ndemonstrate that state-of-the-art learning systems on existing tasks reach only\n12.01% of accuracy with raw input signals. Also, even in the oracle case with\nhuman-annotated descriptions, BERT contextual embedding achieves at most 28% of\naccuracy. Our proposed TrUSt boosts the model performance and reaches 13.94%\nperformance. We also provide detailed analysis topave the way for future\nresearch. TrUMAn is publicly available\nat:https://www.cmlab.csie.ntu.edu.tw/project/trope\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1\">Po-Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_B/0/1/0/all/0/1\">Bing-Chen Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wen-Feng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Ke-Jyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future. (arXiv:2108.04543v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04543","description":"<p>In this article, we perform a review of the state-of-the-art of hybrid\nmachine learning in medical imaging. We start with a short summary of the\ngeneral developments of the past in machine learning and how general and\nspecialized approaches have been in competition in the past decades. A\nparticular focus will be the theoretical and experimental evidence pro and\ncontra hybrid modelling. Next, we inspect several new developments regarding\nhybrid machine learning with a particular focus on so-called known operator\nlearning and how hybrid approaches gain more and more momentum across\nessentially all applications in medical imaging and medical image analysis. As\nwe will point out by numerous examples, hybrid models are taking over in image\nreconstruction and analysis. Even domains such as physical simulation and\nscanner and acquisition design are being addressed using machine learning grey\nbox modelling approaches. Towards the end of the article, we will investigate a\nfew future directions and point out relevant areas in which hybrid modelling,\nmeta learning, and other domains will likely be able to drive the\nstate-of-the-art ahead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1\">Harald K&#xf6;stler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1\">Marco Heisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1\">Patrick Krauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-wise Hard Negative Example Generation for Contrastive Learning in Unpaired Image-to-Image Translation. (arXiv:2108.04547v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04547","description":"<p>Contrastive learning shows great potential in unpaired image-to-image\ntranslation, but sometimes the translated results are in poor quality and the\ncontents are not preserved consistently. In this paper, we uncover that the\nnegative examples play a critical role in the performance of contrastive\nlearning for image translation. The negative examples in previous methods are\nrandomly sampled from the patches of different positions in the source image,\nwhich are not effective to push the positive examples close to the query\nexamples. To address this issue, we present instance-wise hard Negative Example\nGeneration for Contrastive learning in Unpaired image-to-image\nTranslation~(NEGCUT). Specifically, we train a generator to produce negative\nexamples online. The generator is novel from two perspectives: 1) it is\ninstance-wise which means that the generated examples are based on the input\nimage, and 2) it can generate hard negative examples since it is trained with\nan adversarial loss. With the generator, the performance of unpaired\nimage-to-image translation is significantly improved. Experiments on three\nbenchmark datasets demonstrate that the proposed NEGCUT framework achieves\nstate-of-the-art performance compared to previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks. (arXiv:2108.04558v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04558","description":"<p>Human observers engage in selective information uptake when classifying\nvisual patterns. The same is true of deep neural networks, which currently\nconstitute the best performing artificial vision systems. Our goal is to\nexamine the congruence, or lack thereof, in the information-gathering\nstrategies of the two systems. We have operationalized our investigation as a\ncharacter recognition task. We have used eye-tracking to assay the spatial\ndistribution of information hotspots for humans via fixation maps and an\nactivation mapping technique for obtaining analogous distributions for deep\nnetworks through visualization maps. Qualitative comparison between\nvisualization maps and fixation maps reveals an interesting correlate of\ncongruence. The deep learning model considered similar regions in character,\nwhich humans have fixated in the case of correctly classified characters. On\nthe other hand, when the focused regions are different for humans and deep\nnets, the characters are typically misclassified by the latter. Hence, we\npropose to use the visual fixation maps obtained from the eye-tracking\nexperiment as a supervisory input to align the model's focus on relevant\ncharacter regions. We find that such supervision improves the model's\nperformance significantly and does not require any additional parameters. This\napproach has the potential to find applications in diverse domains such as\nmedical analysis and surveillance in which explainability helps to determine\nsystem fidelity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ralekar_C/0/1/0/all/0/1\">Chetan Ralekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1\">Shubham Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan Kumar Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1\">Santanu Chaudhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Metric Learning for Open World Semantic Segmentation. (arXiv:2108.04562v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04562","description":"<p>Classical close-set semantic segmentation networks have limited ability to\ndetect out-of-distribution (OOD) objects, which is important for\nsafety-critical applications such as autonomous driving. Incrementally learning\nthese OOD objects with few annotations is an ideal way to enlarge the knowledge\nbase of the deep learning models. In this paper, we propose an open world\nsemantic segmentation system that includes two modules: (1) an open-set\nsemantic segmentation module to detect both in-distribution and OOD objects.\n(2) an incremental few-shot learning module to gradually incorporate those OOD\nobjects into its existing knowledge base. This open world semantic segmentation\nsystem behaves like a human being, which is able to identify OOD objects and\ngradually learn them with corresponding supervision. We adopt the Deep Metric\nLearning Network (DMLNet) with contrastive clustering to implement open-set\nsemantic segmentation. Compared to other open-set semantic segmentation\nmethods, our DMLNet achieves state-of-the-art performance on three challenging\nopen-set semantic segmentation datasets without using additional data or\ngenerative models. On this basis, two incremental few-shot learning methods are\nfurther proposed to progressively improve the DMLNet with the annotations of\nOOD objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_P/0/1/0/all/0/1\">Peng Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Junhao Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (arXiv:2108.04584v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04584","description":"<p>Scene understanding is crucial for autonomous systems which intend to operate\nin the real world. Single task vision networks extract information only based\non some aspects of the scene. In multi-task learning (MTL), on the other hand,\nthese single tasks are jointly learned, thereby providing an opportunity for\ntasks to share information and obtain a more comprehensive understanding. To\nthis end, we develop UniNet, a unified scene understanding network that\naccurately and efficiently infers vital vision tasks including object\ndetection, semantic segmentation, instance segmentation, monocular depth\nestimation, and monocular instance depth prediction. As these tasks look at\ndifferent semantic and geometric information, they can either complement or\nconflict with each other. Therefore, understanding inter-task relationships can\nprovide useful cues to enable complementary information sharing. We evaluate\nthe task relationships in UniNet through the lens of adversarial attacks based\non the notion that they can exploit learned biases and task interactions in the\nneural network. Extensive experiments on the Cityscapes dataset, using\nuntargeted and targeted attacks reveal that semantic tasks strongly interact\namongst themselves, and the same holds for geometric tasks. Additionally, we\nshow that the relationship between semantic and geometric tasks is asymmetric\nand their interaction becomes weaker as we move towards higher-level\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1\">NareshKumar Gurulingan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Multi-Object Detection and Tracking with Camera-LiDAR Fusion for Autonomous Driving. (arXiv:2108.04602v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04602","description":"<p>Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results\nof object detection, affinity computation and data association in real time.\nThis paper presents an efficient multi-modal MOT framework with online joint\ndetection and tracking schemes and robust data association for autonomous\ndriving applications. The novelty of this work includes: (1) development of an\nend-to-end deep neural network for joint object detection and correlation using\n2D and 3D measurements; (2) development of a robust affinity computation module\nto compute occlusion-aware appearance and motion affinities in 3D space; (3)\ndevelopment of a comprehensive data association module for joint optimization\namong detection confidences, affinities and start-end probabilities. The\nexperiment results on the KITTI tracking benchmark demonstrate the superior\nperformance of the proposed method in terms of both tracking accuracy and\nprocessing speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kemiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Q/0/1/0/all/0/1\">Qi Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition. (arXiv:2108.04603v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04603","description":"<p>This paper proposes a novel model for recognizing images with composite\nattribute-object concepts, notably for composite concepts that are unseen\nduring model training. We aim to explore the three key properties required by\nthe task --- relation-aware, consistent, and decoupled --- to learn rich and\nrobust features for primitive concepts that compose attribute-object pairs. To\nthis end, we propose the Blocked Message Passing Network (BMP-Net). The model\nconsists of two modules. The concept module generates semantically meaningful\nfeatures for primitive concepts, whereas the visual module extracts visual\nfeatures for attributes and objects from input images. A message passing\nmechanism is used in the concept module to capture the relations between\nprimitive concepts. Furthermore, to prevent the model from being biased towards\nseen composite concepts and reduce the entanglement between attributes and\nobjects, we propose a blocking mechanism that equalizes the information\navailable to the model for both seen and unseen concepts. Extensive experiments\nand ablation studies on two benchmarks show the efficacy of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White blood cell subtype detection and classification. (arXiv:2108.04614v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04614","description":"<p>Machine learning has endless applications in the health care industry. White\nblood cell classification is one of the interesting and promising area of\nresearch. The classification of the white blood cells plays an important part\nin the medical diagnosis. In practise white blood cell classification is\nperformed by the haematologist by taking a small smear of blood and careful\nexamination under the microscope. The current procedures to identify the white\nblood cell subtype is more time taking and error-prone. The computer aided\ndetection and diagnosis of the white blood cells tend to avoid the human error\nand reduce the time taken to classify the white blood cells. In the recent\nyears several deep learning approaches have been developed in the context of\nclassification of the white blood cells that are able to identify but are\nunable to localize the positions of white blood cells in the blood cell image.\nFollowing this, the present research proposes to utilize YOLOv3 object\ndetection technique to localize and classify the white blood cells with\nbounding boxes. With exhaustive experimental analysis, the proposed work is\nfound to detect the white blood cell with 99.2% accuracy and classify with 90%\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Praveen_N/0/1/0/all/0/1\">Nalla Praveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Canonical 3D Object Representation for Fine-Grained Recognition. (arXiv:2108.04628v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04628","description":"<p>We propose a novel framework for fine-grained object recognition that learns\nto recover object variation in 3D space from a single image, trained on an\nimage collection without using any ground-truth 3D annotation. We accomplish\nthis by representing an object as a composition of 3D shape and its appearance,\nwhile eliminating the effect of camera viewpoint, in a canonical configuration.\nUnlike conventional methods modeling spatial variation in 2D images only, our\nmethod is capable of reconfiguring the appearance feature in a canonical 3D\nspace, thus enabling the subsequent object classifier to be invariant under 3D\ngeometric variation. Our representation also allows us to go beyond existing\nmethods, by incorporating 3D shape variation as an additional cue for object\nrecognition. To learn the model without ground-truth 3D annotation, we deploy a\ndifferentiable renderer in an analysis-by-synthesis framework. By incorporating\n3D shape and appearance jointly in a deep representation, our method learns the\ndiscriminative representation of the object and achieves competitive\nperformance on fine-grained image recognition and vehicle re-identification. We\nalso demonstrate that the performance of 3D shape reconstruction is improved by\nlearning fine-grained shape deformation in a boosting manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joung_S/0/1/0/all/0/1\">Sunghun Joung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">Ig-Jae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoodLogoDet-1500: A Dataset for Large-Scale Food Logo Detection via Multi-Scale Feature Decoupling Network. (arXiv:2108.04644v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04644","description":"<p>Food logo detection plays an important role in the multimedia for its wide\nreal-world applications, such as food recommendation of the self-service shop\nand infringement detection on e-commerce platforms. A large-scale food logo\ndataset is urgently needed for developing advanced food logo detection\nalgorithms. However, there are no available food logo datasets with food brand\ninformation. To support efforts towards food logo detection, we introduce the\ndataset FoodLogoDet-1500, a new large-scale publicly available food logo\ndataset, which has 1,500 categories, about 100,000 images and about 150,000\nmanually annotated food logo objects. We describe the collection and annotation\nprocess of FoodLogoDet-1500, analyze its scale and diversity, and compare it\nwith other logo datasets. To the best of our knowledge, FoodLogoDet-1500 is the\nfirst largest publicly available high-quality dataset for food logo detection.\nThe challenge of food logo detection lies in the large-scale categories and\nsimilarities between food logo categories. For that, we propose a novel food\nlogo detection method Multi-scale Feature Decoupling Network (MFDNet), which\ndecouples classification and regression into two branches and focuses on the\nclassification branch to solve the problem of distinguishing multiple food logo\ncategories. Specifically, we introduce the feature offset module, which\nutilizes the deformation-learning for optimal classification offset and can\neffectively obtain the most representative features of classification in\ndetection. In addition, we adopt a balanced feature pyramid in MFDNet, which\npays attention to global information, balances the multi-scale feature maps,\nand enhances feature extraction capability. Comprehensive experiments on\nFoodLogoDet-1500 and other two benchmark logo datasets demonstrate the\neffectiveness of the proposed method. The FoodLogoDet-1500 can be found at this\nhttps URL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1\">Weiqing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Sujuan Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations. (arXiv:2108.04658v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04658","description":"<p>Development of deep learning systems for biomedical segmentation often\nrequires access to expert-driven, manually annotated datasets. If more than a\nsingle expert is involved in the annotation of the same images, then the\ninter-expert agreement is not necessarily perfect, and no single expert\nannotation can precisely capture the so-called ground truth of the regions of\ninterest on all images. Also, it is not trivial to generate a reference\nestimate using annotations from multiple experts. Here we present a deep neural\nnetwork, defined as U-Net-and-a-half, which can simultaneously learn from\nannotations performed by multiple experts on the same set of images.\nU-Net-and-a-half contains a convolutional encoder to generate features from the\ninput images, multiple decoders that allow simultaneous learning from image\nmasks obtained from annotations that were independently generated by multiple\nexperts, and a shared low-dimensional feature space. To demonstrate the\napplicability of our framework, we used two distinct datasets from digital\npathology and radiology, respectively. Specifically, we trained two separate\nmodels using pathologist-driven annotations of glomeruli on whole slide images\nof human kidney biopsies (10 patients), and radiologist-driven annotations of\nlumen cross-sections of human arteriovenous fistulae obtained from\nintravascular ultrasound images (10 patients), respectively. The models based\non U-Net-and-a-half exceeded the performance of the traditional U-Net models\ntrained on single expert annotations alone, thus expanding the scope of\nmultitask learning in the context of biomedical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1\">Jesper Kers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1\">Clarissa A. Cassol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1\">Joris J. Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1\">Najia Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1\">Alik Farber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1\">Samir Haroon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Kevin P. Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Suvranu Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1\">Vipul C. Chitalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1\">Vijaya B. Kolachalama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Breast Cancer Classification: Enhanced Tangent Function. (arXiv:2108.04663v1 [eess.IV])","link":"http://arxiv.org/abs/2108.04663","description":"<p>Background and Aim: Recently, deep learning using convolutional neural\nnetwork has been used successfully to classify the images of breast cells\naccurately. However, the accuracy of manual classification of those\nhistopathological images is comparatively low. This research aims to increase\nthe accuracy of the classification of breast cancer images by utilizing a\nPatch-Based Classifier (PBC) along with deep learning architecture.\nMethodology: The proposed system consists of a Deep Convolutional Neural\nNetwork (DCNN) that helps in enhancing and increasing the accuracy of the\nclassification process. This is done by the use of the Patch-based Classifier\n(PBC). CNN has completely different layers where images are first fed through\nconvolutional layers using hyperbolic tangent function together with the\nmax-pooling layer, drop out layers, and SoftMax function for classification.\nFurther, the output obtained is fed to a patch-based classifier that consists\nof patch-wise classification output followed by majority voting. Results: The\nresults are obtained throughout the classification stage for breast cancer\nimages that are collected from breast-histology datasets. The proposed solution\nimproves the accuracy of classification whether or not the images had normal,\nbenign, in-situ, or invasive carcinoma from 87% to 94% with a decrease in\nprocessing time from 0.45 s to 0.2s on average. Conclusion: The proposed\nsolution focused on increasing the accuracy of classifying cancer in the breast\nby enhancing the image contrast and reducing the vanishing gradient. Finally,\nthis solution for the implementation of the Contrast Limited Adaptive Histogram\nEqualization (CLAHE) technique and modified tangent function helps in\nincreasing the accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thapa_A/0/1/0/all/0/1\">Ashu Thapa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alsadoon_A/0/1/0/all/0/1\">Abeer Alsadoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1\">P.W.C. Prasad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajaj_S/0/1/0/all/0/1\">Simi Bajaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alsadoon_O/0/1/0/all/0/1\">Omar Hisham Alsadoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_R/0/1/0/all/0/1\">Rasha S. Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jerew_O/0/1/0/all/0/1\">Oday D. Jerew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Camera Trajectory Forecasting with Trajectory Tensors. (arXiv:2108.04694v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04694","description":"<p>We introduce the problem of multi-camera trajectory forecasting (MCTF), which\ninvolves predicting the trajectory of a moving object across a network of\ncameras. While multi-camera setups are widespread for applications such as\nsurveillance and traffic monitoring, existing trajectory forecasting methods\ntypically focus on single-camera trajectory forecasting (SCTF), limiting their\nuse for such applications. Furthermore, using a single camera limits the\nfield-of-view available, making long-term trajectory forecasting impossible. We\naddress these shortcomings of SCTF by developing an MCTF framework that\nsimultaneously uses all estimated relative object locations from several\nviewpoints and predicts the object's future location in all possible\nviewpoints. Our framework follows a Which-When-Where approach that predicts in\nwhich camera(s) the objects appear and when and where within the camera views\nthey appear. To this end, we propose the concept of trajectory tensors: a new\ntechnique to encode trajectories across multiple camera views and the\nassociated uncertainties. We develop several encoder-decoder MCTF models for\ntrajectory tensors and present extensive experiments on our own database\n(comprising 600 hours of video data from 15 camera views) created particularly\nfor the MCTF task. Results show that our trajectory tensor models outperform\ncoordinate trajectory-based MCTF models and existing SCTF methods adapted for\nMCTF. Code is available from: https://github.com/olly-styles/Trajectory-Tensors\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Styles_O/0/1/0/all/0/1\">Olly Styles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_T/0/1/0/all/0/1\">Tanaya Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIDCD - Bosch Industrial Depth Completion Dataset. (arXiv:2108.04706v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04706","description":"<p>We introduce BIDCD - the Bosch Industrial Depth Completion Dataset. BIDCD is\na new RGBD dataset of metallic industrial objects, collected with a depth\ncamera mounted on a robotic manipulator. The main purpose of this dataset is to\nfacilitate the training of domain-specific depth completion models, to be used\nin logistics and manufacturing tasks. We trained a State-of-the-Art depth\ncompletion model on this dataset, and report the results, setting an initial\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Botach_A/0/1/0/all/0/1\">Adam Botach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_Y/0/1/0/all/0/1\">Yuri Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_Y/0/1/0/all/0/1\">Yakov Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapiro_Y/0/1/0/all/0/1\">Yoel Shapiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1\">Dotan Di Castro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds. (arXiv:2108.04728v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04728","description":"<p>Current 3D single object tracking approaches track the target based on a\nfeature comparison between the target template and the search area. However,\ndue to the common occlusion in LiDAR scans, it is non-trivial to conduct\naccurate feature comparisons on severe sparse and incomplete shapes. In this\nwork, we exploit the ground truth bounding box given in the first frame as a\nstrong cue to enhance the feature description of the target object, enabling a\nmore accurate feature comparison in a simple yet effective way. In particular,\nwe first propose the BoxCloud, an informative and robust representation, to\ndepict an object using the point-to-box relation. We further design an\nefficient box-aware feature fusion module, which leverages the aforementioned\nBoxCloud for reliable feature matching and embedding. Integrating the proposed\ngeneral components into an existing model P2B, we construct a superior\nbox-aware tracker (BAT). Experiments confirm that our proposed BAT outperforms\nthe previous state-of-the-art by a large margin on both KITTI and NuScenes\nbenchmarks, achieving a 12.8% improvement in terms of precision while running\n~20% faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaoda Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiantao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weibing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-STGCNN: A Semantics-guided Spatial-Temporal Graph Convolutional Network for Multi-class Trajectory Prediction. (arXiv:2108.04740v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04740","description":"<p>Predicting the movement trajectories of multiple classes of road users in\nreal-world scenarios is a challenging task due to the diverse trajectory\npatterns. While recent works of pedestrian trajectory prediction successfully\nmodelled the influence of surrounding neighbours based on the relative\ndistances, they are ineffective on multi-class trajectory prediction. This is\nbecause they ignore the impact of the implicit correlations between different\ntypes of road users on the trajectory to be predicted - for example, a nearby\npedestrian has a different level of influence from a nearby car. In this paper,\nwe propose to introduce class information into a graph convolutional neural\nnetwork to better predict the trajectory of an individual. We embed the class\nlabels of the surrounding objects into the label adjacency matrix (LAM), which\nis combined with the velocity-based adjacency matrix (VAM) comprised of the\nobjects' velocity, thereby generating a semantics-guided graph adjacency (SAM).\nSAM effectively models semantic information with trainable parameters to\nautomatically learn the embedded label features that will contribute to the\nfixed velocity-based trajectory. Such information of spatial and temporal\ndependencies is passed to a graph convolutional and temporal convolutional\nnetwork to estimate the predicted trajectory distributions. We further propose\nnew metrics, known as Average2 Displacement Error (aADE) and Average Final\nDisplacement Error (aFDE), that assess network accuracy more accurately. We\ncall our framework Semantics-STGCNN. It consistently shows superior performance\nto the state-of-the-arts in existing and the newly proposed metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rainbow_B/0/1/0/all/0/1\">Ben A. Rainbow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_Q/0/1/0/all/0/1\">Qianhui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Hubert P. H. Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUNet: Symmetric Undistortion Network for Rolling Shutter Correction. (arXiv:2108.04775v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04775","description":"<p>The vast majority of modern consumer-grade cameras employ a rolling shutter\nmechanism, leading to image distortions if the camera moves during image\nacquisition. In this paper, we present a novel deep network to solve the\ngeneric rolling shutter correction problem with two consecutive frames. Our\npipeline is symmetrically designed to predict the global shutter image\ncorresponding to the intermediate time of these two frames, which is difficult\nfor existing methods because it corresponds to a camera pose that differs most\nfrom the two frames. First, two time-symmetric dense undistortion flows are\nestimated by using well-established principles: pyramidal construction,\nwarping, and cost volume processing. Then, both rolling shutter images are\nwarped into a common global shutter one in the feature space, respectively.\nFinally, a symmetric consistency constraint is constructed in the image decoder\nto effectively aggregate the contextual cues of two rolling shutter images,\nthereby recovering the high-quality global shutter image. Extensive experiments\nwith both synthetic and real data from public benchmarks demonstrate the\nsuperiority of our proposed approach over the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Bin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingyi He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-repository of screening mammography classifiers. (arXiv:2108.04800v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04800","description":"<p>Artificial intelligence (AI) is transforming medicine and showing promise in\nimproving clinical diagnosis. In breast cancer screening, several recent\nstudies show that AI has the potential to improve radiologists' accuracy,\nsubsequently helping in early cancer diagnosis and reducing unnecessary workup.\nAs the number of proposed models and their complexity grows, it is becoming\nincreasingly difficult to re-implement them in order to reproduce the results\nand to compare different approaches. To enable reproducibility of research in\nthis application area and to enable comparison between different methods, we\nrelease a meta-repository containing deep learning models for classification of\nscreening mammograms. This meta-repository creates a framework that enables the\nevaluation of machine learning models on any private or public screening\nmammography data set. At its inception, our meta-repository contains five\nstate-of-the-art models with open-source implementations and cross-platform\ncompatibility. We compare their performance on five international data sets:\ntwo private New York University breast cancer screening data sets as well as\nthree public (DDSM, INbreast and Chinese Mammography Database) data sets. Our\nframework has a flexible design that can be generalized to other medical image\nanalysis tasks. The meta-repository is available at\nhttps://www.github.com/nyukat/mammography_metarepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1\">Benjamin Stadnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1\">Jan Witowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1\">Vishwaesh Rajiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes. (arXiv:2108.04814v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04814","description":"<p>While self-supervised monocular depth estimation in driving scenarios has\nachieved comparable performance to supervised approaches, violations of the\nstatic world assumption can still lead to erroneous depth predictions of\ntraffic participants, posing a potential safety issue. In this paper, we\npresent R4Dyn, a novel set of techniques to use cost-efficient radar data on\ntop of a self-supervised depth estimation framework. In particular, we show how\nradar can be used during training as weak supervision signal, as well as an\nextra input to enhance the estimation robustness at inference time. Since\nautomotive radars are readily available, this allows to collect training data\nfrom a variety of existing vehicles. Moreover, by filtering and expanding the\nsignal to make it compatible with learning-based approaches, we address radar\ninherent issues, such as noise and sparsity. With R4Dyn we are able to overcome\na major limitation of self-supervised depth estimation, i.e. the prediction of\ntraffic participants. We substantially improve the estimation on dynamic\nobjects, such as cars by 37% on the challenging nuScenes dataset, hence\ndemonstrating that radar is a valuable additional sensor for monocular depth\nestimation in autonomous vehicles. Additionally, we plan on making the code\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1\">Stefano Gasperini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Patrick Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1\">Vinzenz Dallabetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of the Loss on Generalization: Empirical Study on Synthetic Lung Nodule Data. (arXiv:2108.04815v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04815","description":"<p>Convolutional Neural Networks (CNNs) are widely used for image classification\nin a variety of fields, including medical imaging. While most studies deploy\ncross-entropy as the loss function in such tasks, a growing number of\napproaches have turned to a family of contrastive learning-based losses. Even\nthough performance metrics such as accuracy, sensitivity and specificity are\nregularly used for the evaluation of CNN classifiers, the features that these\nclassifiers actually learn are rarely identified and their effect on the\nclassification performance on out-of-distribution test samples is\ninsufficiently explored. In this paper, motivated by the real-world task of\nlung nodule classification, we investigate the features that a CNN learns when\ntrained and tested on different distributions of a synthetic dataset with\ncontrolled modes of variation. We show that different loss functions lead to\ndifferent features being learned and consequently affect the generalization\nability of the classifier on unseen data. This study provides some important\ninsights into the design of deep learning solutions for medical imaging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1\">Vasileios Baltatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1\">Loic Le Folgoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1\">Sam Ellis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1\">Octavio E. Martinez Manzanera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1\">Kyriaki-Margarita Bintsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Arjun Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Sujal Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1\">Julia A. Schnabel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Behavior Analysis using 4D Curvature Statistics for Presentation Attack Detection. (arXiv:1910.06056v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1910.06056","description":"<p>The human face has a high potential for biometric identification due to its\nmany individual traits. At the same time, such identification is vulnerable to\nbiometric copies. These presentation attacks pose a great challenge in\nunsupervised authentication settings. As a countermeasure, we propose a method\nthat automatically analyzes the plausibility of facial behavior based on a\nsequence of 3D face scans. A compact feature representation measures facial\nbehavior using the temporal curvature change. Finally, we train our method only\non genuine faces in an anomaly detection scenario. Our method can detect\npresentation attacks using elastic 3D masks, bent photographs with eye holes,\nand monitor replay-attacks. For evaluation, we recorded a challenging database\ncontaining such cases using a high-quality 3D sensor. It features 109 4D face\nscans including eleven different types of presentation attacks. We achieve\nerror rates of 11% and 6% for APCER and BPCER, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thummel_M/0/1/0/all/0/1\">Martin Th&#xfc;mmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sickert_S/0/1/0/all/0/1\">Sven Sickert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments. (arXiv:1910.14442v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1910.14442","description":"<p>We present Interactive Gibson Benchmark, the first comprehensive benchmark\nfor training and evaluating Interactive Navigation: robot navigation strategies\nwhere physical interaction with objects is allowed and even encouraged to\naccomplish a task. For example, the robot can move objects if needed in order\nto clear a path leading to the goal location. Our benchmark comprises two novel\nelements: 1) a new experimental setup, the Interactive Gibson Environment\n(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high\nfidelity physical dynamics of the robot and common objects found in these\nscenes; 2) a set of Interactive Navigation metrics which allows one to study\nthe interplay between navigation and physical interaction. We present and\nevaluate multiple learning-based baselines in Interactive Gibson, and provide\ninsights into regimes of navigation with different trade-offs between\nnavigation path efficiency and disturbance of surrounding objects. We make our\nbenchmark publicly\navailable(https://sites.google.com/view/interactivegibsonenv) and encourage\nresearchers from all disciplines in robotics (e.g. planning, learning, control)\nto propose, evaluate, and compare their Interactive Navigation solutions in\nInteractive Gibson.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">William B. Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1\">Priya Kasimbeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification. (arXiv:2002.04264v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.04264","description":"<p>Key for solving fine-grained image categorization is finding discriminate and\nlocal regions that correspond to subtle visual traits. Great strides have been\nmade, with complex networks designed specifically to learn part-level\ndiscriminate feature representations. In this paper, we show it is possible to\ncultivate subtle details without the need for overly complicated network\ndesigns or training mechanisms -- a single loss is all it takes. The main trick\nlies with how we delve into individual feature channels early on, as opposed to\nthe convention of starting from a consolidated feature map. The proposed loss\nfunction, termed as mutual-channel loss (MC-Loss), consists of two\nchannel-specific components: a discriminality component and a diversity\ncomponent. The discriminality component forces all feature channels belonging\nto the same class to be discriminative, through a novel channel-wise attention\nmechanism. The diversity component additionally constraints channels so that\nthey become mutually exclusive on spatial-wise. The end result is therefore a\nset of feature channels that each reflects different locally discriminative\nregions for a specific class. The MC-Loss can be trained end-to-end, without\nthe need for any bounding-box/part annotations, and yields highly\ndiscriminative regions during inference. Experimental results show our MC-Loss\nwhen implemented on top of common base networks can achieve state-of-the-art\nperformance on all four fine-grained categorization datasets (CUB-Birds,\nFGVC-Aircraft, Flowers-102, and Stanford-Cars). Ablative studies further\ndemonstrate the superiority of MC-Loss when compared with other recently\nproposed general-purpose losses for visual classification, on two different\nbase networks. Code available at\nhttps://github.com/dongliangchang/Mutual-Channel-Loss\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Sparse Adversarial Attack on Sequence-based Gait Recognition. (arXiv:2002.09674v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.09674","description":"<p>Gait recognition is widely used in social security applications due to its\nadvantages in long-distance human identification. Recently, sequence-based\nmethods have achieved high accuracy by learning abundant temporal and spatial\ninformation. However, their robustness under adversarial attacks has not been\nclearly explored. In this paper, we demonstrate that the state-of-the-art gait\nrecognition model is vulnerable to such attacks. To this end, we propose a\nnovel temporal sparse adversarial attack method. Different from previous\nadditive noise models which add perturbations on original samples, we employ a\ngenerative adversarial network based architecture to semantically generate\nadversarial high-quality gait silhouettes or video frames. Moreover, by\nsparsely substituting or inserting a few adversarial gait silhouettes, the\nproposed method ensures its imperceptibility and achieves a high attack success\nrate. The experimental results show that if only one-fortieth of the frames are\nattacked, the accuracy of the target model drops dramatically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Ziwen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Solution to Product detection in Densely Packed Scenes. (arXiv:2007.11946v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.11946","description":"<p>This work is a solution to densely packed scenes dataset SKU-110k. Our work\nis modified from Cascade R-CNN. To solve the problem, we proposed a random crop\nstrategy to ensure both the sampling rate and input scale is relatively\nsufficient as a contrast to the regular random crop. And we adopted some of\ntrick and optimized the hyper-parameters. To grasp the essential feature of the\ndensely packed scenes, we analysis the stages of a detector and investigate the\nbottleneck which limits the performance. As a result, our method obtains 58.7\nmAP on test set of SKU-110k.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1\">Tianze Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanjia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongxiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yichao Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation. (arXiv:2008.07588v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.07588","description":"<p>Deep learning motivated by convolutional neural networks has been highly\nsuccessful in a range of medical imaging problems like image classification,\nimage segmentation, image synthesis etc. However for validation and\ninterpretability, not only do we need the predictions made by the model but\nalso how confident it is while making those predictions. This is important in\nsafety critical applications for the people to accept it. In this work, we used\nan encoder decoder architecture based on variational inference techniques for\nsegmenting brain tumour images. We evaluate our work on the publicly available\nBRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over\nUnion (IOU) as the evaluation metrics. Our model is able to segment brain\ntumours while taking into account both aleatoric uncertainty and epistemic\nuncertainty in a principled bayesian manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Search on Binary Codes by Weighted Hamming Distance. (arXiv:2009.08591v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.08591","description":"<p>Weighted Hamming distance, as a similarity measure between binary codes and\nbinary queries, provides superior accuracy in search tasks than Hamming\ndistance. However, how to efficiently and accurately find $K$ binary codes that\nhave the smallest weighted Hamming distance to the query remains an open issue.\nIn this paper, a fast search algorithm is proposed to perform the\nnon-exhaustive search for $K$ nearest binary codes by weighted Hamming\ndistance. By using binary codes as direct bucket indices in a hash table, the\nsearch algorithm generates a sequence to probe the buckets based on the\nindependence characteristic of the weights for each bit. Furthermore, a fast\nsearch framework based on the proposed search algorithm is designed to solve\nthe problem of long binary codes. Specifically, long binary codes are split\ninto substrings and multiple hash tables are built on them. Then, the search\nalgorithm probes the buckets to obtain candidates according to the generated\nsubstring indices, and a merging algorithm is proposed to find the nearest\nbinary codes by merging the candidates. Theoretical analysis and experimental\nresults demonstrate that the search algorithm improves the search accuracy\ncompared to other non-exhaustive algorithms and provides orders-of-magnitude\nfaster search than the linear scan baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1\">Zhenyu Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuesheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruixin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.10850","description":"<p>Data hiding is the procedure of encoding desired information into an image to\nresist potential noises while ensuring the embedded image has little perceptual\nperturbations from the original image. Recently, with the tremendous successes\ngained by deep neural networks in various fields, data hiding areas have\nattracted increasing number of attentions. The neglect of considering the pixel\nsensitivity within the cover image of deep neural methods will inevitably\naffect the model robustness for information hiding. Targeting at the problem,\nin this paper, we propose a novel deep data hiding scheme with Inverse Gradient\nAttention (IGA), combing the ideas of adversarial learning and attention\nmechanism to endow different sensitivity to different pixels. With the proposed\ncomponent, the model can spotlight pixels with more robustness for embedding\ndata. Empirically, extensive experiments show that the proposed model\noutperforms the state-of-the-art methods on two prevalent datasets under\nmultiple settings. Besides, we further identify and discuss the connections\nbetween the proposed inverse gradient attention and high-frequency regions\nwithin images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuanzhouhan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Camera Convolutional Color Constancy. (arXiv:2011.11890v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11890","description":"<p>We present \"Cross-Camera Convolutional Color Constancy\" (C5), a\nlearning-based method, trained on images from multiple cameras, that accurately\nestimates a scene's illuminant color from raw images captured by a new camera\npreviously unseen during training. C5 is a hypernetwork-like extension of the\nconvolutional color constancy (CCC) approach: C5 learns to generate the weights\nof a CCC model that is then evaluated on the input image, with the CCC weights\ndynamically adapted to different input content. Unlike prior cross-camera color\nconstancy models, which are usually designed to be agnostic to the spectral\nproperties of test-set images from unobserved cameras, C5 approaches this\nproblem through the lens of transductive inference: additional unlabeled images\nare provided as input to the model at test time, which allows the model to\ncalibrate itself to the spectral properties of the test-set camera during\ninference. C5 achieves state-of-the-art accuracy for cross-camera color\nconstancy on several datasets, is fast to evaluate (~7 and ~90 ms per image on\na GPU or CPU, respectively), and requires little memory (~2 MB), and, thus, is\na practical solution to the problem of calibration-free automatic white balance\nfor mobile photography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGendre_C/0/1/0/all/0/1\">Chloe LeGendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yun-Ta Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleibel_F/0/1/0/all/0/1\">Francois Bleibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good and Bad Boundaries in Ultrasound Compounding: Preserving Anatomic Boundaries While Suppressing Artifacts. (arXiv:2011.11962v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2011.11962","description":"<p>Ultrasound 3D compounding is important for volumetric reconstruction, but as\nof yet there is no consensus on best practices for compounding. Ultrasound\nimages depend on probe direction and the path sound waves pass through, so when\nmultiple intersecting B-scans of the same spot from different perspectives\nyield different pixel values, there is not a single, ideal representation for\ncompounding (i.e. combining) the overlapping pixel values. Current popular\nmethods inevitably suppress or altogether leave out bright or dark regions that\nare useful, and potentially introduce new artifacts. In this work, we establish\na new algorithm to compound the overlapping pixels from different view points\nin ultrasound. We uniquely leverage Laplacian and Gaussian Pyramids to preserve\nthe maximum boundary contrast without overemphasizing noise and speckle. We\nevaluate our algorithm by comparing ours with previous algorithms, and we show\nthat our approach not only preserves both light and dark details, but also\nsomewhat suppresses artifacts, rather than amplifying them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1\">Alex Ling Yu Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1\">John Galeotti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iGibson 1.0: a Simulation Environment for Interactive Tasks in Large Realistic Scenes. (arXiv:2012.02924v6 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2012.02924","description":"<p>We present iGibson 1.0, a novel simulation environment to develop robotic\nsolutions for interactive tasks in large-scale realistic scenes. Our\nenvironment contains 15 fully interactive home-sized scenes with 108 rooms\npopulated with rigid and articulated objects. The scenes are replicas of\nreal-world homes, with distribution and the layout of objects aligned to those\nof the real world. iGibson 1.0 integrates several key features to facilitate\nthe study of interactive tasks: i) generation of high-quality virtual sensor\nsignals (RGB, depth, segmentation, LiDAR, flow and so on), ii) domain\nrandomization to change the materials of the objects (both visual and physical)\nand/or their shapes, iii) integrated sampling-based motion planners to generate\ncollision-free trajectories for robot bases and arms, and iv) intuitive\nhuman-iGibson interface that enables efficient collection of human\ndemonstrations. Through experiments, we show that the full interactivity of the\nscenes enables agents to learn useful visual representations that accelerate\nthe training of downstream manipulation tasks. We also show that iGibson 1.0\nfeatures enable the generalization of navigation agents, and that the\nhuman-iGibson interface and integrated motion planners facilitate efficient\nimitation learning of human demonstrated (mobile) manipulation behaviors.\niGibson 1.0 is open-source, equipped with comprehensive examples and\ndocumentation. For more information, visit our project website:\n<a href=\"http://svl.stanford.edu/igibson/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_DArpino_C/0/1/0/all/0/1\">Claudia P&#xe9;rez-D&#x27;Arpino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_L/0/1/0/all/0/1\">Lyne P. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael E. Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Josiah Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INeRF: Inverting Neural Radiance Fields for Pose Estimation. (arXiv:2012.05877v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05877","description":"<p>We present iNeRF, a framework that performs mesh-free pose estimation by\n\"inverting\" a Neural RadianceField (NeRF). NeRFs have been shown to be\nremarkably effective for the task of view synthesis - synthesizing\nphotorealistic novel views of real-world scenes or objects. In this work, we\ninvestigate whether we can apply analysis-by-synthesis via NeRF for mesh-free,\nRGB-only 6DoF pose estimation - given an image, find the translation and\nrotation of a camera relative to a 3D object or scene. Our method assumes that\nno object mesh models are available during either training or test time.\nStarting from an initial pose estimate, we use gradient descent to minimize the\nresidual between pixels rendered from a NeRF and pixels in an observed image.\nIn our experiments, we first study 1) how to sample rays during pose refinement\nfor iNeRF to collect informative gradients and 2) how different batch sizes of\nrays affect iNeRF on a synthetic dataset. We then show that for complex\nreal-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating\nthe camera poses of novel images and using these images as additional training\ndata for NeRF. Finally, we show iNeRF can perform category-level object pose\nestimation, including object instances not seen during training, with RGB\nimages by inverting a NeRF model inferred from a single view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiniVLM: A Smaller and Faster Vision-Language Model. (arXiv:2012.06946v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.06946","description":"<p>Recent vision-language (VL) studies have shown remarkable progress by\nlearning generic representations from massive image-text pairs with transformer\nmodels and then fine-tuning on downstream VL tasks. While existing research has\nbeen focused on achieving high accuracy with large pre-trained models, building\na lightweight model is of great value in practice but is less explored. In this\npaper, we propose a smaller and faster VL model, MiniVLM, which can be\nfinetuned with good performance on various downstream tasks like its larger\ncounterpart. MiniVLM consists of two modules, a vision feature extractor and a\ntransformer-based vision-language fusion module. We design a Two-stage\nEfficient feature Extractor (TEE), inspired by the one-stage EfficientDet\nnetwork, to significantly reduce the time cost of visual feature extraction by\n$95\\%$, compared to a baseline model. We adopt the MiniLM structure to reduce\nthe computation cost of the transformer module after comparing different\ncompact BERT models. In addition, we improve the MiniVLM pre-training by adding\n$7M$ Open Images data, which are pseudo-labeled by a state-of-the-art\ncaptioning model. We also pre-train with high-quality image tags obtained from\na strong tagging model to enhance cross-modality alignment. The large models\nare used offline without adding any overhead in fine-tuning and inference. With\nthe above design choices, our MiniVLM reduces the model size by $73\\%$ and the\ninference time cost by $94\\%$ while being able to retain $94-97\\%$ of the\naccuracy on multiple VL tasks. We hope that MiniVLM helps ease the use of the\nstate-of-the-art VL research for on-the-edge applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Enhanced Prohibited Items Recognition Model. (arXiv:2102.12256v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.12256","description":"<p>We proposed a new modeling method to promote the performance of prohibited\nitems recognition via X-ray image. We analyzed the characteristics of\nprohibited items and X-ray images. We found the fact that the scales of some\nitems are too small to be recognized which encumber the model performance. Then\nwe adopted a set of data augmentation and modified the model to adapt the field\nof prohibited items recognition. The Convolutional Block Attention Module(CBAM)\nand rescoring mechanism has been assembled into the model. By the modification,\nour model achieved a mAP of 89.9% on SIXray10, mAP of 74.8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rong_T/0/1/0/all/0/1\">Tianze Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongxiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yichao Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network. (arXiv:2103.00793v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00793","description":"<p>In real applications, different computation-resource devices need\ndifferent-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,\nexisting methods either design multiple networks and train them independently,\nor construct depth-level/width-level dynamic neural networks which is hard to\nprove the accuracy of each sub-net. In this article, we propose an elegant\nDepth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets\nof similar architectures. To improve the generalization of sub-nets, we design\nthe Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to\nimplement knowledge transfer from the teacher (full-net) to multiple students\n(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to\nconstrain the posterior class probability consistency between full-net and\nsub-nets, and self-attention distillation on the same resolution feature of\ndifferent depth is addressed to drive more abundant feature representations of\nsub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in\na DDNN via the online knowledge distillation in each training iteration without\nextra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet\ndatasets demonstrate that sub-nets in DDNN with EKD training achieve better\nperformance than individually training networks while preserving the original\nperformance of full-nets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Ting-Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05568","description":"<p>Multimodal IR, spanning text corpus, knowledge graph and images, called\noutside knowledge visual question answering (OKVQA), is of much recent\ninterest. However, the popular data set has serious limitations. A surprisingly\nlarge fraction of queries do not assess the ability to integrate cross-modal\ninformation. Instead, some are independent of the image, some depend on\nspeculation, some require OCR or are otherwise answerable from the image alone.\nTo add to the above limitations, frequency-based guessing is very effective\nbecause of (unintended) widespread answer overlaps between the train and test\nfolds. Overall, it is hard to determine when state-of-the-art systems exploit\nthese weaknesses rather than really infer the answers, because they are opaque\nand their 'reasoning' process is uninterpretable. An equally important\nlimitation is that the dataset is designed for the quantitative assessment only\nof the end-to-end answer retrieval task, with no provision for assessing the\ncorrect(semantic) interpretation of the input query. In response, we identify a\nkey structural idiom in OKVQA ,viz., S3 (select, substitute and search), and\nbuild a new data set and challenge around it. Specifically, the questioner\nidentifies an entity in the image and asks a question involving that entity\nwhich can be answered only by consulting a knowledge graph or corpus passage\nmentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA\nannotated based on the structural idiom and (ii)S3VQA, a new dataset built from\nscratch. We also present a neural but structurally transparent OKVQA system,\nS3, that explicitly addresses our challenge dataset, and outperforms recent\ncompetitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aman Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1\">Mayank Kothyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.07770","description":"<p>Video quality assessment (VQA) is now a fastgrowing subject, beginning to\nmature in the full reference (FR) case, while the burgeoning no reference (NR)\ncase remains challenging. We investigate variants of the popular VMAF video\nquality assessment algorithm for the FR case, using support vector regression\nand feedforward neural networks, and extend it to the NR case, using the same\nlearning architectures, to develop a partially unified framework for VQA. When\nheavily trained, algorithms such as VMAF perform well on test datasets, with\n90%+ match; but predicting performance in the wild is better done by\ntraining/testing from scratch, as we do. Even from scratch, we achieve 90%+\nperformance in FR, with gains over VMAF. And we greatly reduce complexity vs.\nleading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our\npreliminary testing, we find the improvements in trainability, while also\nconstraining computational complexity, as quite encouraging, suggesting further\nstudy and analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2103.13689","description":"<p>Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RetrievalFuse: Neural 3D Scene Reconstruction with a Database. (arXiv:2104.00024v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00024","description":"<p>3D reconstruction of large scenes is a challenging problem due to the\nhigh-complexity nature of the solution space, in particular for generative\nneural networks. In contrast to traditional generative learned models which\nencode the full generative process into a neural network and can struggle with\nmaintaining local details at the scene level, we introduce a new method that\ndirectly leverages scene geometry from the training database. First, we learn\nto synthesize an initial estimate for a 3D scene, constructed by retrieving a\ntop-k set of volumetric chunks from the scene database. These candidates are\nthen refined to a final scene generation with an attention-based refinement\nthat can effectively select the most consistent set of geometry from the\ncandidates and combine them together to create an output scene, facilitating\ntransfer of coherent structures and local detail from train scene geometry. We\ndemonstrate our neural scene reconstruction with a database for the tasks of 3D\nsuper resolution and surface reconstruction from sparse point clouds, showing\nthat our approach enables generation of more coherent, accurate 3D scenes,\nimproving on average by over 8% in IoU over state-of-the-art scene\nreconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_Y/0/1/0/all/0/1\">Yawar Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fangchang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Q/0/1/0/all/0/1\">Qi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (arXiv:2105.01622v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.01622","description":"<p>Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n</p>\n<p>We study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n</p>\n<p>We find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FusionPainting: Multimodal Fusion with Adaptive Attention for 3D Object Detection. (arXiv:2106.12449v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12449","description":"<p>Accurate detection of obstacles in 3D is an essential task for autonomous\ndriving and intelligent transportation. In this work, we propose a general\nmultimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D\npoint clouds at a semantic level for boosting the 3D object detection task.\nEspecially, the FusionPainting framework consists of three main modules: a\nmulti-modal semantic segmentation module, an adaptive attention-based semantic\nfusion module, and a 3D object detector. First, semantic information is\nobtained for 2D images and 3D Lidar point clouds based on 2D and 3D\nsegmentation approaches. Then the segmentation results from different sensors\nare adaptively fused based on the proposed attention-based semantic fusion\nmodule. Finally, the point clouds painted with the fused semantic label are\nsent to the 3D detector for obtaining the 3D objection results. The\neffectiveness of the proposed framework has been verified on the large-scale\nnuScenes detection benchmark by comparing it with three different baselines.\nThe experimental results show that the fusion strategy can significantly\nimprove the detection performance compared to the methods using only point\nclouds, and the methods using point clouds only painted with 2D segmentation\ninformation. Furthermore, the proposed approach outperforms other\nstate-of-the-art methods on the nuScenes testing benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shaoqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dingfu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Junbo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bin_Z/0/1/0/all/0/1\">Zhou Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning point embedding for 3D data processing. (arXiv:2107.08565v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08565","description":"<p>Among 2D convolutional networks on point clouds, point-based approaches\nconsume point clouds of fixed size directly. By analysis of PointNet, a pioneer\nin introducing deep learning into point sets, we reveal that current\npoint-based methods are essentially spatial relationship processing networks.\nIn this paper, we take a different approach. Our architecture, named PE-Net,\nlearns the representation of point clouds in high-dimensional space, and\nencodes the unordered input points to feature vectors, which standard 2D CNNs\ncan be applied to. The recommended network can adapt to changes in the number\nof input points which is the limit of current methods. Experiments show that in\nthe tasks of classification and part segmentation, PE-Net achieves the\nstate-of-the-art performance in multiple challenging datasets, such as ModelNet\nand ShapeNetPart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+li_Y/0/1/0/all/0/1\">Yuan li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11170","description":"<p>Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection. (arXiv:2107.12664v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12664","description":"<p>Arbitrary shape text detection is a challenging task due to the high\ncomplexity and variety of scene texts. In this work, we propose a novel\nadaptive boundary proposal network for arbitrary shape text detection, which\ncan learn to directly produce accurate boundary for arbitrary shape text\nwithout any post-processing. Our method mainly consists of a boundary proposal\nmodel and an innovative adaptive boundary deformation model. The boundary\nproposal model constructed by multi-layer dilated convolutions is adopted to\nproduce prior information (including classification map, distance field, and\ndirection field) and coarse boundary proposals. The adaptive boundary\ndeformation model is an encoder-decoder network, in which the encoder mainly\nconsists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network\n(RNN). It aims to perform boundary deformation in an iterative way for\nobtaining text instance shape guided by prior information from the boundary\nproposal model. In this way, our method can directly and efficiently generate\naccurate text boundaries without complex post-processing. Extensive experiments\non publicly available datasets demonstrate the state-of-the-art performance of\nour method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shi-Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UIBert: Learning Generic Multimodal Representations for UI Understanding. (arXiv:2107.13731v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13731","description":"<p>To improve the accessibility of smart devices and to simplify their usage,\nbuilding models which understand user interfaces (UIs) and assist users to\ncomplete their tasks is critical. However, unique challenges are proposed by\nUI-specific characteristics, such as how to effectively leverage multimodal UI\nfeatures that involve image, text, and structural metadata and how to achieve\ngood performance when high-quality labeled data is unavailable. To address such\nchallenges we introduce UIBert, a transformer-based joint image-text model\ntrained through novel pre-training tasks on large-scale unlabeled UI data to\nlearn generic feature representations for a UI and its components. Our key\nintuition is that the heterogeneous features in a UI are self-aligned, i.e.,\nthe image and text features of UI components, are predictive of each other. We\npropose five pretraining tasks utilizing this self-alignment among different\nfeatures of a UI component and across various components in the same UI. We\nevaluate our method on nine real-world downstream UI tasks where UIBert\noutperforms strong multimodal baselines by up to 9.26% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chongyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1\">Xiaoxue Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1\">Srinivas Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jindong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1\">Blaise Aguera y Arcas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2108.01077","description":"<p>A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any\nuser-information. We optimize these faces, by using an evolutionary algorithm\nin the latent embedding space of the StyleGAN face generator. Multiple\nevolutionary strategies are compared, and we propose a novel approach that\nemploys a neural network in order to direct the search in the direction of\npromising samples, without adding fitness evaluations. The results we present\ndemonstrate that it is possible to obtain a high coverage of the LFW identities\n(over 40%) with less than 10 master faces, for three leading deep face\nrecognition systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internal Video Inpainting by Implicit Long-range Propagation. (arXiv:2108.01912v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01912","description":"<p>We propose a novel framework for video inpainting by adopting an internal\nlearning strategy. Unlike previous methods that use optical flow for\ncross-frame context propagation to inpaint unknown regions, we show that this\ncan be achieved implicitly by fitting a convolutional neural network to known\nregions. Moreover, to handle challenging sequences with ambiguous backgrounds\nor long-term occlusion, we design two regularization terms to preserve\nhigh-frequency details and long-term temporal consistency. Extensive\nexperiments on the DAVIS dataset demonstrate that the proposed method achieves\nstate-of-the-art inpainting quality quantitatively and qualitatively. We\nfurther extend the proposed method to another challenging task: learning to\nremove an object from a video giving a single object mask in only one frame in\na 4K video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1\">Hao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02923","description":"<p>Structured text understanding on Visually Rich Documents (VRDs) is a crucial\npart of Document Intelligence. Due to the complexity of content and layout in\nVRDs, structured text understanding has been a challenging task. Most existing\nstudies decoupled this problem into two sub-tasks: entity labeling and entity\nlinking, which require an entire understanding of the context of documents at\nboth token and segment levels. However, little work has been concerned with the\nsolutions that efficiently extract the structured data from different levels.\nThis paper proposes a unified framework named StrucTexT, which is flexible and\neffective for handling both sub-tasks. Specifically, based on the transformer,\nwe introduce a segment-token aligned encoder to deal with the entity labeling\nand entity linking tasks at different levels of granularity. Moreover, we\ndesign a novel pre-training strategy with three self-supervised tasks to learn\na richer representation. StrucTexT uses the existing Masked Visual Language\nModeling task and the new Sentence Length Prediction and Paired Boxes Direction\ntasks to incorporate the multi-modal information across text, image, and\nlayout. We evaluate our method for structured text understanding at\nsegment-level and token-level and show it outperforms the state-of-the-art\ncounterparts with significantly superior performance on the FUNSD, SROIE, and\nEPHOIE datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuxi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiameng Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1\">Kun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Meta-class Memory for Few-Shot Semantic Segmentation. (arXiv:2108.02958v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02958","description":"<p>Currently, the state-of-the-art methods treat few-shot semantic segmentation\ntask as a conditional foreground-background segmentation problem, assuming each\nclass is independent. In this paper, we introduce the concept of meta-class,\nwhich is the meta information (e.g. certain middle-level features) shareable\namong all classes. To explicitly learn meta-class representations in few-shot\nsegmentation task, we propose a novel Meta-class Memory based few-shot\nsegmentation method (MM-Net), where we introduce a set of learnable memory\nembeddings to memorize the meta-class information during the base class\ntraining and transfer to novel classes during the inference stage. Moreover,\nfor the $k$-shot scenario, we propose a novel image quality measurement module\nto select images from the set of support images. A high-quality class prototype\ncould be obtained with the weighted sum of support image features based on the\nquality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that\nour proposed method is able to achieve state-of-the-art results in both 1-shot\nand 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\\% mIoU on\nthe COCO dataset in 1-shot setting, which is 5.1\\% higher than the previous\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1\">Guosheng lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Segmentation Networks to New Domains by Disentangling Latent Representations. (arXiv:2108.03021v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03021","description":"<p>Deep learning models achieve outstanding accuracy in semantic segmentation,\nhowever they require a huge amount of labeled data for their optimization.\nHence, domain adaptation approaches have come into play to transfer knowledge\nacquired on a label-abundant source domain to a related label-scarce target\ndomain. However, such models do not generalize well to data with statistical\nproperties not perfectly matching the ones of the training samples. In this\nwork, we design and carefully analyze multiple latent space-shaping\nregularization strategies that work in conjunction to reduce the domain\ndiscrepancy in semantic segmentation. In particular, we devise a feature\nclustering strategy to increase domain alignment, a feature perpendicularity\nconstraint to space apart feature belonging to different semantic classes,\nincluding those not present in the current batch, and a feature norm alignment\nstrategy to separate active and inactive channels. Additionally, we propose a\nnovel performance metric to capture the relative efficacy of an adaptation\nstrategy compared to supervised training. We verify the effectiveness of our\nframework in synthetic-to-real and real-to-real adaptation scenarios,\noutperforming previous state-of-the-art methods on multiple road scenes\nbenchmarks and using different backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1\">Francesco Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.03272","description":"<p>Recent research in embodied AI has been boosted by the use of simulation\nenvironments to develop and train robot learning approaches. However, the use\nof simulation has skewed the attention to tasks that only require what robotics\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\nan open-source simulation environment that supports the simulation of a more\ndiverse set of household tasks through three key innovations. First, iGibson\n2.0 supports object states, including temperature, wetness level, cleanliness\nlevel, and toggled and sliced states, necessary to cover a wider range of\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\nit. This functionality can generate potentially infinite instances of tasks\nwith minimal effort from the users. The sampling mechanism allows our scenes to\nbe more densely populated with small objects in semantically meaningful\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\nimmerse humans in its scenes to collect demonstrations. As a result, we can\ncollect demonstrations from humans on these new types of tasks, and use them\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\nenable robot learning of novel tasks, in the hope of demonstrating the\npotential of this new simulator to support new research in embodied AI. iGibson\n2.0 and its new dataset will be publicly available at\n<a href=\"http://svl.stanford.edu/igibson/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1\">Gokul Dharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expressive Power and Loss Surfaces of Deep Learning Models. (arXiv:2108.03579v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.03579","description":"<p>The goals of this paper are two-fold. The first goal is to serve as an\nexpository tutorial on the working of deep learning models which emphasizes\ngeometrical intuition about the reasons for success of deep learning. The\nsecond goal is to complement the current results on the expressive power of\ndeep learning models and their loss surfaces with novel insights and results.\nIn particular, we describe how deep neural networks carve out manifolds\nespecially when the multiplication neurons are introduced. Multiplication is\nused in dot products and the attention mechanism and it is employed in capsule\nnetworks and self-attention based transformers. We also describe how random\npolynomial, random matrix, spin glass and computational complexity perspectives\non the loss surfaces are interconnected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1\">Simant Dube</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Latent Semantic Graph for Video Captioning. (arXiv:2108.03662v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03662","description":"<p>Video captioning aims to automatically generate natural language sentences\nthat can describe the visual contents of a given video. Existing generative\nmodels like encoder-decoder frameworks cannot explicitly explore the\nobject-level interactions and frame-level information from complex\nspatio-temporal data to generate semantic-rich captions. Our main contribution\nis to identify three key problems in a joint framework for future video\nsummarization tasks. 1) Enhanced Object Proposal: we propose a novel\nConditional Graph that can fuse spatio-temporal information into latent object\nproposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to\ndynamically extract visual words with higher semantic levels. 3) Sentence\nValidation: A novel Discriminative Language Validator is proposed to verify\ngenerated captions so that key semantic concepts can be effectively preserved.\nOur experiments on two public datasets (MVSD and MSR-VTT) manifest significant\nimprovements over state-of-the-art approaches on all metrics, especially for\nBLEU-4 and CIDEr. Our code is available at\nhttps://github.com/baiyang4/D-LSG-Video-Caption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Bingzhang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1\">Maurice Pagnucco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning of Occlusion Aware Flow Guided 3D Geometry Perception with Adaptive Cross Weighted Loss from Monocular Videos. (arXiv:2108.03893v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03893","description":"<p>Self-supervised deep learning-based 3D scene understanding methods can\novercome the difficulty of acquiring the densely labeled ground-truth and have\nmade a lot of advances. However, occlusions and moving objects are still some\nof the major limitations. In this paper, we explore the learnable occlusion\naware optical flow guided self-supervised depth and camera pose estimation by\nan adaptive cross weighted loss to address the above limitations. Firstly, we\nexplore to train the learnable occlusion mask fused optical flow network by an\nocclusion-aware photometric loss with the temporally supplemental information\nand backward-forward consistency of adjacent views. And then, we design an\nadaptive cross-weighted loss between the depth-pose and optical flow loss of\nthe geometric and photometric error to distinguish the moving objects which\nviolate the static scene assumption. Our method shows promising results on\nKITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good\ngeneralization ability under a variety of challenging scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiaojiao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning methods for automatic evaluation of delayed enhancement-MRI. The results of the EMIDEC challenge. (arXiv:2108.04016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04016","description":"<p>A key factor for assessing the state of the heart after myocardial infarction\n(MI) is to measure whether the myocardium segment is viable after reperfusion\nor revascularization therapy. Delayed enhancement-MRI or DE-MRI, which is\nperformed several minutes after injection of the contrast agent, provides high\ncontrast between viable and nonviable myocardium and is therefore a method of\nchoice to evaluate the extent of MI. To automatically assess myocardial status,\nthe results of the EMIDEC challenge that focused on this task are presented in\nthis paper. The challenge's main objectives were twofold. First, to evaluate if\ndeep learning methods can distinguish between normal and pathological cases.\nSecond, to automatically calculate the extent of myocardial infarction. The\npublicly available database consists of 150 exams divided into 50 cases with\nnormal MRI after injection of a contrast agent and 100 cases with myocardial\ninfarction (and then with a hyperenhanced area on DE-MRI), whatever their\ninclusion in the cardiac emergency department. Along with MRI, clinical\ncharacteristics are also provided. The obtained results issued from several\nworks show that the automatic classification of an exam is a reachable task\n(the best method providing an accuracy of 0.92), and the automatic segmentation\nof the myocardium is possible. However, the segmentation of the diseased area\nneeds to be improved, mainly due to the small size of these areas and the lack\nof contrast with the surrounding structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pommier_T/0/1/0/all/0/1\">Thibaut Pommier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decourselle_T/0/1/0/all/0/1\">Thomas Decourselle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qayyum_A/0/1/0/all/0/1\">Abdul Qayyum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salomon_M/0/1/0/all/0/1\">Michel Salomon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginhac_D/0/1/0/all/0/1\">Dominique Ginhac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boucher_A/0/1/0/all/0/1\">Arnaud Boucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahim_K/0/1/0/all/0/1\">Khawla Brahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1\">Marleen de Bruijne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camarasa_R/0/1/0/all/0/1\">Robin Camarasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_T/0/1/0/all/0/1\">Teresa M. Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girum_K/0/1/0/all/0/1\">Kibrom B. Girum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennemuth_A/0/1/0/all/0/1\">Anja Hennemuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huellebrand_M/0/1/0/all/0/1\">Markus Huellebrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1\">Raabid Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivantsits_M/0/1/0/all/0/1\">Matthias Ivantsits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1\">Craig Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishabh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jixi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsekos_N/0/1/0/all/0/1\">Nikolaos V. Tsekos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varela_M/0/1/0/all/0/1\">Marta Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hannu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuncheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Raphael Couturier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meriaudeau_F/0/1/0/all/0/1\">Fabrice Meriaudeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No-Reference Image Quality Assessment by Hallucinating Pristine Features. (arXiv:2108.04165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04165","description":"<p>In this paper, we propose a no-reference (NR) image quality assessment (IQA)\nmethod via feature level pseudo-reference (PR) hallucination. The proposed\nquality assessment framework is grounded on the prior models of natural image\nstatistical behaviors and rooted in the view that the perceptually meaningful\nfeatures could be well exploited to characterize the visual quality. Herein,\nthe PR features from the distorted images are learned by a mutual learning\nscheme with the pristine reference as the supervision, and the discriminative\ncharacteristics of PR features are further ensured with the triplet\nconstraints. Given a distorted image for quality inference, the feature level\ndisentanglement is performed with an invertible neural layer for final quality\nprediction, leading to the PR and the corresponding distortion features for\ncomparison. The effectiveness of our proposed method is demonstrated on four\npopular IQA databases, and superior performance on cross-database evaluation\nalso reveals the high generalization capability of our method. The\nimplementation of our method is publicly available on\nhttps://github.com/Baoliang93/FPR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Gradient Adversarial Attack. (arXiv:2108.04204v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04204","description":"<p>In recent years, research on adversarial attacks has become a hot spot.\nAlthough current literature on the transfer-based adversarial attack has\nachieved promising results for improving the transferability to unseen\nblack-box models, it still leaves a long way to go. Inspired by the idea of\nmeta-learning, this paper proposes a novel architecture called Meta Gradient\nAdversarial Attack (MGAA), which is plug-and-play and can be integrated with\nany existing gradient-based attack method for improving the cross-model\ntransferability. Specifically, we randomly sample multiple models from a model\nzoo to compose different tasks and iteratively simulate a white-box attack and\na black-box attack in each task. By narrowing the gap between the gradient\ndirections in white-box and black-box attacks, the transferability of\nadversarial examples on the black-box setting can be improved. Extensive\nexperiments on the CIFAR10 and ImageNet datasets show that our architecture\noutperforms the state-of-the-art methods for both black-box and white-box\nattack settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yunpei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04212","description":"<p>Action recognition is a crucial task for video understanding. In this paper,\nwe present AutoVideo, a Python system for automated video action recognition.\nIt currently supports seven action recognition algorithms and various\npre-processing modules. Unlike the existing libraries that only provide model\nzoos, AutoVideo is built with the standard pipeline language. The basic\nbuilding block is primitive, which wraps a pre-processing module or an\nalgorithm with some hyperparameters. AutoVideo is highly modular and\nextendable. It can be easily combined with AutoML searchers. The pipeline\nlanguage is quite general so that we can easily enrich AutoVideo with\nalgorithms for various other video-related tasks in the future. AutoVideo is\nreleased under MIT license at https://github.com/datamllab/autovideo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anmoll Kumar Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Mohammad Qazim Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation of VHR EO Images using Unsupervised Learning. (arXiv:2108.04222v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04222","description":"<p>Semantic segmentation is a crucial step in many Earth observation tasks.\nLarge quantity of pixel-level annotation is required to train deep networks for\nsemantic segmentation. Earth observation techniques are applied to varieties of\napplications and since classes vary widely depending on the applications,\ntherefore, domain knowledge is often required to label Earth observation\nimages, impeding availability of labeled training data in many Earth\nobservation applications. To tackle these challenges, in this paper we propose\nan unsupervised semantic segmentation method that can be trained using just a\nsingle unlabeled scene. Remote sensing scenes are generally large. The proposed\nmethod exploits this property to sample smaller patches from the larger scene\nand uses deep clustering and contrastive learning to refine the weights of a\nlightweight deep model composed of a series of the convolution layers along\nwith an embedded channel attention. After unsupervised training on the target\nimage/scene, the model automatically segregates the major classes present in\nthe scene and produces the segmentation map. Experimental results on the\nVaihingen dataset demonstrate the efficacy of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahzad_M/0/1/0/all/0/1\">Muhammad Shahzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation. (arXiv:1907.12975v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/1907.12975","description":"<p>Separating and labeling each instance of a nucleus (instance-aware\nsegmentation) is the key challenge in segmenting single cell nuclei on\nfluorescence microscopy images. Deep Neural Networks can learn the implicit\ntransformation of a nuclear image into a probability map indicating the class\nmembership of each pixel (nucleus or background), but the use of\npost-processing steps to turn the probability map into a labeled object mask is\nerror-prone. This especially accounts for nuclear images of tissue sections and\nnuclear images across varying tissue preparations. In this work, we aim to\nevaluate the performance of state-of-the-art deep learning architectures to\nsegment nuclei in fluorescence images of various tissue origins and sample\npreparation types without post-processing. We compare architectures that\noperate on pixel to pixel translation and an architecture that operates on\nobject detection and subsequent locally applied segmentation. In addition, we\npropose a novel strategy to create artificial images to extend the training\nset. We evaluate the influence of ground truth annotation quality, image scale\nand segmentation complexity on segmentation performance. Results show that\nthree out of four deep learning architectures (U-Net, U-Net with ResNet34\nbackbone, Mask R-CNN) can segment fluorescent nuclear images on most of the\nsample preparation types and tissue origins with satisfactory segmentation\nperformance. Mask R-CNN, an architecture designed to address instance aware\nsegmentation tasks, outperforms other architectures. Equal nuclear mean size,\nconsistent nuclear annotations and the use of artificially generated images\nresult in overall acceptable precision and recall across different tissues and\nsample preparation types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1\">Florian Kromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1\">Lukas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1\">Eva Bozsaky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1\">Inge Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1\">Wolfgang Doerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1\">Sabine Taschner-Mandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1\">Peter Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.02940","description":"<p>In recent years, many deep learning models have been adopted in autonomous\ndriving. At the same time, these models introduce new vulnerabilities that may\ncompromise the safety of autonomous vehicles. Specifically, recent studies have\ndemonstrated that adversarial attacks can cause a significant decline in\ndetection precision of deep learning-based 3D object detection models. Although\ndriving safety is the ultimate concern for autonomous driving, there is no\ncomprehensive study on the linkage between the performance of deep learning\nmodels and the driving safety of autonomous vehicles under adversarial attacks.\nIn this paper, we investigate the impact of two primary types of adversarial\nattacks, perturbation attacks and patch attacks, on the driving safety of\nvision-based autonomous vehicles rather than the detection precision of deep\nlearning models. In particular, we consider two state-of-the-art models in\nvision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving\nsafety, we propose an end-to-end evaluation framework with a set of driving\nsafety performance metrics. By analyzing the results of our extensive\nevaluation experiments, we find that (1) the attack's impact on the driving\nsafety of autonomous vehicles and the attack's impact on the precision of 3D\nobject detectors are decoupled, and (2) the DSGN model demonstrates stronger\nrobustness to adversarial attacks than the Stereo R-CNN model. In addition, we\nfurther investigate the causes behind the two findings with an ablation study.\nThe findings of this paper provide a new perspective to evaluate adversarial\nattacks and guide the selection of deep learning models in autonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kejie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohua Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.IR updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.IR","description":"Computer Science -- Information Retrieval (cs.IR) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"High Quality Related Search Query Suggestions using Deep Reinforcement Learning. (arXiv:2108.04452v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04452","description":"<p>\"High Quality Related Search Query Suggestions\" task aims at recommending\nsearch queries which are real, accurate, diverse, relevant and engaging.\nObtaining large amounts of query-quality human annotations is expensive. Prior\nwork on supervised query suggestion models suffered from selection and exposure\nbias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),\nleading to low quality suggestions. Reinforcement Learning techniques employed\nto reformulate a query using terms from search results, have limited\nscalability to large-scale industry applications. To recommend high quality\nrelated search queries, we train a Deep Reinforcement Learning model to predict\nthe query a user would enter next. The reward signal is composed of long-term\nsession-based user feedback, syntactic relatedness and estimated naturalness of\ngenerated query. Over the baseline supervised model, our proposed approach\nachieves a significant relative improvement in terms of recommendation\ndiversity (3%), down-stream user-engagement (4.2%) and per-sentence word\nrepetitions (82%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1\">Praveen Kumar Bodigutla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End User Behavior Retrieval in Click-Through RatePrediction Model. (arXiv:2108.04468v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04468","description":"<p>Click-Through Rate (CTR) prediction is one of the core tasks in recommender\nsystems (RS). It predicts a personalized click probability for each user-item\npair. Recently, researchers have found that the performance of CTR model can be\nimproved greatly by taking user behavior sequence into consideration,\nespecially long-term user behavior sequence. The report on an e-commerce\nwebsite shows that 23\\% of users have more than 1000 clicks during the past 5\nmonths. Though there are numerous works focus on modeling sequential user\nbehaviors, few works can handle long-term user behavior sequence due to the\nstrict inference time constraint in real world system. Two-stage methods are\nproposed to push the limit for better performance. At the first stage, an\nauxiliary task is designed to retrieve the top-$k$ similar items from long-term\nuser behavior sequence. At the second stage, the classical attention mechanism\nis conducted between the candidate item and $k$ items selected in the first\nstage. However, information gap happens between retrieval stage and the main\nCTR task. This goal divergence can greatly diminishing the performance gain of\nlong-term user sequence. In this paper, inspired by Reformer, we propose a\nlocality-sensitive hashing (LSH) method called ETA (End-to-end Target\nAttention) which can greatly reduce the training and inference cost and make\nthe end-to-end training with long-term user behavior sequence possible. Both\noffline and online experiments confirm the effectiveness of our model. We\ndeploy ETA into a large-scale real world E-commerce system and achieve extra\n3.1\\% improvements on GMV (Gross Merchandise Value) compared to a two-stage\nlong user sequence CTR model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_C/0/1/0/all/0/1\">Changhua Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_S/0/1/0/all/0/1\">Shanshan Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1\">Junfeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Wenwu Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized Graph Collaborative Filtering. (arXiv:2108.04475v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04475","description":"<p>User-item interactions in recommendations can be naturally de-noted as a\nuser-item bipartite graph. Given the success of graph neural networks (GNNs) in\ngraph representation learning, GNN-based C methods have been proposed to\nadvance recommender systems. These methods often make recommendations based on\nthe learned user and item embeddings. However, we found that they do not\nperform well wit sparse user-item graphs which are quite common in real-world\nrecommendations. Therefore, in this work, we introduce a novel perspective to\nbuild GNN-based CF methods for recommendations which leads to the proposed\nframework Localized Graph Collaborative Filtering (LGCF). One key advantage of\nLGCF is that it does not need to learn embeddings for each user and item, which\nis challenging in sparse scenarios.\n</p>\n<p>Alternatively, LGCF aims at encoding useful CF information into a localized\ngraph and making recommendations based on such graph. Extensive experiments on\nvarious datasets validate the effectiveness of LGCF especially in sparse\nscenarios. Furthermore, empirical results demonstrate that LGCF provides\ncomplementary information to the embedding-based CF model which can be utilized\nto boost recommendation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Hyperbolic Graph Convolution Network for Recommendation. (arXiv:2108.04607v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04607","description":"<p>Recently, Graph Convolution Network (GCN) based methods have achieved\noutstanding performance for recommendation. These methods embed users and items\nin Euclidean space, and perform graph convolution on user-item interaction\ngraphs. However, real-world datasets usually exhibit tree-like hierarchical\nstructures, which make Euclidean space less effective in capturing user-item\nrelationship. In contrast, hyperbolic space, as a continuous analogue of a\ntree-graph, provides a promising alternative. In this paper, we propose a fully\nhyperbolic GCN model for recommendation, where all operations are performed in\nhyperbolic space. Utilizing the advantage of hyperbolic space, our method is\nable to embed users/items with less distortion and capture user-item\ninteraction relationship more accurately. Extensive experiments on public\nbenchmark datasets show that our method outperforms both Euclidean and\nhyperbolic counterparts and requires far lower embedding dimensionality to\nachieve comparable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Latent Relation Modeling for Collaborative Metric Learning. (arXiv:2108.04655v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04655","description":"<p>Collaborative Metric Learning (CML) recently emerged as a powerful paradigm\nfor recommendation based on implicit feedback collaborative filtering. However,\nstandard CML methods learn fixed user and item representations, which fails to\ncapture the complex interests of users. Existing extensions of CML also either\nignore the heterogeneity of user-item relations, i.e. that a user can\nsimultaneously like very different items, or the latent item-item relations,\ni.e. that a user's preference for an item depends, not only on its intrinsic\ncharacteristics, but also on items they previously interacted with. In this\npaper, we present a hierarchical CML model that jointly captures latent\nuser-item and item-item relations from implicit data. Our approach is inspired\nby translation mechanisms from knowledge graph embedding and leverages\nmemory-based attention networks. We empirically show the relevance of this\njoint relational modeling, by outperforming existing CML models on\nrecommendation tasks on several real-world datasets. Our experiments also\nemphasize the limits of current CML relational models on very sparse datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1\">Guillaume Salha-Galvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1\">Manuel Moussallam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"POSO: Personalized Cold Start Modules for Large-scale Recommender Systems. (arXiv:2108.04690v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04690","description":"<p>Recommendation for new users, also called user cold start, has been a\nwell-recognized challenge for online recommender systems. Most existing methods\nview the crux as the lack of initial data. However, in this paper, we argue\nthat there are neglected problems: 1) New users' behaviour follows much\ndifferent distributions from regular users. 2) Although personalized features\nare involved, heavily imbalanced samples prevent the model from balancing\nnew/regular user distributions, as if the personalized features are\noverwhelmed. We name the problem as the ``submergence\" of personalization. To\ntackle this problem, we propose a novel module: Personalized COld Start MOdules\n(POSO). Considering from a model architecture perspective, POSO personalizes\nexisting modules by introducing multiple user-group-specialized sub-modules.\nThen, it fuses their outputs by personalized gates, resulting in comprehensive\nrepresentations. In such way, POSO projects imbalanced features to even\nmodules. POSO can be flexibly integrated into many existing modules and\neffectively improves their performance with negligible computational overheads.\nThe proposed method shows remarkable advantage in industrial scenario. It has\nbeen deployed on the large-scale recommender system of Kwai, and improves new\nuser Watch Time by a large margin (+7.75%). Moreover, POSO can be further\ngeneralized to regular users, inactive users and returning users (+2%-3% on\nWatch Time), as well as item cold start (+3.8% on Watch Time). Its\neffectiveness has also been verified on public dataset (MovieLens 20M). We\nbelieve such practical experience can be well generalized to other scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shangfeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haobin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Honghuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+PinghuaGong/0/1/0/all/0/1\">PinghuaGong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMUSED: An Annotation Framework of Multi-modal Social Media Data. (arXiv:2010.00502v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2010.00502","description":"<p>In this paper, we present a semi-automated framework called AMUSED for\ngathering multi-modal annotated data from the multiple social media platforms.\nThe framework is designed to mitigate the issues of collecting and annotating\nsocial media data by cohesively combining machine and human in the data\ncollection process. From a given list of the articles from professional news\nmedia or blog, AMUSED detects links to the social media posts from news\narticles and then downloads contents of the same post from the respective\nsocial media platform to gather details about that specific post. The framework\nis capable of fetching the annotated data from multiple platforms like Twitter,\nYouTube, Reddit. The framework aims to reduce the workload and problems behind\nthe data annotation from the social media platforms. AMUSED can be applied in\nmultiple application domains, as a use case, we have implemented the framework\nfor collecting COVID-19 misinformation data from different social media\nplatforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahi_G/0/1/0/all/0/1\">Gautam Kishore Shahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Information Retrieval"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.LG updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.LG","description":"Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deep Transfer Learning for Identifications of Slope Surface Cracks. (arXiv:2108.04235v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04235","description":"<p>Geohazards such as landslides have caused great losses to the safety of\npeople's lives and property, which is often accompanied with surface cracks. If\nsuch surface cracks could be identified in time, it is of great significance\nfor the monitoring and early warning of geohazards. Currently, the most common\nmethod for crack identification is manual detection, which is with low\nefficiency and accuracy. In this paper, a deep transfer learning framework is\nproposed to effectively and efficiently identify slope surface cracks for the\nsake of fast monitoring and early warning of geohazards such as landslides. The\nessential idea is to employ transfer learning by training (a) the large sample\ndataset of concrete cracks and (b) the small sample dataset of soil and rock\nmasses cracks. In the proposed framework, (1) pretrained cracks identification\nmodels are constructed based on the large sample dataset of concrete cracks;\n(2) refined cracks identification models are further constructed based on the\nsmall sample dataset of soil and rock masses cracks. The proposed framework\ncould be applied to conduct UAV surveys on high-steep slopes to realize the\nmonitoring and early warning of landslides to ensure the safety of people's\nlives and property.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1\">Gang Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation. (arXiv:2108.04238v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04238","description":"<p>Explanation of AI, as well as fairness of algorithms' decisions and the\ntransparency of the decision model, are becoming more and more important. And\nit is crucial to design effective and human-friendly techniques when opening\nthe black-box model. Counterfactual conforms to the human way of thinking and\nprovides a human-friendly explanation, and its corresponding explanation\nalgorithm refers to a strategic alternation of a given data point so that its\nmodel output is \"counter-facted\", i.e. the prediction is reverted. In this\npaper, we adapt counterfactual explanation over fine-grained image\nclassification problem. We demonstrated an adaptive method that could give a\ncounterfactual explanation by showing the composed counterfactual feature map\nusing top-down layer searching algorithm (TDLS). We have proved that our TDLS\nalgorithm could provide more flexible counterfactual visual explanation in an\nefficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,\nwe discussed several applicable scenarios of counterfactual visual\nexplanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Haocheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caleb Chen Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Influenza Hemagglutinin Protein Sequences using Convolutional Neural Networks. (arXiv:2108.04240v1 [q-bio.QM])","link":"http://arxiv.org/abs/2108.04240","description":"<p>The Influenza virus can be considered as one of the most severe viruses that\ncan infect multiple species with often fatal consequences to the hosts. The\nHemagglutinin (HA) gene of the virus can be a target for antiviral drug\ndevelopment realised through accurate identification of its sub-types and\npossible the targeted hosts. This paper focuses on accurately predicting if an\nInfluenza type A virus can infect specific hosts, and more specifically, Human,\nAvian and Swine hosts, using only the protein sequence of the HA gene. In more\ndetail, we propose encoding the protein sequences into numerical signals using\nthe Hydrophobicity Index and subsequently utilising a Convolutional Neural\nNetwork-based predictive model. The Influenza HA protein sequences used in the\nproposed work are obtained from the Influenza Research Database (IRD).\nSpecifically, complete and unique HA protein sequences were used for avian,\nhuman and swine hosts. The data obtained for this work was 17999 human-host\nproteins, 17667 avian-host proteins and 9278 swine-host proteins. Given this\nset of collected proteins, the proposed method yields as much as 10% higher\naccuracy for an individual class (namely, Avian) and 5% higher overall accuracy\nthan in an earlier study. It is also observed that the accuracy for each class\nin this work is more balanced than what was presented in this earlier study. As\nthe results show, the proposed model can distinguish HA protein sequences with\nhigh accuracy whenever the virus under investigation can infect Human, Avian or\nSwine hosts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Chrysostomou_C/0/1/0/all/0/1\">Charalambos Chrysostomou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Alexandrou_F/0/1/0/all/0/1\">Floris Alexandrou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nicolaou_M/0/1/0/all/0/1\">Mihalis A. Nicolaou</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Seker_H/0/1/0/all/0/1\">Huseyin Seker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Olfactory Bulb Segmentation on High Resolutional T2-Weighted MRI. (arXiv:2108.04267v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04267","description":"<p>The neuroimage analysis community has neglected the automated segmentation of\nthe olfactory bulb (OB) despite its crucial role in olfactory function. The\nlack of an automatic processing method for the OB can be explained by its\nchallenging properties. Nonetheless, recent advances in MRI acquisition\ntechniques and resolution have allowed raters to generate more reliable manual\nannotations. Furthermore, the high accuracy of deep learning methods for\nsolving semantic segmentation problems provides us with an option to reliably\nassess even small structures. In this work, we introduce a novel, fast, and\nfully automated deep learning pipeline to accurately segment OB tissue on\nsub-millimeter T2-weighted (T2w) whole-brain MR images. To this end, we\ndesigned a three-stage pipeline: (1) Localization of a region containing both\nOBs using FastSurferCNN, (2) Segmentation of OB tissue within the localized\nregion through four independent AttFastSurferCNN - a novel deep learning\narchitecture with a self-attention mechanism to improve modeling of contextual\ninformation, and (3) Ensemble of the predicted label maps. The OB pipeline\nexhibits high performance in terms of boundary delineation, OB localization,\nand volume estimation across a wide range of ages in 203 participants of the\nRhineland Study. Moreover, it also generalizes to scans of an independent\ndataset never encountered during training, the Human Connectome Project (HCP),\nwith different acquisition parameters and demographics, evaluated in 30 cases\nat the native 0.7mm HCP resolution, and the default 0.8mm pipeline resolution.\nWe extensively validated our pipeline not only with respect to segmentation\naccuracy but also to known OB volume effects, where it can sensitively\nreplicate age effects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Estrada_S/0/1/0/all/0/1\">Santiago Estrada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Ran Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diers_K/0/1/0/all/0/1\">Kersten Diers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Weiyi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehses_P/0/1/0/all/0/1\">Philipp Ehses</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_T/0/1/0/all/0/1\">Tony St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breteler_M/0/1/0/all/0/1\">Monique M.B Breteler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACE: A Novel Approach for the Statistical Analysis of Pairwise Connectivity. (arXiv:2108.04289v1 [q-bio.NC])","link":"http://arxiv.org/abs/2108.04289","description":"<p>Analysing correlations between streams of events is an important problem. It\narises for example in Neurosciences, when the connectivity of neurons should be\ninferred from spike trains that record neurons' individual spiking activity.\nWhile recently some approaches for inferring delayed synaptic connections have\nbeen proposed, they are limited in the types of connectivities and delays they\nare able to handle, or require computation-intensive procedures. This paper\nproposes a faster and more flexible approach for analysing such delayed\ncorrelated activity: a statistical approach for the Analysis of Connectivity in\nspiking Events (ACE), based on the idea of hypothesis testing. It first\ncomputes for any pair of a source and a target neuron the inter-spike delays\nbetween subsequent source- and target-spikes. Then, it derives a null model for\nthe distribution of inter-spike delays for \\emph{uncorrelated}~neurons.\nFinally, it compares the observed distribution of inter-spike delays to this\nnull model and infers pairwise connectivity based on the Pearson's Chi-squared\ntest statistic. Thus, ACE is capable to detect connections with a priori\nunknown, non-discrete (and potentially large) inter-spike delays, which might\nvary between pairs of neurons. Since ACE works incrementally, it has potential\nfor being used in online processing. In our experiments, we visualise the\nadvantages of ACE in varying experimental scenarios (except for one special\ncase) and in a state-of-the-art dataset which has been generated for\nneuro-scientific research under most realistic conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Krempl/0/1/0/all/0/1\">Krempl</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Georg/0/1/0/all/0/1\">Georg</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kottke/0/1/0/all/0/1\">Kottke</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Daniel/0/1/0/all/0/1\">Daniel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Minh_P/0/1/0/all/0/1\">Pham Minh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tuan/0/1/0/all/0/1\">Tuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Numerical Networks for Natura 2000 habitats classification by satellite images. (arXiv:2108.04327v1 [math.NA])","link":"http://arxiv.org/abs/2108.04327","description":"<p>Natural numerical networks are introduced as a new classification algorithm\nbased on the numerical solution of nonlinear partial differential equations of\nforward-backward diffusion type on complete graphs. The proposed natural\nnumerical network is applied to open important environmental and nature\nconservation task, the automated identification of protected habitats by using\nsatellite images. In the natural numerical network, the forward diffusion\ncauses the movement of points in a feature space toward each other. The\nopposite effect, keeping the points away from each other, is caused by backward\ndiffusion. This yields the desired classification. The natural numerical\nnetwork contains a few parameters that are optimized in the learning phase of\nthe method. After learning parameters and optimizing the topology of the\nnetwork graph, classification necessary for habitat identification is\nperformed. A relevancy map for each habitat is introduced as a tool for\nvalidating the classification and finding new Natura 2000 habitat appearances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1\">Karol Mikula</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kollar_M/0/1/0/all/0/1\">Michal Kollar</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ozvat_A/0/1/0/all/0/1\">Aneta A. Ozvat</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ambroz_M/0/1/0/all/0/1\">Martin Ambroz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Cahojova_L/0/1/0/all/0/1\">Lucia Cahojova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jarolimek_I/0/1/0/all/0/1\">Ivan Jarolimek</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibik_J/0/1/0/all/0/1\">Jozef Sibik</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sibikova_M/0/1/0/all/0/1\">Maria Sibikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Generic Multimodal Architecture for Batch and Streaming Big Data Integration. (arXiv:2108.04343v1 [cs.AI])","link":"http://arxiv.org/abs/2108.04343","description":"<p>Big Data are rapidly produced from various heterogeneous data sources. They\nare of different types (text, image, video or audio) and have different levels\nof reliability and completeness. One of the most interesting architectures that\ndeal with the large amount of emerging data at high velocity is called the\nlambda architecture. In fact, it combines two different processing layers\nnamely batch and speed layers, each providing specific views of data while\nensuring robustness, fast and scalable data processing. However, most papers\ndealing with the lambda architecture are focusing one single type of data\ngenerally produced by a single data source. Besides, the layers of the\narchitecture are implemented independently, or, at best, are combined to\nperform basic processing without assessing either the data reliability or\ncompleteness. Therefore, inspired by the lambda architecture, we propose in\nthis paper a generic multimodal architecture that combines both batch and\nstreaming processing in order to build a complete, global and accurate insight\nin near-real-time based on the knowledge extracted from multiple heterogeneous\nBig Data sources. Our architecture uses batch processing to analyze the data\nstructures and contents, build the learning models and calculate the\nreliability index of the involved sources, while the streaming processing uses\nthe built-in models of the batch layer to immediately process incoming data and\nrapidly provide results. We validate our architecture in the context of urban\ntraffic management systems in order to detect congestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousfi_S/0/1/0/all/0/1\">Siham Yousfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhanoui_M/0/1/0/all/0/1\">Maryem Rhanoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiadmi_D/0/1/0/all/0/1\">Dalila Chiadmi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Machine Learning Techniques for Detecting and Diagnosing COVID-19 from Imaging. (arXiv:2108.04344v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04344","description":"<p>Due to the limited availability and high cost of the reverse\ntranscription-polymerase chain reaction (RT-PCR) test, many studies have\nproposed machine learning techniques for detecting COVID-19 from medical\nimaging. The purpose of this study is to systematically review, assess, and\nsynthesize research articles that have used different machine learning\ntechniques to detect and diagnose COVID-19 from chest X-ray and CT scan images.\nA structured literature search was conducted in the relevant bibliographic\ndatabases to ensure that the survey solely centered on reproducible and\nhigh-quality research. We selected papers based on our inclusion criteria. In\nthis survey, we reviewed $98$ articles that fulfilled our inclusion criteria.\nWe have surveyed a complete pipeline of chest imaging analysis techniques\nrelated to COVID-19, including data collection, pre-processing, feature\nextraction, classification, and visualization. We have considered CT scans and\nX-rays as both are widely used to describe the latest developments in medical\nimaging to detect COVID-19. This survey provides researchers with valuable\ninsights into different machine learning techniques and their performance in\nthe detection and diagnosis of COVID-19 from chest imaging. At the end, the\nchallenges and limitations in detecting COVID-19 using machine learning\ntechniques and the future direction of research are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panday_A/0/1/0/all/0/1\">Aishwarza Panday</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabir_M/0/1/0/all/0/1\">Muhammad Ashad Kabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1\">Nihad Karim Chowdhury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable AI and susceptibility to adversarial attacks: a case study in classification of breast ultrasound images. (arXiv:2108.04345v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04345","description":"<p>Ultrasound is a non-invasive imaging modality that can be conveniently used\nto classify suspicious breast nodules and potentially detect the onset of\nbreast cancer. Recently, Convolutional Neural Networks (CNN) techniques have\nshown promising results in classifying ultrasound images of the breast into\nbenign or malignant. However, CNN inference acts as a black-box model, and as\nsuch, its decision-making is not interpretable. Therefore, increasing effort\nhas been dedicated to explaining this process, most notably through GRAD-CAM\nand other techniques that provide visual explanations into inner workings of\nCNNs. In addition to interpretation, these methods provide clinically important\ninformation, such as identifying the location for biopsy or treatment. In this\nwork, we analyze how adversarial assaults that are practically undetectable may\nbe devised to alter these importance maps dramatically. Furthermore, we will\nshow that this change in the importance maps can come with or without altering\nthe classification result, rendering them even harder to detect. As such, care\nmust be taken when using these importance maps to shed light on the inner\nworkings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and\npropose a new network based on ResNet-50 to improve the classification\naccuracies. Our sensitivity and specificity is comparable to the state of the\nart results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasaee_H/0/1/0/all/0/1\">Hamza Rasaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AASeg: Attention Aware Network for Real Time Semantic Segmentation. (arXiv:2108.04349v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04349","description":"<p>In this paper, we present a new network named Attention Aware Network (AASeg)\nfor real time semantic image segmentation. Our network incorporates spatial and\nchannel information using Spatial Attention (SA) and Channel Attention (CA)\nmodules respectively. It also uses dense local multi-scale context information\nusing Multi Scale Context (MSC) module. The feature maps are concatenated\nindividually to produce the final segmentation map. We demonstrate the\neffectiveness of our method using a comprehensive analysis, quantitative\nexperimental results and ablation study using Cityscapes, ADE20K and Camvid\ndatasets. Our network performs better than most previous architectures with a\n74.4\\% Mean IOU on Cityscapes test dataset while running at 202.7 FPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Open Domain Adaption Framework (AODA): Sketch-to-Photo Synthesis. (arXiv:2108.04351v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04351","description":"<p>This paper aims to demonstrate the efficiency of the Adversarial Open Domain\nAdaption framework for sketch-to-photo synthesis. The unsupervised open domain\nadaption for generating realistic photos from a hand-drawn sketch is\nchallenging as there is no such sketch of that class for training data. The\nabsence of learning supervision and the huge domain gap between both the\nfreehand drawing and picture domains make it hard. We present an approach that\nlearns both sketch-to-photo and photo-to-sketch generation to synthesise the\nmissing freehand drawings from pictures. Due to the domain gap between\nsynthetic sketches and genuine ones, the generator trained on false drawings\nmay produce unsatisfactory results when dealing with drawings of lacking\nclasses. To address this problem, we offer a simple but effective open-domain\nsampling and optimization method that tricks the generator into considering\nfalse drawings as genuine. Our approach generalises the learnt sketch-to-photo\nand photo-to-sketch mappings from in-domain input to open-domain categories. On\nthe Scribble and SketchyCOCO datasets, we compared our technique to the most\ncurrent competing methods. For many types of open-domain drawings, our model\noutperforms impressive results in synthesising accurate colour, substance, and\nretaining the structural layout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Amey Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satish_M/0/1/0/all/0/1\">Mega Satish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionInput v2.0 supporting DirectX: A modular library of open-source gesture-based machine learning and computer vision methods for interacting and controlling existing software with a webcam. (arXiv:2108.04357v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04357","description":"<p>Touchless computer interaction has become an important consideration during\nthe COVID-19 pandemic period. Despite progress in machine learning and computer\nvision that allows for advanced gesture recognition, an integrated collection\nof such open-source methods and a user-customisable approach to utilising them\nin a low-cost solution for touchless interaction in existing software is still\nmissing. In this paper, we introduce the MotionInput v2.0 application. This\napplication utilises published open-source libraries and additional gesture\ndefinitions developed to take the video stream from a standard RGB webcam as\ninput. It then maps human motion gestures to input operations for existing\napplications and games. The user can choose their own preferred way of\ninteracting from a series of motion types, including single and bi-modal hand\ngesturing, full-body repetitive or extremities-based exercises, head and facial\nmovements, eye tracking, and combinations of the above. We also introduce a\nseries of bespoke gesture recognition classifications as DirectInput triggers,\nincluding gestures for idle states, auto calibration, depth capture from a 2D\nRGB webcam stream and tracking of facial motions such as mouth motions,\nwinking, and head direction with rotation. Three use case areas assisted the\ndevelopment of the modules: creativity software, office and clinical software,\nand gaming software. A collection of open-source libraries has been integrated\nand provide a layer of modular gesture mapping on top of existing mouse and\nkeyboard controls in Windows via DirectX. With ease of access to webcams\nintegrated into most laptops and desktop computers, touchless computing becomes\nmore available with MotionInput v2.0, in a federated and locally processed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kummen_A/0/1/0/all/0/1\">Ashild Kummen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1\">Ali Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganeva_T/0/1/0/all/0/1\">Teodora Ganeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qianying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1\">Robert Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratwatte_C/0/1/0/all/0/1\">Chenuka Ratwatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yang Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almazov_E/0/1/0/all/0/1\">Emil Almazov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visram_S/0/1/0/all/0/1\">Sheena Visram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Andrew Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebire_N/0/1/0/all/0/1\">Neil J Sebire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stott_L/0/1/0/all/0/1\">Lee Stott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_Y/0/1/0/all/0/1\">Yvonne Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_G/0/1/0/all/0/1\">Graham Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamedally_D/0/1/0/all/0/1\">Dean Mohamedally</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Nets for Diabetic Retinopathy Screening in Bangladeshi Patients. (arXiv:2108.04358v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04358","description":"<p>Diabetes is one of the most prevalent chronic diseases in Bangladesh, and as\na result, Diabetic Retinopathy (DR) is widespread in the population. DR, an eye\nillness caused by diabetes, can lead to blindness if it is not identified and\ntreated in its early stages. Unfortunately, diagnosis of DR requires medically\ntrained professionals, but Bangladesh has limited specialists in comparison to\nits population. Moreover, the screening process is often expensive, prohibiting\nmany from receiving timely and proper diagnosis. To address the problem, we\nintroduce a deep learning algorithm which screens for different stages of DR.\nWe use a state-of-the-art CNN architecture to diagnose patients based on\nretinal fundus imagery. This paper is an experimental evaluation of the\nalgorithm we developed for DR diagnosis and screening specifically for\nBangladeshi patients. We perform this validation study using separate pools of\nretinal image data of real patients from a hospital and field studies in\nBangladesh. Our results show that the algorithm is effective at screening\nBangladeshi eyes even when trained on a public dataset which is out of domain,\nand can accurately determine the stage of DR as well, achieving an overall\naccuracy of 92.27\\% and 93.02\\% on two validation sets of Bangladeshi eyes. The\nresults confirm the ability of the algorithm to be used in real clinical\nsettings and applications due to its high accuracy and classwise metrics. Our\nalgorithm is implemented in the application Drishti, which is used to screen\nfor DR in patients living in rural areas in Bangladesh, where access to\nprofessional screening is limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haque_A/0/1/0/all/0/1\">Ayaan Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutradhar_I/0/1/0/all/0/1\">Ipsita Sutradhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mahziba Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mehedi Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1\">Malabika Sarker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptable image quality assessment using meta-reinforcement learning of task amenability. (arXiv:2108.04359v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04359","description":"<p>The performance of many medical image analysis tasks are strongly associated\nwith image data quality. When developing modern deep learning algorithms,\nrather than relying on subjective (human-based) image quality assessment (IQA),\ntask amenability potentially provides an objective measure of task-specific\nimage quality. To predict task amenability, an IQA agent is trained using\nreinforcement learning (RL) with a simultaneously optimised task predictor,\nsuch as a classification or segmentation neural network. In this work, we\ndevelop transfer learning or adaptation strategies to increase the adaptability\nof both the IQA agent and the task predictor so that they are less dependent on\nhigh-quality, expert-labelled training data. The proposed transfer learning\nstrategy re-formulates the original RL problem for task amenability in a\nmeta-reinforcement learning (meta-RL) framework. The resulting algorithm\nfacilitates efficient adaptation of the agent to different definitions of image\nquality, each with its own Markov decision process environment including\ndifferent images, labels and an adaptable task predictor. Our work demonstrates\nthat the IQA agents pre-trained on non-expert task labels can be adapted to\npredict task amenability as defined by expert task labels, using only a small\nset of expert labels. Using 6644 clinical ultrasound images from 249 prostate\ncancer patients, our results for image classification and segmentation tasks\nshow that the proposed IQA method can be adapted using data with as few as\nrespective 19.7% and 29.6% expert-reviewed consensus labels and still achieve\ncomparable IQA and task performance, which would otherwise require a training\ndataset with 100% expert labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_S/0/1/0/all/0/1\">Shaheer U. Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yunguan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavrinides_V/0/1/0/all/0/1\">Vasilis Stavrinides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M. C. Baum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qianye Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Richard E. Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey A. Sonn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C. Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RaftMLP: Do MLP-based Models Dream of Winning Over Computer Vision?. (arXiv:2108.04384v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04384","description":"<p>For the past ten years, CNN has reigned supreme in the world of computer\nvision, but recently, Transformer is on the rise. However, the quadratic\ncomputational cost of self-attention has become a severe problem of practice.\nThere has been much research on architectures without CNN and self-attention in\nthis context. In particular, MLP-Mixer is a simple idea designed using MLPs and\nhit an accuracy comparable to the Vision Transformer. However, the only\ninductive bias in this architecture is the embedding of tokens. Thus, there is\nstill a possibility to build a non-convolutional inductive bias into the\narchitecture itself, and we built in an inductive bias using two simple ideas.\nA way is to divide the token-mixing block vertically and horizontally. Another\nway is to make spatial correlations denser among some channels of token-mixing.\nWith this approach, we were able to improve the accuracy of the MLP-Mixer while\nreducing its parameters and computational complexity. Compared to other\nMLP-based models, the proposed model, named RaftMLP has a good balance of\ncomputational complexity, the number of parameters, and actual memory usage. In\naddition, our work indicates that MLP-based models have the potential to\nreplace CNNs by adopting inductive bias. The source code in PyTorch version is\navailable at \\url{https://github.com/okojoalg/raft-mlp}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1\">Yuki Tatsunami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1\">Masato Taki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diversity-aware Web APIs Recommendation with Compatibility Guarantee. (arXiv:2108.04389v1 [cs.SE])","link":"http://arxiv.org/abs/2108.04389","description":"<p>With the ever-increasing prevalence of web APIs (Application Programming\nInterfaces) in enabling smart software developments, finding and composing a\nlist of existing web APIs that can corporately fulfil the software developers'\nfunctional needs have become a promising way to develop a successful mobile\napp, economically and conveniently. However, the big volume and diversity of\ncandidate web APIs put additional burden on the app developers' web APIs\nselection decision-makings, since it is often a challenging task to\nsimultaneously guarantee the diversity and compatibility of the finally\nselected a set of web APIs. Considering this challenge, a Diversity-aware and\nCompatibility-driven web APIs Recommendation approach, namely DivCAR, is put\nforward in this paper. First, to achieve diversity, DivCAR employs random walk\nsampling technique on a pre-built correlation graph to generate diverse\ncorrelation subgraphs. Afterwards, with the diverse correlation subgraphs, we\nmodel the compatible web APIs recommendation problem to be a minimum group\nSteiner tree search problem. Through solving the minimum group Steiner tree\nsearch problem, manifold sets of compatible and diverse web APIs ranked are\nreturned to the app developers. At last, we design and enact a set of\nexperiments on a real-world dataset crawled from www.programmableWeb.com.\nExperimental results validate the effectiveness and efficiency of our proposed\nDivCAR approach in balancing the web APIs recommendation diversity and\ncompatibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonga_W/0/1/0/all/0/1\">Wenwen Gonga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yucong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chena_Y/0/1/0/all/0/1\">Yifei Chena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lianyong Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Architecture Selection in Differentiable NAS. (arXiv:2108.04392v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04392","description":"<p>Differentiable Neural Architecture Search is one of the most popular Neural\nArchitecture Search (NAS) methods for its search efficiency and simplicity,\naccomplished by jointly optimizing the model weight and architecture parameters\nin a weight-sharing supernet via gradient-based algorithms. At the end of the\nsearch phase, the operations with the largest architecture parameters will be\nselected to form the final architecture, with the implicit assumption that the\nvalues of architecture parameters reflect the operation strength. While much\nhas been discussed about the supernet's optimization, the architecture\nselection process has received little attention. We provide empirical and\ntheoretical analysis to show that the magnitude of architecture parameters does\nnot necessarily indicate how much the operation contributes to the supernet's\nperformance. We propose an alternative perturbation-based architecture\nselection that directly measures each operation's influence on the supernet. We\nre-evaluate several differentiable NAS methods with the proposed architecture\nselection and find that it is able to extract significantly improved\narchitectures from the underlying supernets consistently. Furthermore, we find\nthat several failure modes of DARTS can be greatly alleviated with the proposed\nselection method, indicating that much of the poor generalization observed in\nDARTS can be attributed to the failure of magnitude-based architecture\nselection rather than entirely the optimization of its supernet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Minhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangning Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaocheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Procedural Adversarial Noise Attack And Defense. (arXiv:2108.04409v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04409","description":"<p>Deep Neural Networks (DNNs) are vulnerable to adversarial examples which\nwould inveigle neural networks to make prediction errors with small per-\nturbations on the input images. Researchers have been devoted to promoting the\nresearch on the universal adversarial perturbations (UAPs) which are\ngradient-free and have little prior knowledge on data distributions. Procedural\nadversarial noise at- tack is a data-free universal perturbation generation\nmethod. In this paper, we propose two universal adversarial perturbation (UAP)\ngeneration methods based on procedural noise functions: Simplex noise and\nWorley noise. In our framework, the shading which disturbs visual\nclassification is generated with rendering technology. Without changing the\nsemantic representations, the adversarial examples generated via our methods\nshow superior performance on the attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Huilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wancheng Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy-Preserving Machine Learning: Methods, Challenges and Directions. (arXiv:2108.04417v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04417","description":"<p>Machine learning (ML) is increasingly being adopted in a wide variety of\napplication domains. Usually, a well-performing ML model, especially, emerging\ndeep neural network model, relies on a large volume of training data and\nhigh-powered computational resources. The need for a vast volume of available\ndata raises serious privacy concerns because of the risk of leakage of highly\nprivacy-sensitive information and the evolving regulatory environments that\nincreasingly restrict access to and use of privacy-sensitive data. Furthermore,\na trained ML model may also be vulnerable to adversarial attacks such as\nmembership/property inference attacks and model inversion attacks. Hence,\nwell-designed privacy-preserving ML (PPML) solutions are crucial and have\nattracted increasing research interest from academia and industry. More and\nmore efforts of PPML are proposed via integrating privacy-preserving techniques\ninto ML algorithms, fusing privacy-preserving approaches into ML pipeline, or\ndesigning various privacy-preserving architectures for existing ML systems. In\nparticular, existing PPML arts cross-cut ML, system, security, and privacy;\nhence, there is a critical need to understand state-of-art studies, related\nchallenges, and a roadmap for future research. This paper systematically\nreviews and summarizes existing privacy-preserving approaches and proposes a\nPGU model to guide evaluation for various PPML solutions through elaborately\ndecomposing their privacy-preserving functionalities. The PGU model is designed\nas the triad of Phase, Guarantee, and technical Utility. Furthermore, we also\ndiscuss the unique characteristics and challenges of PPML and outline possible\ndirections of future work that benefit a wide range of research communities\namong ML, distributed systems, security, and privacy areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1\">Nathalie Baracaldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_J/0/1/0/all/0/1\">James Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised classification of radiology images with NoTeacher: A Teacher that is not Mean. (arXiv:2108.04423v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04423","description":"<p>Deep learning models achieve strong performance for radiology image\nclassification, but their practical application is bottlenecked by the need for\nlarge labeled training datasets. Semi-supervised learning (SSL) approaches\nleverage small labeled datasets alongside larger unlabeled datasets and offer\npotential for reducing labeling cost. In this work, we introduce NoTeacher, a\nnovel consistency-based SSL framework which incorporates probabilistic\ngraphical models. Unlike Mean Teacher which maintains a teacher network updated\nvia a temporal ensemble, NoTeacher employs two independent networks, thereby\neliminating the need for a teacher network. We demonstrate how NoTeacher can be\ncustomized to handle a range of challenges in radiology image classification.\nSpecifically, we describe adaptations for scenarios with 2D and 3D inputs, uni\nand multi-label classification, and class distribution mismatch between labeled\nand unlabeled portions of the training data. In realistic empirical evaluations\non three public benchmark datasets spanning the workhorse modalities of\nradiology (X-Ray, CT, MRI), we show that NoTeacher achieves over 90-95% of the\nfully supervised AUROC with less than 5-15% labeling budget. Further, NoTeacher\noutperforms established SSL methods with minimal hyperparameter tuning, and has\nimplications as a principled and practical option for semisupervised learning\nin radiology applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Unnikrishnan_B/0/1/0/all/0/1\">Balagopal Unnikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaram_S/0/1/0/all/0/1\">Shafa Balaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan Sheng Foo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnaswamy_P/0/1/0/all/0/1\">Pavitra Krishnaswamy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Principal Component Analysis in High Dimensional CP Models. (arXiv:2108.04428v1 [stat.ML])","link":"http://arxiv.org/abs/2108.04428","description":"<p>The CP decomposition for high dimensional non-orthogonal spike tensors is an\nimportant problem with broad applications across many disciplines. However,\nprevious works with theoretical guarantee typically assume restrictive\nincoherence conditions on the basis vectors for the CP components. In this\npaper, we propose new computationally efficient composite PCA and concurrent\northogonalization algorithms for tensor CP decomposition with theoretical\nguarantees under mild incoherence conditions. The composite PCA applies the\nprincipal component or singular value decompositions twice, first to a matrix\nunfolding of the tensor data to obtain singular vectors and then to the matrix\nfolding of the singular vectors obtained in the first step. It can be used as\nan initialization for any iterative optimization schemes for the tensor CP\ndecomposition. The concurrent orthogonalization algorithm iteratively estimates\nthe basis vector in each mode of the tensor by simultaneously applying\nprojections to the orthogonal complements of the spaces generated by others CP\ncomponents in other modes. It is designed to improve the alternating least\nsquares estimator and other forms of the high order orthogonal iteration for\ntensors with low or moderately high CP ranks. Our theoretical investigation\nprovides estimation accuracy and statistical convergence rates for the two\nproposed algorithms. Our implementations on synthetic data demonstrate\nsignificant practical superiority of our approach over existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Han_Y/0/1/0/all/0/1\">Yuefeng Han</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_C/0/1/0/all/0/1\">Cun-Hui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Knowledge Tracing via Adversarial Training. (arXiv:2108.04430v1 [cs.CY])","link":"http://arxiv.org/abs/2108.04430","description":"<p>We study the problem of knowledge tracing (KT) where the goal is to trace the\nstudents' knowledge mastery over time so as to make predictions on their future\nperformance. Owing to the good representation capacity of deep neural networks\n(DNNs), recent advances on KT have increasingly concentrated on exploring DNNs\nto improve the performance of KT. However, we empirically reveal that the DNNs\nbased KT models may run the risk of overfitting, especially on small datasets,\nleading to limited generalization. In this paper, by leveraging the current\nadvances in adversarial training (AT), we propose an efficient AT based KT\nmethod (ATKT) to enhance KT model's generalization and thus push the limit of\nKT. Specifically, we first construct adversarial perturbations and add them on\nthe original interaction embeddings as adversarial examples. The original and\nadversarial examples are further used to jointly train the KT model, forcing it\nis not only to be robust to the adversarial examples, but also to enhance the\ngeneralization over the original ones. To better implement AT, we then present\nan efficient attentive-LSTM model as KT backbone, where the key is a proposed\nknowledge hidden state attention module that adaptively aggregates information\nfrom previous knowledge hidden states while simultaneously highlighting the\nimportance of current knowledge hidden state to make a more accurate\nprediction. Extensive experiments on four public benchmark datasets demonstrate\nthat our ATKT achieves new state-of-the-art performance. Code is available at:\n\\color{blue} {\\url{https://github.com/xiaopengguo/ATKT}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaopeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_M/0/1/0/all/0/1\">Mingyu Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1\">Maojing Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisit the Fundamental Theorem of Linear Algebra. (arXiv:2108.04432v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04432","description":"<p>This survey is meant to provide an introduction to the fundamental theorem of\nlinear algebra and the theories behind them. Our goal is to give a rigorous\nintroduction to the readers with prior exposure to linear algebra.\nSpecifically, we provide some details and proofs of some results from (Strang,\n1993). We then describe the fundamental theorem of linear algebra from\ndifferent views and find the properties and relationships behind the views. The\nfundamental theorem of linear algebra is essential in many fields, such as\nelectrical engineering, computer science, machine learning, and deep learning.\nThis survey is primarily a summary of purpose, significance of important\ntheories behind it.\n</p>\n<p>The sole aim of this survey is to give a self-contained introduction to\nconcepts and mathematical tools in theory behind the fundamental theorem of\nlinear algebra and rigorous analysis in order to seamlessly introduce its\nproperties in four subspaces in subsequent sections. However, we clearly\nrealize our inability to cover all the useful and interesting results and given\nthe paucity of scope to present this discussion, e.g., the separated analysis\nof the (orthogonal) projection matrices. We refer the reader to literature in\nthe field of linear algebra for a more detailed introduction to the related\nfields. Some excellent examples include (Rose, 1982; Strang, 2009; Trefethen\nand Bau III, 1997; Strang, 2019, 2021).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Enhanced Dynamic Mode Decomposition. (arXiv:2108.04433v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04433","description":"<p>Koopman operator theory shows how nonlinear dynamical systems can be\nrepresented as an infinite-dimensional, linear operator acting on a Hilbert\nspace of observables of the system. However, determining the relevant modes and\neigenvalues of this infinite-dimensional operator can be difficult. The\nextended dynamic mode decomposition (EDMD) is one such method for generating\napproximations to Koopman spectra and modes, but the EDMD method faces its own\nset of challenges due to the need of user defined observables. To address this\nissue, we explore the use of convolutional autoencoder networks to\nsimultaneously find optimal families of observables which also generate both\naccurate embeddings of the flow into a space of observables and immersions of\nthe observables back into flow coordinates. This network results in a global\ntransformation of the flow and affords future state prediction via EDMD and the\ndecoder network. We call this method deep learning dynamic mode decomposition\n(DLDMD). The method is tested on canonical nonlinear data sets and is shown to\nproduce results that outperform a standard DMD approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Curtis_C/0/1/0/all/0/1\">Christopher W. Curtis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alford_Lago_D/0/1/0/all/0/1\">Daniel Jay Alford-Lago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Issan_O/0/1/0/all/0/1\">Opal Issan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generalizable Model-and-Data Driven Approach for Open-Set RFF Authentication. (arXiv:2108.04436v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04436","description":"<p>Radio-frequency fingerprints~(RFFs) are promising solutions for realizing\nlow-cost physical layer authentication. Machine learning-based methods have\nbeen proposed for RFF extraction and discrimination. However, most existing\nmethods are designed for the closed-set scenario where the set of devices is\nremains unchanged. These methods can not be generalized to the RFF\ndiscrimination of unknown devices. To enable the discrimination of RFF from\nboth known and unknown devices, we propose a new end-to-end deep learning\nframework for extracting RFFs from raw received signals. The proposed framework\ncomprises a novel preprocessing module, called neural synchronization~(NS),\nwhich incorporates the data-driven learning with signal processing priors as an\ninductive bias from communication-model based processing. Compared to\ntraditional carrier synchronization techniques, which are static, this module\nestimates offsets by two learnable deep neural networks jointly trained by the\nRFF extractor. Additionally, a hypersphere representation is proposed to\nfurther improve the discrimination of RFF. Theoretical analysis shows that such\na data-and-model framework can better optimize the mutual information between\ndevice identity and the RFF, which naturally leads to better performance.\nExperimental results verify that the proposed RFF significantly outperforms\npurely data-driven DNN-design and existing handcrafted RFF methods in terms of\nboth discrimination and network generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Renjie Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiabao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Aiqun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_D/0/1/0/all/0/1\">Derrick Wing Kwan Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swindlehurst_A/0/1/0/all/0/1\">A. Lee Swindlehurst</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaRNN: Adaptive Learning and Forecasting of Time Series. (arXiv:2108.04443v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04443","description":"<p>Time series has wide applications in the real world and is known to be\ndifficult to forecast. Since its statistical properties change over time, its\ndistribution also changes temporally, which will cause severe distribution\nshift problem to existing methods. However, it remains unexplored to model the\ntime series in the distribution perspective. In this paper, we term this as\nTemporal Covariate Shift (TCS). This paper proposes Adaptive RNNs (AdaRNN) to\ntackle the TCS problem by building an adaptive model that generalizes well on\nthe unseen test data. AdaRNN is sequentially composed of two novel algorithms.\nFirst, we propose Temporal Distribution Characterization to better characterize\nthe distribution information in the TS. Second, we propose Temporal\nDistribution Matching to reduce the distribution mismatch in TS to learn the\nadaptive TS model. AdaRNN is a general framework with flexible distribution\ndistances integrated. Experiments on human activity recognition, air quality\nprediction, and financial analysis show that AdaRNN outperforms the latest\nmethods by a classification accuracy of 2.6% and significantly reduces the RMSE\nby 9.0%. We also show that the temporal distribution matching algorithm can be\nextended in Transformer structure to boost its performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wenjie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Sinno Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decentralized Composite Optimization with Compression. (arXiv:2108.04448v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04448","description":"<p>Decentralized optimization and communication compression have exhibited their\ngreat potential in accelerating distributed machine learning by mitigating the\ncommunication bottleneck in practice. While existing decentralized algorithms\nwith communication compression mostly focus on the problems with only smooth\ncomponents, we study the decentralized stochastic composite optimization\nproblem with a potentially non-smooth component. A \\underline{Prox}imal\ngradient \\underline{L}in\\underline{EA}r convergent \\underline{D}ecentralized\nalgorithm with compression, Prox-LEAD, is proposed with rigorous theoretical\nanalyses in the general stochastic setting and the finite-sum setting. Our\ntheorems indicate that Prox-LEAD works with arbitrary compression precision,\nand it tremendously reduces the communication cost almost for free. The\nsuperiorities of the proposed algorithms are demonstrated through the\ncomparison with state-of-the-art algorithms in terms of convergence\ncomplexities and numerical experiments. Our algorithmic framework also\ngenerally enlightens the compressed communication on other primal-dual\nalgorithms by reducing the impact of inexact iterations, which might be of\nindependent interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kun Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An empirical investigation into audio pipeline approaches for classifying bird species. (arXiv:2108.04449v1 [cs.SD])","link":"http://arxiv.org/abs/2108.04449","description":"<p>This paper is an investigation into aspects of an audio classification\npipeline that will be appropriate for the monitoring of bird species on edges\ndevices. These aspects include transfer learning, data augmentation and model\noptimization. The hope is that the resulting models will be good candidates to\ndeploy on edge devices to monitor bird populations. Two classification\napproaches will be taken into consideration, one which explores the\neffectiveness of a traditional Deep Neural Network(DNN) and another that makes\nuse of Convolutional layers.This study aims to contribute empirical evidence of\nthe merits and demerits of each approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Behr_D/0/1/0/all/0/1\">David Behr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maina_C/0/1/0/all/0/1\">Ciira wa Maina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Quality Related Search Query Suggestions using Deep Reinforcement Learning. (arXiv:2108.04452v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04452","description":"<p>\"High Quality Related Search Query Suggestions\" task aims at recommending\nsearch queries which are real, accurate, diverse, relevant and engaging.\nObtaining large amounts of query-quality human annotations is expensive. Prior\nwork on supervised query suggestion models suffered from selection and exposure\nbias, and relied on sparse and noisy immediate user-feedback (e.g., clicks),\nleading to low quality suggestions. Reinforcement Learning techniques employed\nto reformulate a query using terms from search results, have limited\nscalability to large-scale industry applications. To recommend high quality\nrelated search queries, we train a Deep Reinforcement Learning model to predict\nthe query a user would enter next. The reward signal is composed of long-term\nsession-based user feedback, syntactic relatedness and estimated naturalness of\ngenerated query. Over the baseline supervised model, our proposed approach\nachieves a significant relative improvement in terms of recommendation\ndiversity (3%), down-stream user-engagement (4.2%) and per-sentence word\nrepetitions (82%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bodigutla_P/0/1/0/all/0/1\">Praveen Kumar Bodigutla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Reinforcement Learning for Demand Driven Services in Logistics and Transportation Systems: A Survey. (arXiv:2108.04462v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04462","description":"<p>Recent technology development brings the booming of numerous new\nDemand-Driven Services (DDS) into urban lives, including ridesharing, on-demand\ndelivery, express systems and warehousing. In DDS, a service loop is an\nelemental structure, including its service worker, the service providers and\ncorresponding service targets. The service workers should transport either\nhumans or parcels from the providers to the target locations. Various planning\ntasks within DDS can thus be classified into two individual stages: 1)\nDispatching, which is to form service loops from demand/supply distributions,\nand 2)Routing, which is to decide specific serving orders within the\nconstructed loops. Generating high-quality strategies in both stages is\nimportant to develop DDS but faces several challenging. Meanwhile, deep\nreinforcement learning (DRL) has been developed rapidly in recent years. It is\na powerful tool to solve these problems since DRL can learn a parametric model\nwithout relying on too many problem-based assumptions and optimize long-term\neffect by learning sequential decisions. In this survey, we first define DDS,\nthen highlight common applications and important decision/control problems\nwithin. For each problem, we comprehensively introduce the existing DRL\nsolutions, and further summarize them in\n\\textit{https://github.com/tsinghua-fib-lab/DDS\\_Survey}. We also introduce\nopen simulation environments for development and evaluation of DDS\napplications. Finally, we analyze remaining challenges and discuss further\nresearch opportunities in DRL solutions for DDS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Zefang Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Tao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Depeng/0/1/0/all/0/1\">Depeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computational complexity of Inexact Proximal Point Algorithm for Convex Optimization under Holderian Growth. (arXiv:2108.04482v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04482","description":"<p>Several decades ago the Proximal Point Algorithm (PPA) started to gain much\nattraction for both abstract operator theory and the numerical optimization\ncommunities. Even in modern applications, researchers still use proximal\nminimization theory to design scalable algorithms that overcome nonsmoothness\nin high dimensional models. Several remarkable references as\n\\cite{Fer:91,Ber:82constrained,Ber:89parallel,Tom:11} analyzed the tight local\nrelations between the convergence rate of PPA and the regularity of the\nobjective function. However, without taking into account the concrete\ncomputational effort paid for computing each PPA iteration, any iteration\ncomplexity remains abstract and purely informative. In this manuscript we aim\nto evaluate the computational complexity of practical PPA in terms of\n(proximal) gradient/subgradient iterations, which might allow a fair\npositioning of the famous PPA numerical performance in the class of first order\nmethods. First, we derive nonasymptotic iteration complexity estimates of exact\nand inexact PPA to minimize convex functions under $\\gamma-$Holderian growth:\n$\\BigO{\\log(1/\\epsilon)}$ (for $\\gamma \\in [1,2]$) and\n$\\BigO{1/\\epsilon^{\\gamma - 2}}$ (for $\\gamma &gt; 2$). In particular, we recover\nwell-known results on exact PPA: finite convergence for sharp minima and linear\nconvergence for quadratic growth, even under presence of inexactness. Second,\nassuming that an usual (proximal) gradient/subgradient method subroutine is\nemployed to compute inexact PPA iteration, we show novel computational\ncomplexity bounds on a restarted variant of the inexact PPA, available when no\ninformation on the growth of the objective function is known. In the numerical\nexperiments we confirm the practical performance and implementability of our\nschemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patrascu_A/0/1/0/all/0/1\">Andrei Patrascu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irofti_P/0/1/0/all/0/1\">Paul Irofti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularized Sequential Latent Variable Models with Adversarial Neural Networks. (arXiv:2108.04496v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04496","description":"<p>The recurrent neural networks (RNN) with richly distributed internal states\nand flexible non-linear transition functions, have overtaken the dynamic\nBayesian networks such as the hidden Markov models (HMMs) in the task of\nmodeling highly structured sequential data. These data, such as from speech and\nhandwriting, often contain complex relationships between the underlaying\nvariational factors and the observed data. The standard RNN model has very\nlimited randomness or variability in its structure, coming from the output\nconditional probability model. This paper will present different ways of using\nhigh level latent random variables in RNN to model the variability in the\nsequential data, and the training method of such RNN model under the VAE\n(Variational Autoencoder) principle. We will explore possible ways of using\nadversarial method to train a variational RNN model. Contrary to competing\napproaches, our approach has theoretical optimum in the model training and\nprovides better model training stability. Our approach also improves the\nposterior approximation in the variational inference network by a separated\nadversarial training step. Numerical results simulated from TIMIT speech data\nshow that reconstruction loss and evidence lower bound converge to the same\nlevel and adversarial training loss converges to 0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Ming Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Reinforcement Learning for Data Processing and Analytics. (arXiv:2108.04526v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04526","description":"<p>Data processing and analytics are fundamental and pervasive. Algorithms play\na vital role in data processing and analytics where many algorithm designs have\nincorporated heuristics and general rules from human knowledge and experience\nto improve their effectiveness. Recently, reinforcement learning, deep\nreinforcement learning (DRL) in particular, is increasingly explored and\nexploited in many areas because it can learn better strategies in complicated\nenvironments it is interacting with than statically designed algorithms.\nMotivated by this trend, we provide a comprehensive review of recent works\nfocusing on utilizing deep reinforcement learning to improve data processing\nand analytics. First, we present an introduction to key concepts, theories, and\nmethods in deep reinforcement learning. Next, we discuss deep reinforcement\nlearning deployment on database systems, facilitating data processing and\nanalytics in various aspects, including data organization, scheduling, tuning,\nand indexing. Then, we survey the application of deep reinforcement learning in\ndata processing and analytics, ranging from data preparation, natural language\ninterface to healthcare, fintech, etc. Finally, we discuss important open\nchallenges and future research directions of using deep reinforcement learning\nin data processing and analytics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1\">Qingpeng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yiyuan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongle Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-Granular Spatio-Temporal Graph Network for Skeleton-based Action Recognition. (arXiv:2108.04536v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04536","description":"<p>The task of skeleton-based action recognition remains a core challenge in\nhuman-centred scene understanding due to the multiple granularities and large\nvariation in human motion. Existing approaches typically employ a single neural\nrepresentation for different motion patterns, which has difficulty in capturing\nfine-grained action classes given limited training data. To address the\naforementioned problems, we propose a novel multi-granular spatio-temporal\ngraph network for skeleton-based action classification that jointly models the\ncoarse- and fine-grained skeleton motion patterns. To this end, we develop a\ndual-head graph network consisting of two interleaved branches, which enables\nus to extract features at two spatio-temporal resolutions in an effective and\nefficient manner. Moreover, our network utilises a cross-head communication\nstrategy to mutually enhance the representations of both heads. We conducted\nextensive experiments on three large-scale datasets, namely NTU RGB+D 60, NTU\nRGB+D 120, and Kinetics-Skeleton, and achieves the state-of-the-art performance\non all the benchmarks, which validates the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tailin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Known Operator Learning and Hybrid Machine Learning in Medical Imaging --- A Review of the Past, the Present, and the Future. (arXiv:2108.04543v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04543","description":"<p>In this article, we perform a review of the state-of-the-art of hybrid\nmachine learning in medical imaging. We start with a short summary of the\ngeneral developments of the past in machine learning and how general and\nspecialized approaches have been in competition in the past decades. A\nparticular focus will be the theoretical and experimental evidence pro and\ncontra hybrid modelling. Next, we inspect several new developments regarding\nhybrid machine learning with a particular focus on so-called known operator\nlearning and how hybrid approaches gain more and more momentum across\nessentially all applications in medical imaging and medical image analysis. As\nwe will point out by numerous examples, hybrid models are taking over in image\nreconstruction and analysis. Even domains such as physical simulation and\nscanner and acquisition design are being addressed using machine learning grey\nbox modelling approaches. Towards the end of the article, we will investigate a\nfew future directions and point out relevant areas in which hybrid modelling,\nmeta learning, and other domains will likely be able to drive the\nstate-of-the-art ahead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kostler_H/0/1/0/all/0/1\">Harald K&#xf6;stler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heisig_M/0/1/0/all/0/1\">Marco Heisig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauss_P/0/1/0/all/0/1\">Patrick Krauss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seung Hee Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABC-FL: Anomalous and Benign client Classification in Federated Learning. (arXiv:2108.04551v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04551","description":"<p>Federated Learning is a distributed machine learning framework designed for\ndata privacy preservation i.e., local data remain private throughout the entire\ntraining and testing procedure. Federated Learning is gaining popularity\nbecause it allows one to use machine learning techniques while preserving\nprivacy. However, it inherits the vulnerabilities and susceptibilities raised\nin deep learning techniques. For instance, Federated Learning is particularly\nvulnerable to data poisoning attacks that may deteriorate its performance and\nintegrity due to its distributed nature and inaccessibility to the raw data. In\naddition, it is extremely difficult to correctly identify malicious clients due\nto the non-Independently and/or Identically Distributed (non-IID) data. The\nreal-world data can be complex and diverse, making them hardly distinguishable\nfrom the malicious data without direct access to the raw data. Prior research\nhas focused on detecting malicious clients while treating only the clients\nhaving IID data as benign. In this study, we propose a method that detects and\nclassifies anomalous clients from benign clients when benign ones have non-IID\ndata. Our proposed method leverages feature dimension reduction, dynamic\nclustering, and cosine similarity-based clipping. The experimental results\nvalidates that our proposed method not only classifies the malicious clients\nbut also alleviates their negative influences from the entire procedure. Our\nfindings may be used in future studies to effectively eliminate anomalous\nclients when building a model with diverse data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hyejun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Joonyong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tai Myung Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Benefits of Implicit Regularization from SGD in Least Squares Problems. (arXiv:2108.04552v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04552","description":"<p>Stochastic gradient descent (SGD) exhibits strong algorithmic regularization\neffects in practice, which has been hypothesized to play an important role in\nthe generalization of modern machine learning approaches. In this work, we seek\nto understand these issues in the simpler setting of linear regression\n(including both underparameterized and overparameterized regimes), where our\ngoal is to make sharp instance-based comparisons of the implicit regularization\nafforded by (unregularized) average SGD with the explicit regularization of\nridge regression. For a broad class of least squares problem instances (that\nare natural in high-dimensional settings), we show: (1) for every problem\ninstance and for every ridge parameter, (unregularized) SGD, when provided with\nlogarithmically more samples than that provided to the ridge algorithm,\ngeneralizes no worse than the ridge solution (provided SGD uses a tuned\nconstant stepsize); (2) conversely, there exist instances (in this wide problem\nclass) where optimally-tuned ridge regression requires quadratically more\nsamples than SGD in order to have the same generalization performance. Taken\ntogether, our results show that, up to the logarithmic factors, the\ngeneralization performance of SGD is always no worse than that of ridge\nregression in a wide range of overparameterized problems, and, in fact, could\nbe much better for some problem instances. More generally, our results show how\nalgorithmic regularization has important consequences even in simpler\n(overparameterized) convex settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1\">Difan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jingfeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1\">Dean P. Foster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham M. Kakade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Driven VRP: A Neural Network Model to Learn Hidden Preferences for VRP. (arXiv:2108.04578v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04578","description":"<p>The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the\ntotal distance of the routes under the capacity constraints of the vehicles.\nBut more often, the objective involves multiple criteria including not only the\ntotal distance of the tour but also other factors such as travel costs, travel\ntime, and fuel consumption.Moreover, in reality, there are numerous implicit\npreferences ingrained in the minds of the route planners and the drivers.\nDrivers, for instance, have familiarity with certain neighborhoods and\nknowledge of the state of roads, and often consider the best places for rest\nand lunch breaks. This knowledge is difficult to formulate and balance when\noperational routing decisions have to be made. This motivates us to learn the\nimplicit preferences from past solutions and to incorporate these learned\npreferences in the optimization process. These preferences are in the form of\narc probabilities, i.e., the more preferred a route is, the higher is the joint\nprobability. The novelty of this work is the use of a neural network model to\nestimate the arc probabilities, which allows for additional features and\nautomatic parameter estimation. This first requires identifying suitable\nfeatures, neural architectures and loss functions, taking into account that\nthere is typically few data available. We investigate the difference with a\nprior weighted Markov counting approach, and study the applicability of neural\nnetworks in this setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1\">Jayanta Mandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canoy_R/0/1/0/all/0/1\">Rocsildes Canoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucarey_V/0/1/0/all/0/1\">V&#xed;ctor Bucarey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1\">Tias Guns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniNet: A Unified Scene Understanding Network and Exploring Multi-Task Relationships through the Lens of Adversarial Attacks. (arXiv:2108.04584v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04584","description":"<p>Scene understanding is crucial for autonomous systems which intend to operate\nin the real world. Single task vision networks extract information only based\non some aspects of the scene. In multi-task learning (MTL), on the other hand,\nthese single tasks are jointly learned, thereby providing an opportunity for\ntasks to share information and obtain a more comprehensive understanding. To\nthis end, we develop UniNet, a unified scene understanding network that\naccurately and efficiently infers vital vision tasks including object\ndetection, semantic segmentation, instance segmentation, monocular depth\nestimation, and monocular instance depth prediction. As these tasks look at\ndifferent semantic and geometric information, they can either complement or\nconflict with each other. Therefore, understanding inter-task relationships can\nprovide useful cues to enable complementary information sharing. We evaluate\nthe task relationships in UniNet through the lens of adversarial attacks based\non the notion that they can exploit learned biases and task interactions in the\nneural network. Extensive experiments on the Cityscapes dataset, using\nuntargeted and targeted attacks reveal that semantic tasks strongly interact\namongst themselves, and the same holds for geometric tasks. Additionally, we\nshow that the relationship between semantic and geometric tasks is asymmetric\nand their interaction becomes weaker as we move towards higher-level\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gurulingan_N/0/1/0/all/0/1\">NareshKumar Gurulingan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent neural network-based Internal Model Control of unknown nonlinear stable systems. (arXiv:2108.04585v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04585","description":"<p>Owing to their superior modeling capabilities, gated Recurrent Neural\nNetworks (RNNs), such as Gated Recurrent Units (GRUs) and Long Short-Term\nMemory networks (LSTMs), have become popular tools for learning dynamical\nsystems. This paper aims to discuss how these networks can be adopted for the\nsynthesis of Internal Model Control (IMC) architectures. To this end, a first\ngated RNN is used to learn a model of the unknown input-output stable plant.\nThen, another gated RNN approximating the model inverse is trained. The\nproposed scheme is able to cope with the saturation of the control variables,\nand it can be deployed on low-power embedded controllers since it does not\nrequire any online computation. The approach is then tested on the Quadruple\nTank benchmark system, resulting in satisfactory closed-loop performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonassi_F/0/1/0/all/0/1\">Fabio Bonassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scattolini_R/0/1/0/all/0/1\">Riccardo Scattolini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Learning and Testing Decision Tree. (arXiv:2108.04587v1 [cs.DS])","link":"http://arxiv.org/abs/2108.04587","description":"<p>In this paper, we study learning and testing decision tree of size and depth\nthat are significantly smaller than the number of attributes $n$.\n</p>\n<p>Our main result addresses the problem of poly$(n,1/\\epsilon)$ time algorithms\nwith poly$(s,1/\\epsilon)$ query complexity (independent of $n$) that\ndistinguish between functions that are decision trees of size $s$ from\nfunctions that are $\\epsilon$-far from any decision tree of size\n$\\phi(s,1/\\epsilon)$, for some function $\\phi &gt; s$. The best known result is\nthe recent one that follows from Blank, Lange and Tan,~\\cite{BlancLT20}, that\ngives $\\phi(s,1/\\epsilon)=2^{O((\\log^3s)/\\epsilon^3)}$. In this paper, we give\na new algorithm that achieves $\\phi(s,1/\\epsilon)=2^{O(\\log^2 (s/\\epsilon))}$.\n</p>\n<p>Moreover, we study the testability of depth-$d$ decision tree and give a {\\it\ndistribution free} tester that distinguishes between depth-$d$ decision tree\nand functions that are $\\epsilon$-far from depth-$d^2$ decision tree. In\nparticular, for decision trees of size $s$, the above result holds in the\ndistribution-free model when the tree depth is $O(\\log(s/\\epsilon))$.\n</p>\n<p>We also give other new results in learning and testing of size-$s$ decision\ntrees and depth-$d$ decision trees that follow from results in the literature\nand some results we prove in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bshouty_N/0/1/0/all/0/1\">Nader H. Bshouty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_Zaknoon_C/0/1/0/all/0/1\">Catherine A. Haddad-Zaknoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-informed Graph Structure Learning for Node Classification. (arXiv:2108.04595v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04595","description":"<p>Graph Neural Networks (GNNs) have achieved great success among various\ndomains. Nevertheless, most GNN methods are sensitive to the quality of graph\nstructures. To tackle this problem, some studies exploit different graph\nstructure learning strategies to refine the original graph structure. However,\nthese methods only consider feature information while ignoring available label\ninformation. In this paper, we propose a novel label-informed graph structure\nlearning framework which incorporates label information explicitly through a\nclass transition matrix. We conduct extensive experiments on seven node\nclassification benchmark datasets and the results show that our method\noutperforms or matches the state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fenyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions. (arXiv:2108.04620v1 [math.OC])","link":"http://arxiv.org/abs/2108.04620","description":"<p>Gradient descent (GD) type optimization methods are the standard instrument\nto train artificial neural networks (ANNs) with rectified linear unit (ReLU)\nactivation. Despite the great success of GD type optimization methods in\nnumerical simulations for the training of ANNs with ReLU activation, it remains\n- even in the simplest situation of the plain vanilla GD optimization method\nwith random initializations and ANNs with one hidden layer - an open problem to\nprove (or disprove) the conjecture that the risk of the GD optimization method\nconverges in the training of such ANNs to zero as the width of the ANNs, the\nnumber of independent random initializations, and the number of GD steps\nincrease to infinity. In this article we prove this conjecture in the situation\nwhere the probability distribution of the input data is equivalent to the\ncontinuous uniform distribution on a compact interval, where the probability\ndistributions for the random initializations of the ANN parameters are standard\nnormal distributions, and where the target function under consideration is\ncontinuous and piecewise affine linear. Roughly speaking, the key ingredients\nin our mathematical convergence analysis are (i) to prove that suitable sets of\nglobal minima of the risk functions are \\emph{twice continuously differentiable\nsubmanifolds of the ANN parameter spaces}, (ii) to prove that the Hessians of\nthe risk functions on these sets of global minima satisfy an appropriate\n\\emph{maximal rank condition}, and, thereafter, (iii) to apply the machinery in\n[Fehrman, B., Gess, B., Jentzen, A., Convergence rates for the stochastic\ngradient descent method for non-convex objective functions. J. Mach. Learn.\nRes. 21(136): 1--48, 2020] to establish convergence of the GD optimization\nmethod with random initializations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Riekert_A/0/1/0/all/0/1\">Adrian Riekert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Maximize Influence. (arXiv:2108.04623v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04623","description":"<p>As the field of machine learning for combinatorial optimization advances,\ntraditional problems are resurfaced and readdressed through this new\nperspective. The overwhelming majority of the literature focuses on small graph\nproblems, while several real-world problems are devoted to large graphs. Here,\nwe focus on two such problems that are related: influence estimation, a\n\\#P-hard counting problem, and influence maximization, an NP-hard problem. We\ndevelop GLIE, a Graph Neural Network (GNN) that inherently parameterizes an\nupper bound of influence estimation and train it on small simulated graphs.\nExperiments show that GLIE can provide accurate predictions faster than the\nalternatives for graphs 10 times larger than the train set. More importantly,\nit can be used on arbitrary large graphs for influence maximization, as the\npredictions can rank effectively seed sets even when the accuracy deteriorates.\nTo showcase this, we propose a version of a standard Influence Maximization\n(IM) algorithm where we substitute traditional influence estimation with the\npredictions of GLIE.We also transfer GLIE into a reinforcement learning model\nthat learns how to choose seeds to maximize influence sequentially using GLIE's\nhidden representations and predictions. The final results show that the\nproposed methods surpasses a previous GNN-RL approach and perform on par with a\nstate-of-the-art IM algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulos_G/0/1/0/all/0/1\">George Panagopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tziortziotis_N/0/1/0/all/0/1\">Nikolaos Tziortziotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1\">Fragkiskos D. Malliaros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Latent Relation Modeling for Collaborative Metric Learning. (arXiv:2108.04655v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04655","description":"<p>Collaborative Metric Learning (CML) recently emerged as a powerful paradigm\nfor recommendation based on implicit feedback collaborative filtering. However,\nstandard CML methods learn fixed user and item representations, which fails to\ncapture the complex interests of users. Existing extensions of CML also either\nignore the heterogeneity of user-item relations, i.e. that a user can\nsimultaneously like very different items, or the latent item-item relations,\ni.e. that a user's preference for an item depends, not only on its intrinsic\ncharacteristics, but also on items they previously interacted with. In this\npaper, we present a hierarchical CML model that jointly captures latent\nuser-item and item-item relations from implicit data. Our approach is inspired\nby translation mechanisms from knowledge graph embedding and leverages\nmemory-based attention networks. We empirically show the relevance of this\njoint relational modeling, by outperforming existing CML models on\nrecommendation tasks on several real-world datasets. Our experiments also\nemphasize the limits of current CML relational models on very sparse datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salha_Galvan_G/0/1/0/all/0/1\">Guillaume Salha-Galvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1\">Manuel Moussallam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Analysis on Effectiveness of NLP Methods for Predicting Code Smell. (arXiv:2108.04656v1 [cs.SE])","link":"http://arxiv.org/abs/2108.04656","description":"<p>A code smell is a surface indicator of an inherent problem in the system,\nmost often due to deviation from standard coding practices on the developers\npart during the development phase. Studies observe that code smells made the\ncode more susceptible to call for modifications and corrections than code that\ndid not contain code smells. Restructuring the code at the early stage of\ndevelopment saves the exponentially increasing amount of effort it would\nrequire to address the issues stemming from the presence of these code smells.\nInstead of using traditional features to detect code smells, we use user\ncomments to manually construct features to predict code smells. We use three\nExtreme learning machine kernels over 629 packages to identify eight code\nsmells by leveraging feature engineering aspects and using sampling techniques.\nOur findings indicate that the radial basis functional kernel performs best out\nof the three kernel methods with a mean accuracy of 98.52.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulanikar_A/0/1/0/all/0/1\">Abhiram Anand Gulanikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lov Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1\">Lalita Bhanu Murthy Neti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-Net-and-a-half: Convolutional network for biomedical image segmentation using multiple expert-driven annotations. (arXiv:2108.04658v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04658","description":"<p>Development of deep learning systems for biomedical segmentation often\nrequires access to expert-driven, manually annotated datasets. If more than a\nsingle expert is involved in the annotation of the same images, then the\ninter-expert agreement is not necessarily perfect, and no single expert\nannotation can precisely capture the so-called ground truth of the regions of\ninterest on all images. Also, it is not trivial to generate a reference\nestimate using annotations from multiple experts. Here we present a deep neural\nnetwork, defined as U-Net-and-a-half, which can simultaneously learn from\nannotations performed by multiple experts on the same set of images.\nU-Net-and-a-half contains a convolutional encoder to generate features from the\ninput images, multiple decoders that allow simultaneous learning from image\nmasks obtained from annotations that were independently generated by multiple\nexperts, and a shared low-dimensional feature space. To demonstrate the\napplicability of our framework, we used two distinct datasets from digital\npathology and radiology, respectively. Specifically, we trained two separate\nmodels using pathologist-driven annotations of glomeruli on whole slide images\nof human kidney biopsies (10 patients), and radiologist-driven annotations of\nlumen cross-sections of human arteriovenous fistulae obtained from\nintravascular ultrasound images (10 patients), respectively. The models based\non U-Net-and-a-half exceeded the performance of the traditional U-Net models\ntrained on single expert annotations alone, thus expanding the scope of\nmultitask learning in the context of biomedical image segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1\">Jesper Kers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassol_C/0/1/0/all/0/1\">Clarissa A. Cassol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1\">Joris J. Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idrees_N/0/1/0/all/0/1\">Najia Idrees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_A/0/1/0/all/0/1\">Alik Farber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haroon_S/0/1/0/all/0/1\">Samir Haroon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daly_K/0/1/0/all/0/1\">Kevin P. Daly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Suvranu Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitalia_V/0/1/0/all/0/1\">Vipul C. Chitalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolachalama_V/0/1/0/all/0/1\">Vijaya B. Kolachalama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on Predictability of Software Code Smell Using Deep Learning Models. (arXiv:2108.04659v1 [cs.SE])","link":"http://arxiv.org/abs/2108.04659","description":"<p>Code Smell, similar to a bad smell, is a surface indication of something\ntainted but in terms of software writing practices. This metric is an\nindication of a deeper problem lies within the code and is associated with an\nissue which is prominent to experienced software developers with acceptable\ncoding practices. Recent studies have often observed that codes having code\nsmells are often prone to a higher probability of change in the software\ndevelopment cycle. In this paper, we developed code smell prediction models\nwith the help of features extracted from source code to predict eight types of\ncode smell. Our work also presents the application of data sampling techniques\nto handle class imbalance problem and feature selection techniques to find\nrelevant feature sets. Previous studies had made use of techniques such as\nNaive - Bayes and Random forest but had not explored deep learning methods to\npredict code smell. A total of 576 distinct Deep Learning models were trained\nusing the features and datasets mentioned above. The study concluded that the\ndeep learning models which used data from Synthetic Minority Oversampling\nTechnique gave better results in terms of accuracy, AUC with the accuracy of\nsome models improving from 88.47 to 96.84.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1\">Himanshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_T/0/1/0/all/0/1\">Tanmay G. Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_L/0/1/0/all/0/1\">Lov Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neti_L/0/1/0/all/0/1\">Lalita Bhanu Murthy Neti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Aneesh Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChemiRise: a data-driven retrosynthesis engine. (arXiv:2108.04682v1 [physics.chem-ph])","link":"http://arxiv.org/abs/2108.04682","description":"<p>We have developed an end-to-end, retrosynthesis system, named ChemiRise, that\ncan propose complete retrosynthesis routes for organic compounds rapidly and\nreliably. The system was trained on a processed patent database of over 3\nmillion organic reactions. Experimental reactions were atom-mapped, clustered,\nand extracted into reaction templates. We then trained a graph convolutional\nneural network-based one-step reaction proposer using template embeddings and\ndeveloped a guiding algorithm on the directed acyclic graph (DAG) of chemical\ncompounds to find the best candidate to explore. The atom-mapping algorithm and\nthe one-step reaction proposer were benchmarked against previous studies and\nshowed better results. The final product was demonstrated by retrosynthesis\nroutes reviewed and rated by human experts, showing satisfying functionality\nand a potential productivity boost in real-life use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Sun_X/0/1/0/all/0/1\">Xiangyan Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lin_Y/0/1/0/all/0/1\">Yuquan Lin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wu_L/0/1/0/all/0/1\">Lingjie Wu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xing_H/0/1/0/all/0/1\">Haoming Xing</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gao_M/0/1/0/all/0/1\">Minghong Gao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tan_S/0/1/0/all/0/1\">Suocheng Tan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ni_Z/0/1/0/all/0/1\">Zekun Ni</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Han_Q/0/1/0/all/0/1\">Qi Han</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wu_J/0/1/0/all/0/1\">Junqiu Wu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fan_J/0/1/0/all/0/1\">Jie Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Transition State Calculation. (arXiv:2108.04698v1 [stat.ML])","link":"http://arxiv.org/abs/2108.04698","description":"<p>The transition state (TS) calculation is a grand challenge for computational\nintensive energy function. The traditional methods need to evaluate the\ngradients of the energy function at a very large number of locations. To reduce\nthe number of expensive computations of the true gradients, we propose an\nactive learning framework consisting of a statistical surrogate model, Gaussian\nprocess regression (GPR) for the energy function, and a single-walker dynamics\nmethod, gentle accent dynamics (GAD), for the saddle-type transition states. TS\nis detected by the GAD applied to the GPR surrogate for the gradient vector and\nthe Hessian matrix. Our key ingredient for efficiency improvements is an active\nlearning method which sequentially designs the most informative locations and\ntakes evaluations of the original model at these locations to train GPR. We\nformulate this active learning task as the optimal experimental design problem\nand propose a very efficient sample-based sub-optimal criterion to construct\nthe optimal locations. We show that the new method significantly decreases the\nrequired number of energy or force evaluations of the original model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Gu_S/0/1/0/all/0/1\">Shuting Gu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_H/0/1/0/all/0/1\">Hongqiao Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRECODE - A Generic Model Extension to Prevent Deep Gradient Leakage. (arXiv:2108.04725v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04725","description":"<p>Collaborative training of neural networks leverages distributed data by\nexchanging gradient information between different clients. Although training\ndata entirely resides with the clients, recent work shows that training data\ncan be reconstructed from such exchanged gradient information. To enhance\nprivacy, gradient perturbation techniques have been proposed. However, they\ncome at the cost of reduced model performance, increased convergence time, or\nincreased data demand. In this paper, we introduce PRECODE, a PRivacy EnhanCing\nmODulE that can be used as generic extension for arbitrary model architectures.\nWe propose a simple yet effective realization of PRECODE using variational\nmodeling. The stochastic sampling induced by variational modeling effectively\nprevents privacy leakage from gradients and in turn preserves privacy of data\nowners. We evaluate PRECODE using state of the art gradient inversion attacks\non two different model architectures trained on three datasets. In contrast to\ncommonly used defense mechanisms, we find that our proposed modification\nconsistently reduces the attack success rate to 0% while having almost no\nnegative impact on model training and final performance. As a result, PRECODE\nreveals a promising path towards privacy enhancing model extensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scheliga_D/0/1/0/all/0/1\">Daniel Scheliga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mader_P/0/1/0/all/0/1\">Patrick M&#xe4;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeland_M/0/1/0/all/0/1\">Marco Seeland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crowdsourced Databases and Sui Generis Rights. (arXiv:2108.04727v1 [cs.DB])","link":"http://arxiv.org/abs/2108.04727","description":"<p>In this study we propose a new concept of databases (crowdsourced databases),\nadding a new conceptual approach to the debate on legal protection of databases\nin Europe. We also summarise the current legal framework and current indexing\nand web scraping practices - it would not be prudent to suggest a new theory\nwithout contextualising it in the legal and practical context in which it is\ndeveloped.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almeida_G/0/1/0/all/0/1\">Gon&#xe7;alo Sim&#xf5;es de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abreu_G/0/1/0/all/0/1\">Gon&#xe7;alo Faria Abreu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correlation Clustering Reconstruction in Semi-Adversarial Models. (arXiv:2108.04729v1 [cs.DS])","link":"http://arxiv.org/abs/2108.04729","description":"<p>Correlation Clustering is an important clustering problem with many\napplications. We study the reconstruction version of this problem in which one\nis seeking to reconstruct a latent clustering that has been corrupted by random\nnoise and adversarial modifications.\n</p>\n<p>Concerning the latter, we study a standard \"post-adversarial\" model, in which\nadversarial modifications come after the noise, and also introduce and analyze\na \"pre-adversarial\" model in which adversarial modifications come before the\nnoise. Given an input coming from such a semi-adversarial generative model, the\ngoal is to reconstruct almost perfectly and with high probability the latent\nclustering.\n</p>\n<p>We focus on the case where the hidden clusters have equal size and show the\nfollowing. In the pre-adversarial setting, spectral algorithms are optimal, in\nthe sense that they reconstruct all the way to the information-theoretic\nthreshold beyond which no reconstruction is possible. In contrast, in the\npost-adversarial setting their ability to restore the hidden clusters stops\nbefore the threshold, but the gap is optimally filled by SDP-based algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chierichetti_F/0/1/0/all/0/1\">Flavio Chierichetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panconesi_A/0/1/0/all/0/1\">Alessandro Panconesi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_G/0/1/0/all/0/1\">Giuseppe Re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trevisan_L/0/1/0/all/0/1\">Luca Trevisan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Factors Aware Dual-Attentional Knowledge Tracing. (arXiv:2108.04741v1 [cs.AI])","link":"http://arxiv.org/abs/2108.04741","description":"<p>With the increasing demands of personalized learning, knowledge tracing has\nbecome important which traces students' knowledge states based on their\nhistorical practices. Factor analysis methods mainly use two kinds of factors\nwhich are separately related to students and questions to model students'\nknowledge states. These methods use the total number of attempts of students to\nmodel students' learning progress and hardly highlight the impact of the most\nrecent relevant practices. Besides, current factor analysis methods ignore rich\ninformation contained in questions. In this paper, we propose Multi-Factors\nAware Dual-Attentional model (MF-DAKT) which enriches question representations\nand utilizes multiple factors to model students' learning progress based on a\ndual-attentional mechanism. More specifically, we propose a novel\nstudent-related factor which records the most recent attempts on relevant\nconcepts of students to highlight the impact of recent exercises. To enrich\nquestions representations, we use a pre-training method to incorporate two\nkinds of question information including questions' relation and difficulty\nlevel. We also add a regularization term about questions' difficulty level to\nrestrict pre-trained question representations to fine-tuning during the process\nof predicting students' performance. Moreover, we apply a dual-attentional\nmechanism to differentiate contributions of factors and factor interactions to\nfinal prediction in different practice records. At last, we conduct experiments\non several real-world datasets and results show that MF-DAKT can outperform\nexisting knowledge tracing methods. We also conduct several studies to validate\nthe effects of each component of MF-DAKT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Moyu Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinning Zhu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunhong Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yang Ji</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Feng Pan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Changchuan Yin</a> (1) ((1) Beijing University of Posts and Telecommunications)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The information of attribute uncertainties: what convolutional neural networks can learn about errors in input data. (arXiv:2108.04742v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04742","description":"<p>Errors in measurements are key to weighting the value of data, but are often\nneglected in Machine Learning (ML). We show how Convolutional Neural Networks\n(CNNs) are able to learn about the context and patterns of signal and noise,\nleading to improvements in the performance of classification methods. We\nconstruct a model whereby two classes of objects follow an underlying Gaussian\ndistribution, and where the features (the input data) have varying, but known,\nlevels of noise. This model mimics the nature of scientific data sets, where\nthe noises arise as realizations of some random processes whose underlying\ndistributions are known. The classification of these objects can then be\nperformed using standard statistical techniques (e.g., least-squares\nminimization or Markov-Chain Monte Carlo), as well as ML techniques. This\nallows us to take advantage of a maximum likelihood approach to object\nclassification, and to measure the amount by which the ML methods are\nincorporating the information in the input data uncertainties. We show that,\nwhen each data point is subject to different levels of noise (i.e., noises with\ndifferent distribution functions), that information can be learned by the CNNs,\nraising the ML performance to at least the same level of the least-squares\nmethod -- and sometimes even surpassing it. Furthermore, we show that, with\nvarying noise levels, the confidence of the ML classifiers serves as a proxy\nfor the underlying cumulative distribution function, but only if the\ninformation about specific input data uncertainties is provided to the CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_N/0/1/0/all/0/1\">Nat&#xe1;lia V. N. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abramo_L/0/1/0/all/0/1\">L. Raul Abramo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirata_N/0/1/0/all/0/1\">Nina S. Hirata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedPAGE: A Fast Local Stochastic Gradient Method for Communication-Efficient Federated Learning. (arXiv:2108.04755v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04755","description":"<p>Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)\nis a classical federated learning algorithm in which clients run multiple local\nSGD steps before communicating their update to an orchestrating server. We\npropose a new federated learning algorithm, FedPAGE, able to further reduce the\ncommunication complexity by utilizing the recent optimal PAGE method (Li et\nal., 2021) instead of plain SGD in FedAvg. We show that FedPAGE uses much fewer\ncommunication rounds than previous local methods for both federated convex and\nnonconvex optimization. Concretely, 1) in the convex setting, the number of\ncommunication rounds of FedPAGE is $O(\\frac{N^{3/4}}{S\\epsilon})$, improving\nthe best-known result $O(\\frac{N}{S\\epsilon})$ of SCAFFOLD (Karimireddy et\nal.,2020) by a factor of $N^{1/4}$, where $N$ is the total number of clients\n(usually is very large in federated learning), $S$ is the sampled subset of\nclients in each communication round, and $\\epsilon$ is the target error; 2) in\nthe nonconvex setting, the number of communication rounds of FedPAGE is\n$O(\\frac{\\sqrt{N}+S}{S\\epsilon^2})$, improving the best-known result\n$O(\\frac{N^{2/3}}{S^{2/3}\\epsilon^2})$ of SCAFFOLD (Karimireddy et al.,2020) by\na factor of $N^{1/6}S^{1/3}$, if the sampled clients $S\\leq \\sqrt{N}$. Note\nthat in both settings, the communication cost for each round is the same for\nboth FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new state-of-the-art\nresults in terms of communication complexity for both federated convex and\nnonconvex optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhize Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imitation Learning by Reinforcement Learning. (arXiv:2108.04763v1 [stat.ML])","link":"http://arxiv.org/abs/2108.04763","description":"<p>Imitation Learning algorithms learn a policy from demonstrations of expert\nbehavior. Somewhat counterintuitively, we show that, for deterministic experts,\nimitation learning can be done by reduction to reinforcement learning, which is\ncommonly considered more difficult. We conduct experiments which confirm that\nour reduction works well in practice for a continuous control task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Ciosek_K/0/1/0/all/0/1\">Kamil Ciosek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bandit Algorithms for Precision Medicine. (arXiv:2108.04782v1 [stat.ML])","link":"http://arxiv.org/abs/2108.04782","description":"<p>The Oxford English Dictionary defines precision medicine as \"medical care\ndesigned to optimize efficiency or therapeutic benefit for particular groups of\npatients, especially by using genetic or molecular profiling.\" It is not an\nentirely new idea: physicians from ancient times have recognized that medical\ntreatment needs to consider individual variations in patient characteristics.\nHowever, the modern precision medicine movement has been enabled by a\nconfluence of events: scientific advances in fields such as genetics and\npharmacology, technological advances in mobile devices and wearable sensors,\nand methodological advances in computing and data sciences.\n</p>\n<p>This chapter is about bandit algorithms: an area of data science of special\nrelevance to precision medicine. With their roots in the seminal work of\nBellman, Robbins, Lai and others, bandit algorithms have come to occupy a\ncentral place in modern data science ( Lattimore and Szepesvari, 2020). Bandit\nalgorithms can be used in any situation where treatment decisions need to be\nmade to optimize some health outcome. Since precision medicine focuses on the\nuse of patient characteristics to guide treatment, contextual bandit algorithms\nare especially useful since they are designed to take such information into\naccount. The role of bandit algorithms in areas of precision medicine such as\nmobile health and digital phenotyping has been reviewed before (Tewari and\nMurphy, 2017; Rabbi et al., 2019). Since these reviews were published, bandit\nalgorithms have continued to find uses in mobile health and several new topics\nhave emerged in the research on bandit algorithms. This chapter is written for\nquantitative researchers in fields such as statistics, machine learning, and\noperations research who might be interested in knowing more about the\nalgorithmic and mathematical details of bandit algorithms that have been used\nin mobile health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1\">Yangyi Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1\">Ziping Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1\">Ambuj Tewari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Effects of The COVID-19 Pandemic on Road Traffic Safety: The Cases of New York City, Los Angeles, and Boston. (arXiv:2108.04787v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04787","description":"<p>The COVID-19 pandemic has resulted in significant social and economic impacts\nthroughout the world. In addition to the health consequences, the impacts on\ntraffic behaviors have also been sudden and dramatic. We have analyzed how the\nroad traffic safety of New York City, Los Angeles, and Boston in the U.S. have\nbeen impacted by the pandemic and corresponding local government orders and\nrestrictions. To be specific, we have studied the accident hotspots'\ndistributions before and after the outbreak of the pandemic and found that\ntraffic accidents have shifted in both location and time compared to previous\nyears. In addition, we have studied the road network characteristics in those\nhotspot regions with the hope to understand the underlying cause of the hotspot\nshifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karadla_L/0/1/0/all/0/1\">Lahari Karadla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weizi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-repository of screening mammography classifiers. (arXiv:2108.04800v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04800","description":"<p>Artificial intelligence (AI) is transforming medicine and showing promise in\nimproving clinical diagnosis. In breast cancer screening, several recent\nstudies show that AI has the potential to improve radiologists' accuracy,\nsubsequently helping in early cancer diagnosis and reducing unnecessary workup.\nAs the number of proposed models and their complexity grows, it is becoming\nincreasingly difficult to re-implement them in order to reproduce the results\nand to compare different approaches. To enable reproducibility of research in\nthis application area and to enable comparison between different methods, we\nrelease a meta-repository containing deep learning models for classification of\nscreening mammograms. This meta-repository creates a framework that enables the\nevaluation of machine learning models on any private or public screening\nmammography data set. At its inception, our meta-repository contains five\nstate-of-the-art models with open-source implementations and cross-platform\ncompatibility. We compare their performance on five international data sets:\ntwo private New York University breast cancer screening data sets as well as\nthree public (DDSM, INbreast and Chinese Mammography Database) data sets. Our\nframework has a flexible design that can be generalized to other medical image\nanalysis tasks. The meta-repository is available at\nhttps://www.github.com/nyukat/mammography_metarepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stadnick_B/0/1/0/all/0/1\">Benjamin Stadnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Witowski_J/0/1/0/all/0/1\">Jan Witowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajiv_V/0/1/0/all/0/1\">Vishwaesh Rajiv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chledowski_J/0/1/0/all/0/1\">Jakub Ch&#x142;&#x119;dowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamout_F/0/1/0/all/0/1\">Farah E. Shamout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1\">Krzysztof J. Geras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spiderweb nanomechanical resonators via Bayesian optimization: inspired by nature and guided by machine learning. (arXiv:2108.04809v1 [cond-mat.mes-hall])","link":"http://arxiv.org/abs/2108.04809","description":"<p>From ultra-sensitive detectors of fundamental forces to quantum networks and\nsensors, mechanical resonators are enabling next-generation technologies to\noperate in room temperature environments. Currently, silicon nitride\nnanoresonators stand as a leading microchip platform in these advances by\nallowing for mechanical resonators whose motion is remarkably isolated from\nambient thermal noise. However, to date, human intuition has remained the\ndriving force behind design processes. Here, inspired by nature and guided by\nmachine learning, a spiderweb nanomechanical resonator is developed that\nexhibits vibration modes which are isolated from ambient thermal environments\nvia a novel \"torsional soft-clamping\" mechanism discovered by the data-driven\noptimization algorithm. This bio-inspired resonator is then fabricated;\nexperimentally confirming a new paradigm in mechanics with quality factors\nabove 1 billion in room temperature environments. In contrast to other\nstate-of-the-art resonators, this milestone is achieved with a compact design\nwhich does not require sub-micron lithographic features or complex phononic\nbandgaps, making it significantly easier and cheaper to manufacture at large\nscales. Here we demonstrate the ability of machine learning to work in tandem\nwith human intuition to augment creative possibilities and uncover new\nstrategies in computing and nanotechnology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Shin_D/0/1/0/all/0/1\">Dongil Shin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Cupertino_A/0/1/0/all/0/1\">Andrea Cupertino</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jong_M/0/1/0/all/0/1\">Matthijs H. J. de Jong</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Steeneken_P/0/1/0/all/0/1\">Peter G. Steeneken</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Bessa_M/0/1/0/all/0/1\">Miguel A. Bessa</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Norte_R/0/1/0/all/0/1\">Richard A. Norte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary Complex Neural Network Acceleration on FPGA. (arXiv:2108.04811v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04811","description":"<p>Being able to learn from complex data with phase information is imperative\nfor many signal processing applications. Today' s real-valued deep neural\nnetworks (DNNs) have shown efficiency in latent information analysis but fall\nshort when applied to the complex domain. Deep complex networks (DCN), in\ncontrast, can learn from complex data, but have high computational costs;\ntherefore, they cannot satisfy the instant decision-making requirements of many\ndeployable systems dealing with short observations or short signal bursts.\nRecent, Binarized Complex Neural Network (BCNN), which integrates DCNs with\nbinarized neural networks (BNN), shows great potential in classifying complex\ndata in real-time. In this paper, we propose a structural pruning based\naccelerator of BCNN, which is able to provide more than 5000 frames/s inference\nthroughput on edge devices. The high performance comes from both the algorithm\nand hardware sides. On the algorithm side, we conduct structural pruning to the\noriginal BCNN models and obtain 20 $\\times$ pruning rates with negligible\naccuracy loss; on the hardware side, we propose a novel 2D convolution\noperation accelerator for the binary complex neural network. Experimental\nresults show that the proposed design works with over 90% utilization and is\nable to achieve the inference throughput of 5882 frames/s and 4938 frames/s for\ncomplex NIN-Net and ResNet-18 using CIFAR-10 dataset and Alveo U280 Board.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hongwu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weitze_S/0/1/0/all/0/1\">Scott Weitze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sahidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_T/0/1/0/all/0/1\">Tong Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Minghu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mimi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. (arXiv:2108.04812v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04812","description":"<p>We study continual learning for natural language instruction generation, by\nobserving human users' instruction execution. We focus on a collaborative\nscenario, where the system both acts and delegates tasks to human users using\nnatural language. We compare user execution of generated instructions to the\noriginal system intent as an indication to the system's success communicating\nits intent. We show how to use this signal to improve the system's ability to\ngenerate instructions via contextual bandit learning. In interaction with real\nusers, our system demonstrates dramatic improvements in its ability to generate\nlanguage over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_N/0/1/0/all/0/1\">Noriyuki Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R4Dyn: Exploring Radar for Self-Supervised Monocular Depth Estimation of Dynamic Scenes. (arXiv:2108.04814v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04814","description":"<p>While self-supervised monocular depth estimation in driving scenarios has\nachieved comparable performance to supervised approaches, violations of the\nstatic world assumption can still lead to erroneous depth predictions of\ntraffic participants, posing a potential safety issue. In this paper, we\npresent R4Dyn, a novel set of techniques to use cost-efficient radar data on\ntop of a self-supervised depth estimation framework. In particular, we show how\nradar can be used during training as weak supervision signal, as well as an\nextra input to enhance the estimation robustness at inference time. Since\nautomotive radars are readily available, this allows to collect training data\nfrom a variety of existing vehicles. Moreover, by filtering and expanding the\nsignal to make it compatible with learning-based approaches, we address radar\ninherent issues, such as noise and sparsity. With R4Dyn we are able to overcome\na major limitation of self-supervised depth estimation, i.e. the prediction of\ntraffic participants. We substantially improve the estimation on dynamic\nobjects, such as cars by 37% on the challenging nuScenes dataset, hence\ndemonstrating that radar is a valuable additional sensor for monocular depth\nestimation in autonomous vehicles. Additionally, we plan on making the code\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gasperini_S/0/1/0/all/0/1\">Stefano Gasperini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_P/0/1/0/all/0/1\">Patrick Koch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1\">Vinzenz Dallabetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel Compressible Adaptive Spectral Mixture Kernels for Gaussian Processes with Sparse Time and Phase Delay Structures. (arXiv:1808.00560v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1808.00560","description":"<p>Spectral mixture (SM) kernels comprise a powerful class of kernels for\nGaussian processes (GPs) capable of discovering structurally complex patterns\nand modeling negative covariances. Being a linear superposition of\nquasi-periodical kernel components, the state-of-the-art SM kernel does not\nconsider component compression and dependency structures between components. In\nthis paper, we investigate the benefits of component compression and modeling\nof both time and phase delay structures between basis components in the SM\nkernel. By verifying the presence of dependencies between function components\nusing Gaussian conditionals and posterior covariance, we first propose a new SM\nkernel variant with a time and phase delay dependency structure (SMD) and then\nprovide a structure adaptation (SA) algorithm for the SMD. The SMD kernel is\nconstructed in two steps: first, time delay and phase delay are incorporated\ninto each basis component; next, cross-convolution between a basis component\nand the reversed complex conjugate of another basis component is performed,\nwhich yields a complex-valued and positive definite kernel incorporating\ndependency structures between basis components. The model compression and\ndependency sparsity of the SMD kernel can be obtained by using automatic\npruning in SA. We perform a thorough comparative experimental analysis of the\nSMD on both synthetic and real-life datasets. The results corroborate the\nefficacy of the dependency structure and SA in the SMD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yijue Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1\">Feng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1\">Elena Marchiori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1\">Sergios Theodoridis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor-based computation of metastable and coherent sets. (arXiv:1908.04741v3 [math.NA] UPDATED)","link":"http://arxiv.org/abs/1908.04741","description":"<p>Recent years have seen rapid advances in the data-driven analysis of\ndynamical systems based on Koopman operator theory and related approaches. On\nthe other hand, low-rank tensor product approximations -- in particular the\ntensor train (TT) format -- have become a valuable tool for the solution of\nlarge-scale problems in a number of fields. In this work, we combine\nKoopman-based models and the TT format, enabling their application to\nhigh-dimensional problems in conjunction with a rich set of basis functions or\nfeatures. We derive efficient algorithms to obtain a reduced matrix\nrepresentation of the system's evolution operator starting from an appropriate\nlow-rank representation of the data. These algorithms can be applied to both\nstationary and non-stationary systems. We establish the infinite-data limit of\nthese matrix representations, and demonstrate our methods' capabilities using\nseveral benchmark data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Nuske_F/0/1/0/all/0/1\">Feliks N&#xfc;ske</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gelss_P/0/1/0/all/0/1\">Patrick Gel&#xdf;</a>, <a href=\"http://arxiv.org/find/math/1/au:+Klus_S/0/1/0/all/0/1\">Stefan Klus</a>, <a href=\"http://arxiv.org/find/math/1/au:+Clementi_C/0/1/0/all/0/1\">Cecilia Clementi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Gibson Benchmark (iGibson 0.5): A Benchmark for Interactive Navigation in Cluttered Environments. (arXiv:1910.14442v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1910.14442","description":"<p>We present Interactive Gibson Benchmark, the first comprehensive benchmark\nfor training and evaluating Interactive Navigation: robot navigation strategies\nwhere physical interaction with objects is allowed and even encouraged to\naccomplish a task. For example, the robot can move objects if needed in order\nto clear a path leading to the goal location. Our benchmark comprises two novel\nelements: 1) a new experimental setup, the Interactive Gibson Environment\n(iGibson 0.5), which simulates high fidelity visuals of indoor scenes, and high\nfidelity physical dynamics of the robot and common objects found in these\nscenes; 2) a set of Interactive Navigation metrics which allows one to study\nthe interplay between navigation and physical interaction. We present and\nevaluate multiple learning-based baselines in Interactive Gibson, and provide\ninsights into regimes of navigation with different trade-offs between\nnavigation path efficiency and disturbance of surrounding objects. We make our\nbenchmark publicly\navailable(https://sites.google.com/view/interactivegibsonenv) and encourage\nresearchers from all disciplines in robotics (e.g. planning, learning, control)\nto propose, evaluate, and compare their Interactive Navigation solutions in\nInteractive Gibson.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">William B. Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasimbeg_P/0/1/0/all/0/1\">Priya Kasimbeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tchapmi_M/0/1/0/all/0/1\">Micael Tchapmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generate Levels From Nothing. (arXiv:2002.05259v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2002.05259","description":"<p>Machine learning for procedural content generation has recently become an\nactive area of research. Levels vary in both form and function and are mostly\nunrelated to each other across games. This has made it difficult to assemble\nsuitably large datasets to bring machine learning to level design in the same\nway as it's been used for image generation. Here we propose Generative Playing\nNetworks which design levels for itself to play. The algorithm is built in two\nparts; an agent that learns to play game levels, and a generator that learns\nthe distribution of playable levels. As the agent learns and improves its\nability, the space of playable levels, as defined by the agent, grows. The\ngenerator targets the agent's playability estimates to then update its\nunderstanding of what constitutes a playable level. We call this process of\nlearning the distribution of data found through self-discovery with an\nenvironment, self-supervised inductive learning. Unlike previous approaches to\nprocedural content generation, Generative Playing Networks are end-to-end\ndifferentiable and do not require human-designed examples or domain knowledge.\nWe demonstrate the capability of this framework by training an agent and level\ngenerator for a 2D dungeon crawler game.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bontrager_P/0/1/0/all/0/1\">Philip Bontrager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1\">Julian Togelius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPCA: A Probabilistic Framework for Gaussian Process Embedded Channel Attention. (arXiv:2003.04575v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2003.04575","description":"<p>Channel attention mechanisms have been commonly applied in many visual tasks\nfor effective performance improvement. It is able to reinforce the informative\nchannels as well as to suppress the useless channels. Recently, different\nchannel attention modules have been proposed and implemented in various ways.\nGenerally speaking, they are mainly based on convolution and pooling\noperations. In this paper, we propose Gaussian process embedded channel\nattention (GPCA) module and further interpret the channel attention schemes in\na probabilistic way. The GPCA module intends to model the correlations among\nthe channels, which are assumed to be captured by beta distributed variables.\nAs the beta distribution cannot be integrated into the end-to-end training of\nconvolutional neural networks (CNNs) with a mathematically tractable solution,\nwe utilize an approximation of the beta distribution to solve this problem. To\nspecify, we adapt a Sigmoid-Gaussian approximation, in which the Gaussian\ndistributed variables are transferred into the interval [0,1]. The Gaussian\nprocess is then utilized to model the correlations among different channels. In\nthis case, a mathematically tractable solution is derived. The GPCA module can\nbe efficiently implemented and integrated into the end-to-end training of the\nCNNs. Experimental results demonstrate the promising performance of the\nproposed GPCA module. Codes are available at https://github.com/PRIS-CV/GPCA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise Robust Named Entity Understanding for Voice Assistants. (arXiv:2005.14408v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.14408","description":"<p>Named Entity Recognition (NER) and Entity Linking (EL) play an essential role\nin voice assistant interaction, but are challenging due to the special\ndifficulties associated with spoken user queries. In this paper, we propose a\nnovel architecture that jointly solves the NER and EL tasks by combining them\nin a joint reranking module. We show that our proposed framework improves NER\naccuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features\nused also lead to better accuracies in other natural language understanding\ntasks, such as domain classification and semantic parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muralidharan_D/0/1/0/all/0/1\">Deepak Muralidharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniz_J/0/1/0/all/0/1\">Joel Ruben Antony Moniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Sida Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Justine Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulman_S/0/1/0/all/0/1\">Stephen Pulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Atish Kothari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ray Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yinying Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_V/0/1/0/all/0/1\">Vivek Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1\">Mubarak Seyed Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_G/0/1/0/all/0/1\">Gang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dun_N/0/1/0/all/0/1\">Nan Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yidan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+O_A/0/1/0/all/0/1\">Andy O</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitkara_P/0/1/0/all/0/1\">Pooja Chitkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Alkesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tayal_K/0/1/0/all/0/1\">Kushal Tayal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Roger Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasch_P/0/1/0/all/0/1\">Peter Grasch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jason D. Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Baseline for Shapley Values in MLPs: from Missingness to Neutrality. (arXiv:2006.04896v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.04896","description":"<p>Deep neural networks have gained momentum based on their accuracy, but their\ninterpretability is often criticised. As a result, they are labelled as black\nboxes. In response, several methods have been proposed in the literature to\nexplain their predictions. Among the explanatory methods, Shapley values is a\nfeature attribution method favoured for its robust theoretical foundation.\nHowever, the analysis of feature attributions using Shapley values requires\nchoosing a baseline that represents the concept of missingness. An arbitrary\nchoice of baseline could negatively impact the explanatory power of the method\nand possibly lead to incorrect interpretations. In this paper, we present a\nmethod for choosing a baseline according to a neutrality value: as a parameter\nselected by decision-makers, the point at which their choices are determined by\nthe model predictions being either above or below it. Hence, the proposed\nbaseline is set based on a parameter that depends on the actual use of the\nmodel. This procedure stands in contrast to how other baselines are set, i.e.\nwithout accounting for how the model is used. We empirically validate our\nchoice of baseline in the context of binary classification tasks, using two\ndatasets: a synthetic dataset and a dataset derived from the financial domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Izzo_C/0/1/0/all/0/1\">Cosimo Izzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhrati_R/0/1/0/all/0/1\">Ramin Okhrati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medda_F/0/1/0/all/0/1\">Francesca Medda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bandits with Partially Observable Confounded Data. (arXiv:2006.06731v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.06731","description":"<p>We study linear contextual bandits with access to a large, confounded,\noffline dataset that was sampled from some fixed policy. We show that this\nproblem is closely related to a variant of the bandit problem with side\ninformation. We construct a linear bandit algorithm that takes advantage of the\nprojected information, and prove regret bounds. Our results demonstrate the\nability to take advantage of confounded offline data. Particularly, we prove\nregret bounds that improve current bounds by a factor related to the visible\ndimensionality of the contexts in the data. Our results indicate that\nconfounded offline data can significantly improve online learning algorithms.\nFinally, we demonstrate various characteristics of our approach through\nsynthetic simulations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1\">Guy Tennenholtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1\">Uri Shalit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1\">Yonathan Efroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provable Training Set Debugging for Linear Regression. (arXiv:2006.09009v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.09009","description":"<p>We investigate problems in penalized $M$-estimation, inspired by applications\nin machine learning debugging. Data are collected from two pools, one\ncontaining data with possibly contaminated labels, and the other which is known\nto contain only cleanly labeled points. We first formulate a general\nstatistical algorithm for identifying buggy points and provide rigorous\ntheoretical guarantees under the assumption that the data follow a linear\nmodel. We then present two case studies to illustrate the results of our\ngeneral theory and the dependence of our estimator on clean versus buggy\npoints. We further propose an algorithm for tuning parameter selection of our\nLasso-based algorithm and provide corresponding theoretical guarantees.\nFinally, we consider a two-person \"game\" played between a bug generator and a\ndebugger, where the debugger can augment the contaminated data set with cleanly\nlabeled versions of points in the original data pool. We establish a\ntheoretical result showing a sufficient condition under which the bug generator\ncan always fool the debugger. Nonetheless, we provide empirical results showing\nthat such a situation may not occur in practice, making it possible for natural\naugmentation strategies combined with our Lasso debugging algorithm to succeed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaomin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaojin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loh_P/0/1/0/all/0/1\">Po-Ling Loh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Backdoor. (arXiv:2006.11890v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.11890","description":"<p>One intriguing property of deep neural networks (DNNs) is their inherent\nvulnerability to backdoor attacks -- a trojan model responds to\ntrigger-embedded inputs in a highly predictable manner while functioning\nnormally otherwise. Despite the plethora of prior work on DNNs for continuous\ndata (e.g., images), the vulnerability of graph neural networks (GNNs) for\ndiscrete-structured data (e.g., graphs) is largely unexplored, which is highly\nconcerning given their increasing use in security-sensitive domains. To bridge\nthis gap, we present GTA, the first backdoor attack on GNNs. Compared with\nprior work, GTA departs in significant ways: graph-oriented -- it defines\ntriggers as specific subgraphs, including both topological structures and\ndescriptive features, entailing a large design spectrum for the adversary;\ninput-tailored -- it dynamically adapts triggers to individual graphs, thereby\noptimizing both attack effectiveness and evasiveness; downstream model-agnostic\n-- it can be readily launched without knowledge regarding downstream models or\nfine-tuning strategies; and attack-extensible -- it can be instantiated for\nboth transductive (e.g., node classification) and inductive (e.g., graph\nclassification) tasks, constituting severe threats for a range of\nsecurity-critical applications. Through extensive evaluation using benchmark\ndatasets and state-of-the-art models, we demonstrate the effectiveness of GTA.\nWe further provide analytical justification for its effectiveness and discuss\npotential countermeasures, pointing to several promising research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1\">Zhaohan Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ren Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Quantification using Variational Inference for Biomedical Image Segmentation. (arXiv:2008.07588v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.07588","description":"<p>Deep learning motivated by convolutional neural networks has been highly\nsuccessful in a range of medical imaging problems like image classification,\nimage segmentation, image synthesis etc. However for validation and\ninterpretability, not only do we need the predictions made by the model but\nalso how confident it is while making those predictions. This is important in\nsafety critical applications for the people to accept it. In this work, we used\nan encoder decoder architecture based on variational inference techniques for\nsegmenting brain tumour images. We evaluate our work on the publicly available\nBRATS dataset using Dice Similarity Coefficient (DSC) and Intersection Over\nUnion (IOU) as the evaluation metrics. Our model is able to segment brain\ntumours while taking into account both aleatoric uncertainty and epistemic\nuncertainty in a principled bayesian manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sagar_A/0/1/0/all/0/1\">Abhinav Sagar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applications of Deep Neural Networks. (arXiv:2009.05673v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.05673","description":"<p>Deep learning is a group of exciting new technologies for neural networks.\nThrough a combination of advanced training techniques and neural network\narchitectural components, it is now possible to create neural networks that can\nhandle tabular data, images, text, and audio as both input and output. Deep\nlearning allows a neural network to learn hierarchies of information in a way\nthat is like the function of the human brain. This course will introduce the\nstudent to classic neural network structures, Convolution Neural Networks\n(CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU),\nGeneral Adversarial Networks (GAN), and reinforcement learning. Application of\nthese architectures to computer vision, time series, security, natural language\nprocessing (NLP), and data generation will be covered. High-Performance\nComputing (HPC) aspects will demonstrate how deep learning can be leveraged\nboth on graphical processing units (GPUs), as well as grids. Focus is primarily\nupon the application of deep learning to problems, with some introduction to\nmathematical foundations. Readers will use the Python programming language to\nimplement deep learning using Google TensorFlow and Keras. It is not necessary\nto know Python prior to this book; however, familiarity with at least one\nprogramming language is assumed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heaton_J/0/1/0/all/0/1\">Jeff Heaton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Into the Unknown: Active Monitoring of Neural Networks. (arXiv:2009.06429v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.06429","description":"<p>Neural-network classifiers achieve high accuracy when predicting the class of\nan input that they were trained to identify. Maintaining this accuracy in\ndynamic environments, where inputs frequently fall outside the fixed set of\ninitially known classes, remains a challenge. The typical approach is to detect\ninputs from novel classes and retrain the classifier on an augmented dataset.\nHowever, not only the classifier but also the detection mechanism needs to\nadapt in order to distinguish between newly learned and yet unknown input\nclasses. To address this challenge, we introduce an algorithmic framework for\nactive monitoring of a neural network. A monitor wrapped in our framework\noperates in parallel with the neural network and interacts with a human user\nvia a series of interpretable labeling queries for incremental adaptation. In\naddition, we propose an adaptive quantitative monitor to improve precision. An\nexperimental evaluation on a diverse set of benchmarks with varying numbers of\nclasses confirms the benefits of our active monitoring framework in dynamic\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lukina_A/0/1/0/all/0/1\">Anna Lukina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1\">Christian Schilling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1\">Thomas A. Henzinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Sample Size, Dimensionality, and Generalization in Covariate Shift Adaptation. (arXiv:2010.01184v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2010.01184","description":"<p>In supervised learning, training and test datasets are often sampled from\ndistinct distributions. Domain adaptation techniques are thus required.\nCovariate shift adaptation yields good generalization performance when domains\ndiffer only by the marginal distribution of features. Covariate shift\nadaptation is usually implemented using importance weighting, which may fail,\naccording to common wisdom, due to small effective sample sizes (ESS). Previous\nresearch argues this scenario is more common in high-dimensional settings.\nHowever, how effective sample size, dimensionality, and model\nperformance/generalization are formally related in supervised learning,\nconsidering the context of covariate shift adaptation, is still somewhat\nobscure in the literature. Thus, a main challenge is presenting a unified\ntheory connecting those points. Hence, in this paper, we focus on building a\nunified view connecting the ESS, data dimensionality, and generalization in the\ncontext of covariate shift adaptation. Moreover, we also demonstrate how\ndimensionality reduction or feature selection can increase the ESS, and argue\nthat our results support dimensionality reduction before covariate shift\nadaptation as a good practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1\">Felipe Maia Polo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vicente_R/0/1/0/all/0/1\">Renato Vicente</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advanced Dropout: A Model-free Methodology for Bayesian Dropout Optimization. (arXiv:2010.05244v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.05244","description":"<p>Due to lack of data, overfitting ubiquitously exists in real-world\napplications of deep neural networks (DNNs). We propose advanced dropout, a\nmodel-free methodology, to mitigate overfitting and improve the performance of\nDNNs. The advanced dropout technique applies a model-free and easily\nimplemented distribution with parametric prior, and adaptively adjusts dropout\nrate. Specifically, the distribution parameters are optimized by stochastic\ngradient variational Bayes in order to carry out an end-to-end training. We\nevaluate the effectiveness of the advanced dropout against nine dropout\ntechniques on seven computer vision datasets (five small-scale datasets and two\nlarge-scale datasets) with various base models. The advanced dropout\noutperforms all the referred techniques on all the datasets.We further compare\nthe effectiveness ratios and find that advanced dropout achieves the highest\none on most cases. Next, we conduct a set of analysis of dropout rate\ncharacteristics, including convergence of the adaptive dropout rate, the\nlearned distributions of dropout masks, and a comparison with dropout rate\ngeneration without an explicit distribution. In addition, the ability of\noverfitting prevention is evaluated and confirmed. Finally, we extend the\napplication of the advanced dropout to uncertainty inference, network pruning,\ntext classification, and regression. The proposed advanced dropout is also\nsuperior to the corresponding referred methods. Codes are available at\nhttps://github.com/PRIS-CV/AdvancedDropout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_a/0/1/0/all/0/1\">and Jianjun Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1\">Jing-Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zheng-Hua Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep tree-ensembles for multi-output prediction. (arXiv:2011.02829v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.02829","description":"<p>Recently, deep neural networks have expanded the state-of-art in various\nscientific fields and provided solutions to long standing problems across\nmultiple application domains. Nevertheless, they also suffer from weaknesses\nsince their optimal performance depends on massive amounts of training data and\nthe tuning of an extended number of parameters. As a countermeasure, some\ndeep-forest methods have been recently proposed, as efficient and low-scale\nsolutions. Despite that, these approaches simply employ label classification\nprobabilities as induced features and primarily focus on traditional\nclassification and regression tasks, leaving multi-output prediction\nunder-explored. Moreover, recent work has demonstrated that tree-embeddings are\nhighly representative, especially in structured output prediction. In this\ndirection, we propose a novel deep tree-ensemble (DTE) model, where every layer\nenriches the original feature set with a representation learning component\nbased on tree-embeddings. In this paper, we specifically focus on two\nstructured output prediction tasks, namely multi-label classification and\nmulti-target regression. We conducted experiments using multiple benchmark\ndatasets and the obtained results confirm that our method provides superior\nresults to state-of-the-art methods in both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakano_F/0/1/0/all/0/1\">Felipe Kenji Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pliakos_K/0/1/0/all/0/1\">Konstantinos Pliakos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vens_C/0/1/0/all/0/1\">Celine Vens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Synthetic Over-sampling method with Minority and Majority classes for imbalance problems. (arXiv:2011.04170v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.04170","description":"<p>Class imbalance is a substantial challenge in classifying many real-world\ncases. Synthetic over-sampling methods have been effective to improve the\nperformance of classifiers for imbalance problems. However, most synthetic\nover-sampling methods generate non-diverse synthetic instances within the\nconvex hull formed by the existing minority instances as they only concentrate\non the minority class and ignore the vast information provided by the majority\nclass. They also often do not perform well for extremely imbalanced data as the\nfewer the minority instances, the less information to generate synthetic\ninstances. Moreover, existing methods that generate synthetic instances using\nthe majority class distributional information cannot perform effectively when\nthe majority class has a multi-modal distribution. We propose a new method to\ngenerate diverse and adaptable synthetic instances using Synthetic\nOver-sampling with Minority and Majority classes (SOMM). SOMM generates\nsynthetic instances diversely within the minority data space. It updates the\ngenerated instances adaptively to the neighbourhood including both classes.\nThus, SOMM performs well for both binary and multiclass imbalance problems. We\nexamine the performance of SOMM for binary and multiclass problems using\nbenchmark data sets for different imbalance levels. The empirical results show\nthe superiority of SOMM compared to other existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khorshidi_H/0/1/0/all/0/1\">Hadi A. Khorshidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aickelin_U/0/1/0/all/0/1\">Uwe Aickelin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Laplace for Bayesian neural networks. (arXiv:2011.10443v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2011.10443","description":"<p>We develop variational Laplace for Bayesian neural networks (BNNs) which\nexploits a local approximation of the curvature of the likelihood to estimate\nthe ELBO without the need for stochastic sampling of the neural-network\nweights. The Variational Laplace objective is simple to evaluate, as it is (in\nessence) the log-likelihood, plus weight-decay, plus a squared-gradient\nregularizer. Variational Laplace gave better test performance and expected\ncalibration errors than maximum a-posteriori inference and standard\nsampling-based variational inference, despite using the same variational\napproximate posterior. Finally, we emphasise care needed in benchmarking\nstandard VI as there is a risk of stopping before the variance parameters have\nconverged. We show that early-stopping can be avoided by increasing the\nlearning rate for the variance parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Unlu_A/0/1/0/all/0/1\">Ali Unlu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Utilizing Concept Drift for Measuring the Effectiveness of Policy Interventions: The Case of the COVID-19 Pandemic. (arXiv:2012.03728v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2012.03728","description":"<p>As a reaction to the high infectiousness and lethality of the COVID-19 virus,\ncountries around the world have adopted drastic policy measures to contain the\npandemic. However, it remains unclear which effect these measures, so-called\nnon-pharmaceutical interventions (NPIs), have on the spread of the virus. In\nthis article, we use machine learning and apply drift detection methods in a\nnovel way to predict the time lag of policy interventions with respect to the\ndevelopment of daily case numbers of COVID-19 across 9 European countries and\n28 US states. Our analysis shows that there are, on average, more than two\nweeks between NPI enactment and a drift in the case numbers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baier_L/0/1/0/all/0/1\">Lucas Baier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1\">Niklas K&#xfc;hl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoffer_J/0/1/0/all/0/1\">Jakob Sch&#xf6;ffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satzger_G/0/1/0/all/0/1\">Gerhard Satzger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Average Treatment Effects via Orthogonal Regularization. (arXiv:2101.08490v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.08490","description":"<p>Decision-making often requires accurate estimation of treatment effects from\nobservational data. This is challenging as outcomes of alternative decisions\nare not observed and have to be estimated. Previous methods estimate outcomes\nbased on unconfoundedness but neglect any constraints that unconfoundedness\nimposes on the outcomes. In this paper, we propose a novel regularization\nframework for estimating average treatment effects that exploits\nunconfoundedness. To this end, we formalize unconfoundedness as an\northogonality constraint, which ensures that the outcomes are orthogonal to the\ntreatment assignment. This orthogonality constraint is then included in the\nloss function via a regularization. Based on our regularization framework, we\ndevelop deep orthogonal networks for unconfounded treatments (DONUT), which\nlearn outcomes that are orthogonal to the treatment assignment. Using a variety\nof benchmark datasets for estimating average treatment effects, we demonstrate\nthat DONUT outperforms the state-of-the-art substantially.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hatt_T/0/1/0/all/0/1\">Tobias Hatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1\">Stefan Feuerriegel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks. (arXiv:2102.03322v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.03322","description":"<p>Given the increasing promise of Graph Neural Networks (GNNs) in real-world\napplications, several methods have been developed for explaining their\npredictions. So far, these methods have primarily focused on generating\nsubgraphs that are especially relevant for a particular prediction. However,\nsuch methods do not provide a clear opportunity for recourse: given a\nprediction, we want to understand how the prediction can be changed in order to\nachieve a more desirable outcome. In this work, we propose a method for\ngenerating counterfactual (CF) explanations for GNNs: the minimal perturbation\nto the input (graph) data such that the prediction changes. Using only edge\ndeletions, we find that our method, CF-GNNExplainer can generate CF\nexplanations for the majority of instances across three widely used datasets\nfor GNN explanations, while removing less than 3 edges on average, with at\nleast 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes\nedges that are crucial for the original predictions, resulting in minimal CF\nexplanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1\">Ana Lucic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1\">Maartje ter Hoeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolomei_G/0/1/0/all/0/1\">Gabriele Tolomei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Fabrizio Silvestri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations. (arXiv:2102.06559v3 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2102.06559","description":"<p>We perform scalable approximate inference in a continuous-depth Bayesian\nneural network family. In this model class, uncertainty about separate weights\nin each layer gives hidden units that follow a stochastic differential\nequation. We demonstrate gradient-based stochastic variational inference in\nthis infinite-parameter setting, producing arbitrarily-flexible approximate\nposteriors. We also derive a novel gradient estimator that approaches zero\nvariance as the approximate posterior over weights approaches the true\nposterior. This approach brings continuous-depth Bayesian neural nets to a\ncompetitive comparison against discrete-depth alternatives, while inheriting\nthe memory-efficient training and tunable precision of Neural ODEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Xu_W/0/1/0/all/0/1\">Winnie Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1\">Ricky T.Q. Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuechen Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Duvenaud_D/0/1/0/all/0/1\">David Duvenaud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Provable Super-Convergence with a Large Cyclical Learning Rate. (arXiv:2102.10734v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.10734","description":"<p>Conventional wisdom dictates that learning rate should be in the stable\nregime so that gradient-based algorithms don't blow up. This letter introduces\na simple scenario where an unstably large learning rate scheme leads to a super\nfast convergence, with the convergence rate depending only logarithmically on\nthe condition number of the problem. Our scheme uses a Cyclical Learning Rate\n(CLR) where we periodically take one large unstable step and several small\nstable steps to compensate for the instability. These findings also help\nexplain the empirical observations of [Smith and Topin, 2019] where they show\nthat CLR with a large maximum learning rate can dramatically accelerate\nlearning and lead to so-called \"super-convergence\". We prove that our scheme\nexcels in the problems where Hessian exhibits a bimodal spectrum and the\neigenvalues can be grouped into two clusters (small and large). The unstably\nlarge step is the key to enabling fast convergence over the small\neigen-spectrum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1\">Samet Oymak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-based Deep Reinforcement Learning for POMDPs. (arXiv:2102.12344v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.12344","description":"<p>A promising characteristic of Deep Reinforcement Learning (DRL) is its\ncapability to learn optimal policy in an end-to-end manner without relying on\nfeature engineering. However, most approaches assume a fully observable state\nspace, i.e. fully observable Markov Decision Processes (MDPs). In real-world\nrobotics, this assumption is unpractical, because of issues such as sensor\nsensitivity limitations and sensor noise, and the lack of knowledge about\nwhether the observation design is complete or not. These scenarios lead to\nPartially Observable MDPs (POMDPs). In this paper, we propose\nLong-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient\n(LSTM-TD3) by introducing a memory component to TD3, and compare its\nperformance with other DRL algorithms in both MDPs and POMDPs. Our results\ndemonstrate the significant advantages of the memory component in addressing\nPOMDPs, including the ability to handle missing and noisy observation data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingheng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1\">Rob Gorbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1\">Dana Kuli&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embedded Knowledge Distillation in Depth-Level Dynamic Neural Network. (arXiv:2103.00793v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00793","description":"<p>In real applications, different computation-resource devices need\ndifferent-depth networks (e.g., ResNet-18/34/50) with high-accuracy. Usually,\nexisting methods either design multiple networks and train them independently,\nor construct depth-level/width-level dynamic neural networks which is hard to\nprove the accuracy of each sub-net. In this article, we propose an elegant\nDepth-Level Dynamic Neural Network (DDNN) integrated different-depth sub-nets\nof similar architectures. To improve the generalization of sub-nets, we design\nthe Embedded-Knowledge-Distillation (EKD) training mechanism for the DDNN to\nimplement knowledge transfer from the teacher (full-net) to multiple students\n(sub-nets). Specifically, the Kullback-Leibler (KL) divergence is introduced to\nconstrain the posterior class probability consistency between full-net and\nsub-nets, and self-attention distillation on the same resolution feature of\ndifferent depth is addressed to drive more abundant feature representations of\nsub-nets. Thus, we can obtain multiple high-accuracy sub-nets simultaneously in\na DDNN via the online knowledge distillation in each training iteration without\nextra computation cost. Extensive experiments on CIFAR-10/100, and ImageNet\ndatasets demonstrate that sub-nets in DDNN with EKD training achieve better\nperformance than individually training networks while preserving the original\nperformance of full-nets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Shuchang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Ting-Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering. (arXiv:2103.05568v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.05568","description":"<p>Multimodal IR, spanning text corpus, knowledge graph and images, called\noutside knowledge visual question answering (OKVQA), is of much recent\ninterest. However, the popular data set has serious limitations. A surprisingly\nlarge fraction of queries do not assess the ability to integrate cross-modal\ninformation. Instead, some are independent of the image, some depend on\nspeculation, some require OCR or are otherwise answerable from the image alone.\nTo add to the above limitations, frequency-based guessing is very effective\nbecause of (unintended) widespread answer overlaps between the train and test\nfolds. Overall, it is hard to determine when state-of-the-art systems exploit\nthese weaknesses rather than really infer the answers, because they are opaque\nand their 'reasoning' process is uninterpretable. An equally important\nlimitation is that the dataset is designed for the quantitative assessment only\nof the end-to-end answer retrieval task, with no provision for assessing the\ncorrect(semantic) interpretation of the input query. In response, we identify a\nkey structural idiom in OKVQA ,viz., S3 (select, substitute and search), and\nbuild a new data set and challenge around it. Specifically, the questioner\nidentifies an entity in the image and asks a question involving that entity\nwhich can be answered only by consulting a knowledge graph or corpus passage\nmentioning the entity. Our challenge consists of (i)OKVQAS3, a subset of OKVQA\nannotated based on the structural idiom and (ii)S3VQA, a new dataset built from\nscratch. We also present a neural but structurally transparent OKVQA system,\nS3, that explicitly addresses our challenge dataset, and outperforms recent\ncompetitive baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aman Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothyari_M/0/1/0/all/0/1\">Mayank Kothyari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vishwajeet Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1\">Preethi Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2103.13689","description":"<p>Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poisoning the Unlabeled Dataset of Semi-Supervised Learning. (arXiv:2105.01622v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.01622","description":"<p>Semi-supervised machine learning models learn from a (small) set of labeled\ntraining examples, and a (large) set of unlabeled training examples.\nState-of-the-art models can reach within a few percentage points of\nfully-supervised training, while requiring 100x less labeled data.\n</p>\n<p>We study a new class of vulnerabilities: poisoning attacks that modify the\nunlabeled dataset. In order to be useful, unlabeled datasets are given strictly\nless review than labeled datasets, and adversaries can therefore poison them\neasily. By inserting maliciously-crafted unlabeled examples totaling just 0.1%\nof the dataset size, we can manipulate a model trained on this poisoned dataset\nto misclassify arbitrary examples at test time (as any desired label). Our\nattacks are highly effective across datasets and semi-supervised learning\nmethods.\n</p>\n<p>We find that more accurate methods (thus more likely to be used) are\nsignificantly more vulnerable to poisoning attacks, and as such better training\nmethods are unlikely to prevent this attack. To counter this we explore the\nspace of defenses, and propose two methods that mitigate our attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIPA: General Information Propagation Algorithm for Graph Learning. (arXiv:2105.06035v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.06035","description":"<p>Graph neural networks (GNNs) have been popularly used in analyzing\ngraph-structured data, showing promising results in various applications such\nas node classification, link prediction and network recommendation. In this\npaper, we present a new graph attention neural network, namely GIPA, for\nattributed graph data learning. GIPA consists of three key components:\nattention, feature propagation and aggregation. Specifically, the attention\ncomponent introduces a new multi-layer perceptron based multi-head to generate\nbetter non-linear feature mapping and representation than conventional\nimplementations such as dot-product. The propagation component considers not\nonly node features but also edge features, which differs from existing GNNs\nthat merely consider node features. The aggregation component uses a residual\nconnection to generate the final embedding. We evaluate the performance of GIPA\nusing the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The\nexperimental results reveal that GIPA can beat the state-of-the-art models in\nterms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of\n$0.8700\\pm 0.0010$ and outperforms all the previous methods listed in the\nogbn-proteins leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinkai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhixiong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xintan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongchao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding the Condensation of Two-layer Neural Networks at Initial Training. (arXiv:2105.11686v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.11686","description":"<p>Studying the implicit regularization effect of the nonlinear training\ndynamics of neural networks (NNs) is important for understanding why\nover-parameterized neural networks often generalize well on real dataset.\nEmpirically, for two-layer NN, existing works have shown that input weights of\nhidden neurons (the input weight of a hidden neuron consists of the weight from\nits input layer to the hidden neuron and its bias term) condense on isolated\norientations with a small initialization. The condensation dynamics implies\nthat NNs can learn features from the training data with a network configuration\neffectively equivalent to a much smaller network during the training. In this\nwork, we show that the multiple roots of activation function at origin\n(referred as ``multiplicity'') is a key factor for understanding the\ncondensation at the initial stage of training. Our experiments of multilayer\nnetworks suggest that the maximal number of condensed orientations is twice the\nmultiplicity of the activation function used. Our theoretical analysis of\ntwo-layer networks confirms experiments for two cases, one is for the\nactivation function of multiplicity one, which contains many common activation\nfunctions, and the other is for the one-dimensional input. This work makes a\nstep towards understanding how small initialization implicitly leads NNs to\ncondensation at initial training stage, which lays a foundation for the future\nstudy of the nonlinear dynamics of NNs and its implicit regularization effect\nat a later stage of training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhi-Qin John Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hanxu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaoyu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack. (arXiv:2106.06895v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2106.06895","description":"<p>To reduce the time-to-market and access to state-of-the-art techniques, CNN\nhardware mapping and deployment on embedded accelerators are often outsourced\nto untrusted third parties, which is going to be more prevalent in futuristic\nartificial intelligence of things (AIoT) systems. These AIoT systems anticipate\nhorizontal collaboration among different resource-constrained AIoT node\ndevices, where CNN layers are partitioned and these devices collaboratively\ncompute complex CNN tasks. This horizontal collaboration opens another attack\nsurface to the CNN-based application, like inserting the hardware Trojans (HT)\ninto the embedded accelerators designed for the CNN. Therefore, there is a dire\nneed to explore this attack surface for designing secure embedded hardware\naccelerators for CNNs. Towards this goal, in this paper, we exploited this\nattack surface to propose an HT-based attack called FeSHI. Since in horizontal\ncollaboration of RC AIoT devices different sections of CNN architectures are\noutsourced to different untrusted third parties, the attacker may not know the\ninput image, but it has access to the layer-by-layer output feature maps\ninformation for the assigned sections of the CNN architecture. This attack\nexploits the statistical distribution, i.e., Gaussian distribution, of the\nlayer-by-layer feature maps of the CNN to design two triggers for stealthy HT\nwith a very low probability of triggering. Also, three different novel,\nstealthy and effective trigger designs are proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Odetola_T/0/1/0/all/0/1\">Tolulope Odetola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_F/0/1/0/all/0/1\">Faiq Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandefur_T/0/1/0/all/0/1\">Travis Sandefur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_H/0/1/0/all/0/1\">Hawzhin Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Syed Rafay Hasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.07160","description":"<p>We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1\">Kim Hammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1\">Rolf Stadler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep generative model for probabilistic energy forecasting in power systems: normalizing flows. (arXiv:2106.09370v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09370","description":"<p>Greater direct electrification of end-use sectors with a higher share of\nrenewables is one of the pillars to power a carbon-neutral society by 2050.\nHowever, in contrast to conventional power plants, renewable energy is subject\nto uncertainty raising challenges for their interaction with power systems.\nScenario-based probabilistic forecasting models have become an important tool\nto equip decision-makers. This paper proposes to present to the power systems\nforecasting practitioners a recent deep learning technique, the normalizing\nflows, to produce accurate scenario-based probabilistic forecasts that are\ncrucial to face the new challenges in power systems applications. The strength\nof this technique is to directly learn the stochastic multivariate distribution\nof the underlying process by maximizing the likelihood. Through comprehensive\nempirical evaluations using the open data of the Global Energy Forecasting\nCompetition 2014, we demonstrate that this methodology is competitive with\nother state-of-the-art deep learning generative models: generative adversarial\nnetworks and variational autoencoders. The models producing weather-based wind,\nsolar power, and load scenarios are properly compared both in terms of forecast\nvalue, by considering the case study of an energy retailer, and quality using\nseveral complementary metrics. The numerical experiments are simple and easily\nreproducible. Thus, we hope it will encourage other forecasting practitioners\nto test and use normalizing flows in power system applications such as bidding\non electricity markets, scheduling of power systems with high renewable energy\nsources penetration, energy management of virtual power plan or microgrids, and\nunit commitment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1\">Antoine Wehenkel Damien Lanaspeze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1\">Bertrand Corn&#xe9;lusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes. (arXiv:2106.15380v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.15380","description":"<p>In this work we present a novel approach to hierarchical reinforcement\nlearning for linearly-solvable Markov decision processes. Our approach assumes\nthat the state space is partitioned, and the subtasks consist in moving between\nthe partitions. We represent value functions on several levels of abstraction,\nand use the compositionality of subtasks to estimate the optimal values of the\nstates in each partition. The policy is implicitly defined on these optimal\nvalue estimates, rather than being decomposed among the subtasks. As a\nconsequence, our approach can learn the globally optimal policy, and does not\nsuffer from the non-stationarity of high-level decisions. If several partitions\nhave equivalent dynamics, the subtasks of those partitions can be shared. If\nthe set of boundary states is smaller than the entire state space, our approach\ncan have significantly smaller sample complexity than that of a flat learner,\nand we validate this empirically in several experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Infante_G/0/1/0/all/0/1\">Guillermo Infante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1\">Anders Jonsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Distance Measure for Privacy-preserving Process Mining based on Feature Learning. (arXiv:2107.06578v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2107.06578","description":"<p>To enable process analysis based on an event log without compromising the\nprivacy of individuals involved in process execution, a log may be anonymized.\nSuch anonymization strives to transform a log so that it satisfies provable\nprivacy guarantees, while largely maintaining its utility for process analysis.\nExisting techniques perform anonymization using simple, syntactic measures to\nidentify suitable transformation operations. This way, the semantics of the\nactivities referenced by the events in a trace are neglected, potentially\nleading to transformations in which events of unrelated activities are merged.\nTo avoid this and incorporate the semantics of activities during anonymization,\nwe propose to instead incorporate a distance measure based on feature learning.\nSpecifically, we show how embeddings of events enable the definition of a\ndistance measure for traces to guide event log anonymization. Our experiments\nwith real-world data indicate that anonymization using this measure, compared\nto a syntactic one, yields logs that are closer to the original log in various\ndimensions and, hence, have higher utility for process analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosel_F/0/1/0/all/0/1\">Fabian R&#xf6;sel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrenkrog_Petersen_S/0/1/0/all/0/1\">Stephan A. Fahrenkrog-Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1\">Han van der Aa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1\">Matthias Weidlich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-To-Ensemble by Contextual Rank Aggregation in E-Commerce. (arXiv:2107.08598v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.08598","description":"<p>Ensemble models in E-commerce combine predictions from multiple sub-models\nfor ranking and revenue improvement. Industrial ensemble models are typically\ndeep neural networks, following the supervised learning paradigm to infer\nconversion rate given inputs from sub-models. However, this process has the\nfollowing two problems. Firstly, the point-wise scoring approach disregards the\nrelationships between items and leads to homogeneous displayed results, while\ndiversified display benefits user experience and revenue. Secondly, the\nlearning paradigm focuses on the ranking metrics and does not directly optimize\nthe revenue. In our work, we propose a new Learning-To-Ensemble (LTE) framework\nRAEGO, which replaces the ensemble model with a contextual Rank Aggregator (RA)\nand explores the best weights of sub-models by the Evaluator-Generator\nOptimization (EGO). To achieve the best online performance, we propose a new\nrank aggregation algorithm TournamentGreedy as a refinement of classic rank\naggregators, which also produces the best average weighted Kendall Tau Distance\n(KTD) amongst all the considered algorithms with quadratic time complexity.\nUnder the assumption that the best output list should be Pareto Optimal on the\nKTD metric for sub-models, we show that our RA algorithm has higher efficiency\nand coverage in exploring the optimal weights. Combined with the idea of\nBayesian Optimization and gradient descent, we solve the online contextual\nBlack-Box Optimization task that finds the optimal weights for sub-models given\na chosen RA model. RA-EGO has been deployed in our online system and has\nimproved the revenue significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuesi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huzhang_G/0/1/0/all/0/1\">Guangda Huzhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qianying Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Da_Q/0/1/0/all/0/1\">Qing Da</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bias Loss for Mobile Neural Networks. (arXiv:2107.11170v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11170","description":"<p>Compact convolutional neural networks (CNNs) have witnessed exceptional\nimprovements in performance in recent years. However, they still fail to\nprovide the same predictive power as CNNs with a large number of parameters.\nThe diverse and even abundant features captured by the layers is an important\ncharacteristic of these successful CNNs. However, differences in this\ncharacteristic between large CNNs and their compact counterparts have rarely\nbeen investigated. In compact CNNs, due to the limited number of parameters,\nabundant features are unlikely to be obtained, and feature diversity becomes an\nessential characteristic. Diverse features present in the activation maps\nderived from a data point during model inference may indicate the presence of a\nset of unique descriptors necessary to distinguish between objects of different\nclasses. In contrast, data points with low feature diversity may not provide a\nsufficient amount of unique descriptors to make a valid prediction; we refer to\nthem as random predictions. Random predictions can negatively impact the\noptimization process and harm the final performance. This paper proposes\naddressing the problem raised by random predictions by reshaping the standard\ncross-entropy to make it biased toward data points with a limited number of\nunique descriptive features. Our novel Bias Loss focuses the training on a set\nof valuable data points and prevents the vast number of samples with poor\nlearning features from misleading the optimization process. Furthermore, to\nshow the importance of diversity, we present a family of SkipNet models whose\narchitectures are brought to boost the number of unique descriptors in the last\nlayers. Our Skipnet-M can achieve 1% higher classification accuracy than\nMobileNetV3 Large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abrahamyan_L/0/1/0/all/0/1\">Lusine Abrahamyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziatchin_V/0/1/0/all/0/1\">Valentin Ziatchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Causal Inference in Heterogeneous Observational Data. (arXiv:2107.11732v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.11732","description":"<p>Analyzing observational data from multiple sources can be useful for\nincreasing statistical power to detect a treatment effect; however, practical\nconstraints such as privacy considerations may restrict individual-level\ninformation sharing across data sets. This paper develops federated methods\nthat only utilize summary-level information from heterogeneous data sets. Our\nfederated methods provide doubly-robust point estimates of treatment effects as\nwell as variance estimates. We derive the asymptotic distributions of our\nfederated estimators, which are shown to be asymptotically equivalent to the\ncorresponding estimators from the combined, individual-level data. We show that\nto achieve these properties, federated methods should be adjusted based on\nconditions such as whether models are correctly specified and stable across\nheterogeneous data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruoxuan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koenecke_A/0/1/0/all/0/1\">Allison Koenecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1\">Michael Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1\">Susan Athey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel Density Estimation by Stagewise Algorithm with a Simple Dictionary. (arXiv:2107.13430v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2107.13430","description":"<p>This study proposes multivariate kernel density estimation by stagewise\nminimization algorithm based on $U$-divergence and a simple dictionary. The\ndictionary consists of an appropriate scalar bandwidth matrix and a part of the\noriginal data. The resulting estimator brings us data-adaptive weighting\nparameters and bandwidth matrices, and realizes a sparse representation of\nkernel density estimation. We develop the non-asymptotic error bound of\nestimator obtained via the proposed stagewise minimization algorithm. It is\nconfirmed from simulation studies that the proposed estimator performs\ncompetitive to or sometime better than other well-known density estimators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Nishida_K/0/1/0/all/0/1\">Kiheiji Nishida</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Naito_K/0/1/0/all/0/1\">Kanta Naito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing Open Source Educational Resources for Machine Learning and Data Science. (arXiv:2107.14330v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2107.14330","description":"<p>Education should not be a privilege but a common good. It should be openly\naccessible to everyone, with as few barriers as possible; even more so for key\ntechnologies such as Machine Learning (ML) and Data Science (DS). Open\nEducational Resources (OER) are a crucial factor for greater educational\nequity. In this paper, we describe the specific requirements for OER in ML and\nDS and argue that it is especially important for these fields to make source\nfiles publicly available, leading to Open Source Educational Resources (OSER).\nWe present our view on the collaborative development of OSER, the challenges\nthis poses, and first steps towards their solutions. We outline how OSER can be\nused for blended learning scenarios and share our experiences in university\neducation. Finally, we discuss additional challenges such as credit assignment\nor granting certificates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1\">Ludwig Bothmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strickroth_S/0/1/0/all/0/1\">Sven Strickroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casalicchio_G/0/1/0/all/0/1\">Giuseppe Casalicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1\">Marius Lindauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheipl_F/0/1/0/all/0/1\">Fabian Scheipl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralDP Differentially private neural networks by design. (arXiv:2107.14582v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14582","description":"<p>The application of differential privacy to the training of deep neural\nnetworks holds the promise of allowing large-scale (decentralized) use of\nsensitive data while providing rigorous privacy guarantees to the individual.\nThe predominant approach to differentially private training of neural networks\nis DP-SGD, which relies on norm-based gradient clipping as a method for\nbounding sensitivity, followed by the addition of appropriately calibrated\nGaussian noise. In this work we propose NeuralDP, a technique for privatising\nactivations of some layer within a neural network, which by the post-processing\nproperties of differential privacy yields a differentially private network. We\nexperimentally demonstrate on two datasets (MNIST and Pediatric Pneumonia\nDataset (PPD)) that our method offers substantially improved privacy-utility\ntrade-offs compared to DP-SGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1\">Moritz Knolle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1\">Dmitrii Usynin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1\">Alexander Ziller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1\">Georgios Kaissis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2108.01077","description":"<p>A master face is a face image that passes face-based identity-authentication\nfor a large portion of the population. These faces can be used to impersonate,\nwith a high probability of success, any user, without having access to any\nuser-information. We optimize these faces, by using an evolutionary algorithm\nin the latent embedding space of the StyleGAN face generator. Multiple\nevolutionary strategies are compared, and we propose a novel approach that\nemploys a neural network in order to direct the search in the direction of\npromising samples, without adding fitness evaluations. The results we present\ndemonstrate that it is possible to obtain a high coverage of the LFW identities\n(over 40%) with less than 10 master faces, for three leading deep face\nrecognition systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1\">Ron Shmelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1\">Tomer Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01139","description":"<p>EuroVoc is a multilingual thesaurus that was built for organizing the\nlegislative documentary of the European Union institutions. It contains\nthousands of categories at different levels of specificity and its descriptors\nare targeted by legal texts in almost thirty languages. In this work we propose\na unified framework for EuroVoc classification on 22 languages by fine-tuning\nmodern Transformer-based pretrained language models. We study extensively the\nperformance of our trained models and show that they significantly improve the\nresults obtained by a similar tool - JEX - on the same dataset. The code and\nthe fine-tuned models were open sourced, together with a programmatic interface\nthat eases the process of loading the weights of a trained model and of\nclassifying a new document.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdvRush: Searching for Adversarially Robust Neural Architectures. (arXiv:2108.01289v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.01289","description":"<p>Deep neural networks continue to awe the world with their remarkable\nperformance. Their predictions, however, are prone to be corrupted by\nadversarial examples that are imperceptible to humans. Current efforts to\nimprove the robustness of neural networks against adversarial examples are\nfocused on developing robust training methods, which update the weights of a\nneural network in a more robust direction. In this work, we take a step beyond\ntraining of the weight parameters and consider the problem of designing an\nadversarially robust neural architecture with high intrinsic robustness. We\npropose AdvRush, a novel adversarial robustness-aware neural architecture\nsearch algorithm, based upon a finding that independent of the training method,\nthe intrinsic robustness of a neural network can be represented with the\nsmoothness of its input loss landscape. Through a regularizer that favors a\ncandidate architecture with a smoother input loss landscape, AdvRush\nsuccessfully discovers an adversarially robust neural architecture. Along with\na comprehensive theoretical motivation for AdvRush, we conduct an extensive\namount of experiments to demonstrate the efficacy of AdvRush on various\nbenchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust\naccuracy under FGSM attack after standard training and 50.04% robust accuracy\nunder AutoAttack after 7-step PGD adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1\">Jisoo Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1\">Byunggook Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1\">Hyeokjun Choe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Federated Learning with Clustering: Non-IID Heart Rate Variability Data Application. (arXiv:2108.01903v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.01903","description":"<p>While machine learning techniques are being applied to various fields for\ntheir exceptional ability to find complex relations in large datasets, the\nstrengthening of regulations on data ownership and privacy is causing\nincreasing difficulty in its application to medical data. In light of this,\nFederated Learning has recently been proposed as a solution to train on private\ndata without breach of confidentiality. This conservation of privacy is\nparticularly appealing in the field of healthcare, where patient data is highly\nconfidential. However, many studies have shown that its assumption of\nIndependent and Identically Distributed data is unrealistic for medical data.\nIn this paper, we propose Personalized Federated Cluster Models, a hierarchical\nclustering-based FL process, to predict Major Depressive Disorder severity from\nHeart Rate Variability. By allowing clients to receive more personalized model,\nwe address problems caused by non-IID data, showing an accuracy increase in\nseverity prediction. This increase in performance may be sufficient to use\nPersonalized Federated Cluster Models in many existing Federated Learning\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">J.H.Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">H.M.Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">H.Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">E-H.Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">A.Y.Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">H.Y.Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">H.J.Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">T-M.Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Knowledge Distillation for Multilingual Social Event Detection. (arXiv:2108.03084v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.03084","description":"<p>Recently published graph neural networks (GNNs) show promising performance at\nsocial event detection tasks. However, most studies are oriented toward\nmonolingual data in languages with abundant training samples. This has left the\nmore common multilingual settings and lesser-spoken languages relatively\nunexplored. Thus, we present a GNN that incorporates cross-lingual word\nembeddings for detecting events in multilingual data streams. The first exploit\nis to make the GNN work with multilingual data. For this, we outline a\nconstruction strategy that aligns messages in different languages at both the\nnode and semantic levels. Relationships between messages are established by\nmerging entities that are the same but are referred to in different languages.\nNon-English message representations are converted into English semantic space\nvia the cross-lingual word embeddings. The resulting message graph is then\nuniformly encoded by a GNN model. In special cases where a lesser-spoken\nlanguage needs to be detected, a novel cross-lingual knowledge distillation\nframework, called CLKD, exploits prior knowledge learned from similar threads\nin English to make up for the paucity of annotated data. Experiments on both\nsynthetic and real-world datasets show the framework to be highly effective at\ndetection in both multilingual data and in languages where training samples are\nscarce.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiaqian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yongxin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lihong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.03272","description":"<p>Recent research in embodied AI has been boosted by the use of simulation\nenvironments to develop and train robot learning approaches. However, the use\nof simulation has skewed the attention to tasks that only require what robotics\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\nan open-source simulation environment that supports the simulation of a more\ndiverse set of household tasks through three key innovations. First, iGibson\n2.0 supports object states, including temperature, wetness level, cleanliness\nlevel, and toggled and sliced states, necessary to cover a wider range of\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\nit. This functionality can generate potentially infinite instances of tasks\nwith minimal effort from the users. The sampling mechanism allows our scenes to\nbe more densely populated with small objects in semantically meaningful\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\nimmerse humans in its scenes to collect demonstrations. As a result, we can\ncollect demonstrations from humans on these new types of tasks, and use them\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\nenable robot learning of novel tasks, in the hope of demonstrating the\npotential of this new simulator to support new research in embodied AI. iGibson\n2.0 and its new dataset will be publicly available at\n<a href=\"http://svl.stanford.edu/igibson/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1\">Gokul Dharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expressive Power and Loss Surfaces of Deep Learning Models. (arXiv:2108.03579v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.03579","description":"<p>The goals of this paper are two-fold. The first goal is to serve as an\nexpository tutorial on the working of deep learning models which emphasizes\ngeometrical intuition about the reasons for success of deep learning. The\nsecond goal is to complement the current results on the expressive power of\ndeep learning models and their loss surfaces with novel insights and results.\nIn particular, we describe how deep neural networks carve out manifolds\nespecially when the multiplication neurons are introduced. Multiplication is\nused in dot products and the attention mechanism and it is employed in capsule\nnetworks and self-attention based transformers. We also describe how random\npolynomial, random matrix, spin glass and computational complexity perspectives\non the loss surfaces are interconnected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1\">Simant Dube</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Network for DrawiNg Networks, (DNN)^2. (arXiv:2108.03632v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.03632","description":"<p>By leveraging recent progress of stochastic gradient descent methods, several\nworks have shown that graphs could be efficiently laid out through the\noptimization of a tailored objective function. In the meantime, Deep Learning\n(DL) techniques achieved great performances in many applications. We\ndemonstrate that it is possible to use DL techniques to learn a graph-to-layout\nsequence of operations thanks to a graph-related objective function. In this\npaper, we present a novel graph drawing framework called (DNN)^2: Deep Neural\nNetwork for DrawiNg Networks. Our method uses Graph Convolution Networks to\nlearn a model. Learning is achieved by optimizing a graph topology related loss\nfunction that evaluates (DNN)^2 generated layouts during training. Once\ntrained, the (DNN)^ model is able to quickly lay any input graph out. We\nexperiment (DNN)^2 and statistically compare it to optimization-based and\nregular graph layout algorithms. The results show that (DNN)^2 performs well\nand are encouraging as the Deep Learning approach to Graph Drawing is novel and\nmany leads for future works are identified.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giovannangeli_L/0/1/0/all/0/1\">Loann Giovannangeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalanne_F/0/1/0/all/0/1\">Frederic Lalanne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auber_D/0/1/0/all/0/1\">David Auber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giot_R/0/1/0/all/0/1\">Romain Giot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourqui_R/0/1/0/all/0/1\">Romain Bourqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoVideo: An Automated Video Action Recognition System. (arXiv:2108.04212v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04212","description":"<p>Action recognition is a crucial task for video understanding. In this paper,\nwe present AutoVideo, a Python system for automated video action recognition.\nIt currently supports seven action recognition algorithms and various\npre-processing modules. Unlike the existing libraries that only provide model\nzoos, AutoVideo is built with the standard pipeline language. The basic\nbuilding block is primitive, which wraps a pre-processing module or an\nalgorithm with some hyperparameters. AutoVideo is highly modular and\nextendable. It can be easily combined with AutoML searchers. The pipeline\nlanguage is quite general so that we can easily enrich AutoVideo with\nalgorithms for various other video-related tasks in the future. AutoVideo is\nreleased under MIT license at https://github.com/datamllab/autovideo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1\">Daochen Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_Z/0/1/0/all/0/1\">Zaid Pervaiz Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Sirui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Anmoll Kumar Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Mohammad Qazim Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1\">Kwei-Herng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaben Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_N/0/1/0/all/0/1\">Na Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation. (arXiv:1907.12975v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/1907.12975","description":"<p>Separating and labeling each instance of a nucleus (instance-aware\nsegmentation) is the key challenge in segmenting single cell nuclei on\nfluorescence microscopy images. Deep Neural Networks can learn the implicit\ntransformation of a nuclear image into a probability map indicating the class\nmembership of each pixel (nucleus or background), but the use of\npost-processing steps to turn the probability map into a labeled object mask is\nerror-prone. This especially accounts for nuclear images of tissue sections and\nnuclear images across varying tissue preparations. In this work, we aim to\nevaluate the performance of state-of-the-art deep learning architectures to\nsegment nuclei in fluorescence images of various tissue origins and sample\npreparation types without post-processing. We compare architectures that\noperate on pixel to pixel translation and an architecture that operates on\nobject detection and subsequent locally applied segmentation. In addition, we\npropose a novel strategy to create artificial images to extend the training\nset. We evaluate the influence of ground truth annotation quality, image scale\nand segmentation complexity on segmentation performance. Results show that\nthree out of four deep learning architectures (U-Net, U-Net with ResNet34\nbackbone, Mask R-CNN) can segment fluorescent nuclear images on most of the\nsample preparation types and tissue origins with satisfactory segmentation\nperformance. Mask R-CNN, an architecture designed to address instance aware\nsegmentation tasks, outperforms other architectures. Equal nuclear mean size,\nconsistent nuclear annotations and the use of artificially generated images\nresult in overall acceptable precision and recall across different tissues and\nsample preparation types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kromp_F/0/1/0/all/0/1\">Florian Kromp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1\">Lukas Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozsaky_E/0/1/0/all/0/1\">Eva Bozsaky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_I/0/1/0/all/0/1\">Inge Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerr_W/0/1/0/all/0/1\">Wolfgang Doerr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschner_Mandl_S/0/1/0/all/0/1\">Sabine Taschner-Mandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambros_P/0/1/0/all/0/1\">Peter Ambros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.02940","description":"<p>In recent years, many deep learning models have been adopted in autonomous\ndriving. At the same time, these models introduce new vulnerabilities that may\ncompromise the safety of autonomous vehicles. Specifically, recent studies have\ndemonstrated that adversarial attacks can cause a significant decline in\ndetection precision of deep learning-based 3D object detection models. Although\ndriving safety is the ultimate concern for autonomous driving, there is no\ncomprehensive study on the linkage between the performance of deep learning\nmodels and the driving safety of autonomous vehicles under adversarial attacks.\nIn this paper, we investigate the impact of two primary types of adversarial\nattacks, perturbation attacks and patch attacks, on the driving safety of\nvision-based autonomous vehicles rather than the detection precision of deep\nlearning models. In particular, we consider two state-of-the-art models in\nvision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving\nsafety, we propose an end-to-end evaluation framework with a set of driving\nsafety performance metrics. By analyzing the results of our extensive\nevaluation experiments, we find that (1) the attack's impact on the driving\nsafety of autonomous vehicles and the attack's impact on the precision of 3D\nobject detectors are decoupled, and (2) the DSGN model demonstrates stronger\nrobustness to adversarial attacks than the Stereo R-CNN model. In addition, we\nfurther investigate the causes behind the two findings with an ablation study.\nThe findings of this paper provide a new perspective to evaluate adversarial\nattacks and guide the selection of deep learning models in autonomous driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jindi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kejie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohua Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Machine Learning"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.MM updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.MM","description":"Computer Science -- Multimedia (cs.MM) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning to Cut by Watching Movies. (arXiv:2108.04294v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04294","description":"<p>Video content creation keeps growing at an incredible pace; yet, creating\nengaging stories remains challenging and requires non-trivial video editing\nexpertise. Many video editing components are astonishingly hard to automate\nprimarily due to the lack of raw video materials. This paper focuses on a new\ntask for computational video editing, namely the task of raking cut\nplausibility. Our key idea is to leverage content that has already been edited\nto learn fine-grained audiovisual patterns that trigger cuts. To do this, we\nfirst collected a data source of more than 10K videos, from which we extract\nmore than 255K cuts. We devise a model that learns to discriminate between real\nand artificial cuts via contrastive learning. We set up a new task and a set of\nbaselines to benchmark video cut generation. We observe that our proposed model\noutperforms the baselines by large margins. To demonstrate our model in\nreal-world applications, we conduct human studies in a collection of unedited\nvideos. The results show that our model does a better job at cutting than\nrandom and alternative baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Open Framework for Analyzing and Modeling XR Network Traffic. (arXiv:2108.04577v1 [cs.NI])","link":"http://arxiv.org/abs/2108.04577","description":"<p>Thanks to recent advancements in the technology, eXtended Reality (XR)\napplications are gaining a lot of momentum, and they will surely become\nincreasingly popular in the next decade. These new applications, however,\nrequire a step forward also in terms of models to simulate and analyze this\ntype of traffic sources in modern communication networks, in order to guarantee\nto the users state of the art performance and Quality of Experience (QoE).\nRecognizing this need, in this work, we present a novel open-source traffic\nmodel, which researchers can use as a starting point both for improvements of\nthe model itself and for the design of optimized algorithms for the\ntransmission of these peculiar data flows. Along with the mathematical model\nand the code, we also share with the community the traces that we gathered for\nour study, collected from freely available applications such as Minecraft VR,\nGoogle Earth VR, and Virus Popper. Finally, we propose a roadmap for the\nconstruction of an end-to-end framework that fills this gap in the current\nstate of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lecci_M/0/1/0/all/0/1\">Mattia Lecci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drago_M/0/1/0/all/0/1\">Matteo Drago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanella_A/0/1/0/all/0/1\">Andrea Zanella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorzi_M/0/1/0/all/0/1\">Michele Zorzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-aware Compositional Zero-shot Learning for Attribute-Object Pair Recognition. (arXiv:2108.04603v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04603","description":"<p>This paper proposes a novel model for recognizing images with composite\nattribute-object concepts, notably for composite concepts that are unseen\nduring model training. We aim to explore the three key properties required by\nthe task --- relation-aware, consistent, and decoupled --- to learn rich and\nrobust features for primitive concepts that compose attribute-object pairs. To\nthis end, we propose the Blocked Message Passing Network (BMP-Net). The model\nconsists of two modules. The concept module generates semantically meaningful\nfeatures for primitive concepts, whereas the visual module extracts visual\nfeatures for attributes and objects from input images. A message passing\nmechanism is used in the concept module to capture the relations between\nprimitive concepts. Furthermore, to prevent the model from being biased towards\nseen composite concepts and reduce the entanglement between attributes and\nobjects, we propose a blocking mechanism that equalizes the information\navailable to the model for both seen and unseen concepts. Extensive experiments\nand ablation studies on two benchmarks show the efficacy of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MCTSteg: A Monte Carlo Tree Search-based Reinforcement Learning Framework for Universal Non-additive Steganography. (arXiv:2103.13689v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2103.13689","description":"<p>Recent research has shown that non-additive image steganographic frameworks\neffectively improve security performance through adjusting distortion\ndistribution. However, as far as we know, all of the existing non-additive\nproposals are based on handcrafted policies, and can only be applied to a\nspecific image domain, which heavily prevent non-additive steganography from\nreleasing its full potentiality. In this paper, we propose an automatic\nnon-additive steganographic distortion learning framework called MCTSteg to\nremove the above restrictions. Guided by the reinforcement learning paradigm,\nwe combine Monte Carlo Tree Search (MCTS) and steganalyzer-based environmental\nmodel to build MCTSteg. MCTS makes sequential decisions to adjust distortion\ndistribution without human intervention. Our proposed environmental model is\nused to obtain feedbacks from each decision. Due to its self-learning\ncharacteristic and domain-independent reward function, MCTSteg has become the\nfirst reported universal non-additive steganographic framework which can work\nin both spatial and JPEG domains. Extensive experimental results show that\nMCTSteg can effectively withstand the detection of both hand-crafted\nfeature-based and deep-learning-based steganalyzers. In both spatial and JPEG\ndomains, the security performance of MCTSteg steadily outperforms the state of\nthe art by a clear margin under different scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xianbo Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Multimedia"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}