{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-23T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models. (arXiv:2108.08877v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08877","description":"<p>We provide the first exploration of text-to-text transformers (T5) sentence\nembeddings. Sentence embeddings are broadly useful for language processing\ntasks. While T5 achieves impressive performance on language tasks cast as\nsequence-to-sequence mapping problems, it is unclear how to produce sentence\nembeddings from encoder-decoder models. We investigate three methods for\nextracting T5 sentence embeddings: two utilize only the T5 encoder and one uses\nthe full T5 encoder-decoder model. Our encoder-only models outperforms\nBERT-based sentence embeddings on both transfer tasks and semantic textual\nsimilarity (STS). Our encoder-decoder method achieves further improvement on\nSTS. Scaling up T5 from millions to billions of parameters is found to produce\nconsistent improvements on downstream tasks. Finally, we introduce a two-stage\ncontrastive learning approach that achieves a new state-of-art on STS using\nsentence embeddings, outperforming both Sentence BERT and SimCSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7BA%7Dbrego_G/0/1/0/all/0/1\">Gustavo Hern&#xe1;ndez {&#xc1;}brego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_K/0/1/0/all/0/1\">Keith B. Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Neural Topic Modeling of Text Corpora. (arXiv:2108.08946v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08946","description":"<p>Topic Modeling refers to the problem of discovering the main topics that have\noccurred in corpora of textual data, with solutions finding crucial\napplications in numerous fields. In this work, inspired by the recent\nadvancements in the Natural Language Processing domain, we introduce FAME, an\nopen-source framework enabling an efficient mechanism of extracting and\nincorporating textual features and utilizing them in discovering topics and\nclustering text documents that are semantically similar in a corpus. These\nfeatures range from traditional approaches (e.g., frequency-based) to the most\nrecent auto-encoding embeddings from transformer-based language models such as\nBERT model family. To demonstrate the effectiveness of this library, we\nconducted experiments on the well-known News-Group dataset. The library is\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fazeli_S/0/1/0/all/0/1\">Shayan Fazeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarrafzadeh_M/0/1/0/all/0/1\">Majid Sarrafzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIGLI: Conditional Image Generation from Language & Image. (arXiv:2108.08955v1 [cs.CV])","link":"http://arxiv.org/abs/2108.08955","description":"<p>Multi-modal generation has been widely explored in recent years. Current\nresearch directions involve generating text based on an image or vice versa. In\nthis paper, we propose a new task called CIGLI: Conditional Image Generation\nfrom Language and Image. Instead of generating an image based on text as in\ntext-image generation, this task requires the generation of an image from a\ntextual description and an image prompt. We designed a new dataset to ensure\nthat the text description describes information from both images, and that\nsolely analyzing the description is insufficient to generate an image. We then\npropose a novel language-image fusion model which improves the performance over\ntwo established baseline methods, as evaluated by quantitative (automatic) and\nqualitative (human) evaluations. The code and dataset is available at\nhttps://github.com/vincentlux/CIGLI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1\">Jared Fernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localize, Group, and Select: Boosting Text-VQA by Scene Text Modeling. (arXiv:2108.08965v1 [cs.CV])","link":"http://arxiv.org/abs/2108.08965","description":"<p>As an important task in multimodal context understanding, Text-VQA (Visual\nQuestion Answering) aims at question answering through reading text information\nin images. It differentiates from the original VQA task as Text-VQA requires\nlarge amounts of scene-text relationship understanding, in addition to the\ncross-modal grounding capability. In this paper, we propose Localize, Group,\nand Select (LOGOS), a novel model which attempts to tackle this problem from\nmultiple aspects. LOGOS leverages two grounding tasks to better localize the\nkey information of the image, utilizes scene text clustering to group\nindividual OCR tokens, and learns to select the best answer from different\nsources of OCR (Optical Character Recognition) texts. Experiments show that\nLOGOS outperforms previous state-of-the-art methods on two Text-VQA benchmarks\nwithout using additional OCR annotation data. Ablation studies and analysis\ndemonstrate the capability of LOGOS to bridge different modalities and better\nunderstand scene text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yansen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_C/0/1/0/all/0/1\">Carolyn P. Rose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining. (arXiv:2108.08983v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08983","description":"<p>Recently, the performance of Pre-trained Language Models (PLMs) has been\nsignificantly improved by injecting knowledge facts to enhance their abilities\nof language understanding. For medical domains, the background knowledge\nsources are especially useful, due to the massive medical terms and their\ncomplicated relations are difficult to understand in text. In this work, we\nintroduce SMedBERT, a medical PLM trained on large-scale medical corpora,\nincorporating deep structured semantic knowledge from neighbors of\nlinked-entity.In SMedBERT, the mention-neighbor hybrid attention is proposed to\nlearn heterogeneous-entity information, which infuses the semantic\nrepresentations of entity types into the homogeneous neighboring entity\nstructure. Apart from knowledge integration as external features, we propose to\nemploy the neighbors of linked-entities in the knowledge graph as additional\nglobal contexts of text mentions, allowing them to communicate via shared\nneighbors, thus enrich their semantic representations. Experiments demonstrate\nthat SMedBERT significantly outperforms strong baselines in various\nknowledge-intensive Chinese medical tasks. It also improves the performance of\nother tasks such as question answering, question matching and natural language\ninference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zerui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bite Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v1 [cs.CL])","link":"http://arxiv.org/abs/2108.08988","description":"<p>Social media platforms provide convenient means for users to participate in\nmultiple online activities on various contents and create fast widespread\ninteractions. However, this rapidly growing access has also increased the\ndiverse information, and characterizing user types to understand people's\nlifestyle decisions shared in social media is challenging. In this paper, we\npropose a weakly supervised graph embedding based framework for understanding\nuser types. We evaluate the user embedding learned using weak supervision over\nwell-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.\nExperiments on real-world datasets demonstrate that the proposed framework\noutperforms the baselines for detecting user types. Finally, we illustrate data\nanalysis on different types of users (e.g., practitioner vs. promotional) from\nour dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our\nmethod for constructing user representation readily generalizes to other\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoMeSci- A 5 Star Open Data Gold Standard Knowledge Graph of Software Mentions in Scientific Articles. (arXiv:2108.09070v1 [cs.IR])","link":"http://arxiv.org/abs/2108.09070","description":"<p>Knowledge about software used in scientific investigations is important for\nseveral reasons, for instance, to enable an understanding of provenance and\nmethods involved in data handling. However, software is usually not formally\ncited, but rather mentioned informally within the scholarly description of the\ninvestigation, raising the need for automatic information extraction and\ndisambiguation. Given the lack of reliable ground truth data, we present\nSoMeSci (Software Mentions in Science) a gold standard knowledge graph of\nsoftware mentions in scientific articles. It contains high quality annotations\n(IRR: $\\kappa{=}.82$) of 3756 software mentions in 1367 PubMed Central\narticles. Besides the plain mention of the software, we also provide relation\nlabels for additional information, such as the version, the developer, a URL or\ncitations. Moreover, we distinguish between different types, such as\napplication, plugin or programming environment, as well as different types of\nmentions, such as usage or creation. To the best of our knowledge, SoMeSci is\nthe most comprehensive corpus about software mentions in scientific articles,\nproviding training samples for Named Entity Recognition, Relation Extraction,\nEntity Disambiguation, and Entity Linking. Finally, we sketch potential use\ncases and provide baseline results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schindler_D/0/1/0/all/0/1\">David Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bensmann_F/0/1/0/all/0/1\">Felix Bensmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1\">Stefan Dietze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1\">Frank Kr&#xfc;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention is All You Need. (arXiv:2108.09084v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEDIT: Geographic-Enhanced and Dependency-Guided Tagging for Joint POI and Accessibility Extraction at Baidu Maps. (arXiv:2108.09104v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09104","description":"<p>Providing timely accessibility reminders of a point-of-interest (POI) plays a\nvital role in improving user satisfaction of finding places and making visiting\ndecisions. However, it is difficult to keep the POI database in sync with the\nreal-world counterparts due to the dynamic nature of business changes. To\nalleviate this problem, we formulate and present a practical solution that\njointly extracts POI mentions and identifies their coupled accessibility labels\nfrom unstructured text. We approach this task as a sequence tagging problem,\nwhere the goal is to produce &lt;POI name, accessibility label&gt; pairs from\nunstructured text. This task is challenging because of two main issues: (1) POI\nnames are often newly-coined words so as to successfully register new entities\nor brands and (2) there may exist multiple pairs in the text, which\nnecessitates dealing with one-to-many or many-to-one mapping to make each POI\ncoupled with its accessibility label. To this end, we propose a\nGeographic-Enhanced and Dependency-guIded sequence Tagging (GEDIT) model to\nconcurrently address the two challenges. First, to alleviate challenge #1, we\ndevelop a geographic-enhanced pre-trained model to learn the text\nrepresentations. Second, to mitigate challenge #2, we apply a relational graph\nconvolutional network to learn the tree node representations from the parsed\ndependency tree. Finally, we construct a neural sequence tagging model by\nintegrating and feeding the previously pre-learned representations into a CRF\nlayer. Extensive experiments conducted on a real-world dataset demonstrate the\nsuperiority and effectiveness of GEDIT. In addition, it has already been\ndeployed in production at Baidu Maps. Statistics show that the proposed\nsolution can save significant human effort and labor costs to deal with the\nsame amount of documents, which confirms that it is a practical way for POI\naccessibility maintenance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jizhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunyuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1\">Miao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Airbert: In-domain Pretraining for Vision-and-Language Navigation. (arXiv:2108.09105v1 [cs.CV])","link":"http://arxiv.org/abs/2108.09105","description":"<p>Vision-and-language navigation (VLN) aims to enable embodied agents to\nnavigate in realistic environments using natural language instructions. Given\nthe scarcity of domain-specific training data and the high diversity of image\nand language inputs, the generalization of VLN agents to unseen environments\nremains challenging. Recent methods explore pretraining to improve\ngeneralization, however, the use of generic image-caption datasets or existing\nsmall-scale VLN environments is suboptimal and results in limited improvements.\nIn this work, we introduce BnB, a large-scale and diverse in-domain VLN\ndataset. We first collect image-caption (IC) pairs from hundreds of thousands\nof listings from online rental marketplaces. Using IC pairs we next propose\nautomatic strategies to generate millions of VLN path-instruction (PI) pairs.\nWe further propose a shuffling loss that improves the learning of temporal\norder inside PI pairs. We use BnB pretrain our Airbert model that can be\nadapted to discriminative and generative settings and show that it outperforms\nstate of the art for Room-to-Room (R2R) navigation and Remote Referring\nExpression (REVERIE) benchmarks. Moreover, our in-domain pretraining\nsignificantly increases performance on a challenging few-shot VLN evaluation,\nwhere we train the model only on VLN instructions from a few houses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guhur_P/0/1/0/all/0/1\">Pierre-Louis Guhur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tapaswi_M/0/1/0/all/0/1\">Makarand Tapaswi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Communication with Adaptive Universal Transformer. (arXiv:2108.09119v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09119","description":"<p>With the development of deep learning (DL), natural language processing (NLP)\nmakes it possible for us to analyze and understand a large amount of language\ntexts. Accordingly, we can achieve a semantic communication in terms of joint\nsemantic source and channel coding over a noisy channel with the help of NLP.\nHowever, the existing method to realize this goal is to use a fixed transformer\nof NLP while ignoring the difference of semantic information contained in each\nsentence. To solve this problem, we propose a new semantic communication system\nbased on Universal Transformer. Compared with the traditional transformer, an\nadaptive circulation mechanism is introduced in the Universal Transformer.\nThrough the introduction of the circulation mechanism, the new semantic\ncommunication system can be more flexible to transmit sentences with different\nsemantic information, and achieve better end-to-end performance under various\nchannel conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chenghui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group-based Distinctive Image Captioning with Memory Attention. (arXiv:2108.09151v1 [cs.CV])","link":"http://arxiv.org/abs/2108.09151","description":"<p>Describing images using natural language is widely known as image captioning,\nwhich has made consistent progress due to the development of computer vision\nand natural language generation techniques. Though conventional captioning\nmodels achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and\nSPICE, the ability of captions to distinguish the target image from other\nsimilar images is under-explored. To generate distinctive captions, a few\npioneers employ contrastive learning or re-weighted the ground-truth captions,\nwhich focuses on one single input image. However, the relationships between\nobjects in a similar image group (e.g., items or properties within the same\nalbum or fine-grained events) are neglected. In this paper, we improve the\ndistinctiveness of image captions using a Group-based Distinctive Captioning\nModel (GdisCap), which compares each image with other images in one similar\ngroup and highlights the uniqueness of each image. In particular, we propose a\ngroup-based memory attention (GMA) module, which stores object features that\nare unique among the image group (i.e., with low similarity to objects in other\nimages). These unique object features are highlighted when generating captions,\nresulting in more distinctive captions. Furthermore, the distinctive words in\nthe ground-truth captions are selected to supervise the language decoder and\nGMA. Finally, we propose a new evaluation metric, distinctive word rate\n(DisWordRate) to measure the distinctiveness of captions. Quantitative results\nindicate that the proposed method significantly improves the distinctiveness of\nseveral baseline models, and achieves the state-of-the-art performance on both\naccuracy and distinctiveness. Results of a user study agree with the\nquantitative evaluation and demonstrate the rationality of the new metric\nDisWordRate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiuniu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Antoni B. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Conversation Generation Model via Equivalent Shared Memory Investigation. (arXiv:2108.09164v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09164","description":"<p>Conversation generation as a challenging task in Natural Language Generation\n(NLG) has been increasingly attracting attention over the last years. A number\nof recent works adopted sequence-to-sequence structures along with external\nknowledge, which successfully enhanced the quality of generated conversations.\nNevertheless, few works utilized the knowledge extracted from similar\nconversations for utterance generation. Taking conversations in customer\nservice and court debate domains as examples, it is evident that essential\nentities/phrases, as well as their associated logic and inter-relationships can\nbe extracted and borrowed from similar conversation instances. Such information\ncould provide useful signals for improving conversation generation. In this\npaper, we propose a novel reading and memory framework called Deep Reading\nMemory Network (DRMN) which is capable of remembering useful information of\nsimilar conversations for improving utterance generation. We apply our model to\ntwo large-scale conversation datasets of justice and e-commerce fields.\nExperiments prove that the proposed model outperforms the state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_C/0/1/0/all/0/1\">Changzhen Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yating Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Conghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09193","description":"<p>Transformer has achieved great success in NLP. However, the quadratic\ncomplexity of the self-attention mechanism in Transformer makes it inefficient\nin handling long sequences. Many existing works explore to accelerate\nTransformers by computing sparse self-attention instead of a dense one, which\nusually attends to tokens at certain positions or randomly selected tokens.\nHowever, manually selected or random tokens may be uninformative for context\nmodeling. In this paper, we propose Smart Bird, which is an efficient and\neffective Transformer with learnable sparse attention. In Smart Bird, we first\ncompute a sketched attention matrix with a single-head low-dimensional\nTransformer, which aims to find potential important interactions between\ntokens. We then sample token pairs based on their probability scores derived\nfrom the sketched attention matrix to generate different sparse attention index\nmatrices for different attention heads. Finally, we select token embeddings\naccording to the index matrices to form the input of sparse attention networks.\nExtensive experiments on six benchmark datasets for different tasks validate\nthe efficiency and effectiveness of Smart Bird in text modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Radiological Findings With Normalized Anatomical Information Using a Span-Based BERT Relation Extraction Model. (arXiv:2108.09211v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09211","description":"<p>Medical imaging is critical to the diagnosis and treatment of numerous\nmedical problems, including many forms of cancer. Medical imaging reports\ndistill the findings and observations of radiologists, creating an unstructured\ntextual representation of unstructured medical images. Large-scale use of this\ntext-encoded information requires converting the unstructured text to a\nstructured, semantic representation. We explore the extraction and\nnormalization of anatomical information in radiology reports that is associated\nwith radiological findings. We investigate this extraction and normalization\ntask using a span-based relation extraction model that jointly extracts\nentities and relations using BERT. This work examines the factors that\ninfluence extraction and normalization performance, including the body\npart/organ system, frequency of occurrence, span length, and span diversity. It\ndiscusses approaches for improving performance and creating high-quality\nsemantic representations of radiological phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damani_A/0/1/0/all/0/1\">Aashka Damani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunn_M/0/1/0/all/0/1\">Martin Gunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzuner_O/0/1/0/all/0/1\">Ozlem Uzuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Relation Modeling: Learning to Define Relations between Entities. (arXiv:2108.09241v1 [cs.CL])","link":"http://arxiv.org/abs/2108.09241","description":"<p>Relations between entities can be represented by different instances, e.g., a\nsentence containing both entities or a fact in a Knowledge Graph (KG). However,\nthese instances may not well capture the general relations between entities,\nmay be difficult to understand by humans, even may not be found due to the\nincompleteness of the knowledge source.\n</p>\n<p>In this paper, we introduce the Open Relation Modeling task - given two\nentities, generate a coherent sentence describing the relation between them. To\nsolve this task, we propose to teach machines to generate definition-like\nrelation descriptions by letting them learn from definitions of entities.\nSpecifically, we fine-tune Pre-trained Language Models (PLMs) to produce\ndefinitions conditioned on extracted entity pairs. To help PLMs reason between\nentities and provide additional relational knowledge to PLMs for open relation\nmodeling, we incorporate reasoning paths in KGs and include a reasoning path\nselection mechanism. We show that PLMs can select interpretable and informative\nreasoning paths by confidence estimation, and the selected path can guide PLMs\nto generate better relation descriptions. Experimental results show that our\nmodel can generate concise but informative relation descriptions that capture\nthe representative characteristics of entities and relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwu_W/0/1/0/all/0/1\">Wen-mei Hwu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v9 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.02358","description":"<p>Sinhala is the native language of the Sinhalese people who make up the\nlargest ethnic group of Sri Lanka. The language belongs to the globe-spanning\nlanguage tree, Indo-European. However, due to poverty in both linguistic and\neconomic capital, Sinhala, in the perspective of Natural Language Processing\ntools and research, remains a resource-poor language which has neither the\neconomic drive its cousin English has nor the sheer push of the law of numbers\na language such as Chinese has. A number of research groups from Sri Lanka have\nnoticed this dearth and the resultant dire need for proper tools and research\nfor Sinhala natural language processing. However, due to various reasons, these\nattempts seem to lack coordination and awareness of each other. The objective\nof this paper is to fill that gap of a comprehensive literature survey of the\npublicly available Sinhala natural language tools and research so that the\nresearchers working in this field can better utilize contributions of their\npeers. As such, we shall be uploading this paper to arXiv and perpetually\nupdate it periodically to reflect the advances made in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_N/0/1/0/all/0/1\">Nisansa de Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Empowered Representation Learning for Chinese Medical Reading Comprehension: Task, Model and Resources. (arXiv:2008.10327v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.10327","description":"<p>Machine Reading Comprehension (MRC) aims to extract answers to questions\ngiven a passage. It has been widely studied recently, especially in open\ndomains. However, few efforts have been made on closed-domain MRC, mainly due\nto the lack of large-scale training data. In this paper, we introduce a\nmulti-target MRC task for the medical domain, whose goal is to predict answers\nto medical questions and the corresponding support sentences from medical\ninformation sources simultaneously, in order to ensure the high reliability of\nmedical knowledge serving. A high-quality dataset is manually constructed for\nthe purpose, named Multi-task Chinese Medical MRC dataset (CMedMRC), with\ndetailed analysis conducted. We further propose the Chinese medical BERT model\nfor the task (CMedBERT), which fuses medical knowledge into pre-trained\nlanguage models by the dynamic fusion mechanism of heterogeneous features and\nthe multi-task learning strategy. Experiments show that CMedBERT consistently\noutperforms strong baselines by fusing context-aware and knowledge-aware token\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Taolin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bite Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation for Singing Voice Detection. (arXiv:2011.04297v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2011.04297","description":"<p>Singing Voice Detection (SVD) has been an active area of research in music\ninformation retrieval (MIR). Currently, two deep neural network-based methods,\none based on CNN and the other on RNN, exist in literature that learn optimized\nfeatures for the voice detection (VD) task and achieve state-of-the-art\nperformance on common datasets. Both these models have a huge number of\nparameters (1.4M for CNN and 65.7K for RNN) and hence not suitable for\ndeployment on devices like smartphones or embedded sensors with limited\ncapacity in terms of memory and computation power. The most popular method to\naddress this issue is known as knowledge distillation in deep learning\nliterature (in addition to model compression) where a large pre-trained network\nknown as the teacher is used to train a smaller student network. Given the wide\napplications of SVD in music information retrieval, to the best of our\nknowledge, model compression for practical deployment has not yet been\nexplored. In this paper, efforts have been made to investigate this issue using\nboth conventional as well as ensemble knowledge distillation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Soumava Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_G/0/1/0/all/0/1\">Gurunath Reddy M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">K Sreenivasa Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1\">Partha Pratim Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EasyTransfer -- A Simple and Scalable Deep Transfer Learning Platform for NLP Applications. (arXiv:2011.09463v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.09463","description":"<p>The literature has witnessed the success of leveraging Pre-trained Language\nModels (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural\nLanguage Processing (NLP) applications, yet it is not easy to build an\neasy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the\nEasyTransfer platform is designed to develop deep TL algorithms for NLP\napplications. EasyTransfer is backended with a high-performance and scalable\nengine for efficient training and inference, and also integrates comprehensive\ndeep TL algorithms, to make the development of industrial-scale TL applications\neasier. In EasyTransfer, the built-in data and model parallelism strategies,\ncombined with AI compiler optimization, show to be 4.0x faster than the\ncommunity version of distributed training. EasyTransfer supports various NLP\nmodels in the ModelZoo, including mainstream PLMs and multi-modality models. It\nalso features various in-house developed TL algorithms, together with the\nAppZoo for NLP applications. The toolkit is convenient for users to quickly\nstart model training, evaluation, and online deployment. EasyTransfer is\ncurrently deployed at Alibaba to support a variety of business scenarios,\nincluding item recommendation, personalized search, conversational question\nanswering, etc. Extensive experiments on real-world datasets and online\napplications show that EasyTransfer is suitable for online production with\ncutting-edge performance for various applications. The source code of\nEasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1\">Minghui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hanjie Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Verification and Reranking for Open Fact Checking Over Tables. (arXiv:2012.15115v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15115","description":"<p>Structured information is an important knowledge source for automatic\nverification of factual claims. Nevertheless, the majority of existing research\ninto this task has focused on textual data, and the few recent inquiries into\nstructured data have been for the closed-domain setting where appropriate\nevidence for each claim is assumed to have already been retrieved. In this\npaper, we investigate verification over structured data in the open-domain\nsetting, introducing a joint reranking-and-verification model which fuses\nevidence documents in the verification component. Our open-domain model\nachieves performance comparable to the closed-domain state-of-the-art on the\nTabFact dataset, and demonstrates performance gains from the inclusion of\nmultiple tables as well as a significant improvement over a heuristic retrieval\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"De-identifying Hospital Discharge Summaries: An End-to-End Framework using Ensemble of Deep Learning Models. (arXiv:2101.00146v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00146","description":"<p>Electronic Medical Records contain clinical narrative text that is of great\npotential value to medical researchers. However, this information is mixed with\nPersonally Identifiable Information that presents risks to patient and\nclinician confidentiality. This paper presents an end-to-end de-identification\nframework to automatically remove PII from hospital discharge summaries. Our\ncorpus included 600 hospital discharge summaries which were extracted from the\nEMRs of two principal referral hospitals in Sydney, Australia. Our end-to-end\nde-identification framework consists of three components: 1) Annotation:\nlabelling of PII in the hospital discharge summaries using five pre-defined\ncategories: person, address, date of birth, individual identification number,\nphone/fax number; 2) Modelling: training six named entity recognition deep\nlearning base-models on balanced and imbalanced datasets; and evaluating\nensembles that combine all six base-models, the three base-models with the best\nF1 scores and the three base-models with the best recall scores respectively,\nusing token-level majority voting and stacking methods; and 3)\nDe-identification: removing PII from the hospital discharge summaries. Our\nresults showed that the ensemble model combined using the stacking Support\nVector Machine method on the three base-models with the best F1 scores achieved\nexcellent results with a F1 score of 99.16% on the test set of our corpus. We\nalso evaluated the robustness of our modelling component on the 2014 i2b2\nde-identification dataset. Our ensemble model, which uses the token-level\nmajority voting method on all six base-models, achieved the highest F1 score of\n96.24% at strict entity matching and the highest F1 score of 98.64% at binary\ntoken-level matching compared to two state-of-the-art methods. The end-to-end\nframework provides a robust solution to de-identifying clinical narrative\ncorpuses safely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Concha_O/0/1/0/all/0/1\">Oscar Perez-Concha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anthony Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_V/0/1/0/all/0/1\">Vicki Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorm_L/0/1/0/all/0/1\">Louisa Jorm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Czert -- Czech BERT-like Model for Language Representation. (arXiv:2103.13031v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.13031","description":"<p>This paper describes the training process of the first Czech monolingual\nlanguage representation models based on BERT and ALBERT architectures. We\npre-train our models on more than 340K of sentences, which is 50 times more\nthan multilingual models that include Czech data. We outperform the\nmultilingual models on 9 out of 11 datasets. In addition, we establish the new\nstate-of-the-art results on nine datasets. At the end, we discuss properties of\nmonolingual and multilingual models based upon our results. We publish all the\npre-trained and fine-tuned models freely for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priban_P/0/1/0/all/0/1\">Pavel P&#x159;ib&#xe1;&#x148;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasek_J/0/1/0/all/0/1\">Jan Pa&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejak_M/0/1/0/all/0/1\">Michal Sej&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Understanding with Privacy-Preserving BERT. (arXiv:2104.07504v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07504","description":"<p>Privacy preservation remains a key challenge in data mining and Natural\nLanguage Understanding (NLU). Previous research shows that the input text or\neven text embeddings can leak private information. This concern motivates our\nresearch on effective privacy preservation approaches for pretrained Language\nModels (LMs). We investigate the privacy and utility implications of applying\ndx-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU\napplications. More importantly, we further propose privacy-adaptive LM\npretraining methods and show that our approach can boost the utility of BERT\ndramatically while retaining the same level of privacy protection. We also\nquantify the level of privacy preservation and provide guidance on privacy\nconfiguration. Our experiments and findings lay the groundwork for future\nexplorations of privacy-preserving NLU with pretrained LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Chen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weize Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendersky_M/0/1/0/all/0/1\">Michael Bendersky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1\">Marc Najork</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning from Reformulations in Conversational Question Answering over Knowledge Graphs. (arXiv:2105.04850v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2105.04850","description":"<p>The rise of personal assistants has made conversational question answering\n(ConvQA) a very popular mechanism for user-system interaction. State-of-the-art\nmethods for ConvQA over knowledge graphs (KGs) can only learn from crisp\nquestion-answer pairs found in popular benchmarks. In reality, however, such\ntraining data is hard to come by: users would rarely mark answers explicitly as\ncorrect or wrong. In this work, we take a step towards a more natural learning\nparadigm - from noisy and implicit feedback via question reformulations. A\nreformulation is likely to be triggered by an incorrect system response,\nwhereas a new follow-up question could be a positive signal on the previous\nturn's answer. We present a reinforcement learning model, termed CONQUER, that\ncan learn from a conversational stream of questions and reformulations. CONQUER\nmodels the answering process as multiple agents walking in parallel on the KG,\nwhere the walks are determined by actions sampled using a policy network. This\npolicy network takes the question along with the conversational context as\ninputs and is trained via noisy rewards obtained from the reformulation\nlikelihood. To evaluate CONQUER, we create and release ConvRef, a benchmark\nwith about 11k natural conversations containing around 205k reformulations.\nExperiments show that CONQUER successfully learns to answer conversational\nquestions from noisy reward signals, significantly improving over a\nstate-of-the-art baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1\">Magdalena Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRASEMap: A Probabilistic Reasoning and Semantic Embedding based Knowledge Graph Alignment System. (arXiv:2106.08801v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08801","description":"<p>Knowledge Graph (KG) alignment aims at finding equivalent entities and\nrelations (i.e., mappings) between two KGs. The existing approaches utilize\neither reasoning-based or semantic embedding-based techniques, but few studies\nexplore their combination. In this demonstration, we present PRASEMap, an\nunsupervised KG alignment system that iteratively computes the Mappings with\nboth Probabilistic Reasoning (PR) And Semantic Embedding (SE) techniques.\nPRASEMap can support various embedding-based KG alignment approaches as the SE\nmodule, and enables easy human computer interaction that additionally provides\nan option for users to feed the mapping annotations back to the system for\nbetter results. The demonstration showcases these features via a stand-alone\nWeb application with user friendly interfaces. The demo is available at\nhttps://prasemap.qizhy.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiyuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Entity Extraction Method Based on Machine Reading Comprehension. (arXiv:2108.06444v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06444","description":"<p>Entity extraction is a key technology for obtaining information from massive\ntexts in natural language processing. The further interaction between them does\nnot meet the standards of human reading comprehension, thus limiting the\nunderstanding of the model, and also the omission or misjudgment of the answer\n(ie the target entity) due to the reasoning question. An effective MRC-based\nentity extraction model-MRC-I2DP, which uses the proposed gated\nattention-attracting mechanism to adjust the restoration of each part of the\ntext pair, creating problems and thinking for multi-level interactive attention\ncalculations to increase the target entity It also uses the proposed 2D\nprobability coding module, TALU function and mask mechanism to strengthen the\ndetection of all possible targets of the target, thereby improving the\nprobability and accuracy of prediction. Experiments have proved that MRC-I2DP\nrepresents an overall state-of-the-art model in 7 from the scientific and\npublic domains, achieving a performance improvement of up to compared to the\nmodel model in F1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaobo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiajun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guangyu Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-22T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}