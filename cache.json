{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DisCoDisCo at the DISRPT2021 Shared Task: A System for Discourse Segmentation, Classification, and Connective Detection. (arXiv:2109.09777v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09777","description":"<p>This paper describes our submission to the DISRPT2021 Shared Task on\nDiscourse Unit Segmentation, Connective Detection, and Relation Classification.\nOur system, called DisCoDisCo, is a Transformer-based neural classifier which\nenhances contextualized word embeddings (CWEs) with hand-crafted features,\nrelying on tokenwise sequence tagging for discourse segmentation and connective\ndetection, and a feature-rich, encoder-less sentence pair classifier for\nrelation classification. Our results for the first two tasks outperform SOTA\nscores from the previous 2019 shared task, and results on relation\nclassification suggest strong performance on the new 2021 benchmark. Ablation\ntests show that including features beyond CWEs are helpful for both tasks, and\na partial evaluation of multiple pre-trained Transformer-based language models\nindicates that models pre-trained on the Next Sentence Prediction (NSP) task\nare optimal for relation classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gessler_L/0/1/0/all/0/1\">Luke Gessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behzad_S/0/1/0/all/0/1\">Shabnam Behzad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Janet Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Siyao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yilun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology. (arXiv:2109.09780v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09780","description":"<p>An important question concerning contextualized word embedding (CWE) models\nlike BERT is how well they can represent different word senses, especially\nthose in the long tail of uncommon senses. Rather than build a WSD system as in\nprevious work, we investigate contextualized embedding neighborhoods directly,\nformulating a query-by-example nearest neighbor retrieval task and examining\nranking performance for words and senses in different frequency bands. In an\nevaluation on two English sense-annotated corpora, we find that several popular\nCWE models all outperform a random baseline even for proportionally rare\nsenses, without explicit sense supervision. However, performance varies\nconsiderably even among models with similar architectures and pretraining\nregimes, with especially large differences for rare word senses, revealing that\nCWE models are not all created equal when it comes to approximating word senses\nin their native representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gessler_L/0/1/0/all/0/1\">Luke Gessler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization. (arXiv:2109.09784v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09784","description":"<p>State-of-the-art abstractive summarization systems often generate\n\\emph{hallucinations}; i.e., content that is not directly inferable from the\nsource text. Despite being assumed incorrect, many of the hallucinated contents\nare consistent with world knowledge (factual hallucinations). Including these\nfactual hallucinations into a summary can be beneficial in providing additional\nbackground information. In this work, we propose a novel detection approach\nthat separates factual from non-factual hallucinations of entities. Our method\nis based on an entity's prior and posterior probabilities according to\npre-trained and finetuned masked language models, respectively. Empirical\nresults suggest that our method vastly outperforms three strong baselines in\nboth accuracy and F1 scores and has a strong correlation with human judgments\non factuality classification tasks. Furthermore, our approach can provide\ninsight into whether a particular hallucination is caused by the summarizer's\npre-training or fine-tuning step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Meng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yue Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dependency Induction Through the Lens of Visual Perception. (arXiv:2109.09790v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09790","description":"<p>Most previous work on grammar induction focuses on learning phrasal or\ndependency structure purely from text. However, because the signal provided by\ntext alone is limited, recently introduced visually grounded syntax models make\nuse of multimodal information leading to improved performance in constituency\ngrammar induction. However, as compared to dependency grammars, constituency\ngrammars do not provide a straightforward way to incorporate visual information\nwithout enforcing language-specific heuristics. In this paper, we propose an\nunsupervised grammar induction model that leverages word concreteness and a\nstructural vision-based heuristic to jointly learn constituency-structure and\ndependency-structure grammars. Our experiments find that concreteness is a\nstrong indicator for learning dependency grammars, improving the direct\nattachment score (DAS) by over 50\\% as compared to state-of-the-art models\ntrained on pure text. Next, we propose an extension of our model that leverages\nboth word concreteness and visual semantic role labels in constituency and\ndependency parsing. Our experiments show that the proposed extension\noutperforms the current state-of-the-art visually grounded models in\nconstituency parsing even with a smaller grammar size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1\">Ruisi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming Fake News: Robust Generalisable News Classification Using Transformers. (arXiv:2109.09796v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09796","description":"<p>As online news has become increasingly popular and fake news increasingly\nprevalent, the ability to audit the veracity of online news content has become\nmore important than ever. Such a task represents a binary classification\nchallenge, for which transformers have achieved state-of-the-art results. Using\nthe publicly available ISOT and Combined Corpus datasets, this study explores\ntransformers' abilities to identify fake news, with particular attention given\nto investigating generalisation to unseen datasets with varying styles, topics\nand class distributions. Moreover, we explore the idea that opinion-based news\narticles cannot be classified as real or fake due to their subjective nature\nand often sensationalised language, and propose a novel two-step classification\npipeline to remove such articles from both model training and the final\ndeployed inference system. Experiments over the ISOT and Combined Corpus\ndatasets show that transformers achieve an increase in F1 scores of up to 4.9%\nfor out of distribution generalisation compared to baseline approaches, with a\nfurther increase of 10.1% following the implementation of our two-step\nclassification pipeline. To the best of our knowledge, this study is the first\nto investigate generalisation of transformers in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blackledge_C/0/1/0/all/0/1\">Ciara Blackledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Span Representation for Domain-adapted Coreference Resolution. (arXiv:2109.09811v1 [cs.LG])","link":"http://arxiv.org/abs/2109.09811","description":"<p>Recent work has shown fine-tuning neural coreference models can produce\nstrong performance when adapting to different domains. However, at the same\ntime, this can require a large amount of annotated target examples. In this\nwork, we focus on supervised domain adaptation for clinical notes, proposing\nthe use of concept knowledge to more efficiently adapt coreference models to a\nnew domain. We develop methods to improve the span representations via (1) a\nretrofitting loss to incentivize span representations to satisfy a\nknowledge-based distance function and (2) a scaffolding loss to guide the\nrecovery of knowledge from the span representation. By integrating these\nlosses, our model is able to improve our baseline precision and F-1 score. In\nparticular, we show that incorporating knowledge with end-to-end coreference\nmodels results in better performance on the most challenging, domain-specific\nspans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_N/0/1/0/all/0/1\">Nupoor Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation Methods for Anaphoric Zero Pronouns. (arXiv:2109.09825v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09825","description":"<p>In pro-drop language like Arabic, Chinese, Italian, Japanese, Spanish, and\nmany others, unrealized (null) arguments in certain syntactic positions can\nrefer to a previously introduced entity, and are thus called anaphoric zero\npronouns. The existing resources for studying anaphoric zero pronoun\ninterpretation are however still limited. In this paper, we use five data\naugmentation methods to generate and detect anaphoric zero pronouns\nautomatically. We use the augmented data as additional training materials for\ntwo anaphoric zero pronoun systems for Arabic. Our experimental results show\nthat data augmentation improves the performance of the two systems, surpassing\nthe state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aloraini_A/0/1/0/all/0/1\">Abdulrahman Aloraini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StreamSide: A Fully-Customizable Open-Source Toolkit for Efficient Annotation of Meaning Representations. (arXiv:2109.09853v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09853","description":"<p>This demonstration paper presents StreamSide, an open-source toolkit for\nannotating multiple kinds of meaning representations. StreamSide supports\nframe-based annotation schemes e.g., Abstract Meaning Representation (AMR) and\nframeless annotation schemes e.g., Widely Interpretable Semantic Representation\n(WISeR). Moreover, it supports both sentence-level and document-level\nannotation by allowing annotators to create multi-rooted graphs for input text.\nIt can open and automatically convert between several types of input formats\nincluding plain text, Penman notation, and its own JSON format enabling richer\nannotation. It features reference frames for AMR predicate argument structures,\nand also concept-to-text alignment. StreamSide is released under the Apache 2.0\nlicense, and is completely open-source so that it can be customized to annotate\nenriched meaning representations in different languages (e.g., Uniform Meaning\nRepresentations). All StreamSide resources are publicly distributed through our\nopen source project at: https://github.com/emorynlp/StreamSide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_G/0/1/0/all/0/1\">Gregor Williamson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intensionalizing Abstract Meaning Representations: Non-Veridicality and Scope. (arXiv:2109.09858v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09858","description":"<p>Abstract Meaning Representation (AMR) is a graphical meaning representation\nlanguage designed to represent propositional information about argument\nstructure. However, at present it is unable to satisfyingly represent\nnon-veridical intensional contexts, often licensing inappropriate inferences.\nIn this paper, we show how to resolve the problem of non-veridicality without\nappealing to layered graphs through a mapping from AMRs into Simply-Typed\nLambda Calculus (STLC). At least for some cases, this requires the introduction\nof a new role :content which functions as an intensional operator. The\ntranslation proposed is inspired by the formal linguistics literature on the\nevent semantics of attitude reports. Next, we address the interaction of\nquantifier scope and intensional operators in so-called de re/de dicto\nambiguities. We adopt a scope node from the literature and provide an explicit\nmultidimensional semantics utilizing Cooper storage which allows us to derive\nthe de re and de dicto scope readings as well as intermediate scope readings\nwhich prove difficult for accounts without a scope node.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Williamson_G/0/1/0/all/0/1\">Gregor Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_P/0/1/0/all/0/1\">Patrick Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuxin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Identification with a Reciprocal Rank Classifier. (arXiv:2109.09862v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09862","description":"<p>Language identification is a critical component of language processing\npipelines (Jauhiainen et al.,2019) and is not a solved problem in real-world\nsettings. We present a lightweight and effective language identifier that is\nrobust to changes of domain and to the absence of copious training data.\n</p>\n<p>The key idea for classification is that the reciprocal of the rank in a\nfrequency table makes an effective additive feature score, hence the term\nReciprocal Rank Classifier (RRC). The key finding for language classification\nis that ranked lists of words and frequencies of characters form a sufficient\nand robust representation of the regularities of key languages and their\northographies.\n</p>\n<p>We test this on two 22-language data sets and demonstrate zero-effort domain\nadaptation from a Wikipedia training set to a Twitter test set. When trained on\nWikipedia but applied to Twitter the macro-averaged F1-score of a\nconventionally trained SVM classifier drops from 90.9% to 77.7%. By contrast,\nthe macro F1-score of RRC drops only from 93.1% to 90.6%. These classifiers are\ncompared with those from fastText and langid. The RRC performs better than\nthese established systems in most experiments, especially on short Wikipedia\ntexts and Twitter.\n</p>\n<p>The RRC classifier can be improved for particular domains and conversational\nsituations by adding words to the ranked lists. Using new terms learned from\nsuch conversations, we demonstrate a further 7.9% increase in accuracy of\nsample message classification, and 1.7% increase for conversation\nclassification. Surprisingly, this made results on Twitter data slightly worse.\n</p>\n<p>The RRC classifier is available as an open source Python package\n(https://github.com/LivePersonInc/lplangid).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Widdows_D/0/1/0/all/0/1\">Dominic Widdows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brew_C/0/1/0/all/0/1\">Chris Brew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Learning for Short Text Clustering. (arXiv:2109.09894v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09894","description":"<p>Effective representation learning is critical for short text clustering due\nto the sparse, high-dimensional and noise attributes of short text corpus.\nExisting pre-trained models (e.g., Word2vec and BERT) have greatly improved the\nexpressiveness for short text representations with more condensed,\nlow-dimensional and continuous features compared to the traditional\nBag-of-Words (BoW) model. However, these models are trained for general\npurposes and thus are suboptimal for the short text clustering task. In this\npaper, we propose two methods to exploit the unsupervised autoencoder (AE)\nframework to further tune the short text representations based on these\npre-trained text models for optimal clustering performance. In our first method\nStructural Text Network Graph Autoencoder (STN-GAE), we exploit the structural\ntext information among the corpus by constructing a text network, and then\nadopt graph convolutional network as encoder to fuse the structural features\nwith the pre-trained text features for text representation learning. In our\nsecond method Soft Cluster Assignment Autoencoder (SCA-AE), we adopt an extra\nsoft cluster assignment constraint on the latent space of autoencoder to\nencourage the learned text representations to be more clustering-friendly. We\ntested two methods on seven popular short text datasets, and the experimental\nresults show that when only using the pre-trained model for short text\nclustering, BERT performs better than BoW and Word2vec. However, as long as we\nfurther tune the pre-trained representations, the proposed method like SCA-AE\ncan greatly increase the clustering performance, and the accuracy improvement\ncompared to use BERT alone could reach as much as 14\\%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hui Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiangyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuiqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guangyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization in Text-based Games via Hierarchical Reinforcement Learning. (arXiv:2109.09968v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09968","description":"<p>Deep reinforcement learning provides a promising approach for text-based\ngames in studying natural language communication between humans and artificial\nagents. However, the generalization still remains a big challenge as the agents\ndepend critically on the complexity and variety of training tasks. In this\npaper, we address this problem by introducing a hierarchical framework built\nupon the knowledge graph-based RL agent. In the high level, a meta-policy is\nexecuted to decompose the whole game into a set of subtasks specified by\ntextual goals, and select one of them based on the KG. Then a sub-policy in the\nlow level is executed to conduct goal-conditioned reinforcement learning. We\ncarry out experiments on games with various difficulty levels and show that the\nproposed method enjoys favorable generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunqiu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Kernel-Smoothed Machine Translation with Retrieved Examples. (arXiv:2109.09991v1 [cs.CL])","link":"http://arxiv.org/abs/2109.09991","description":"<p>How to effectively adapt neural machine translation (NMT) models according to\nemerging cases without retraining? Despite the great success of neural machine\ntranslation, updating the deployed models online remains a challenge. Existing\nnon-parametric approaches that retrieve similar examples from a database to\nguide the translation process are promising but are prone to overfit the\nretrieved examples. However, non-parametric methods are prone to overfit the\nretrieved examples. In this work, we propose to learn Kernel-Smoothed\nTranslation with Example Retrieval (KSTER), an effective approach to adapt\nneural machine translation models online. Experiments on domain adaptation and\nmulti-domain machine translation datasets show that even without expensive\nretraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over\nthe best existing online adaptation methods. The code and trained models are\nreleased at https://github.com/jiangqn/KSTER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qingnan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negation-Instance Based Evaluation of End-to-End Negation Resolution. (arXiv:2109.10013v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10013","description":"<p>In this paper, we revisit the task of negation resolution, which includes the\nsubtasks of cue detection (e.g. \"not\", \"never\") and scope resolution. In the\ncontext of previous shared tasks, a variety of evaluation metrics have been\nproposed. Subsequent works usually use different subsets of these, including\nvariations and custom implementations, rendering meaningful comparisons between\nsystems difficult. Examining the problem both from a linguistic perspective and\nfrom a downstream viewpoint, we here argue for a negation-instance based\napproach to evaluating negation resolution. Our proposed metrics correspond to\nexpectations over per-instance scores and hence are intuitively interpretable.\nTo render research comparable and to foster future work, we provide results for\na set of current state-of-the-art systems for negation resolution on three\nEnglish corpora, and make our implementation of the evaluation scripts publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sineva_E/0/1/0/all/0/1\">Elizaveta Sineva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grunewald_S/0/1/0/all/0/1\">Stefan Gr&#xfc;newald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Annemarie Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhn_J/0/1/0/all/0/1\">Jonas Kuhn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Comments are Equal: Insights into Comment Moderation from a Topic-Aware Model. (arXiv:2109.10033v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10033","description":"<p>Moderation of reader comments is a significant problem for online news\nplatforms. Here, we experiment with models for automatic moderation, using a\ndataset of comments from a popular Croatian newspaper. Our analysis shows that\nwhile comments that violate the moderation rules mostly share common linguistic\nand thematic features, their content varies across the different sections of\nthe newspaper. We therefore make our models topic-aware, incorporating semantic\nfeatures from a topic model into the classification decision. Our results show\nthat topic information improves the performance of the model, increases its\nconfidence in correct outputs, and helps us understand the model's outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zosa_E/0/1/0/all/0/1\">Elaine Zosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_R/0/1/0/all/0/1\">Ravi Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karan_M/0/1/0/all/0/1\">Mladen Karan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10044","description":"<p>This report describes the parsing problem for Combinatory Categorial Grammar\n(CCG), showing how a combination of Transformer-based neural models and a\nsymbolic CCG grammar can lead to substantial gains over existing approaches.\nThe report also documents a 20-year research program, showing how NLP methods\nhave evolved over this time. The staggering accuracy improvements provided by\nneural models for CCG parsing can be seen as a reflection of the improvements\nseen in NLP more generally. The report provides a minimal introduction to CCG\nand CCG parsing, with many pointers to the relevant literature. It then\ndescribes the CCG supertagging problem, and some recent work from Tian et al.\n(2020) which applies Transformer-based models to supertagging with great\neffect. I use this existing model to develop a CCG multitagger, which can serve\nas a front-end to an existing CCG parser. Simply using this new multitagger\nprovides substantial gains in parsing accuracy. I then show how a\nTransformer-based model from the parsing literature can be combined with the\ngrammar-based CCG parser, setting a new state-of-the-art for the CCGbank\nparsing task of almost 93% F-score for labelled dependencies, with complete\nsentence accuracies of over 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1\">Stephen Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?. (arXiv:2109.10052v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10052","description":"<p>In this paper, we investigate what types of stereotypical information are\ncaptured by pretrained language models. We present the first dataset comprising\nstereotypical attributes of a range of social groups and propose a method to\nelicit stereotypes encoded by pretrained language models in an unsupervised\nfashion. Moreover, we link the emergent stereotypes to their manifestation as\nbasic emotions as a means to study their emotional effects in a more\ngeneralized manner. To demonstrate how our methods can be used to analyze\nemotion and stereotype shifts due to linguistic experience, we use fine-tuning\non news sources as a case study. Our experiments expose how attitudes towards\ndifferent social groups vary across models and how quickly emotions and\nstereotypes can shift at the fine-tuning stage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choenni_R/0/1/0/all/0/1\">Rochelle Choenni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1\">Ekaterina Shutova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rooij_R/0/1/0/all/0/1\">Robert van Rooij</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations. (arXiv:2109.10080v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10080","description":"<p>Adverse Drug Event (ADE) extraction mod-els can rapidly examine large\ncollections of so-cial media texts, detecting mentions of drug-related adverse\nreactions and trigger medicalinvestigations. However, despite the recent\nad-vances in NLP, it is currently unknown if suchmodels are robust in face\nofnegation, which ispervasive across language varieties.In this paper we\nevaluate three state-of-the-artsystems, showing their fragility against\nnega-tion, and then we introduce two possible strate-gies to increase the\nrobustness of these mod-els: a pipeline approach, relying on a\nspecificcomponent for negation detection; an augmen-tation of an ADE extraction\ndataset to artifi-cially create negated samples and further trainthe models.We\nshow that both strategies bring significantincreases in performance, lowering\nthe num-ber of spurious entities predicted by the mod-els. Our dataset and code\nwill be publicly re-leased to encourage research on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scaboro_S/0/1/0/all/0/1\">Simone Scaboro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelli_B/0/1/0/all/0/1\">Beatrice Portelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chersoni_E/0/1/0/all/0/1\">Emmanuele Chersoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santus_E/0/1/0/all/0/1\">Enrico Santus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval. (arXiv:2109.10086v1 [cs.IR])","link":"http://arxiv.org/abs/2109.10086","description":"<p>In neural Information Retrieval (IR), ongoing research is directed towards\nimproving the first retriever in ranking pipelines. Learning dense embeddings\nto conduct retrieval using efficient approximate nearest neighbors methods has\nproven to work well. Meanwhile, there has been a growing interest in learning\n\\emph{sparse} representations for documents and queries, that could inherit\nfrom the desirable properties of bag-of-words models such as the exact matching\nof terms and the efficiency of inverted indexes. Introduced recently, the\nSPLADE model provides highly sparse representations and competitive results\nwith respect to state-of-the-art dense and sparse approaches. In this paper, we\nbuild on SPLADE and propose several significant improvements in terms of\neffectiveness and/or efficiency. More specifically, we modify the pooling\nmechanism, benchmark a model solely based on document expansion, and introduce\nmodels trained with distillation. We also report results on the BEIR benchmark.\nOverall, SPLADE is considerably improved with more than $9$\\% gains on NDCG@10\non TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Formal_T/0/1/0/all/0/1\">Thibault Formal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassance_C/0/1/0/all/0/1\">Carlos Lassance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InvBERT: Text Reconstruction from Contextualized Embeddings used for Derived Text Formats of Literary Works. (arXiv:2109.10104v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10104","description":"<p>Digital Humanities and Computational Literary Studies apply text mining\nmethods to investigate literature. Such automated approaches enable\nquantitative studies on large corpora which would not be feasible by manual\ninspection alone. However, due to copyright restrictions, the availability of\nrelevant digitized literary works is limited. Derived Text Formats (DTFs) have\nbeen proposed as a solution. Here, textual materials are transformed in such a\nway that copyright-critical features are removed, but that the use of certain\nanalytical methods remains possible. Contextualized word embeddings produced by\ntransformer-encoders (like BERT) are promising candidates for DTFs because they\nallow for state-of-the-art performance on various analytical tasks and, at\nfirst sight, do not disclose the original text. However, in this paper we\ndemonstrate that under certain conditions the reconstruction of the original\ncopyrighted text becomes feasible and its publication in the form of\ncontextualized word representations is not safe. Our attempts to invert BERT\nsuggest, that publishing parts of the encoder together with the contextualized\nembeddings is critical, since it allows to generate data to train a decoder\nwith a reconstruction accuracy sufficient to violate copyright laws.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hohmann_J/0/1/0/all/0/1\">Johannes H&#xf6;hmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugler_K/0/1/0/all/0/1\">Kai Kugler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Difficulty of Segmenting Words with Attention. (arXiv:2109.10107v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10107","description":"<p>Word segmentation, the problem of finding word boundaries in speech, is of\ninterest for a range of tasks. Previous papers have suggested that for\nsequence-to-sequence models trained on tasks such as speech translation or\nspeech recognition, attention can be used to locate and segment the words. We\nshow, however, that even on monolingual data this approach is brittle. In our\nexperiments with different input types, data sizes, and segmentation\nalgorithms, only models trained to predict phones from words succeed in the\ntask. Models trained to predict words from either phones or speech (i.e., the\nopposite direction needed to generalize to new data), yield much worse results,\nsuggesting that attention-based segmentation is only useful in limited\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanabria_R/0/1/0/all/0/1\">Ramon Sanabria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review on Summarizing Financial News Using Deep Learning. (arXiv:2109.10118v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10118","description":"<p>Investors make investment decisions depending on several factors such as\nfundamental analysis, technical analysis, and quantitative analysis. Another\nfactor on which investors can make investment decisions is through sentiment\nanalysis of news headlines, the sole purpose of this study. Natural Language\nProcessing techniques are typically used to deal with such a large amount of\ndata and get valuable information out of it. NLP algorithms convert raw text\ninto numerical representations that machines can easily understand and\ninterpret. This conversion can be done using various embedding techniques. In\nthis research, embedding techniques used are BoW, TF-IDF, Word2Vec, BERT,\nGloVe, and FastText, and then fed to deep learning models such as RNN and LSTM.\nThis work aims to evaluate these model's performance to choose the robust model\nin identifying the significant factors influencing the prediction. During this\nresearch, it was expected that Deep Leaming would be applied to get the desired\nresults or achieve better accuracy than the state-of-the-art. The models are\ncompared to check their outputs to know which one has performed better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kamal_S/0/1/0/all/0/1\">Saurabh Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sahil Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvFiT: Conversational Fine-Tuning of Pretrained Language Models. (arXiv:2109.10126v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10126","description":"<p>Transformer-based language models (LMs) pretrained on large text collections\nare proven to store a wealth of semantic knowledge. However, 1) they are not\neffective as sentence encoders when used off-the-shelf, and 2) thus typically\nlag behind conversationally pretrained (e.g., via response selection) encoders\non conversational tasks such as intent detection (ID). In this work, we propose\nConvFiT, a simple and efficient two-stage procedure which turns any pretrained\nLM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and\ntask-specialised sentence encoder (after Stage 2). We demonstrate that 1)\nfull-blown conversational pretraining is not required, and that LMs can be\nquickly transformed into effective conversational encoders with much smaller\namounts of unannotated data; 2) pretrained LMs can be fine-tuned into\ntask-specialised sentence encoders, optimised for the fine-grained semantics of\na particular task. Consequently, such specialised sentence encoders allow for\ntreating ID as a simple semantic similarity task based on interpretable nearest\nneighbours retrieval. We validate the robustness and versatility of the ConvFiT\nframework with such similarity-based inference on the standard ID evaluation\nsets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board,\nwith particular gains in the most challenging, few-shot setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1\">Pei-Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coope_S/0/1/0/all/0/1\">Sam Coope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerz_D/0/1/0/all/0/1\">Daniela Gerz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1\">Pawe&#x142; Budzianowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1\">I&#xf1;igo Casanueva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mrksic_N/0/1/0/all/0/1\">Nikola Mrk&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_T/0/1/0/all/0/1\">Tsung-Hsien Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement. (arXiv:2109.10133v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10133","description":"<p>Many recent works have demonstrated that unsupervised sentence\nrepresentations of neural networks encode syntactic information by observing\nthat neural language models are able to predict the agreement between a verb\nand its subject. We take a critical look at this line of research by showing\nthat it is possible to achieve high accuracy on this agreement task with simple\nsurface heuristics, indicating a possible flaw in our assessment of neural\nnetworks' syntactic ability. Our fine-grained analyses of results on the\nlong-range French object-verb agreement show that contrary to LSTMs,\nTransformers are able to capture a non-trivial amount of grammatical structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisniewski_G/0/1/0/all/0/1\">Guillaume Wisniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crabbe_B/0/1/0/all/0/1\">Benoit Crabb&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation with Noisy Labels for Natural Language Understanding. (arXiv:2109.10147v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10147","description":"<p>Knowledge Distillation (KD) is extensively used to compress and deploy large\npre-trained language models on edge devices for real-world applications.\nHowever, one neglected area of research is the impact of noisy (corrupted)\nlabels on KD. We present, to the best of our knowledge, the first study on KD\nwith noisy labels in Natural Language Understanding (NLU). We document the\nscope of the problem and present two methods to mitigate the impact of label\nnoise. Experiments on the GLUE benchmark show that our methods are effective\neven under high noise levels. Nevertheless, our results indicate that more\nresearch is necessary to cope with label noise under the KD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_S/0/1/0/all/0/1\">Shivendra Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_K/0/1/0/all/0/1\">Khalil Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation. (arXiv:2109.10164v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10164","description":"<p>Intermediate layer knowledge distillation (KD) can improve the standard KD\ntechnique (which only targets the output of teacher and student models)\nespecially over large pre-trained language models. However, intermediate layer\ndistillation suffers from excessive computational burdens and engineering\nefforts required for setting up a proper layer mapping. To address these\nproblems, we propose a RAndom Intermediate Layer Knowledge Distillation\n(RAIL-KD) approach in which, intermediate layers from the teacher model are\nselected randomly to be distilled into the intermediate layers of the student\nmodel. This randomized selection enforce that: all teacher layers are taken\ninto account in the training process, while reducing the computational cost of\nintermediate layer distillation. Also, we show that it act as a regularizer for\nimproving the generalizability of the student model. We perform extensive\nexperiments on GLUE tasks as well as on out-of-domain test sets. We show that\nour proposed RAIL-KD approach outperforms other state-of-the-art intermediate\nlayer KD methods considerably in both performance and training-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haidar_M/0/1/0/all/0/1\">Md Akmal Haidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anchuri_N/0/1/0/all/0/1\">Nithin Anchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Familiar Does That Sound? Cross-Lingual Representational Similarity Analysis of Acoustic Word Embeddings. (arXiv:2109.10179v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10179","description":"<p>How do neural networks \"perceive\" speech sounds from unknown languages? Does\nthe typological similarity between the model's training language (L1) and an\nunknown language (L2) have an impact on the model representations of L2 speech\nsignals? To answer these questions, we present a novel experimental design\nbased on representational similarity analysis (RSA) to analyze acoustic word\nembeddings (AWEs) -- vector representations of variable-duration spoken-word\nsegments. First, we train monolingual AWE models on seven Indo-European\nlanguages with various degrees of typological similarity. We then employ RSA to\nquantify the cross-lingual similarity by simulating native and non-native\nspoken-word processing using AWEs. Our experiments show that typological\nsimilarity indeed affects the representational similarity of the models in our\nstudy. We further discuss the implications of our work on modeling speech\nprocessing and language similarity with neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_B/0/1/0/all/0/1\">Badr M. Abdullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaitova_I/0/1/0/all/0/1\">Iuliia Zaitova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avgustinova_T/0/1/0/all/0/1\">Tania Avgustinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobius_B/0/1/0/all/0/1\">Bernd M&#xf6;bius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranslateLocally: Blazing-fast translation running on the local CPU. (arXiv:2109.10194v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10194","description":"<p>Every day, millions of people sacrifice their privacy and browsing habits in\nexchange for online machine translation. Companies and governments with\nconfidentiality requirements often ban online translation or pay a premium to\ndisable logging. To bring control back to the end user and demonstrate speed,\nwe developed translateLocally. Running locally on a desktop or laptop CPU,\ntranslateLocally delivers cloud-like translation speed and quality even on 10\nyear old hardware. The open-source software is based on Marian and runs on\nLinux, Windows, and macOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linde_J/0/1/0/all/0/1\">Jelmer Van der Linde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Source, Two Targets: Challenges and Rewards of Dual Decoding. (arXiv:2109.10197v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10197","description":"<p>Machine translation is generally understood as generating one target text\nfrom an input source document. In this paper, we consider a stronger\nrequirement: to jointly generate two texts so that each output side effectively\ndepends on the other. As we discuss, such a device serves several practical\npurposes, from multi-target machine translation to the generation of controlled\nvariations of the target text. We present an analysis of possible\nimplementations of dual decoding, and experiment with four applications.\nViewing the problem from multiple angles allows us to better highlight the\nchallenges of dual decoding and to also thoroughly analyze the benefits of\ngenerating matched, rather than independent, translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jitao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blindness to Modality Helps Entailment Graph Mining. (arXiv:2109.10227v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10227","description":"<p>Understanding linguistic modality is widely seen as important for downstream\ntasks such as Question Answering and Knowledge Graph Population. Entailment\nGraph learning might also be expected to benefit from attention to modality. We\nbuild Entailment Graphs using a news corpus filtered with a modality parser,\nand show that stripping modal modifiers from predicates in fact increases\nperformance. This suggests that for some tasks, the pragmatics of modal\nmodification of predicates allows them to contribute as evidence of entailment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guillou_L/0/1/0/all/0/1\">Liane Guillou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vroe_S/0/1/0/all/0/1\">Sander Bijl de Vroe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Mark Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets. (arXiv:2109.10234v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10234","description":"<p>We introduce BERTweetFR, the first large-scale pre-trained language model for\nFrench tweets. Our model is initialized using the general-domain French\nlanguage model CamemBERT which follows the base architecture of RoBERTa.\nExperiments show that BERTweetFR outperforms all previous general-domain French\nlanguage models on two downstream Twitter NLP tasks of offensiveness\nidentification and named entity recognition. The dataset used in the\noffensiveness detection task is first created and annotated by our team,\nfilling in the gap of such analytic datasets in French. We make our model\npublicly available in the transformers library with the aim of promoting future\nresearch in analytic tasks for French tweets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanzhu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rennard_V/0/1/0/all/0/1\">Virgile Rennard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xypolopoulos_C/0/1/0/all/0/1\">Christos Xypolopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Vision-and-Language Pretraining Improve Lexical Grounding?. (arXiv:2109.10246v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10246","description":"<p>Linguistic representations derived from text alone have been criticized for\ntheir lack of grounding, i.e., connecting words to their meanings in the\nphysical world. Vision-and-Language (VL) models, trained jointly on text and\nimage or video data, have been offered as a response to such criticisms.\nHowever, while VL pretraining has shown success on multimodal tasks such as\nvisual question answering, it is not yet known how the internal linguistic\nrepresentations themselves compare to their text-only counterparts. This paper\ncompares the semantic representations learned via VL vs. text-only pretraining\nfor two recent VL models using a suite of analyses (clustering, probing, and\nperformance on a commonsense question answering task) in a language-only\nsetting. We find that the multimodal models fail to significantly outperform\nthe text-only variants, suggesting that future work is required if multimodal\npretraining is to be pursued as a means of improving NLP in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_T/0/1/0/all/0/1\">Tian Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audiomer: A Convolutional Transformer for Keyword Spotting. (arXiv:2109.10252v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10252","description":"<p>Transformers have seen an unprecedented rise in Natural Language Processing\nand Computer Vision tasks. However, in audio tasks, they are either infeasible\nto train due to extremely large sequence length of audio waveforms or reach\ncompetitive performance after feature extraction through Fourier-based methods,\nincurring a loss-floor. In this work, we introduce an architecture, Audiomer,\nwhere we combine 1D Residual Networks with Performer Attention to achieve\nstate-of-the-art performance in Keyword Spotting with raw audio waveforms,\nout-performing all previous methods while also being computationally cheaper,\nmuch more parameter and data-efficient. Audiomer allows for deployment in\ncompute-constrained devices and training on smaller datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamdar_J/0/1/0/all/0/1\">Juhi Kamdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Meet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10255","description":"<p>The recognition of hate speech and offensive language (HOF) is commonly\nformulated as a classification task to decide if a text contains HOF. We\ninvestigate whether HOF detection can profit by taking into account the\nrelationships between HOF and similar concepts: (a) HOF is related to sentiment\nanalysis because hate speech is typically a negative statement and expresses a\nnegative opinion; (b) it is related to emotion analysis, as expressed hate\npoints to the author experiencing (or pretending to experience) anger while the\naddressees experience (or are intended to experience) fear. (c) Finally, one\nconstituting element of HOF is the mention of a targeted person or group. On\nthis basis, we hypothesize that HOF detection shows improvements when being\nmodeled jointly with these concepts, in a multi-task learning setup. We base\nour experiments on existing data sets for each of these concepts (sentiment,\nemotion, target of HOF) and evaluate our models as a participant (as team\nIMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection\nexperiments in which we consider multiple available resources and submissions\nto the shared task, we find that the combination of the CrowdFlower emotion\ncorpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target\ndetection data leads to an F1 =.79 in a multi-head multi-task learning model\nbased on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test\ndata, this result is more substantial with an increase by 2pp in F1 and a\nconsiderable increase in recall. Across both data sets (2019, 2021), the recall\nis particularly increased for the class of HOF (6pp for the 2019 data and 3pp\nfor the 2021 data), showing that MTL with emotion, sentiment, and target\nidentification is an appropriate approach for early warning systems that might\nbe deployed in social media platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plaza_del_Arco_F/0/1/0/all/0/1\">Flor Miriam Plaza-del-Arco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halat_S/0/1/0/all/0/1\">Sercan Halat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Trade-offs of Domain Adaptation for Neural Language Models. (arXiv:2109.10274v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10274","description":"<p>In this paper, we connect language model adaptation with concepts of machine\nlearning theory. We consider a training setup with a large out-of-domain set\nand a small in-domain set. As a first contribution, we derive how the benefit\nof training a model on either set depends on the size of the sets and the\ndistance between their underlying distribution. As a second contribution, we\npresent how the most popular data selection techniques -- importance sampling,\nintelligent data selection and influence functions -- can be presented in a\ncommon framework which highlights their similarity and also their subtle\ndifferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iter_D/0/1/0/all/0/1\">Dan Iter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From English to Signal Temporal Logic. (arXiv:2109.10294v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10294","description":"<p>Formal methods provide very powerful tools and techniques for the design and\nanalysis of complex systems. Their practical application remains however\nlimited, due to the widely accepted belief that formal methods require\nextensive expertise and a steep learning curve. Writing correct formal\nspecifications in form of logical formulas is still considered to be a\ndifficult and error prone task.\n</p>\n<p>In this paper we propose DeepSTL, a tool and technique for the translation of\ninformal requirements, given as free English sentences, into Signal Temporal\nLogic (STL), a formal specification language for cyber-physical systems, used\nboth by academia and advanced research labs in industry. A major challenge to\ndevise such a translator is the lack of publicly available informal\nrequirements and formal specifications. We propose a two-step workflow to\naddress this challenge. We first design a grammar-based generation technique of\nsynthetic data, where each output is a random STL formula and its associated\nset of possible English translations. In the second step, we use a\nstate-of-the-art transformer-based neural translation technique, to train an\naccurate attentional translator of English to STL. The experimental results\nshow high translation quality for patterns of English requirements that have\nbeen well trained, making this workflow promising to be extended for processing\nmore complex translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartocci_E/0/1/0/all/0/1\">Ezio Bartocci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickovic_D/0/1/0/all/0/1\">Dejan Ni&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isakovic_H/0/1/0/all/0/1\">Haris Isakovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1\">Radu Grosu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents. (arXiv:2109.10341v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10341","description":"<p>Document-level neural machine translation (DocNMT) delivers coherent\ntranslations by incorporating cross-sentence context. However, for most\nlanguage pairs there's a shortage of parallel documents, although parallel\nsentences are readily available. In this paper, we study whether and how\ncontextual modeling in DocNMT is transferable from sentences to documents in a\nzero-shot fashion (i.e. no parallel documents for student languages) through\nmultilingual modeling. Using simple concatenation-based DocNMT, we explore the\neffect of 3 factors on multilingual transfer: the number of document-supervised\nteacher languages, the data schedule for parallel documents at training, and\nthe data condition of parallel documents (genuine vs. backtranslated). Our\nexperiments on Europarl-7 and IWSLT-10 datasets show the feasibility of\nmultilingual transfer for DocNMT, particularly on document-specific metrics. We\nobserve that more teacher languages and adequate data schedule both contribute\nto better transfer quality. Surprisingly, the transfer is less sensitive to the\ndata condition and multilingual DocNMT achieves comparable performance with\nboth back-translated and genuine document pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabirmoghaddam_A/0/1/0/all/0/1\">Ali Dabirmoghaddam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arivazhagan_N/0/1/0/all/0/1\">Naveen Arivazhagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation-Guided Pre-Training for Open-Domain Question Answering. (arXiv:2109.10346v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10346","description":"<p>Answering complex open-domain questions requires understanding the latent\nrelations between involving entities. However, we found that the existing QA\ndatasets are extremely imbalanced in some types of relations, which hurts the\ngeneralization performance over questions with long-tail relations. To remedy\nthis problem, in this paper, we propose a Relation-Guided Pre-Training\n(RGPT-QA) framework. We first generate a relational QA dataset covering a wide\nrange of relations from both the Wikidata triplets and Wikipedia hyperlinks. We\nthen pre-train a QA model to infer the latent relations from the question, and\nthen conduct extractive QA to get the target answer entity. We demonstrate that\nby pretraining with propoed RGPT-QA techique, the popular open-domain QA model,\nDense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute\nimprovement in Exact Match accuracy on Natural Questions, TriviaQA, and\nWebQuestions. Particularly, we show that RGPT-QA improves significantly on\nquestions with long-tail relations\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informed Sampling for Diversity in Concept-to-Text NLG. (arXiv:2004.14364v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14364","description":"<p>Deep-learning models for language generation tasks tend to produce repetitive\noutput. Various methods have been proposed to encourage lexical diversity\nduring decoding, but this often comes at a cost to the perceived fluency and\nadequacy of the output. In this work, we propose to ameliorate this cost by\nusing an Imitation Learning approach to explore the level of diversity that a\nlanguage generation model can reliably produce. Specifically, we augment the\ndecoding process with a meta-classifier trained to distinguish which words at\nany given timestep will lead to high-quality output. We focus our experiments\non concept-to-text generation where models are sensitive to the inclusion of\nirrelevant words due to the strict relation between input and output. Our\nanalysis shows that previous methods for diversity underperform in this\nsetting, while human evaluation suggests that our proposed method achieves a\nhigh level of diversity with minimal effect to the output's fluency and\nadequacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Giulio Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models. (arXiv:2009.13267v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.13267","description":"<p>The discrepancy between maximum likelihood estimation (MLE) and task measures\nsuch as BLEU score has been studied before for autoregressive neural machine\ntranslation (NMT) and resulted in alternative training algorithms (Ranzato et\nal., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However,\nMLE training remains the de facto approach for autoregressive NMT because of\nits computational efficiency and stability. Despite this mismatch between the\ntraining objective and task measure, we notice that the samples drawn from an\nMLE-based trained NMT support the desired distribution -- there are samples\nwith much higher BLEU score comparing to the beam decoding output. To benefit\nfrom this observation, we train an energy-based model to mimic the behavior of\nthe task measure (i.e., the energy-based model assigns lower energy to samples\nwith higher BLEU score), which is resulted in a re-ranking algorithm based on\nthe samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal\nenergy models (over target sentence) and joint energy models (over both source\nand target sentences). Our EBR with the joint energy model consistently\nimproves the performance of the Transformer-based NMT: +4 BLEU points on\nIWSLT'14 German-English, +3.0 BELU points on Sinhala-English, +1.2 BLEU on\nWMT'16 English-German tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1\">Sumanta Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rooshenas_A/0/1/0/all/0/1\">Amirmohammad Rooshenas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naskar_S/0/1/0/all/0/1\">Subhajit Naskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not all parameters are born equal: Attention is mostly what you need. (arXiv:2010.11859v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11859","description":"<p>Transformers are widely used in state-of-the-art machine translation, but the\nkey to their success is still unknown. To gain insight into this, we consider\nthree groups of parameters: embeddings, attention, and feed forward neural\nnetwork (FFN) layers. We examine the relative importance of each by performing\nan ablation study where we initialise them at random and freeze them, so that\ntheir weights do not change over the course of the training. Through this, we\nshow that the attention and FFN are equally important and fulfil the same\nfunctionality in a model. We show that the decision about whether a component\nis frozen or allowed to train is at least as important for the final model\nperformance as its number of parameters. At the same time, the number of\nparameters alone is not indicative of a component's importance. Finally, while\nthe embedding layer is the least essential for machine translation tasks, it is\nthe most important component for language modelling tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation. (arXiv:2101.00421v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00421","description":"<p>Machine translation systems are vulnerable to domain mismatch, especially in\na low-resource scenario. Out-of-domain translations are often of poor quality\nand prone to hallucinations, due to exposure bias and the decoder acting as a\nlanguage model. We adopt two approaches to alleviate this problem: lexical\nshortlisting restricted by IBM statistical alignments, and hypothesis\nre-ranking based on similarity. The methods are computationally cheap, widely\nknown, but not extensively experimented on domain adaptation. We demonstrate\nsuccess on low-resource out-of-domain test sets, however, the methods are\nineffective when there is sufficient data or too great domain mismatch. This is\ndue to both the IBM model losing its advantage over the implicitly learned\nneural alignment, and issues with subword segmentation of out-of-domain words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Retrieval Conversational Machine Reading. (arXiv:2102.08633v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.08633","description":"<p>In conversational machine reading, systems need to interpret natural language\nrules, answer high-level questions such as \"May I qualify for VA health care\nbenefits?\", and ask follow-up clarification questions whose answer is necessary\nto answer the original question. However, existing works assume the rule text\nis provided for each user question, which neglects the essential retrieval step\nin real scenarios. In this work, we propose and investigate an open-retrieval\nsetting of conversational machine reading. In the open-retrieval setting, the\nrelevant rule texts are unknown so that a system needs to retrieve\nquestion-relevant evidence from a collection of rule texts, and answer users'\nhigh-level questions according to multiple retrieved rule texts in a\nconversational manner. We propose MUDERN, a Multi-passage Discourse-aware\nEntailment Reasoning Network which extracts conditions in the rule texts\nthrough discourse segmentation, conducts multi-passage entailment reasoning to\nanswer user questions directly, or asks clarification follow-up questions to\ninquiry more information. On our created OR-ShARC dataset, MUDERN achieves the\nstate-of-the-art performance, outperforming existing single-passage\nconversational machine reading models as well as a new multi-passage\nconversational machine reading baseline by a large margin. In addition, we\nconduct in-depth analyses to provide new insights into this new setting and our\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Mandarin-English Code-switching Speech Recognition. (arXiv:2104.02258v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.02258","description":"<p>Mandarin-English code-switching (CS) is frequently used among East and\nSoutheast Asian people. However, the intra-sentence language switching of the\ntwo very different languages makes recognizing CS speech challenging.\nMeanwhile, the recent successful non-autoregressive (NAR) ASR models remove the\nneed for left-to-right beam decoding in autoregressive (AR) models and achieved\noutstanding performance and fast inference speed, but it has not been applied\nto Mandarin-English CS speech recognition. This paper takes advantage of the\nMask-CTC NAR ASR framework to tackle the CS speech recognition issue. We\nfurther propose to change the Mandarin output target of the encoder to Pinyin\nfor faster encoder training and introduce the Pinyin-to-Mandarin decoder to\nlearn contextualized information. Moreover, we use word embedding label\nsmoothing to regularize the decoder with contextualized information and\nprojection matrix regularization to bridge that gap between the encoder and\ndecoder. We evaluate these methods on the SEAME corpus and achieved exciting\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-Po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sung-Feng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders. (arXiv:2104.03630v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03630","description":"<p>Powerful sentence encoders trained for multiple languages are on the rise.\nThese systems are capable of embedding a wide range of linguistic properties\ninto vector representations. While explicit probing tasks can be used to verify\nthe presence of specific linguistic properties, it is unclear whether the\nvector representations can be manipulated to indirectly steer such properties.\nFor efficient learning, we investigate the use of a geometric mapping in\nembedding space to transform linguistic properties, without any tuning of the\npre-trained sentence encoder or decoder. We validate our approach on three\nlinguistic properties using a pre-trained multilingual autoencoder and analyze\nthe results in both monolingual and cross-lingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raedt_M/0/1/0/all/0/1\">Maarten De Raedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godin_F/0/1/0/all/0/1\">Fr&#xe9;deric Godin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buteneers_P/0/1/0/all/0/1\">Pieter Buteneers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Condenser: a Pre-training Architecture for Dense Retrieval. (arXiv:2104.08253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08253","description":"<p>Pre-trained Transformer language models (LM) have become go-to text\nrepresentation encoders. Prior research fine-tunes deep LMs to encode text\nsequences such as sentences and passages into single dense vector\nrepresentations for efficient text comparison and retrieval. However, dense\nencoders require a lot of data and sophisticated techniques to effectively\ntrain and suffer in low data situations. This paper finds a key reason is that\nstandard LMs' internal attention structure is not ready-to-use for dense\nencoders, which needs to aggregate text information into the dense\nrepresentation. We propose to pre-train towards dense encoder with a novel\nTransformer architecture, Condenser, where LM prediction CONditions on DENSE\nRepresentation. Our experiments show Condenser improves over standard LM by\nlarge margins on various text retrieval and similarity tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Luyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callan_J/0/1/0/all/0/1\">Jamie Callan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12227","description":"<p>Classic information extraction techniques consist in building questions and\nanswers about the facts. Indeed, it is still a challenge to subjective\ninformation extraction systems to identify opinions and feelings in context. In\nsentiment-based NLP tasks, there are few resources to information extraction,\nabove all offensive or hateful opinions in context. To fill this important gap,\nthis short paper provides a new cross-lingual and contextual offensive lexicon,\nwhich consists of explicit and implicit offensive and swearing expressions of\nopinion, which were annotated in two different classes: context dependent and\ncontext-independent offensive. In addition, we provide markers to identify hate\nspeech. Annotation approach was evaluated at the expression-level and achieves\nhigh human inter-annotator agreement. The provided offensive lexicon is\navailable in Portuguese and English languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12265","description":"<p>This paper provides a new approach for offensive language and hate speech\ndetection on social media. Our approach incorporates an offensive lexicon\ncomposed of implicit and explicit offensive and swearing expressions annotated\nwith binary classes: context-dependent and context-independent offensive. Due\nto the severity of the hate speech and offensive comments in Brazil, and the\nlack of research in Portuguese, Brazilian Portuguese is the language used to\nvalidate the proposed method. Nevertheless, our proposal may be applied to any\nother language or domain. Based on the obtained results, the proposed approach\nshowed high-performance overcoming the current baselines for European and\nBrazilian Portuguese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benevenuto_F/0/1/0/all/0/1\">Fabr&#xed;cio Benevenuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pardo_T/0/1/0/all/0/1\">Thiago Alexandre Salgueiro Pardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiaKG: an Annotated Diabetes Dataset for Medical Knowledge Graph Construction. (arXiv:2105.15033v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.15033","description":"<p>Knowledge Graph has been proven effective in modeling structured information\nand conceptual knowledge, especially in the medical domain. However, the lack\nof high-quality annotated corpora remains a crucial problem for advancing the\nresearch and applications on this task. In order to accelerate the research for\ndomain-specific knowledge graphs in the medical domain, we introduce DiaKG, a\nhigh-quality Chinese dataset for Diabetes knowledge graph, which contains\n22,050 entities and 6,890 relations in total. We implement recent typical\nmethods for Named Entity Recognition and Relation Extraction as a benchmark to\nevaluate the proposed dataset thoroughly. Empirical results show that the DiaKG\nis challenging for most existing methods and further analysis is conducted to\ndiscuss future research direction for improvements. We hope the release of this\ndataset can assist the construction of diabetes knowledge graphs and facilitate\nAI-based applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dejie Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chaozhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bangchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiaobin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Ji Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bilateral Personalized Dialogue Generation with Contrastive Learning. (arXiv:2106.07857v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07857","description":"<p>Generating personalized responses is one of the major challenges in natural\nhuman-robot interaction. Current researches in this field mainly focus on\ngenerating responses consistent with the robot's pre-assigned persona, while\nignoring the user's persona. Such responses may be inappropriate or even\noffensive, which may lead to the bad user experience. Therefore, we propose a\nBilateral Personalized Dialogue Generation (BPDG) method for dyadic\nconversation, which integrates user and robot personas into dialogue generation\nvia designing a dynamic persona-aware fusion method. To bridge the gap between\nthe learning objective function and evaluation metrics, the Conditional Mutual\nInformation Maximum (CMIM) criterion is adopted with contrastive learning to\nselect the proper response from the generated candidates. Moreover, a bilateral\npersona accuracy metric is designed to measure the degree of bilateral\npersonalization. Experimental results demonstrate that, compared with several\nstate-of-the-art methods, the final results of the proposed method are more\npersonalized and consistent with bilateral personas in terms of both automatic\nand manual evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hanjun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.09700","description":"<p>Biomedical knowledge graphs (KGs) hold rich information on entities such as\ndiseases, drugs, and genes. Predicting missing links in these graphs can boost\nmany important applications, such as drug design and repurposing. Recent work\nhas shown that general-domain language models (LMs) can serve as \"soft\" KGs,\nand that they can be fine-tuned for the task of KG completion. In this work, we\nstudy scientific LMs for KG completion, exploring whether we can tap into their\nlatent knowledge to enhance biomedical link prediction. We evaluate several\ndomain-specific LMs, fine-tuning them on datasets centered on drugs and\ndiseases that we represent as KGs and enrich with textual entity descriptions.\nWe integrate the LM-based models with KG embedding models, using a router\nmethod that learns to assign each input example to either type of model and\nprovides a substantial boost in performance. Finally, we demonstrate the\nadvantage of LM models in the inductive setting with novel scientific entities.\nOur datasets and code are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nadkarni_R/0/1/0/all/0/1\">Rahul Nadkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadden_D/0/1/0/all/0/1\">David Wadden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You should evaluate your language model on marginal likelihood over tokenisations. (arXiv:2109.02550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02550","description":"<p>Neural language models typically tokenise input text into sub-word units to\nachieve an open vocabulary. The standard approach is to use a single canonical\ntokenisation at both train and test time. We suggest that this approach is\nunsatisfactory and may bottleneck our evaluation of language model performance.\nUsing only the one-best tokenisation ignores tokeniser uncertainty over\nalternative tokenisations, which may hurt model out-of-domain performance.\n</p>\n<p>In this paper, we argue that instead, language models should be evaluated on\ntheir marginal likelihood over tokenisations. We compare different estimators\nfor the marginal likelihood based on sampling, and show that it is feasible to\nestimate the marginal likelihood with a manageable number of samples. We then\nevaluate pretrained English and German language models on both the\none-best-tokenisation and marginal perplexities, and show that the marginal\nperplexity can be significantly better than the one best, especially on\nout-of-domain data. We link this difference in perplexity to the tokeniser\nuncertainty as measured by tokeniser entropy. We discuss some implications of\nour results for language model training and evaluation, particularly with\nregard to tokenisation robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1\">Kris Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rimell_L/0/1/0/all/0/1\">Laura Rimell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08828","description":"<p>Empathy is a complex cognitive ability based on the reasoning of others'\naffective states. In order to better understand others and express stronger\nempathy in dialogues, we argue that two issues must be tackled at the same\ntime: (i) identifying which word is the cause for the other's emotion from his\nor her utterance and (ii) reflecting those specific words in the response\ngeneration. However, previous approaches for recognizing emotion cause words in\ntext require sub-utterance level annotations, which can be demanding. Taking\ninspiration from social cognition, we leverage a generative estimator to infer\nemotion cause words from utterances with no word-level label. Also, we\nintroduce a novel method based on pragmatics to make dialogue models focus on\ntargeted words in the input during generation. Our method is applicable to any\ndialogue models with no additional training on the fly. We show our approach\nimproves multiple best-performing dialogue agents on generating more focused\nempathetic responses in terms of both automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeongchang Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09105","description":"<p>Language Models (LMs) have been ubiquitously leveraged in various tasks\nincluding spoken language understanding (SLU). Spoken language requires careful\nunderstanding of speaker interactions, dialog states and speech induced\nmultimodal behaviors to generate a meaningful representation of the\nconversation. In this work, we propose to dissect SLU into three representative\nproperties:conversational (disfluency, pause, overtalk), channel (speaker-type,\nturn-tasks) and ASR (insertion, deletion,substitution). We probe BERT based\nlanguage models (BERT, RoBERTa) trained on spoken transcripts to investigate\nits ability to understand multifarious properties in absence of any speech\ncues. Empirical results indicate that LM is surprisingly good at capturing\nconversational properties such as pause prediction and overtalk detection from\nlexical tokens. On the downsides, the LM scores low on turn-tasks and ASR\nerrors predictions. Additionally, pre-training the LM on spoken transcripts\nrestrain its linguistic understanding. Finally, we establish the efficacy and\ntransferability of the mentioned properties on two benchmark datasets:\nSwitchboard Dialog Act and Disfluency datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ayush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundararaman_M/0/1/0/all/0/1\">Mukuntha Narayanan Sundararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vepa_J/0/1/0/all/0/1\">Jithendra Vepa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries. (arXiv:2109.09195v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09195","description":"<p>Current pre-trained models applied to summarization are prone to factual\ninconsistencies which either misrepresent the source text or introduce\nextraneous information. Thus, comparing the factual consistency of summaries is\nnecessary as we develop improved models. However, the optimal human evaluation\nsetup for factual consistency has not been standardized. To address this issue,\nwe crowdsourced evaluations for factual consistency using the rating-based\nLikert scale and ranking-based Best-Worst Scaling protocols, on 100 articles\nfrom each of the CNN-Daily Mail and XSum datasets over four state-of-the-art\nmodels, to determine the most reliable evaluation framework. We find that\nranking-based protocols offer a more reliable measure of summary quality across\ndatasets, while the reliability of Likert ratings depends on the target dataset\nand the evaluation design. Our crowdsourcing templates and summary evaluations\nwill be publicly available to facilitate future research on factual consistency\nin summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_G/0/1/0/all/0/1\">Griffin Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Borui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}