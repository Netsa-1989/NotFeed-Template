{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An Introduction to Automatic Differentiation forMachine Learning. (arXiv:2110.06209v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06209","description":"<p>Machine learning and neural network models in particular have been improving\nthe state of the art performance on many artificial intelligence related tasks.\nNeural network models are typically implemented using frameworks that perform\ngradient based optimization methods to fit a model to a dataset. These\nframeworks use a technique of calculating derivatives called automatic\ndifferentiation (AD) which removes the burden of performing derivative\ncalculations from the model designer. In this report we describe AD, its\nmotivations, and different implementation approaches. We briefly describe\ndataflow programming as it relates to AD. Lastly, we present example programs\nthat are implemented with Tensorflow and PyTorch, which are two commonly used\nAD frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">Davan Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Effect of Natural Language Explanations on Out-of-Distribution Generalization in Few-shot NLI. (arXiv:2110.06223v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06223","description":"<p>Although neural models have shown strong performance in datasets such as\nSNLI, they lack the ability to generalize out-of-distribution (OOD). In this\nwork, we formulate a few-shot learning setup and examine the effects of natural\nlanguage explanations on OOD generalization. We leverage the templates in the\nHANS dataset and construct templated natural language explanations for each\ntemplate. Although generated explanations show competitive BLEU scores against\ngroundtruth explanations, they fail to improve prediction performance. We\nfurther show that generated explanations often hallucinate information and miss\nkey elements that indicate the label.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangqiaoyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chenhao Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Summarization using Restricted Self-Attention. (arXiv:2110.06263v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06263","description":"<p>Speech summarization is typically performed by using a cascade of speech\nrecognition and text summarization models. End-to-end modeling of speech\nsummarization models is challenging due to memory and compute constraints\narising from long input audio sequences. Recent work in document summarization\nhas inspired methods to reduce the complexity of self-attentions, which enables\ntransformer models to handle long sequences. In this work, we introduce a\nsingle model optimized end-to-end for speech summarization. We apply the\nrestricted self-attention technique from text-based models to speech models to\naddress the memory and compute constraints. We demonstrate that the proposed\nmodel learns to directly summarize speech for the How-2 corpus of instructional\nvideos. The proposed end-to-end model outperforms the previously proposed\ncascaded model by 3 points absolute on ROUGE. Further, we consider the spoken\nlanguage understanding task of predicting concepts from speech inputs and show\nthat the proposed end-to-end model outperforms the cascade model by 4 points\nabsolute F-1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Roshan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palaskar_S/0/1/0/all/0/1\">Shruti Palaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sm{\\aa}prat: DialoGPT for Natural Language Generation of Swedish Dialogue by Transfer Learning. (arXiv:2110.06273v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06273","description":"<p>Building open-domain conversational systems (or chatbots) that produce\nconvincing responses is a recognized challenge. Recent state-of-the-art (SoTA)\ntransformer-based models for the generation of natural language dialogue have\ndemonstrated impressive performance in simulating human-like, single-turn\nconversations in English. This work investigates, by an empirical study, the\npotential for transfer learning of such models to Swedish language. DialoGPT,\nan English language pre-trained model, is adapted by training on three\ndifferent Swedish language conversational datasets obtained from publicly\navailable sources. Perplexity score (an automated intrinsic language model\nmetric) and surveys by human evaluation were used to assess the performances of\nthe fine-tuned models, with results that indicate that the capacity for\ntransfer learning can be exploited with considerable success. Human evaluators\nasked to score the simulated dialogue judged over 57% of the chatbot responses\nto be human-like for the model trained on the largest (Swedish) dataset. We\nprovide the demos and model checkpoints of our English and Swedish chatbots on\nthe HuggingFace platform for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adewumi_T/0/1/0/all/0/1\">Tosin Adewumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_N/0/1/0/all/0/1\">Nosheen Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pahlavan_M/0/1/0/all/0/1\">Maryam Pahlavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brannvall_R/0/1/0/all/0/1\">Rickard Br&#xe4;nnvall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabry_S/0/1/0/all/0/1\">Sana Sabah Sabry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1\">Foteini Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiST: Lite Self-training Makes Efficient Few-shot Learners. (arXiv:2110.06274v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06274","description":"<p>We present a new method LiST for efficient fine-tuning of large pre-trained\nlanguage models (PLMs) in few-shot learning settings. LiST significantly\nimproves over recent methods that adopt prompt fine-tuning using two key\ntechniques. The first one is the use of self-training to leverage large amounts\nof unlabeled data for prompt-tuning to significantly boost the model\nperformance in few-shot settings. We use self-training in conjunction with\nmeta-learning for re-weighting noisy pseudo-prompt labels. However, traditional\nself-training is expensive as it requires updating all the model parameters\nrepetitively. Therefore, we use a second technique for light-weight fine-tuning\nwhere we introduce a small number of task-specific adapter parameters that are\nfine-tuned during self-training while keeping the PLM encoder frozen. This also\nsignificantly reduces the overall model footprint across several tasks that can\nnow share a common PLM encoder as backbone for inference. Combining the above\ntechniques, LiST not only improves the model performance for few-shot learning\non target domains but also reduces the model memory footprint. We present a\ncomprehensive study on six NLU tasks to validate the effectiveness of LiST. The\nresults show that LiST improves by 35% over classic fine-tuning methods and 6%\nover prompt-tuning with 96% reduction in number of trainable parameters when\nfine-tuned with no more than 30 labeled examples from each target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S3PRL-VC: Open-source Voice Conversion Framework with Self-supervised Speech Representations. (arXiv:2110.06280v1 [cs.SD])","link":"http://arxiv.org/abs/2110.06280","description":"<p>This paper introduces S3PRL-VC, an open-source voice conversion (VC)\nframework based on the S3PRL toolkit. In the context of recognition-synthesis\nVC, self-supervised speech representation (S3R) is valuable in its potential to\nreplace the expensive supervised representation adopted by state-of-the-art VC\nsystems. Moreover, we claim that VC is a good probing task for S3R analysis. In\nthis work, we provide a series of in-depth analyses by benchmarking on the two\ntasks in VCC2020, namely intra-/cross-lingual any-to-one (A2O) VC, as well as\nan any-to-any (A2A) setting. We also provide comparisons between not only\ndifferent S3Rs but also top systems in VCC2020 with supervised representations.\nSystematic objective and subjective evaluation were conducted, and we show that\nS3R is comparable with VCC2020 top systems in the A2O setting in terms of\nsimilarity, and achieves state-of-the-art in S3R-based A2A VC. We believe the\nextensive analysis, as well as the toolkit itself, contribute to not only the\nS3R community but also the VC community. The codebase is now open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-Wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decision-Theoretic Question Generation for Situated Reference Resolution: An Empirical Study and Computational Model. (arXiv:2110.06288v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06288","description":"<p>Dialogue agents that interact with humans in situated environments need to\nmanage referential ambiguity across multiple modalities and ask for help as\nneeded. However, it is not clear what kinds of questions such agents should ask\nnor how the answers to such questions can be used to resolve ambiguity. To\naddress this, we analyzed dialogue data from an interactive study in which\nparticipants controlled a virtual robot tasked with organizing a set of tools\nwhile engaging in dialogue with a live, remote experimenter. We discovered a\nnumber of novel results, including the distribution of question types used to\nresolve ambiguity and the influence of dialogue-level factors on the reference\nresolution process. Based on these empirical findings we: (1) developed a\ncomputational model for clarification requests using a decision network with an\nentropy-based utility assignment method that operates across modalities, (2)\nevaluated the model, showing that it outperforms a slot-filling baseline in\nenvironments of varying ambiguity, and (3) interpreted the results to offer\ninsight into the ways that agents can ask questions to facilitate situated\nreference resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gervits_F/0/1/0/all/0/1\">Felix Gervits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briggs_G/0/1/0/all/0/1\">Gordon Briggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roque_A/0/1/0/all/0/1\">Antonio Roque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadomatsu_G/0/1/0/all/0/1\">Genki A. Kadomatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thurston_D/0/1/0/all/0/1\">Dean Thurston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheutz_M/0/1/0/all/0/1\">Matthias Scheutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marge_M/0/1/0/all/0/1\">Matthew Marge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained style control in Transformer-based Text-to-speech Synthesis. (arXiv:2110.06306v1 [eess.AS])","link":"http://arxiv.org/abs/2110.06306","description":"<p>In this paper, we present a novel architecture to realize fine-grained style\ncontrol on the transformer-based text-to-speech synthesis (TransformerTTS).\nSpecifically, we model the speaking style by extracting a time sequence of\nlocal style tokens (LST) from the reference speech. The existing content\nencoder in TransformerTTS is then replaced by our designed cross-attention\nblocks for fusion and alignment between content and style. As the fusion is\nperformed along with the skip connection, our cross-attention block provides a\ngood inductive bias to gradually infuse the phoneme representation with a given\nstyle. Additionally, we prevent the style embedding from encoding linguistic\ncontent by randomly truncating LST during training and using wav2vec 2.0\nfeatures. Experiments show that with fine-grained style control, our system\nperforms better in terms of naturalness, intelligibility, and style\ntransferability. Our code and samples are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Wav2vec 2.0 fine-tuning for improved speech emotion recognition. (arXiv:2110.06309v1 [eess.AS])","link":"http://arxiv.org/abs/2110.06309","description":"<p>While wav2vec 2.0 has been proposed for speech recognition (ASR), it can also\nbe used for speech emotion recognition (SER); its performance can be\nsignificantly improved using different fine-tuning strategies. Two baseline\nmethods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are\nfirst presented. We show that V-FT is able to outperform state-of-the-art\nmodels on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy,\nfurther improves the performance on SER. We also introduce a novel fine-tuning\nmethod termed P-TAPT, which modifies the TAPT objective to learn contextualized\nemotion representations. Experiments show that P-TAPT performs better than TAPT\nespecially under low-resource settings. Compared to prior works in this\nliterature, our top-line system achieved a 7.4% absolute improvement on\nunweighted accuracy (UA) over the state-of-the-art performance on IEMOCAP. Our\ncode is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Compact Metrics for MT. (arXiv:2110.06341v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06341","description":"<p>Recent developments in machine translation and multilingual text generation\nhave led researchers to adopt trained metrics such as COMET or BLEURT, which\ntreat evaluation as a regression problem and use representations from\nmultilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on\nrelated tasks suggest that these models are most efficient when they are large,\nwhich is costly and impractical for evaluation. We investigate the trade-off\nbetween multilinguality and model capacity with RemBERT, a state-of-the-art\nmultilingual language model, using data from the WMT Metrics Shared Task. We\npresent a series of experiments which show that model size is indeed a\nbottleneck for cross-lingual transfer, then demonstrate how distillation can\nhelp addressing this bottleneck, by leveraging synthetic data generation and\ntransferring knowledge from one teacher to multiple students trained on related\nlanguages. Our method yields up to 10.5% improvement over vanilla fine-tuning\nand reaches 92.6% of RemBERT's performance using only a third of its\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_A/0/1/0/all/0/1\">Amy Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_A/0/1/0/all/0/1\">Ankur P. Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Me How to Survey: Literature Review Made Simple with Automatic Reading Path Generation. (arXiv:2110.06354v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06354","description":"<p>Recent years have witnessed the dramatic growth of paper volumes with plenty\nof new research papers published every day, especially in the area of computer\nscience. How to glean papers worth reading from the massive literature to do a\nquick survey or keep up with the latest advancement about a specific research\ntopic has become a challenging task. Existing academic search engines such as\nGoogle Scholar return relevant papers by individually calculating the relevance\nbetween each paper and query. However, such systems usually omit the\nprerequisite chains of a research topic and cannot form a meaningful reading\npath. In this paper, we introduce a new task named Reading Path Generation\n(RPG) which aims at automatically producing a path of papers to read for a\ngiven query. To serve as a research benchmark, we further propose SurveyBank, a\ndataset consisting of large quantities of survey papers in the field of\ncomputer science as well as their citation relationships. Each survey paper\ncontains key phrases extracted from its title and multi-level reading lists\ninferred from its references. Furthermore, we propose a\ngraph-optimization-based approach for reading path generation which takes the\nrelationship between papers into account. Extensive evaluations demonstrate\nthat our approach outperforms other baselines. A Real-time Reading Path\nGeneration System (RePaGer) has been also implemented with our designed model.\nTo the best of our knowledge, we are the first to target this important\nresearch problem. Our source code of RePaGer system and SurveyBank dataset can\nbe found on here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiayuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zijing Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangyang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Masking for Temporal Language Models. (arXiv:2110.06366v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06366","description":"<p>Our world is constantly evolving, and so is the content on the web.\nConsequently, our languages, often said to mirror the world, are dynamic in\nnature. However, most current contextual language models are static and cannot\nadapt to changes over time. In this work, we propose a temporal contextual\nlanguage model called TempoBERT, which uses time as an additional context of\ntexts. Our technique is based on modifying texts with temporal information and\nperforming time masking - specific masking for the supplementary time\ninformation. We leverage our approach for the tasks of semantic change\ndetection and sentence time prediction, experimenting on diverse datasets in\nterms of time, size, genre, and language. Our extensive evaluation shows that\nboth tasks benefit from exploiting time masking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_G/0/1/0/all/0/1\">Guy D. Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_I/0/1/0/all/0/1\">Ido Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for Nouns' Semantic Properties and their Prototypicality. (arXiv:2110.06376v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06376","description":"<p>Large scale language models encode rich commonsense knowledge acquired\nthrough exposure to massive data during pre-training, but their understanding\nof entities and their semantic properties is unclear. We probe BERT (Devlin et\nal., 2019) for the properties of English nouns as expressed by adjectives that\ndo not restrict the reference scope of the noun they modify (as in \"red car\"),\nbut instead emphasise some inherent aspect (\"red strawberry\"). We base our\nstudy on psycholinguistics datasets that capture the association strength\nbetween nouns and their semantic features. We probe BERT using cloze tasks and\nin a classification setting, and show that the model has marginal knowledge of\nthese features and their prevalence as expressed in these datasets. We discuss\nfactors that make evaluation challenging and impede drawing general conclusions\nabout the models' knowledge of noun properties. Finally, we show that when\ntested in a fine-tuning setting addressing entailment, BERT successfully\nleverages the information needed for reasoning about the meaning of\nadjective-noun constructions outperforming previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1\">Marianna Apidianaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soler_A/0/1/0/all/0/1\">Aina Gar&#xed; Soler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoNLU: Detecting, root-causing, and fixing NLU model errors. (arXiv:2110.06384v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06384","description":"<p>Improving the quality of Natural Language Understanding (NLU) models, and\nmore specifically, task-oriented semantic parsing models, in production is a\ncumbersome task. In this work, we present a system called AutoNLU, which we\ndesigned to scale the NLU quality improvement process. It adds automation to\nthree key steps: detection, attribution, and correction of model errors, i.e.,\nbugs. We detected four times more failed tasks than with random sampling,\nfinding that even a simple active learning sampling method on an uncalibrated\nmodel is surprisingly effective for this purpose. The AutoNLU tool empowered\nlinguists to fix ten times more semantic parsing bugs than with prior manual\nprocesses, auto-correcting 65% of all identified bugs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sethi_P/0/1/0/all/0/1\">Pooja Sethi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savenkov_D/0/1/0/all/0/1\">Denis Savenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1\">Forough Arabshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goetz_J/0/1/0/all/0/1\">Jack Goetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolliver_M/0/1/0/all/0/1\">Micaela Tolliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheffer_N/0/1/0/all/0/1\">Nicolas Scheffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabul_I/0/1/0/all/0/1\">Ilknur Kabul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_A/0/1/0/all/0/1\">Ahmed Aly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization. (arXiv:2110.06388v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06388","description":"<p>To capture the semantic graph structure from raw text, most existing\nsummarization approaches are built on GNNs with a pre-trained model. However,\nthese methods suffer from cumbersome procedures and inefficient computations\nfor long-text documents. To mitigate these issues, this paper proposes\nHETFORMER, a Transformer-based pre-trained model with multi-granularity sparse\nattentions for long-text extractive summarization. Specifically, we model\ndifferent types of semantic nodes in raw text as a potential heterogeneous\ngraph and directly learn heterogeneous relationships (edges) among nodes by\nTransformer. Extensive experiments on both single- and multi-document\nsummarization tasks show that HETFORMER achieves state-of-the-art performance\nin Rouge F1 while using less memory and fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian-Guo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-guided Generative Models for Extractive Question Answering. (arXiv:2110.06393v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06393","description":"<p>We propose a novel method for applying Transformer models to extractive\nquestion answering (QA) tasks. Recently, pretrained generative\nsequence-to-sequence (seq2seq) models have achieved great success in question\nanswering. Contributing to the success of these models are internal attention\nmechanisms such as cross-attention. We propose a simple strategy to obtain an\nextractive answer span from the generative model by leveraging the decoder\ncross-attention patterns. Viewing cross-attention as an architectural prior, we\napply joint training to further improve QA performance. Empirical results show\nthat on open-domain question answering datasets like NaturalQuestions and\nTriviaQA, our method approaches state-of-the-art performance on both generative\nand extractive inference, all while using much fewer parameters. Furthermore,\nthis strategy allows us to perform hallucination-free inference while\nconferring significant improvements to the model's ability to rerank relevant\npassages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Davis Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Ontologies and Tool Support for COVID-19 Analytics. (arXiv:2110.06397v1 [cs.SE])","link":"http://arxiv.org/abs/2110.06397","description":"<p>The outbreak of the SARS-CoV-2 pandemic of the new COVID-19 disease (COVID-19\nfor short) demands empowering existing medical, economic, and social emergency\nbackend systems with data analytics capabilities. An impediment in taking\nadvantages of data analytics in these systems is the lack of a unified\nframework or reference model. Ontologies are highlighted as a promising\nsolution to bridge this gap by providing a formal representation of COVID-19\nconcepts such as symptoms, infections rate, contact tracing, and drug\nmodelling. Ontology-based solutions enable the integration of diverse data\nsources that leads to a better understanding of pandemic data, management of\nsmart lockdowns by identifying pandemic hotspots, and knowledge-driven\ninference, reasoning, and recommendations to tackle surrounding issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Aakash Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandara_M/0/1/0/all/0/1\">Madhushi Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahmideh_M/0/1/0/all/0/1\">Mahdi Fahmideh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proper_H/0/1/0/all/0/1\">Henderik A. Proper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizzardi_G/0/1/0/all/0/1\">Giancarlo Guizzardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soar_J/0/1/0/all/0/1\">Jeffrey Soar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Natural Language Generation for Personalized Dialogue System. (arXiv:2110.06419v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06419","description":"<p>Neural conversational models have long suffered from the problem of\ninconsistency and lacking coherent personality. To address the issue,\npersona-based models capturing individual characteristics have been proposed,\nbut they still face the dilemma of model adaption and data privacy. To break\nthis dilemma, we propose a novel Federated Natural Language Generation (FedNLG)\nframework, which learns personalized representations from various dataset on\ndistributed devices, and thus implements the personalized dialogue system\nefficiently and safely. FedNLG first pre-trains parameters of standard neural\nconversational model over a large dialogue corpus, and then fine-tune the model\nparameters and persona embeddings on specific datasets, in a federated manner.\nThus, the model could simultaneously learn the persona embeddings in local\nclients and learn shared model parameters by federated aggregation, which\nachieves accuracyprivacy balance. By conducting extensive experiments, we\ndemonstrate the effectiveness of our model by pre-training model over Cornell\nMovie-Dialogs Corpus and fine-tuning the model over two TV series dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Huanli Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yong Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Graph-based Sentence Ordering with Iteratively Predicted Pairwise Orderings. (arXiv:2110.06446v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06446","description":"<p>Dominant sentence ordering models can be classified into pairwise ordering\nmodels and set-to-sequence models. However, there is little attempt to combine\nthese two types of models, which inituitively possess complementary advantages.\nIn this paper, we propose a novel sentence ordering framework which introduces\ntwo classifiers to make better use of pairwise orderings for graph-based\nsentence ordering. Specially, given an initial sentence-entity graph, we first\nintroduce a graph-based classifier to predict pairwise orderings between linked\nsentences. Then, in an iterative manner, based on the graph updated by\npreviously predicted high-confident pairwise orderings, another classifier is\nused to predict the remaining uncertain pairwise orderings. At last, we adapt a\nGRN-based sentence ordering model on the basis of final graph. Experiments on\nfive commonly-used datasets demonstrate the effectiveness and generality of our\nmodel. Particularly, when equipped with BERT and FHDecoder, our model achieves\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shaopeng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ante Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yubin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiali Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Junfeng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Degen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake News Detection in Spanish Using Deep Learning Techniques. (arXiv:2110.06461v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06461","description":"<p>This paper addresses the problem of fake news detection in Spanish using\nMachine Learning techniques. It is fundamentally the same problem tackled for\nthe English language; however, there is not a significant amount of publicly\navailable and adequately labeled fake news in Spanish to effectively train a\nMachine Learning model, similarly to those proposed for the English language.\nTherefore, this work explores different training strategies and architectures\nto establish a baseline for further research in this area. Four datasets were\nused, two in English and two in Spanish, and four experimental schemes were\ntested, including a baseline with classical Machine Learning models, trained\nand validated using a small dataset in Spanish. The remaining schemes include\nstate-of-the-art Deep Learning models trained (or fine-tuned) and validated in\nEnglish, trained and validated in Spanish, and fitted in English and validated\nwith automatic translated Spanish sentences. The Deep Learning architectures\nwere built on top of different pre-trained Word Embedding representations,\nincluding GloVe, ELMo, BERT, and BETO (a BERT version trained on a large corpus\nin Spanish). According to the results, the best strategy was a combination of a\npre-trained BETO model and a Recurrent Neural Network based on LSTM layers,\nyielding an accuracy of up to 80%; nonetheless, a baseline model using a Random\nForest estimator obtained similar outcomes. Additionally, the translation\nstrategy did not yield acceptable results because of the propagation error;\nthere was also observed a significant difference in models performance when\ntrained in English or Spanish, mainly attributable to the number of samples\navailable for each language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gallego_K/0/1/0/all/0/1\">Kevin Mart&#xed;nez-Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Ortiz_A/0/1/0/all/0/1\">Andr&#xe9;s M. &#xc1;lvarez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_Londono_J/0/1/0/all/0/1\">Juli&#xe1;n D. Arias-Londo&#xf1;o</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ActiveEA: Active Learning for Neural Entity Alignment. (arXiv:2110.06474v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06474","description":"<p>Entity Alignment (EA) aims to match equivalent entities across different\nKnowledge Graphs (KGs) and is an essential step of KG fusion. Current\nmainstream methods -- neural EA models -- rely on training with seed alignment,\ni.e., a set of pre-aligned entity pairs which are very costly to annotate. In\nthis paper, we devise a novel Active Learning (AL) framework for neural EA,\naiming to create highly informative seed alignment to obtain more effective EA\nmodels with less annotation cost. Our framework tackles two main challenges\nencountered when applying AL to EA: (1) How to exploit dependencies between\nentities within the AL strategy. Most AL strategies assume that the data\ninstances to sample are independent and identically distributed. However,\nentities in KGs are related. To address this challenge, we propose a\nstructure-aware uncertainty sampling strategy that can measure the uncertainty\nof each entity as well as its impact on its neighbour entities in the KG. (2)\nHow to recognise entities that appear in one KG but not in the other KG (i.e.,\nbachelors). Identifying bachelors would likely save annotation budget. To\naddress this challenge, we devise a bachelor recognizer paying attention to\nalleviate the effect of sampling bias. Empirical results show that our proposed\nAL strategy can significantly improve sampling quality with good generality\nacross different datasets, EA models and amount of bachelors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scells_H/0/1/0/all/0/1\">Harrisen Scells</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Genghong Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding of Emotion Perception from Art. (arXiv:2110.06486v1 [cs.CV])","link":"http://arxiv.org/abs/2110.06486","description":"<p>Computational modeling of the emotions evoked by art in humans is a\nchallenging problem because of the subjective and nuanced nature of art and\naffective signals. In this paper, we consider the above-mentioned problem of\nunderstanding emotions evoked in viewers by artwork using both text and visual\nmodalities. Specifically, we analyze images and the accompanying text captions\nfrom the viewers expressing emotions as a multimodal classification task. Our\nresults show that single-stream multimodal transformer-based models like MMBT\nand VisualBERT perform better compared to both image-only models and\ndual-stream multimodal models having separate pathways for text and image\nmodalities. We also observe improvements in performance for extreme positive\nand negative emotion classes, when a single-stream model like MMBT is compared\nwith a text-only transformer model like BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_D/0/1/0/all/0/1\">Digbalay Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somandepalli_K/0/1/0/all/0/1\">Krishna Somandepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Souvik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_R/0/1/0/all/0/1\">Rimita Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gratch_J/0/1/0/all/0/1\">Jonathan Gratch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dict-BERT: Enhancing Language Model Pre-training with Dictionary. (arXiv:2110.06490v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06490","description":"<p>Pre-trained language models (PLMs) aim to learn universal language\nrepresentations by conducting self-supervised training tasks on large-scale\ncorpora. Since PLMs capture word semantics in different contexts, the quality\nof word representations highly depends on word frequency, which usually follows\na heavy-tailed distributions in the pre-training corpus. Therefore, the\nembeddings of rare words on the tail are usually poorly optimized. In this\nwork, we focus on enhancing language model pre-training by leveraging\ndefinitions of the rare words in dictionaries (e.g., Wiktionary). To\nincorporate a rare word definition as a part of input, we fetch its definition\nfrom the dictionary and append it to the end of the input text sequence. In\naddition to training with the masked language modeling objective, we propose\ntwo novel self-supervised pre-training tasks on word and sentence-level\nalignment between input text sequence and rare word definitions to enhance\nlanguage modeling representation with dictionary. We evaluate the proposed\nDict-BERT model on the language understanding benchmark GLUE and eight\nspecialized domain benchmark datasets. Extensive experiments demonstrate that\nDict-BERT can significantly improve the understanding of rare words and boost\nmodel performance on various NLP downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Donghan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual COVID-19 Fake News Detection. (arXiv:2110.06495v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06495","description":"<p>The COVID-19 pandemic poses a great threat to global public health.\nMeanwhile, there is massive misinformation associated with the pandemic which\nadvocates unfounded or unscientific claims. Even major social media and news\noutlets have made an extra effort in debunking COVID-19 misinformation, most of\nthe fact-checking information is in English, whereas some unmoderated COVID-19\nmisinformation is still circulating in other languages, threatening the health\nof less-informed people in immigrant communities and developing countries. In\nthis paper, we make the first attempt to detect COVID-19 misinformation in a\nlow-resource language (Chinese) only using the fact-checked news in a\nhigh-resource language (English). We start by curating a Chinese real&amp;fake news\ndataset according to existing fact-checking information. Then, we propose a\ndeep learning framework named CrossFake to jointly encode the cross-lingual\nnews body texts and capture the news content as much as possible. Empirical\nresults on our dataset demonstrate the effectiveness of CorssFake under the\ncross-lingual setting and it also outperforms several monolingual and\ncross-lingual fake news detectors. The dataset is available at\nhttps://github.com/YingtongDou/CrossFake.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiangshu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yingtong Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Limeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentially Private Fine-tuning of Language Models. (arXiv:2110.06500v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06500","description":"<p>We give simpler, sparser, and faster algorithms for differentially private\nfine-tuning of large-scale pre-trained language models, which achieve the\nstate-of-the-art privacy versus utility tradeoffs on many standard NLP tasks.\nWe propose a meta-framework for this problem, inspired by the recent success of\nhighly parameter-efficient methods for fine-tuning. Our experiments show that\ndifferentially private adaptations of these approaches outperform previous\nprivate algorithms in three important dimensions: utility, privacy, and the\ncomputational and memory cost of private training. On many commonly studied\ndatasets, the utility of private models approaches that of non-private models.\nFor example, on the MNLI dataset we achieve an accuracy of $87.8\\%$ using\nRoBERTa-Large and $83.5\\%$ using RoBERTa-Base with a privacy budget of\n$\\epsilon = 6.7$. In comparison, absent privacy constraints, RoBERTa-Large\nachieves an accuracy of $90.2\\%$. Our findings are similar for natural language\ngeneration tasks. Privately fine-tuning with DART, GPT-2-Small, GPT-2-Medium,\nGPT-2-Large, and GPT-2-XL achieve BLEU scores of 38.5, 42.0, 43.1, and 43.8\nrespectively (privacy budget of $\\epsilon = 6.8,\\delta=$ 1e-5) whereas the\nnon-private baseline is $48.1$. All our experiments suggest that larger models\nare better suited for private fine-tuning: while they are well known to achieve\nsuperior accuracy non-privately, we find that they also better maintain their\naccuracy when privacy is introduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Da Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_S/0/1/0/all/0/1\">Saurabh Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1\">Gautam Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1\">Janardhan Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manoel_A/0/1/0/all/0/1\">Andre Manoel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1\">Sergey Yekhanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient domain adaptation of language models in ASR systems using Prompt-tuning. (arXiv:2110.06502v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06502","description":"<p>Automatic Speech Recognition (ASR) systems have found their use in numerous\nindustrial applications in very diverse domains. Since domain-specific systems\nperform better than their generic counterparts on in-domain evaluation, the\nneed for memory and compute-efficient domain adaptation is obvious.\nParticularly, adapting parameter-heavy transformer-based language models used\nfor rescoring ASR hypothesis is challenging. In this work, we overcome the\nproblem using prompt-tuning, a methodology that trains a small number of domain\ntoken embedding parameters to prime a transformer-based LM to a particular\ndomain. With just a handful of extra parameters per domain, we achieve much\nbetter perplexity scores over the baseline of using an unadapted LM. Despite\nbeing parameter-efficient, these improvements are comparable to those of\nfully-fine-tuned models with hundreds of millions of parameters. We replicate\nour findings in perplexity numbers to Word Error Rate in a domain-specific ASR\nsystem for one such domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1\">Ashish Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perception Point: Identifying Critical Learning Periods in Speech for Bilingual Networks. (arXiv:2110.06507v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06507","description":"<p>Recent studies in speech perception have been closely linked to fields of\ncognitive psychology, phonology, and phonetics in linguistics. During\nperceptual attunement, a critical and sensitive developmental trajectory has\nbeen examined in bilingual and monolingual infants where they can best\ndiscriminate common phonemes. In this paper, we compare and identify these\ncognitive aspects on deep neural-based visual lip-reading models. We conduct\nexperiments on the two most extensive public visual speech recognition datasets\nfor English and Mandarin. Through our experimental results, we observe a strong\ncorrelation between these theories in cognitive psychology and our unique\nmodeling. We inspect how these computational models develop similar phases in\nspeech perception and acquisitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saraswat_A/0/1/0/all/0/1\">Anuj Saraswat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_M/0/1/0/all/0/1\">Mehar Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dawn of Quantum Natural Language Processing. (arXiv:2110.06510v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06510","description":"<p>In this paper, we discuss the initial attempts at boosting understanding\nhuman language based on deep-learning models with quantum computing. We\nsuccessfully train a quantum-enhanced Long Short-Term Memory network to perform\nthe parts-of-speech tagging task via numerical simulations. Moreover, a\nquantum-enhanced Transformer is proposed to perform the sentiment analysis\nbased on the existing dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sipio_R/0/1/0/all/0/1\">Riccardo Di Sipio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Hong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Samuel Yen-Chi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangini_S/0/1/0/all/0/1\">Stefano Mangini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1\">Marcel Worring</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventBERT: A Pre-Trained Model for Event Correlation Reasoning. (arXiv:2110.06533v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06533","description":"<p>Event correlation reasoning infers whether a natural language paragraph\ncontaining multiple events conforms to human common sense. For example, \"Andrew\nwas very drowsy, so he took a long nap, and now he is very alert\" is sound and\nreasonable. In contrast, \"Andrew was very drowsy, so he stayed up a long time,\nnow he is very alert\" does not comply with human common sense. Such reasoning\ncapability is essential for many downstream tasks, such as script reasoning,\nabductive reasoning, narrative incoherence, story cloze test, etc. However,\nconducting event correlation reasoning is challenging due to a lack of large\namounts of diverse event-based knowledge and difficulty in capturing\ncorrelation among multiple events. In this paper, we propose EventBERT, a\npre-trained model to encapsulate eventuality knowledge from unlabeled text.\nSpecifically, we collect a large volume of training examples by identifying\nnatural language paragraphs that describe multiple correlated events and\nfurther extracting event spans in an unsupervised manner. We then propose three\nnovel event- and correlation-based learning objectives to pre-train an event\ncorrelation model on our created training corpus. Empirical results show\nEventBERT outperforms strong baselines on four downstream tasks, and achieves\nSoTA results on most of them. Besides, it outperforms existing pre-trained\nmodels by a large margin, e.g., 6.5~23%, in zero-shot learning of these tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and the growth of margin. To counteract this deficiency, we\npropose to reward well-classified examples with additive bonuses to revive\ntheir contribution to learning. This counterexample theoretically addresses\nthese three issues. We empirically support this claim by directly verify the\ntheoretical results or through the significant performance improvement with our\ncounterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that\nbecause our idea can solve these three issues, we can deal with complex\nscenarios, such as imbalanced classification, OOD detection, and applications\nunder adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple or Complex? Complexity-Controllable Question Generation with Soft Templates and Deep Mixture of Experts Model. (arXiv:2110.06560v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06560","description":"<p>The ability to generate natural-language questions with controlled complexity\nlevels is highly desirable as it further expands the applicability of question\ngeneration. In this paper, we propose an end-to-end neural\ncomplexity-controllable question generation model, which incorporates a mixture\nof experts (MoE) as the selector of soft templates to improve the accuracy of\ncomplexity control and the quality of generated questions. The soft templates\ncapture question similarity while avoiding the expensive construction of actual\ntemplates. Our method introduces a novel, cross-domain complexity estimator to\nassess the complexity of a question, taking into account the passage, the\nquestion, the answer and their interactions. The experimental results on two\nbenchmark QA datasets demonstrate that our QG model is superior to\nstate-of-the-art methods in both automatic and manual evaluation. Moreover, our\ncomplexity estimator is significantly more accurate than the baselines in both\nin-domain and out-domain settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sheng Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shirong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yinlin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators. (arXiv:2110.06609v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06609","description":"<p>Pre-trained language models have recently been shown to be able to perform\ntranslation without finetuning via prompting. Inspired by these findings, we\nstudy improving the performance of pre-trained language models on translation\ntasks, where training neural machine translation models is the current de facto\napproach. We present Multi-Stage Prompting, a simple and lightweight approach\nfor better adapting pre-trained language models to translation tasks. To make\npre-trained language models better translators, we divide the translation\nprocess via pre-trained language models into three separate stages: the\nencoding stage, the re-encoding stage, and the decoding stage. During each\nstage, we independently apply different continuous prompts for allowing\npre-trained language models better adapting to translation tasks. We conduct\nextensive experiments on low-, medium-, and high-resource translation tasks.\nExperiments show that our method can significantly improve the translation\nperformance of pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Dense Retrieval for Dialogue Response Selection. (arXiv:2110.06612v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06612","description":"<p>Recent research on dialogue response selection has been mainly focused on\nselecting a proper response from a pre-defined small set of candidates using\nsophisticated neural models. Due to their heavy computational overhead, they\nare unable to select responses from a large candidate pool. In this study, we\npresent a solution to directly select proper responses from a large corpus or\neven a nonparallel corpus that only consists of unpaired sentences, using a\ndense retrieval model. We extensively test our proposed approach under two\nexperiment settings: (i) re-rank experiment that aims to rank a small set of\npre-defined candidates; (ii) full-rank experiment where the target is to\ndirectly select proper responses from a full candidate pool that may contain\nmillions of candidates. For re-rank setting, the superiority is quite\nsurprising given its simplicity. For full-rank setting, we can emphasize that\nwe are the first to do such evaluation. Moreover, human evaluation results show\nthat increasing the size of nonparallel corpus leads to further improvement of\nour model performance\\footnote{All our source codes, models and other related\nresources are publically available at\n\\url{https://github.com/gmftbyGMFTBY/SimpleReDial-v1}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximizing Efficiency of Language Model Pre-training for Learning Representation. (arXiv:2110.06620v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06620","description":"<p>Pre-trained language models in the past years have shown exponential growth\nin model parameters and compute time. ELECTRA is a novel approach for improving\nthe compute efficiency of pre-trained language models (e.g. BERT) based on\nmasked language modeling (MLM) by addressing the sample inefficiency problem\nwith the replaced token detection (RTD) task. Our work proposes adaptive early\nexit strategy to maximize the efficiency of the pre-training process by\nrelieving the model's subsequent layers of the need to process latent features\nby leveraging earlier layer representations. Moreover, we evaluate an initial\napproach to the problem that has not succeeded in maintaining the accuracy of\nthe model while showing a promising compute efficiency by thoroughly\ninvestigating the necessity of the generator module of ELECTRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Suwon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeonghwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaeyoung Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1\">Sung-Hyon Myaeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end translation of human neural activity to speech with a dual-dual generative adversarial network. (arXiv:2110.06634v1 [cs.SD])","link":"http://arxiv.org/abs/2110.06634","description":"<p>In a recent study of auditory evoked potential (AEP) based brain-computer\ninterface (BCI), it was shown that, with an encoder-decoder framework, it is\npossible to translate human neural activity to speech (T-CAS). However, current\nencoder-decoder-based methods achieve T-CAS often with a two-step method where\nthe information is passed between the encoder and decoder with a shared\ndimension reduction vector, which may result in a loss of information. A\npotential approach to this problem is to design an end-to-end method by using a\ndual generative adversarial network (DualGAN) without dimension reduction of\npassing information, but it cannot realize one-to-one signal-to-signal\ntranslation (see Fig.1 (a) and (b)). In this paper, we propose an end-to-end\nmodel to translate human neural activity to speech directly, create a new\nelectroencephalogram (EEG) datasets for participants with good attention by\ndesign a device to detect participants' attention, and introduce a dual-dual\ngenerative adversarial network (Dual-DualGAN) (see Fig. 1 (c) and (d)) to\naddress an end-to-end translation of human neural activity to speech (ET-CAS)\nproblem by group labelling EEG signals and speech signals, inserting a\ntransition domain to realize cross-domain mapping. In the transition domain,\nthe transition signals are cascaded by the corresponding EEG and speech signals\nin a certain proportion, which can build bridges for EEG and speech signals\nwithout corresponding features, and realize one-to-one cross-domain\nEEG-to-speech translation. The proposed method can translate word-length and\nsentence-length sequences of neural activity to speech. Experimental evaluation\nhas been conducted to show that the proposed method significantly outperforms\nstate-of-the-art methods on both words and sentences of auditory stimulus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yina Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhenying Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Anhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction. (arXiv:2110.06651v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06651","description":"<p>Keyphrases are phrases in a document providing a concise summary of core\ncontent, helping readers to understand what the article is talking about in a\nminute. However, existing unsupervised works are not robust enough to handle\nvarious types of documents owing to the mismatch of sequence length for\ncomparison. In this paper, we propose a novel unsupervised keyword extraction\nmethod by leveraging the BERT-based model to select and rank candidate\nkeyphrases with a MASK strategy. In addition, we further enhance the model,\ndenoted as Keyphrases Extraction BERT (KPEBERT), via designing a compatible\nself-supervised task and conducting a contrast learning. We conducted extensive\nexperimental evaluation to demonstrate the superiority and robustness of the\nproposed method as well as the effectiveness of KPEBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truthful AI: Developing and governing AI that does not lie. (arXiv:2110.06674v1 [cs.CY])","link":"http://arxiv.org/abs/2110.06674","description":"<p>In many contexts, lying -- the use of verbal falsehoods to deceive -- is\nharmful. While lying has traditionally been a human affair, AI systems that\nmake sophisticated verbal statements are becoming increasingly prevalent. This\nraises the question of how we should limit the harm caused by AI \"lies\" (i.e.\nfalsehoods that are actively selected for). Human truthfulness is governed by\nsocial norms and by laws (against defamation, perjury, and fraud). Differences\nbetween AI and humans present an opportunity to have more precise standards of\ntruthfulness for AI, and to have these standards rise over time. This could\nprovide significant benefits to public epistemics and the economy, and mitigate\nrisks of worst-case AI futures.\n</p>\n<p>Establishing norms or laws of AI truthfulness will require significant work\nto: (1) identify clear truthfulness standards; (2) create institutions that can\njudge adherence to those standards; and (3) develop AI systems that are\nrobustly truthful.\n</p>\n<p>Our initial proposals for these areas include: (1) a standard of avoiding\n\"negligent falsehoods\" (a generalisation of lies that is easier to assess); (2)\ninstitutions to evaluate AI systems before and after real-world deployment; and\n(3) explicitly training AI systems to be truthful via curated datasets and\nhuman interaction.\n</p>\n<p>A concerning possibility is that evaluation mechanisms for eventual\ntruthfulness standards could be captured by political interests, leading to\nharmful censorship and propaganda. Avoiding this might take careful attention.\nAnd since the scale of AI speech acts might grow dramatically over the coming\ndecades, early truthfulness standards might be particularly important because\nof the precedents they set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotton_Barratt_O/0/1/0/all/0/1\">Owen Cotton-Barratt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finnveden_L/0/1/0/all/0/1\">Lukas Finnveden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bales_A/0/1/0/all/0/1\">Adam Bales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balwit_A/0/1/0/all/0/1\">Avital Balwit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wills_P/0/1/0/all/0/1\">Peter Wills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Righetti_L/0/1/0/all/0/1\">Luca Righetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese. (arXiv:2110.06696v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06696","description":"<p>Although pre-trained models (PLMs) have achieved remarkable improvements in a\nwide range of NLP tasks, they are expensive in terms of time and resources.\nThis calls for the study of training more efficient models with less\ncomputation but still ensures impressive performance. Instead of pursuing a\nlarger scale, we are committed to developing lightweight yet more powerful\nmodels trained with equal or less computation and friendly to rapid deployment.\nThis technical report releases our pre-trained model called Mengzi, which\nstands for a family of discriminative, generative, domain-specific, and\nmultimodal pre-trained model variants, capable of a wide range of language and\nvision tasks. Compared with public Chinese PLMs, Mengzi is simple but more\npowerful. Our lightweight model has achieved new state-of-the-art results on\nthe widely-used CLUE benchmark with our optimized pre-training and fine-tuning\ntechniques. Without modifying the model architecture, our model can be easily\nemployed as an alternative to existing PLMs. Our sources are available at\nhttps://github.com/Langboat/Mengzi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_J/0/1/0/all/0/1\">Jingyun Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematic Inequalities in Language Technology Performance across the World's Languages. (arXiv:2110.06733v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06733","description":"<p>Natural language processing (NLP) systems have become a central technology in\ncommunication, education, medicine, artificial intelligence, and many other\ndomains of research and development. While the performance of NLP methods has\ngrown enormously over the last decade, this progress has been restricted to a\nminuscule subset of the world's 6,500 languages. We introduce a framework for\nestimating the global utility of language technologies as revealed in a\ncomprehensive snapshot of recent publications in NLP. Our analyses involve the\nfield at large, but also more in-depth studies on both user-facing technologies\n(machine translation, language understanding, question answering,\ntext-to-speech synthesis) as well as more linguistic NLP tasks (dependency\nparsing, morphological inflection). In the process, we (1) quantify disparities\nin the current state of NLP research, (2) explore some of its associated\nsocietal and academic factors, and (3) produce tailored recommendations for\nevidence-based policy making aimed at promoting more global and equitable\nlanguage technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1\">Dami&#xe1;n Blasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masader: Metadata Sourcing for Arabic Text and Speech Data Resources. (arXiv:2110.06744v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06744","description":"<p>The NLP pipeline has evolved dramatically in the last few years. The first\nstep in the pipeline is to find suitable annotated datasets to evaluate the\ntasks we are trying to solve. Unfortunately, most of the published datasets\nlack metadata annotations that describe their attributes. Not to mention, the\nabsence of a public catalogue that indexes all the publicly available datasets\nrelated to specific regions or languages. When we consider low-resource\ndialectical languages, for example, this issue becomes more prominent. In this\npaper we create \\textit{Masader}, the largest public catalogue for Arabic NLP\ndatasets, which consists of 200 datasets annotated with 25 attributes.\nFurthermore, We develop a metadata annotation strategy that could be extended\nto other languages. We also make remarks and highlight some issues about the\ncurrent status of Arabic NLP datasets and suggest recommendations to address\nthem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoud_M/0/1/0/all/0/1\">Maraim Masoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaleb_M/0/1/0/all/0/1\">Mustafa Ghaleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1\">Maged S. Al-shaibani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Automated Unit Tests for Unsupervised Code Translation. (arXiv:2110.06773v1 [cs.SE])","link":"http://arxiv.org/abs/2110.06773","description":"<p>With little to no parallel data available for programming languages,\nunsupervised methods are well-suited to source code translation. However, the\nmajority of unsupervised machine translation approaches rely on\nback-translation, a method developed in the context of natural language\ntranslation and one that inherently involves training on noisy inputs.\nUnfortunately, source code is highly sensitive to small changes; a single token\ncan result in compilation failures or erroneous programs, unlike natural\nlanguages where small inaccuracies may not change the meaning of a sentence. To\naddress this issue, we propose to leverage an automated unit-testing system to\nfilter out invalid translations, thereby creating a fully tested parallel\ncorpus. We found that fine-tuning an unsupervised model with this filtered data\nset significantly reduces the noise in the translations so-generated,\ncomfortably outperforming the state-of-the-art for all language pairs studied.\nIn particular, for Java $\\to$ Python and Python $\\to$ C++ we outperform the\nbest previous methods by more than 16% and 24% respectively, reducing the error\nrate by more than 35%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1\">Baptiste Roziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie M. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charton_F/0/1/0/all/0/1\">Francois Charton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harman_M/0/1/0/all/0/1\">Mark Harman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lample_G/0/1/0/all/0/1\">Guillaume Lample</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems. (arXiv:2110.06800v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06800","description":"<p>Zero/few-shot transfer to unseen services is a critical challenge in\ntask-oriented dialogue research. The Schema-Guided Dialogue (SGD) dataset\nintroduced a paradigm for enabling models to support an unlimited number of\nservices without additional data collection or re-training through the use of\nschemas. Schemas describe service APIs in natural language, which models\nconsume to understand the services they need to support. However, the impact of\nthe choice of language in these schemas on model performance remains\nunexplored. We address this by releasing SGD-X, a benchmark for measuring the\nrobustness of dialogue systems to linguistic variations in schemas. SGD-X\nextends the SGD dataset with crowdsourced variants for every schema, where\nvariants are semantically similar yet stylistically diverse. We evaluate two\ndialogue state tracking models on SGD-X and observe that neither generalizes\nwell across schema variations, measured by joint goal accuracy and a novel\nmetric for measuring schema sensitivity. Furthermore, we present a simple\nmodel-agnostic data augmentation method to improve schema robustness and\nzero-shot generalization to unseen services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harrison Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raghav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging redundancy in attention with Reuse Transformers. (arXiv:2110.06821v1 [cs.LG])","link":"http://arxiv.org/abs/2110.06821","description":"<p>Pairwise dot product-based attention allows Transformers to exchange\ninformation between tokens in an input-dependent way, and is key to their\nsuccess across diverse applications in language and vision. However, a typical\nTransformer model computes such pairwise attention scores repeatedly for the\nsame sequence, in multiple heads in multiple layers. We systematically analyze\nthe empirical similarity of these scores across heads and layers and find them\nto be considerably redundant, especially adjacent layers showing high\nsimilarity. Motivated by these findings, we propose a novel architecture that\nreuses attention scores computed in one layer in multiple subsequent layers.\nExperiments on a number of standard benchmarks show that reusing attention\ndelivers performance equivalent to or better than standard transformers, while\nreducing both compute and memory usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhojanapalli_S/0/1/0/all/0/1\">Srinadh Bhojanapalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1\">Ayan Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1\">Andreas Veit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasik_M/0/1/0/all/0/1\">Michal Lukasik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1\">Himanshu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yin-Wen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Speaker-Aware Learning Framework for Improving Multi-turn Dialogue Coherence. (arXiv:2110.06823v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06823","description":"<p>This paper presents a novel open-domain dialogue generation framework\nemphasizing the differentiation of speakers in multi-turn conversations.\nDiffering from prior work that solely relies on the content of conversation\nhistory to generate a response, we argue that capturing relative social\nrelations among utterances (i.e., generated by either the same speaker or\ndifferent persons) benefits the machine capturing fine-grained context\ninformation from a conversation history to improve context coherence in the\ngenerated response. Given that, we propose a speaker-aware framework, named\nParallel Hierarchical Attentive Encoder-Decoder (PHAED), that aims to model\neach utterance with the awareness of its speaker and contextual associations\nwith the same speaker's previous messages. Specifically, in a conversation\ninvolving two speakers, we regard the utterances from one speaker as responses\nand those from the other as queries. After understanding queries via our\nencoder with inner-query and inter-query encodings, our decoder reuses the\nhidden states of previously generated responses to generate a new response. Our\nempirical results show that PHAED outperforms the state-of-the-art in both\nautomatic and human evaluations. Furthermore, our ablation study shows that\ndialogue models with speaker tokens can generally decrease the possibility of\ngenerating non-coherent responses regarding the conversation context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Ming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junli Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Language Model Integration for RNN Transducer based Speech Recognition. (arXiv:2110.06841v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06841","description":"<p>The mismatch between an external language model (LM) and the implicitly\nlearned internal LM (ILM) of RNN-Transducer (RNN-T) can limit the performance\nof LM integration such as simple shallow fusion. A Bayesian interpretation\nsuggests to remove this sequence prior as ILM correction. In this work, we\nstudy various ILM correction-based LM integration methods formulated in a\ncommon RNN-T framework. We provide a decoding interpretation on two major\nreasons for performance improvement with ILM correction, which is further\nexperimentally verified with detailed analysis. We also propose an exact-ILM\ntraining framework by extending the proof given in the hybrid autoregressive\ntransducer, which enables a theoretical justification for other ILM approaches.\nSystematic comparison is conducted for both in-domain and cross-domain\nevaluation on the Librispeech and TED-LIUM Release 2 corpora, respectively. Our\nproposed exact-ILM training can further improve the best ILM method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zuoyun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization in Dependency Parsing. (arXiv:2110.06843v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06843","description":"<p>Compositionality, or the ability to combine familiar units like words into\nnovel phrases and sentences, has been the focus of intense interest in\nartificial intelligence in recent years. To test compositional generalization\nin semantic parsing, Keysers et al. (2020) introduced Compositional Freebase\nQueries (CFQ). This dataset maximizes the similarity between the test and train\ndistributions over primitive units, like words, while maximizing the compound\ndivergence: the dissimilarity between test and train distributions over larger\nstructures, like phrases. Dependency parsing, however, lacks a compositional\ngeneralization benchmark. In this work, we introduce a gold-standard set of\ndependency parses for CFQ, and use this to analyze the behavior of a\nstate-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We\nfind that increasing compound divergence degrades dependency parsing\nperformance, although not as dramatically as semantic parsing performance.\nAdditionally, we find the performance of the dependency parser does not\nuniformly degrade relative to compound divergence, and the parser performs\ndifferently on different splits with the same compound divergence. We explore a\nnumber of hypotheses for what causes the non-uniform degradation in dependency\nparsing performance, and identify a number of syntactic structures that drive\nthe dependency parser's lower performance on the most challenging splits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_E/0/1/0/all/0/1\">Emily Goodwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ousiometrics and Telegnomics: The essence of meaning conforms to a two-dimensional powerful-weak and dangerous-safe framework with diverse corpora presenting a safety bias. (arXiv:2110.06847v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06847","description":"<p>We define `ousiometrics' to be the study of essential meaning in whatever\ncontext that meaningful signals are communicated, and `telegnomics' as the\nstudy of remotely sensed knowledge. From work emerging through the middle of\nthe 20th century, the essence of meaning has become generally accepted as being\nwell captured by the three orthogonal dimensions of evaluation, potency, and\nactivation (EPA). By re-examining first types and then tokens for the English\nlanguage, and through the use of automatically annotated histograms --\n`ousiograms' -- we find here that: 1. The essence of meaning conveyed by words\nis instead best described by a compass-like power-danger (PD) framework, and 2.\nAnalysis of a disparate collection of large-scale English language corpora --\nliterature, news, Wikipedia, talk radio, and social media -- shows that natural\nlanguage exhibits a systematic bias toward safe, low danger words -- a\nreinterpretation of the Pollyanna principle's positivity bias for written\nexpression. To help justify our choice of dimension names and to help address\nthe problems with representing observed ousiometric dimensions by bipolar\nadjective pairs, we introduce and explore `synousionyms' and `antousionyms' --\nousiometric counterparts of synonyms and antonyms. We further show that the PD\nframework revises the circumplex model of affect as a more general model of\nstate of mind. Finally, we use our findings to construct and test a prototype\n`ousiometer', a telegnomic instrument that measures ousiometric time series for\ntemporal corpora. We contend that our power-danger ousiometric framework\nprovides a complement for entropy-based measurements, and may be of value for\nthe study of a wide variety of communication across biological and artificial\nlife.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">P. S. Dodds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">T. Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">M. I. Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmerman_J/0/1/0/all/0/1\">J. W. Zimmerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovato_J/0/1/0/all/0/1\">J. Lovato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beaulieu_S/0/1/0/all/0/1\">S. Beaulieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minot_J/0/1/0/all/0/1\">J. R. Minot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">M. V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reagan_A/0/1/0/all/0/1\">A. J. Reagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">C. M. Danforth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphosyntactic Tagging with Pre-trained Language Models for Arabic and its Dialects. (arXiv:2110.06852v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06852","description":"<p>We present state-of-the-art results on morphosyntactic tagging across\ndifferent varieties of Arabic using fine-tuned pre-trained transformer language\nmodels. Our models consistently outperform existing systems in Modern Standard\nArabic and all the Arabic dialects we study, achieving 2.6% absolute\nimprovement over the previous state-of-the-art in Modern Standard Arabic, 2.8%\nin Gulf, 1.6% in Egyptian, and 7.0% in Levantine. We explore different training\nsetups for fine-tuning pre-trained transformer language models, including\ntraining data size, the use of external linguistic resources, and the use of\nannotated data from other dialects in a low-resource scenario. Our results show\nthat strategic fine-tuning using datasets from other high-resource dialects is\nbeneficial for a low-resource dialect. Additionally, we show that high-quality\nmorphological analyzers as external linguistic resources are beneficial\nespecially in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_S/0/1/0/all/0/1\">Salam Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Role Labeling as Dependency Parsing: Exploring Latent Tree Structures Inside Arguments. (arXiv:2110.06865v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06865","description":"<p>Semantic role labeling is a fundamental yet challenging task in the NLP\ncommunity. Recent works of SRL mainly fall into two lines:1) BIO-based and 2)\nspan-based. Despite effectiveness, they share some intrinsic drawbacks of not\nexplicitly considering internal argument structures, which may potentially\nhinder the model's expressiveness. To remedy this, we propose to reduce SRL to\na dependency parsing task and regard the flat argument spans as latent\nsubtrees. In particular, we equip our formulation with a novel span-constrained\nTreeCRF model to make tree structures span-aware, and further extend it to the\nsecond-order case. Experiments on CoNLL05 and CoNLL12 benchmarks reveal that\nthe results of our methods outperform all previous works and achieve the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qingrong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guohong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Essay Scoring Using Transformer Models. (arXiv:2110.06874v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06874","description":"<p>Automated essay scoring (AES) is gaining increasing attention in the\neducation sector as it significantly reduces the burden of manual scoring and\nallows ad hoc feedback for learners. Natural language processing based on\nmachine learning has been shown to be particularly suitable for text\nclassification and AES. While many machine-learning approaches for AES still\nrely on a bag-of-words (BOW) approach, we consider a transformer-based approach\nin this paper, compare its performance to a logistic regression model based on\nthe BOW approach and discuss their differences. The analysis is based on 2,088\nemail responses to a problem-solving task, that were manually labeled in terms\nof politeness. Both transformer models considered in that analysis outperformed\nwithout any hyper-parameter tuning the regression-based model. We argue that\nfor AES tasks such as politeness classification, the transformer-based approach\nhas significant advantages, while a BOW approach suffers from not taking word\norder into account and reducing the words to their stem. Further, we show how\nsuch models can help increase the accuracy of human raters, and we provide a\ndetailed instruction on how to implement transformer-based models for one's own\npurpose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ludwig_S/0/1/0/all/0/1\">Sabrina Ludwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayer_C/0/1/0/all/0/1\">Christian Mayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Christopher Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eilers_K/0/1/0/all/0/1\">Kerstin Eilers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_S/0/1/0/all/0/1\">Steffen Brandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers. (arXiv:2110.06884v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06884","description":"<p>We describe a Question Answering (QA) dataset that contains complex questions\nwith conditional answers, i.e. the answers are only applicable when certain\nconditions apply. We call this dataset ConditionalQA. In addition to\nconditional answers, the dataset also features: (1) long context documents with\ninformation that is related in logically complex ways; (2) multi-hop questions\nthat require compositional logical reasoning; (3) a combination of extractive\nquestions, yes/no questions, questions with multiple answers, and\nnot-answerable questions; (4) questions asked without knowing the answers. We\nshow that ConditionalQA is challenging for many of the existing QA models,\nespecially in selecting answer conditions. We believe that this dataset will\nmotivate further research in answering complex questions over long documents.\nData and leaderboard are publicly available at\n\\url{https://github.com/haitian-sun/ConditionalQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haitian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Visual Scene-Aware Dialog and Reasoning using Audio-Visual Transformers with Joint Student-Teacher Learning. (arXiv:2110.06894v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06894","description":"<p>In previous work, we have proposed the Audio-Visual Scene-Aware Dialog (AVSD)\ntask, collected an AVSD dataset, developed AVSD technologies, and hosted an\nAVSD challenge track at both the 7th and 8th Dialog System Technology\nChallenges (DSTC7, DSTC8). In these challenges, the best-performing systems\nrelied heavily on human-generated descriptions of the video content, which were\navailable in the datasets but would be unavailable in real-world applications.\nTo promote further advancements for real-world applications, we proposed a\nthird AVSD challenge, at DSTC10, with two modifications: 1) the human-created\ndescription is unavailable at inference time, and 2) systems must demonstrate\ntemporal reasoning by finding evidence from the video to support each answer.\nThis paper introduces the new task that includes temporal reasoning and our new\nextension of the AVSD dataset for DSTC10, for which we collected\nhuman-generated temporal reasoning data. We also introduce a baseline system\nbuilt using an AV-transformer, which we released along with the new dataset.\nFinally, this paper introduces a new system that extends our baseline system\nwith attentional multimodal fusion, joint student-teacher learning (JSTL), and\nmodel combination techniques, achieving state-of-the-art performances on the\nAVSD datasets for DSTC7, DSTC8, and DSTC10. We also propose two temporal\nreasoning methods for AVSD: one attention-based, and one based on a time-domain\nregion proposal network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit P. Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1\">Takaaki Hori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_T/0/1/0/all/0/1\">Tim K. Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1\">Jonathan Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1\">Chiori Hori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Models new APIs: Domain-Agnostic Simulators for Task Oriented Dialogue. (arXiv:2110.06905v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06905","description":"<p>We demonstrate that large language models are able to simulate Task Oriented\nDialogues in novel domains, provided only with an API implementation and a list\nof goals. We show these simulations can formulate online, automatic metrics\nthat correlate well with human evaluations. Furthermore, by checking for\nwhether the User's goals are met, we can use simulation to repeatedly generate\ntraining data and improve the quality of simulations themselves. With no human\nintervention or domain-specific training data, our simulations bootstrap\nend-to-end models which achieve a 37\\% error reduction in previously unseen\ndomains. By including as few as 32 domain-specific conversations, bootstrapped\nmodels can match the performance of a fully-supervised model with $10\\times$\nmore data. To our knowledge, this is the first time simulations have been shown\nto be effective at bootstrapping models without explicitly requiring any\ndomain-specific training data, rule-engineering, or humans-in-the-loop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crook_P/0/1/0/all/0/1\">Paul A. Crook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roller_S/0/1/0/all/0/1\">Stephen Roller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?. (arXiv:2110.06918v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06918","description":"<p>Despite their recent popularity and well known advantages, dense retrievers\nstill lag behind sparse methods such as BM25 in their ability to reliably match\nsalient phrases and rare entities in the query. It has been argued that this is\nan inherent limitation of dense models. We disprove this claim by introducing\nthe Salient Phrase Aware Retriever (SPAR), a dense retriever with the lexical\nmatching capacity of a sparse model. In particular, we show that a dense\nretriever {\\Lambda} can be trained to imitate a sparse one, and SPAR is built\nby augmenting a standard dense retriever with {\\Lambda}. When evaluated on five\nopen-domain question answering datasets and the MS MARCO passage retrieval\ntask, SPAR sets a new state of the art for dense and sparse retrievers and can\nmatch or exceed the performance of more complicated dense-sparse hybrid\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anchit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1\">Stan Peshterliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1\">Yashar Mehdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantics-aware Attention Improves Neural Machine Translation. (arXiv:2110.06920v1 [cs.CL])","link":"http://arxiv.org/abs/2110.06920","description":"<p>The integration of syntactic structures into Transformer machine translation\nhas shown positive results, but to our knowledge, no work has attempted to do\nso with semantic structures. In this work we propose two novel parameter-free\nmethods for injecting semantic information into Transformers, both rely on\nsemantics-aware masking of (some of) the attention heads. One such method\noperates on the encoder, through a Scene-Aware Self-Attention (SASA) head.\nAnother on the decoder, through a Scene-Aware Cross-Attention (SACrA) head. We\nshow a consistent improvement over the vanilla Transformer and syntax-aware\nmodels for four language pairs. We further show an additional gain when using\nboth semantic and syntactic structures in some language pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Slobodkin_A/0/1/0/all/0/1\">Aviv Slobodkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Representations for Modeling Variation in Speech. (arXiv:2011.12649v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.12649","description":"<p>Variation in speech is often represented and investigated using phonetic\ntranscriptions, but transcribing speech is time-consuming and error prone. As\nan alternative representation, therefore, we investigate the extraction of\nacoustic embeddings from several self-supervised neural models. We use these\nrepresentations to compute word-based pronunciation differences between\nnon-native and native speakers of English, and between different dialect\npronunciations, and evaluate these differences by comparing them with available\nhuman native-likeness judgments. We show that Transformer-based speech\nrepresentations lead to significant performance gains over the use of phonetic\ntranscriptions, and find that feature-based use of Transformer models is most\neffective with one of the middle layers instead of the final layer. We also\ndemonstrate that these neural speech representations not only capture segmental\ndifferences, but also intonational and durational differences that cannot be\nrepresented by a set of discrete symbols used in phonetic transcriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_W/0/1/0/all/0/1\">Wietse de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanal_F/0/1/0/all/0/1\">Faraz Sanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_C/0/1/0/all/0/1\">Caitlin Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liberman_M/0/1/0/all/0/1\">Mark Liberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negation in Cognitive Reasoning. (arXiv:2012.12641v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.12641","description":"<p>Negation is both an operation in formal logic and in natural language by\nwhich a proposition is replaced by one stating the opposite, as by the addition\nof \"not\" or another negation cue. Treating negation in an adequate way is\nrequired for cognitive reasoning, which aims at modeling the human ability to\ndraw meaningful conclusions despite incomplete and inconsistent knowledge. One\ntask of cognitive reasoning is answering questions given by sentences in\nnatural language. There are tools based on discourse representation theory to\nconvert sentences automatically into a formal logic representation, and\nadditional knowledge can be added using the predicate names in the formula and\nknowledge databases. However, the knowledge in logic databases in practice\nalways is incomplete. Hence, forward reasoning of automated reasoning systems\nalone does not suffice to derive answers to questions because, instead of\ncomplete proofs, often only partial positive knowledge can be derived, while\nnegative knowledge is used only during the reasoning process. In consequence,\nwe aim at eliminating syntactic negation, strictly speaking, the negated event\nor property. In this paper, we describe an effective procedure to determine the\nnegated event or property in order to replace it by its inverse. This lays the\nbasis of cognitive reasoning, employing both logic and machine learning for\ngeneral question answering. We evaluate our procedure by several benchmarks and\ndemonstrate its practical usefulness in our cognitive reasoning system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schon_C/0/1/0/all/0/1\">Claudia Schon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siebert_S/0/1/0/all/0/1\">Sophie Siebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1\">Frieder Stolzenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Specifying and Interpreting Reinforcement Learning Policies through Simulatable Machine Learning. (arXiv:2101.07140v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.07140","description":"<p>Human-AI collaborative policy synthesis is a procedure in which (1) a human\ninitializes an autonomous agent's behavior, (2) Reinforcement Learning improves\nthe human specified behavior, and (3) the agent can explain the final optimized\npolicy to the user. This paradigm leverages human expertise and facilitates a\ngreater insight into the learned behaviors of an agent. Existing approaches to\nenabling collaborative policy specification involve black box methods which are\nunintelligible and are not catered towards non-expert end-users. In this paper,\nwe develop a novel collaborative framework to enable humans to initialize and\ninterpret an autonomous agent's behavior, rooted in principles of\nhuman-centered design. Through our framework, we enable humans to specify an\ninitial behavior model in the form of unstructured, natural language, which we\nthen convert to lexical decision trees. Next, we are able to leverage these\nhuman-specified policies, to warm-start reinforcement learning and further\nallow the agent to optimize the policies through reinforcement learning.\nFinally, to close the loop on human-specification, we produce explanations of\nthe final learned policy, in multiple modalities, to provide the user a final\ndepiction about the learned policy of the agent. We validate our approach by\nshowing that our model can produce &gt;80% accuracy, and that human-initialized\npolicies are able to successfully warm-start RL. We then conduct a novel\nhuman-subjects study quantifying the relative subjective and objective benefits\nof varying XAI modalities(e.g., Tree, Language, and Program) for explaining\nlearned policies to end-users, in terms of usability and interpretability and\nidentify the circumstances that influence these measures. Our findings\nemphasize the need for personalized explainable systems that can facilitate\nuser-centric policy explanations for a variety of end-users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tambwekar_P/0/1/0/all/0/1\">Pradyumna Tambwekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Andrew Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1\">Nakul Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1\">Matthew Gombolay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Proofs via Smart Contracts: Succinct and Informative Mathematical Derivations via Decentralized Markets. (arXiv:2102.03044v4 [cs.GT] UPDATED)","link":"http://arxiv.org/abs/2102.03044","description":"<p>Modern mathematics is built on the idea that proofs should be translatable\ninto formal proofs, whose validity is an objective question, decidable by a\ncomputer. Yet, in practice, proofs are informal and may omit many details. An\nagent considers a proof valid if they trust that it could be expanded into a\nmachine-verifiable proof. A proof's validity can thus become a subjective\nmatter and lead to a debate, which may be difficult to settle. Hence, while the\nconcept of valid proof is well-defined, the process to establish validity is\nitself a complex multi-agent problem.\n</p>\n<p>We introduce the SPRIG protocol. SPRIG allows agents to propose and verify\nsuccinct and informative proofs in a decentralized fashion; the trust is\nestablished by agents being able to request more details in the proof steps;\ndebates, if they arise, must isolate details of proofs and, if they persist, go\ndown to machine-level details, where they are automatically settled. A\nstructure of bounties and stakes is set to incentivize agents to act in good\nfaith.\n</p>\n<p>We propose a game-theoretic discussion of SPRIG, showing how agents with\nvarious types of information interact, leading to a proof tree with an\nappropriate level of detail and to the invalidation of wrong proofs, and we\ndiscuss resilience against various attacks. We then analyze a simplified model,\ncharacterize its equilibria and compute the agents' level of trust.\n</p>\n<p>SPRIG is designed to run as a smart contract on a blockchain platform. This\nallows anonymous agents to participate in the verification debate, and to\ncontribute with their information. The smart contract mediates the\ninteractions, settles debates, and guarantees that bounties and stakes are paid\nas specified.\n</p>\n<p>SPRIG enables new applications, such as the issuance of bounties for open\nproblems, and the creation of derivatives markets, allowing agents to inject\nmore information pertaining to proofs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carre_S/0/1/0/all/0/1\">Sylvain Carr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabriel_F/0/1/0/all/0/1\">Franck Gabriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongler_C/0/1/0/all/0/1\">Cl&#xe9;ment Hongler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacerda_G/0/1/0/all/0/1\">Gustavo Lacerda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capano_G/0/1/0/all/0/1\">Gloria Capano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances in Multi-turn Dialogue Comprehension: A Survey. (arXiv:2103.03125v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.03125","description":"<p>Training machines to understand natural language and interact with humans is\nan elusive and essential task of artificial intelligence. A diversity of\ndialogue systems has been designed with the rapid development of deep learning\ntechniques, especially the recent pre-trained language models (PrLMs). Among\nthese studies, the fundamental yet challenging type of task is dialogue\ncomprehension whose role is to teach the machines to read and comprehend the\ndialogue context before responding. In this paper, we review the previous\nmethods from the technical perspective of dialogue modeling for the dialogue\ncomprehension task. We summarize the characteristics and challenges of dialogue\ncomprehension in contrast to plain-text reading comprehension. Then, we discuss\nthree typical patterns of dialogue modeling. In addition, we categorize\ndialogue-related pre-training techniques which are employed to enhance PrLMs in\ndialogue scenarios. Finally, we highlight the technical advances in recent\nyears and point out the lessons from the empirical analysis and the prospects\ntowards a new frontier of researches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema. (arXiv:2104.08161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08161","description":"<p>The Winograd Schema (WS) has been proposed as a test for measuring\ncommonsense capabilities of models. Recently, pre-trained language model-based\napproaches have boosted performance on some WS benchmarks but the source of\nimprovement is still not clear. This paper suggests that the apparent progress\non WS may not necessarily reflect progress in commonsense reasoning. To support\nthis claim, we first show that the current evaluation method of WS is\nsub-optimal and propose a modification that uses twin sentences for evaluation.\nWe also propose two new baselines that indicate the existence of artifacts in\nWS benchmarks. We then develop a method for evaluating WS-like sentences in a\nzero-shot setting to account for the commonsense reasoning abilities acquired\nduring the pretraining and observe that popular language models perform\nrandomly in this setting when using our more strict evaluation. We conclude\nthat the observed progress is mostly due to the use of supervision in training\nWS models, which is not likely to successfully support all the required\ncommonsense reasoning skills and knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGG: Learning to Select, Guide, and Generate for Keyphrase Generation. (arXiv:2105.02544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.02544","description":"<p>Keyphrases, that concisely summarize the high-level topics discussed in a\ndocument, can be categorized into present keyphrase which explicitly appears in\nthe source text, and absent keyphrase which does not match any contiguous\nsubsequence but is highly semantically related to the source. Most existing\nkeyphrase generation approaches synchronously generate present and absent\nkeyphrases without explicitly distinguishing these two categories. In this\npaper, a Select-Guide-Generate (SGG) approach is proposed to deal with present\nand absent keyphrase generation separately with different mechanisms.\nSpecifically, SGG is a hierarchical neural network which consists of a\npointing-based selector at low layer concentrated on present keyphrase\ngeneration, a selection-guided generator at high layer dedicated to absent\nkeyphrase generation, and a guider in the middle to transfer information from\nselector to generator. Experimental results on four keyphrase generation\nbenchmarks demonstrate the effectiveness of our model, which significantly\noutperforms the strong baselines for both present and absent keyphrases\ngeneration. Furthermore, we extend SGG to a title generation task which\nindicates its extensibility in natural language generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05541","description":"<p>Chatbots are intelligent software built to be used as a replacement for human\ninteraction. Existing studies typically do not provide enough support for\nlow-resource languages like Bangla. Due to the increasing popularity of social\nmedia, we can also see the rise of interactions in Bangla transliteration\n(mostly in English) among the native Bangla speakers. In this paper, we propose\na novel approach to build a Bangla chatbot aimed to be used as a business\nassistant which can communicate in low-resource languages like Bangla and\nBangla Transliteration in English with high confidence consistently. Since\nannotated data was not available for this purpose, we had to work on the whole\nmachine learning life cycle (data preparation, machine learning modeling, and\nmodel deployment) using Rasa Open Source Framework, fastText embeddings,\nPolyglot embeddings, Flask, and other systems as building blocks. While working\nwith the skewed annotated dataset, we try out different components and\npipelines to evaluate which works best and provide possible reasoning behind\nthe observed results. Finally, we present a pipeline for intent classification\nand entity extraction which achieves reasonable performance (accuracy: 83.02%,\nprecision: 80.82%, recall: 83.02%, F1-score: 80%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Answer Similarity for Evaluating Question Answering Models. (arXiv:2108.06130v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06130","description":"<p>The evaluation of question answering models compares ground-truth annotations\nwith model predictions. However, as of today, this comparison is mostly\nlexical-based and therefore misses out on answers that have no lexical overlap\nbut are still semantically similar, thus treating correct answers as false.\nThis underestimation of the true performance of models hinders user acceptance\nin applications and complicates a fair comparison of different models.\nTherefore, there is a need for an evaluation metric that is based on semantics\ninstead of pure string similarity. In this short paper, we present SAS, a\ncross-encoder-based metric for the estimation of semantic answer similarity,\nand compare it to seven existing metrics. To this end, we create an English and\na German three-way annotated evaluation dataset containing pairs of answers\nalong with human judgment of their semantic similarity, which we release along\nwith an implementation of the SAS metric and the experiments. We find that\nsemantic similarity metrics based on recent transformer models correlate much\nbetter with human judgment than traditional lexical similarity metrics on our\ntwo newly created datasets and one dataset from related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutsch_J/0/1/0/all/0/1\">Julian Gutsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietsch_M/0/1/0/all/0/1\">Malte Pietsch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06314","description":"<p>Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and align them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes challenges in the aspect of both temporal understanding and temporal\nreasoning. We evaluate different SoTA long-document QA systems like BigBird and\nFiD on our dataset. The best-performing model FiD can only achieve 46\\%\naccuracy, still far behind the human performance of 87\\%. We demonstrate that\nthese models are still lacking the ability to perform consistent temporal\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\ndevelop NLP models more sensitive to temporal shift. The dataset and code are\nreleased in~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-Based Self-Critical Training For Question Generation. (arXiv:2108.12026v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12026","description":"<p>Question generation is a conditioned language generation task that consists\nin generating a context-aware question given a context and the targeted answer.\nTrain language modelling with a mere likelihood maximization has been widely\nused while suffering from exposure bias and the discordance between the\ntraining and the test metrics. In the way of addressing this issue, The\npresented work portrays a fully Transformer-based reinforcement learning\ngenerator-evaluation architecture for neural question generation. To edge the\nflexibility of the generation, a semantic-based reward score was externally\ninfused during the training to drive the training of the language model. The\nglobal architecture is laid out in a generator-evaluator fashion optimized\ndirectly to n-gram and semantic-based metrics. Evaluation metrics for language\nmodelling only based on n-gram overlapping do not consider semantic relations\nbetween reference and candidate sequences. To improve the evaluation step, a\ntwo-fold evaluation was carried out. On the one side, an n-gram overlapping\nevaluation using the BLEU score. On the other side, a semantic-based assessment\nusing BERTScore and NUBIA. The results were corroborated by a binary human\nevaluation of the semantic relatedness of the generated question and the ground\ntruth. The results obtained showed that use a semantic-based REINFORCE\nalgorithm for the question generation syntactically reshapes the generated\nquestions while preserving their underlying semantic meaning. Many downstream\napplications can be drawn from a successful question generation including the\nenlargement of question answering datasets, the improvement of conversational\nsystems, the enhancement of autonomous educational assessment systems, and so\nforth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo%5C%22ic/0/1/0/all/0/1\">Lo&#xef;c</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dassi_K/0/1/0/all/0/1\">Kwate Dassi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03137","description":"<p>Existing generative pre-trained language models (e.g., GPT) focus on modeling\nthe language structure and semantics of general texts. However, those models do\nnot consider the numerical properties of numbers and cannot perform robustly on\nnumerical reasoning tasks (e.g., math word problems and measurement\nestimation). In this paper, we propose NumGPT, a generative pre-trained model\nthat explicitly models the numerical properties of numbers in texts.\nSpecifically, it leverages a prototype-based numeral embedding to encode the\nmantissa of the number and an individual embedding to encode the exponent of\nthe number. A numeral-aware loss function is designed to integrate numerals\ninto the pre-training objective of NumGPT. We conduct extensive experiments on\nfour different datasets to evaluate the numeracy ability of NumGPT. The\nexperiment results show that NumGPT outperforms baseline models (e.g., GPT and\nGPT with DICE) on a range of numerical reasoning tasks such as measurement\nestimation, number comparison, math word problems, and magnitude\nclassification. Ablation studies are also conducted to evaluate the impact of\npre-training and model hyperparameters on the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does External Knowledge Help Explainable Natural Language Inference? Automatic Evaluation vs. Human Ratings. (arXiv:2109.07833v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07833","description":"<p>Natural language inference (NLI) requires models to learn and apply\ncommonsense knowledge. These reasoning abilities are particularly important for\nexplainable NLI systems that generate a natural language explanation in\naddition to their label prediction. The integration of external knowledge has\nbeen shown to improve NLI systems, here we investigate whether it can also\nimprove their explanation capabilities. For this, we investigate different\nsources of external knowledge and evaluate the performance of our models on\nin-domain data as well as on special transfer datasets that are designed to\nassess fine-grained reasoning capabilities. We find that different sources of\nknowledge have a different effect on reasoning abilities, for example, implicit\nknowledge stored in language models can hinder reasoning on numbers and\nnegations. Finally, we conduct the largest and most fine-grained explainable\nNLI crowdsourcing study to date. It reveals that even large differences in\nautomatic performance scores do neither reflect in human ratings of label,\nexplanation, commonsense nor grammar correctness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schuff_H/0/1/0/all/0/1\">Hendrik Schuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsiu-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10255","description":"<p>The recognition of hate speech and offensive language (HOF) is commonly\nformulated as a classification task to decide if a text contains HOF. We\ninvestigate whether HOF detection can profit by taking into account the\nrelationships between HOF and similar concepts: (a) HOF is related to sentiment\nanalysis because hate speech is typically a negative statement and expresses a\nnegative opinion; (b) it is related to emotion analysis, as expressed hate\npoints to the author experiencing (or pretending to experience) anger while the\naddressees experience (or are intended to experience) fear. (c) Finally, one\nconstituting element of HOF is the mention of a targeted person or group. On\nthis basis, we hypothesize that HOF detection shows improvements when being\nmodeled jointly with these concepts, in a multi-task learning setup. We base\nour experiments on existing data sets for each of these concepts (sentiment,\nemotion, target of HOF) and evaluate our models as a participant (as team\nIMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection\nexperiments in which we consider multiple available resources and submissions\nto the shared task, we find that the combination of the CrowdFlower emotion\ncorpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target\ndetection data leads to an F1 =.79 in a multi-head multi-task learning model\nbased on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test\ndata, this result is more substantial with an increase by 2pp in F1 and a\nconsiderable increase in recall. Across both data sets (2019, 2021), the recall\nis particularly increased for the class of HOF (6pp for the 2019 data and 3pp\nfor the 2021 data), showing that MTL with emotion, sentiment, and target\nidentification is an appropriate approach for early warning systems that might\nbe deployed in social media platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plaza_del_Arco_F/0/1/0/all/0/1\">Flor Miriam Plaza-del-Arco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halat_S/0/1/0/all/0/1\">Sercan Halat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v3 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2109.13662","description":"<p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- Hinge Loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. We evaluate DeepPSL on a zero shot learning problem in\nimage classification. State of the art results demonstrate the utility and\nflexibility of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel P. Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puranam_S/0/1/0/all/0/1\">Sai Akhil Puranam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dasaratha_S/0/1/0/all/0/1\">Sridhar Dasaratha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phogat_K/0/1/0/all/0/1\">Karmvir Singh Phogat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiyyagura_S/0/1/0/all/0/1\">Sunil Reddy Tiyyagura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.00976","description":"<p>Law, interpretations of law, legal arguments, agreements, etc. are typically\nexpressed in writing, leading to the production of vast corpora of legal text.\nTheir analysis, which is at the center of legal practice, becomes increasingly\nelaborate as these collections grow in size. Natural language understanding\n(NLU) technologies can be a valuable tool to support legal practitioners in\nthese endeavors. Their usefulness, however, largely depends on whether current\nstate-of-the-art models can generalize across various tasks in the legal\ndomain. To answer this currently open question, we introduce the Legal General\nLanguage Understanding Evaluation (LexGLUE) benchmark, a collection of datasets\nfor evaluating model performance across a diverse set of legal NLU tasks in a\nstandardized way. We also provide an evaluation and analysis of several generic\nand legal-oriented models demonstrating that the latter consistently offer\nperformance improvements across multiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1\">Abhik Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_D/0/1/0/all/0/1\">Dirk Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael Bommarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QTN-VQC: An End-to-End Learning framework for Quantum Neural Networks. (arXiv:2110.03861v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2110.03861","description":"<p>The advent of noisy intermediate-scale quantum (NISQ) computers raises a\ncrucial challenge to design quantum neural networks for fully quantum learning\ntasks. To bridge the gap, this work proposes an end-to-end learning framework\nnamed QTN-VQC, by introducing a trainable quantum tensor network (QTN) for\nquantum embedding on a variational quantum circuit (VQC). The architecture of\nQTN is composed of a parametric tensor-train network for feature extraction and\na tensor product encoding for quantum encoding. We highlight the QTN for\nquantum embedding in terms of two perspectives: (1) we theoretically\ncharacterize QTN by analyzing its representation power of input features; (2)\nQTN enables an end-to-end parametric model pipeline, namely QTN-VQC, from the\ngeneration of quantum embedding to the output measurement. Our experiments on\nthe MNIST dataset demonstrate the advantages of QTN for quantum embedding over\nother quantum embedding approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Qi_J/0/1/0/all/0/1\">Jun Qi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. (arXiv:2110.03888v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03888","description":"<p>Recent expeditious developments in deep learning algorithms, distributed\ntraining, and even hardware design for large models have enabled training\nextreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of\nbillions or even trillions of parameters. However, under limited resources,\nextreme-scale model training that requires enormous amounts of computes and\nmemory footprint suffers from frustratingly low efficiency in model\nconvergence. In this paper, we propose a simple training strategy called\n\"Pseudo-to-Real\" for high-memory-footprint-required large models.\nPseudo-to-Real is compatible with large models with architecture of sequential\nlayers. We demonstrate a practice of pretraining unprecedented\n10-trillion-parameter model, an order of magnitude larger than the\nstate-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the\napplication of Pseudo-to-Real, we also provide a technique, Granular CPU\noffloading, to manage CPU memory for training large model and maintain high GPU\nutilities. Fast training of extreme-scale models on a decent amount of\nresources can bring much smaller carbon footprint and contribute to greener AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinze Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HydraSum -- Disentangling Stylistic Features in Text Summarization using Multi-Decoder Models. (arXiv:2110.04400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04400","description":"<p>Existing abstractive summarization models lack explicit control mechanisms\nthat would allow users to influence the stylistic features of the model\noutputs. This results in generating generic summaries that do not cater to the\nusers needs or preferences. To address this issue we introduce HydraSum, a new\nsummarization architecture that extends the single decoder framework of current\nmodels, e.g. BART, to a mixture-of-experts version consisting of multiple\ndecoders. Our proposed model encourages each expert, i.e. decoder, to learn and\ngenerate stylistically-distinct summaries along dimensions such as\nabstractiveness, length, specificity, and others. At each time step, HydraSum\nemploys a gating mechanism that decides the contribution of each individual\ndecoder to the next token's output probability distribution. Through\nexperiments on three summarization datasets (CNN, Newsroom, XSum), we\ndemonstrate that this gating mechanism automatically learns to assign\ncontrasting summary styles to different HydraSum decoders under the standard\ntraining objective without the need for additional supervision. We further show\nthat a guided version of the training process can explicitly govern which\nsummary style is partitioned between decoders, e.g. high abstractiveness vs.\nlow abstractiveness or high specificity vs. low specificity, and also increase\nthe stylistic-difference between individual decoders. Finally, our experiments\ndemonstrate that our decoder framework is highly flexible: during inference, we\ncan sample from individual decoders or mixtures of different subsets of the\ndecoders to yield a diverse set of summaries and enforce single- and\nmulti-style control over summary generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_T/0/1/0/all/0/1\">Tanya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SEPP: Similarity Estimation of Predicted Probabilities for Defending and Detecting Adversarial Text. (arXiv:2110.05748v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05748","description":"<p>There are two cases describing how a classifier processes input text, namely,\nmisclassification and correct classification. In terms of misclassified texts,\na classifier handles the texts with both incorrect predictions and adversarial\ntexts, which are generated to fool the classifier, which is called a victim.\nBoth types are misunderstood by the victim, but they can still be recognized by\nother classifiers. This induces large gaps in predicted probabilities between\nthe victim and the other classifiers. In contrast, text correctly classified by\nthe victim is often successfully predicted by the others and induces small\ngaps. In this paper, we propose an ensemble model based on similarity\nestimation of predicted probabilities (SEPP) to exploit the large gaps in the\nmisclassified predictions in contrast to small gaps in the correct\nclassification. SEPP then corrects the incorrect predictions of the\nmisclassified texts. We demonstrate the resilience of SEPP in defending and\ndetecting adversarial texts through different types of victim classifiers,\nclassification tasks, and adversarial attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Son_H/0/1/0/all/0/1\">Hoang-Quoc Nguyen-Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidano_S/0/1/0/all/0/1\">Seira Hidano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukushima_K/0/1/0/all/0/1\">Kazuhide Fukushima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyomoto_S/0/1/0/all/0/1\">Shinsaku Kiyomoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaoPLM: Pre-trained Language Models for Lao. (arXiv:2110.05896v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05896","description":"<p>Trained on the large corpus, pre-trained language models (PLMs) can capture\ndifferent levels of concepts in context and hence generate universal language\nrepresentations. They can benefit multiple downstream natural language\nprocessing (NLP) tasks. Although PTMs have been widely used in most NLP\napplications, especially for high-resource languages such as English, it is\nunder-represented in Lao NLP research. Previous work on Lao has been hampered\nby the lack of annotated datasets and the sparsity of language resources. In\nthis work, we construct a text classification dataset to alleviate the\nresource-scare situation of the Lao language. We additionally present the first\ntransformer-based PTMs for Lao with four versions: BERT-small, BERT-base,\nELECTRA-small and ELECTRA-base, and evaluate it over two downstream tasks:\npart-of-speech tagging and text classification. Experiments demonstrate the\neffectiveness of our Lao models. We will release our models and datasets to the\ncommunity, hoping to facilitate the future development of Lao NLP applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1\">Nankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shengyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}