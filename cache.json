{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-12T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Post-hoc Interpretability for Neural NLP: A Survey. (arXiv:2108.04840v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04840","description":"<p>Natural Language Processing (NLP) models have become increasingly more\ncomplex and widespread. With recent developments in neural networks, a growing\nconcern is whether it is responsible to use these models. Concerns such as\nsafety and ethics can be partially addressed by providing explanations.\nFurthermore, when models do fail, providing explanations is paramount for\naccountability purposes. To this end, interpretability serves to provide these\nexplanations in terms that are understandable to humans. Central to what is\nunderstandable is how explanations are communicated. Therefore, this survey\nprovides a categorization of how recent interpretability methods communicate\nexplanations and discusses the methods in depth. Furthermore, the survey\nfocuses on post-hoc methods, which provide explanations after a model is\nlearned and generally model-agnostic. A common concern for this class of\nmethods is whether they accurately reflect the model. Hence, how these post-hoc\nmethods are evaluated is discussed throughout the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1\">Andreas Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Sarath Chandar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion. (arXiv:2108.04927v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04927","description":"<p>Language-guided robots performing home and office tasks must navigate in and\ninteract with the world. Grounding language instructions against visual\nobservations and actions to take in an environment is an open challenge. We\npresent Embodied BERT (EmBERT), a transformer-based model which can attend to\nhigh-dimensional, multi-modal inputs across long temporal horizons for\nlanguage-conditioned task completion. Additionally, we bridge the gap between\nsuccessful object-centric navigation models used for non-interactive agents and\nthe language-guided visual task completion benchmark, ALFRED, by introducing\nobject navigation targets for EmBERT training. We achieve competitive\nperformance on the ALFRED benchmark, and EmBERT marks the first\ntransformer-based model to successfully handle the long-horizon, dense,\nmulti-modal histories of ALFRED, and the first ALFRED model to utilize\nobject-centric navigation targets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1\">Alessandro Suglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhatme_G/0/1/0/all/0/1\">Gaurav Sukhatme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis. (arXiv:2108.04938v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04938","description":"<p>Vision-and-language(V&amp;L) models take image and text as input and learn to\ncapture the associations between them. Prior studies show that pre-trained V&amp;L\nmodels can significantly improve the model performance for downstream tasks\nsuch as Visual Question Answering (VQA). However, V&amp;L models are less effective\nwhen applied in the medical domain (e.g., on X-ray images and clinical notes)\ndue to the domain gap. In this paper, we investigate the challenges of applying\npre-trained V&amp;L models in medical applications. In particular, we identify that\nthe visual representation in general V&amp;L models is not suitable for processing\nmedical data. To overcome this limitation, we propose BERTHop, a\ntransformer-based model based on PixelHop++ and VisualBERT, for better\ncapturing the associations between the two modalities. Experiments on the OpenI\ndataset, a commonly used thoracic disease diagnosis benchmark, show that\nBERTHop achieves an average Area Under the Curve (AUC) of 98.12% which is 1.62%\nhigher than state-of-the-art (SOTA) while it is trained on a 9 times smaller\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouhsedaghat_M/0/1/0/all/0/1\">Mozhdeh Rouhsedaghat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_A/0/1/0/all/0/1\">Aichi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scalzo_F/0/1/0/all/0/1\">Fabien Scalzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Social and Behavioral Determinants of Health in Lung Cancer Patients Using Transformers-based Natural Language Processing Models. (arXiv:2108.04949v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04949","description":"<p>Social and behavioral determinants of health (SBDoH) have important roles in\nshaping people's health. In clinical research studies, especially comparative\neffectiveness studies, failure to adjust for SBDoH factors will potentially\ncause confounding issues and misclassification errors in either statistical\nanalyses and machine learning-based models. However, there are limited studies\nto examine SBDoH factors in clinical outcomes due to the lack of structured\nSBDoH information in current electronic health record (EHR) systems, while much\nof the SBDoH information is documented in clinical narratives. Natural language\nprocessing (NLP) is thus the key technology to extract such information from\nunstructured clinical text. However, there is not a mature clinical NLP system\nfocusing on SBDoH. In this study, we examined two state-of-the-art\ntransformer-based NLP models, including BERT and RoBERTa, to extract SBDoH\nconcepts from clinical narratives, applied the best performing model to extract\nSBDoH concepts on a lung cancer screening patient cohort, and examined the\ndifference of SBDoH information between NLP extracted results and structured\nEHRs (SBDoH information captured in standard vocabularies such as the\nInternational Classification of Diseases codes). The experimental results show\nthat the BERT-based NLP model achieved the best strict/lenient F1-score of\n0.8791 and 0.8999, respectively. The comparison between NLP extracted SBDoH\ninformation and structured EHRs in the lung cancer patient cohort of 864\npatients with 161,933 various types of clinical notes showed that much more\ndetailed information about smoking, education, and employment were only\ncaptured in clinical narratives and that it is necessary to use both clinical\nnarratives and structured EHRs to construct a more complete picture of\npatients' SBDoH factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zehao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_C/0/1/0/all/0/1\">Chong Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Songzi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adekkanattu_P/0/1/0/all/0/1\">Prakash Adekkanattu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_J/0/1/0/all/0/1\">Jyotishman Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1\">Thomas J. George</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogan_W/0/1/0/all/0/1\">William R. Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jiang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing. (arXiv:2108.04990v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04990","description":"<p>Interpretability methods like Integrated Gradient and LIME are popular\nchoices for explaining natural language model predictions with relative word\nimportance scores. These interpretations need to be robust for trustworthy NLP\napplications in high-stake areas like medicine or finance. Our paper\ndemonstrates how interpretations can be manipulated by making simple word\nperturbations on an input text. Via a small portion of word-level swaps, these\nadversarial perturbations aim to make the resulting text semantically and\nspatially similar to its seed input (therefore sharing similar\ninterpretations). Simultaneously, the generated examples achieve the same\nprediction label as the seed yet are given a substantially different\nexplanation by the interpretation methods. Our experiments generate fragile\ninterpretations to attack two SOTA interpretation methods, across three popular\nTransformer models and on two different NLP datasets. We observe that the rank\norder correlation drops by over 20% when less than 10% of words are perturbed\non average. Further, rank-order correlation keeps decreasing as more words get\nperturbed. Furthermore, we demonstrate that candidates generated from our\nmethod have good quality metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Sanchit Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekhon_A/0/1/0/all/0/1\">Arshdeep Sekhon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transformer-based Math Language Model for Handwritten Math Expression Recognition. (arXiv:2108.05002v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05002","description":"<p>Handwritten mathematical expressions (HMEs) contain ambiguities in their\ninterpretations, even for humans sometimes. Several math symbols are very\nsimilar in the writing style, such as dot and comma or 0, O, and o, which is a\nchallenge for HME recognition systems to handle without using contextual\ninformation. To address this problem, this paper presents a Transformer-based\nMath Language Model (TMLM). Based on the self-attention mechanism, the\nhigh-level representation of an input token in a sequence of tokens is computed\nby how it is related to the previous tokens. Thus, TMLM can capture long\ndependencies and correlations among symbols and relations in a mathematical\nexpression (ME). We trained the proposed language model using a corpus of\napproximately 70,000 LaTeX sequences provided in CROHME 2016. TMLM achieved the\nperplexity of 4.42, which outperformed the previous math language models, i.e.,\nthe N-gram and recurrent neural network-based language models. In addition, we\ncombine TMLM into a stochastic context-free grammar-based HME recognition\nsystem using a weighting parameter to re-rank the top-10 best candidates. The\nexpression rates on the testing sets of CROHME 2016 and CROHME 2019 were\nimproved by 2.97 and 0.83 percentage points, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ung_H/0/1/0/all/0/1\">Huy Quang Ung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cuong Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Nghia Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEMix Layers: Disentangling Domains for Modular Language Modeling. (arXiv:2108.05036v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05036","description":"<p>We introduce a new domain expert mixture (DEMix) layer that enables\nconditioning a language model (LM) on the domain of the input text. A DEMix\nlayer is a collection of expert feedforward networks, each specialized to a\ndomain, that makes the LM modular: experts can be mixed, added or removed after\ninitial training. Extensive experiments with autoregressive transformer LMs (up\nto 1.3B parameters) show that DEMix layers reduce test-time perplexity,\nincrease training efficiency, and enable rapid adaptation with little overhead.\nWe show that mixing experts during inference, using a parameter-free weighted\nensemble, allows the model to better generalize to heterogeneous or unseen\ndomains. We also show that experts can be added to iteratively incorporate new\ndomains without forgetting older ones, and that experts can be removed to\nrestrict access to unwanted domains, without additional training. Overall,\nthese results demonstrate benefits of explicitly conditioning on textual\ndomains during language modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning. (arXiv:2108.05067v1 [cs.CV])","link":"http://arxiv.org/abs/2108.05067","description":"<p>Medical imaging technologies, including computed tomography (CT) or chest\nX-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19.\nSince manual report writing is usually too time-consuming, a more intelligent\nauxiliary medical system that could generate medical reports automatically and\nimmediately is urgently needed. In this article, we propose to use the medical\nvisual language BERT (Medical-VLBERT) model to identify the abnormality on the\nCOVID-19 scans and generate the medical report automatically based on the\ndetected lesion regions. To produce more accurate medical reports and minimize\nthe visual-and-linguistic differences, this model adopts an alternate learning\nstrategy with two procedures that are knowledge pretraining and transferring.\nTo be more precise, the knowledge pretraining procedure is to memorize the\nknowledge from medical texts, while the transferring procedure is to utilize\nthe acquired knowledge for professional medical sentences generations through\nobservations of medical images. In practice, for automatic medical report\ngeneration on the COVID-19 cases, we constructed a dataset of 368 medical\nfindings in Chinese and 1104 chest CT scans from The First Affiliated Hospital\nof Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun\nYat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of\nthe COVID-19 training samples, our model was first trained on the large-scale\nChinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for\nfurther fine-tuning. The experimental results showed that Medical-VLBERT\nachieved state-of-the-art performances on terminology prediction and report\ngeneration with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The\nChinese COVID-19 CT dataset is available at https://covid19ct.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fuyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuixing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PGCD: a position-guied contributive distribution unit for aspect based sentiment analysis. (arXiv:2108.05098v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05098","description":"<p>Aspect based sentiment analysis (ABSA), exploring sentim- ent polarity of\naspect-given sentence, has drawn widespread applications in social media and\npublic opinion. Previously researches typically derive aspect-independent\nrepresentation by sentence feature generation only depending on text data. In\nthis paper, we propose a Position-Guided Contributive Distribution (PGCD) unit.\nIt achieves a position-dependent contributive pattern and generates\naspect-related statement feature for ABSA task. Quoted from Shapley Value, PGCD\ncan gain position-guided contextual contribution and enhance the aspect-based\nrepresentation. Furthermore, the unit can be used for improving effects on\nmultimodal ABSA task, whose datasets restructured by ourselves. Extensive\nexperiments on both text and text-audio level using dataset (SemEval) show that\nby applying the proposed unit, the mainstream models advance performance in\naccuracy and F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangfeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeliData: A dataset for deliberation in multi-party problem solving. (arXiv:2108.05271v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05271","description":"<p>Dialogue systems research is traditionally focused on dialogues between two\ninterlocutors, largely ignoring group conversations. Moreover, most previous\nresearch is focused either on task-oriented dialogue (e.g.\\ restaurant\nbookings) or user engagement (chatbots), while research on systems for\ncollaborative dialogues is an under-explored area. To this end, we introduce\nthe first publicly available dataset containing collaborative conversations on\nsolving a cognitive task, consisting of 500 group dialogues and 14k utterances.\nFurthermore, we propose a novel annotation schema that captures deliberation\ncues and release 50 dialogues annotated with it. Finally, we demonstrate the\nusefulness of the annotated data in training classifiers to predict the\nconstructiveness of a conversation. The data collection platform, dataset and\nannotated corpus are publicly available at https://delibot.xyz\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karadzhov_G/0/1/0/all/0/1\">Georgi Karadzhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stafford_T/0/1/0/all/0/1\">Tom Stafford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Icelandic Parallel Abstracts Corpus. (arXiv:2108.05289v1 [cs.CL])","link":"http://arxiv.org/abs/2108.05289","description":"<p>We present a new Icelandic-English parallel corpus, the Icelandic Parallel\nAbstracts Corpus (IPAC), composed of abstracts from student theses and\ndissertations. The texts were collected from the Skemman repository which keeps\nrecords of all theses, dissertations and final projects from students at\nIcelandic universities. The corpus was aligned based on sentence-level BLEU\nscores, in both translation directions, from NMT models using Bleualign. The\nresult is a corpus of 64k sentence pairs from over 6 thousand parallel\nabstracts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simonarson_H/0/1/0/all/0/1\">Haukur Barri S&#xed;monarson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1\">V&#xe9;steinn Sn&#xe6;bjarnarson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Compression for Domain Adaptation through Causal Effect Estimation. (arXiv:2101.07086v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.07086","description":"<p>Recent improvements in the predictive quality of natural language processing\nsystems are often dependent on a substantial increase in the number of model\nparameters. This has led to various attempts of compressing such models, but\nexisting methods have not considered the differences in the predictive power of\nvarious model components or in the generalizability of the compressed models.\nTo understand the connection between model compression and out-of-distribution\ngeneralization, we define the task of compressing language representation\nmodels such that they perform best in a domain adaptation setting. We choose to\naddress this problem from a causal perspective, attempting to estimate the\naverage treatment effect (ATE) of a model component, such as a single layer, on\nthe model's predictions. Our proposed ATE-guided Model Compression scheme\n(AMoC), generates many model candidates, differing by the model components that\nwere removed. Then, we select the best candidate through a stepwise regression\nmodel that utilizes the ATE to predict the expected performance on the target\ndomain. AMoC outperforms strong baselines on dozens of domain pairs across\nthree text classification and sequence tagging tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rotman_G/0/1/0/all/0/1\">Guy Rotman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Models: Past, Present and Future. (arXiv:2106.07139v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2106.07139","description":"<p>Large-scale pre-trained models (PTMs) such as BERT and GPT have recently\nachieved great success and become a milestone in the field of artificial\nintelligence (AI). Owing to sophisticated pre-training objectives and huge\nmodel parameters, large-scale PTMs can effectively capture knowledge from\nmassive labeled and unlabeled data. By storing knowledge into huge parameters\nand fine-tuning on specific tasks, the rich knowledge implicitly encoded in\nhuge parameters can benefit a variety of downstream tasks, which has been\nextensively demonstrated via experimental verification and empirical analysis.\nIt is now the consensus of the AI community to adopt PTMs as backbone for\ndownstream tasks rather than learning models from scratch. In this paper, we\ntake a deep look into the history of pre-training, especially its special\nrelation with transfer learning and self-supervised learning, to reveal the\ncrucial position of PTMs in the AI development spectrum. Further, we\ncomprehensively review the latest breakthroughs of PTMs. These breakthroughs\nare driven by the surge of computational power and the increasing availability\nof data, towards four important directions: designing effective architectures,\nutilizing rich contexts, improving computational efficiency, and conducting\ninterpretation and theoretical analysis. Finally, we discuss a series of open\nproblems and research directions of PTMs, and hope our view can inspire and\nadvance the future study of PTMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jiezhong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wentao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jinhui Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-Oriented Script Construction. (arXiv:2107.13189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.13189","description":"<p>The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-11T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}