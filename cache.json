{"site_title":"ArxivDaily","build_time":"2021-08-10T03:18:13.988261121Z","days":[{"date":"2021-08-10","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Facebook AI WMT21 News Translation Task Submission. (arXiv:2108.03265v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03265","description":"<p>We describe Facebook's multilingual model submission to the WMT2021 shared\ntask on news translation. We participate in 14 language directions: English to\nand from Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese. To\ndevelop systems covering all these directions, we focus on multilingual models.\nWe utilize data from all available sources --- WMT, large-scale data mining,\nand in-domain backtranslation --- to create high quality bilingual and\nmultilingual baselines. Subsequently, we investigate strategies for scaling\nmultilingual model size, such that one system has sufficient capacity for high\nquality representations of all eight languages. Our final submission is an\nensemble of dense and sparse Mixture-of-Expert multilingual translation models,\nfollowed by finetuning on in-domain news data and noisy channel reranking.\nCompared to previous year's winning submissions, our multilingual system\nimproved the translation quality on all language directions, with an average\nimprovement of 2.0 BLEU. In the WMT2021 task, our system ranks first in 10\ndirections based on automatic evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Chau Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edunov_S/0/1/0/all/0/1\">Sergey Edunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offensive Language and Hate Speech Detection with Deep Learning and Transfer Learning. (arXiv:2108.03305v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03305","description":"<p>Toxic online speech has become a crucial problem nowadays due to an\nexponential increase in the use of internet by people from different cultures\nand educational backgrounds. Differentiating if a text message belongs to hate\nspeech and offensive language is a key challenge in automatic detection of\ntoxic text content. In this paper, we propose an approach to automatically\nclassify tweets into three classes: Hate, offensive and Neither. Using public\ntweet data set, we first perform experiments to build BI-LSTM models from empty\nembedding and then we also try the same neural network architecture with\npre-trained Glove embedding. Next, we introduce a transfer learning approach\nfor hate speech detection using an existing pre-trained language model BERT\n(Bidirectional Encoder Representations from Transformers), DistilBert\n(Distilled version of BERT) and GPT-2 (Generative Pre-Training). We perform\nhyper parameters tuning analysis of our best model (BI-LSTM) considering\ndifferent neural network architectures, learn-ratings and normalization methods\netc. After tuning the model and with the best combination of parameters, we\nachieve over 92 percent accuracy upon evaluating it on test data. We also\ncreate a class module which contains main functionality including text\nclassification, sentiment checking and text data augmentation. This model could\nserve as an intermediate module between user and Twitter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Bencheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jason Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Ajay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umair_H/0/1/0/all/0/1\">Hafiza Umair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vovor_A/0/1/0/all/0/1\">Atsu Vovor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durzynski_N/0/1/0/all/0/1\">Natalie Durzynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-shot Language Modeling. (arXiv:2108.03334v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03334","description":"<p>Can we construct a neural model that is inductively biased towards learning\nhuman languages? Motivated by this question, we aim at constructing an\ninformative prior over neural weights, in order to adapt quickly to held-out\nlanguages in the task of character-level language modeling. We infer this\ndistribution from a sample of typologically diverse training languages via\nLaplace approximation. The use of such a prior outperforms baseline models with\nan uninformative prior (so-called \"fine-tuning\") in both zero-shot and few-shot\nsettings. This shows that the prior is imbued with universal phonological\nknowledge. Moreover, we harness additional language-specific side information\nas distant supervision for held-out languages. Specifically, we condition\nlanguage models on features from typological databases, by concatenating them\nto hidden states or generating weights with hyper-networks. These features\nappear beneficial in the few-shot setting, but not in the zero-shot setting.\nSince the paucity of digital texts affects the majority of the world's\nlanguages, we hope that these findings will help broaden the scope of\napplications for language technology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiny Neural Models for Seq2Seq. (arXiv:2108.03340v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03340","description":"<p>Semantic parsing models with applications in task oriented dialog systems\nrequire efficient sequence to sequence (seq2seq) architectures to be run\non-device. To this end, we propose a projection based encoder-decoder model\nreferred to as pQRNN-MAtt. Studies based on projection methods were restricted\nto encoder-only models, and we believe this is the first study extending it to\nseq2seq architectures. The resulting quantized models are less than 3.5MB in\nsize and are well suited for on-device latency critical applications. We show\nthat on MTOP, a challenging multilingual semantic parsing dataset, the average\nmodel performance surpasses LSTM based seq2seq model that uses pre-trained\nembeddings despite being 85x smaller. Furthermore, the model can be an\neffective student for distilling large pre-trained models such as T5/BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kandoor_A/0/1/0/all/0/1\">Arun Kandoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What do Bias Measures Measure?. (arXiv:2108.03362v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03362","description":"<p>Natural Language Processing (NLP) models propagate social biases about\nprotected attributes such as gender, race, and nationality. To create\ninterventions and mitigate these biases and associated harms, it is vital to be\nable to detect and measure such biases. While many existing works propose bias\nevaluation methodologies for different tasks, there remains a need to\ncohesively understand what biases and normative harms each of these measures\ncaptures and how different measures compare. To address this gap, this work\npresents a comprehensive survey of existing bias measures in NLP as a function\nof the associated NLP tasks, metrics, datasets, and social biases and\ncorresponding harms. This survey also organizes metrics into different\ncategories to present advantages and disadvantages. Finally, we propose a\ndocumentation standard for bias measures to aid their development,\ncategorization, and appropriate usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1\">Emily Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jieyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanseverino_M/0/1/0/all/0/1\">Mattie Sanseverino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Personalized Dialogue via Multi-Task Meta-Learning. (arXiv:2108.03377v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03377","description":"<p>Conventional approaches to personalized dialogue generation typically require\na large corpus, as well as predefined persona information. However, in a\nreal-world setting, neither a large corpus of training data nor persona\ninformation are readily available. To address these practical limitations, we\npropose a novel multi-task meta-learning approach which involves training a\nmodel to adapt to new personas without relying on a large corpus, or on any\npredefined persona information. Instead, the model is tasked with generating\npersonalized responses based on only the dialogue context. Unlike prior work,\nour approach leverages on the provided persona information only during training\nvia the introduction of an auxiliary persona reconstruction task. In this\npaper, we introduce 2 frameworks that adopt the proposed multi-task\nmeta-learning approach: the Multi-Task Meta-Learning (MTML) framework, and the\nAlternating Multi-Task Meta-Learning (AMTML) framework. Experimental results\nshow that utilizing MTML and AMTML results in dialogue responses with greater\npersona consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jing Yang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kong Aik Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Summarization with Constrained Markov Decision Process. (arXiv:2108.03405v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03405","description":"<p>We study controllable text summarization which allows users to gain control\non a particular attribute (e.g., length limit) of the generated summaries. In\nthis work, we propose a novel training framework based on Constrained Markov\nDecision Process (CMDP), which conveniently includes a reward function along\nwith a set of constraints, to facilitate better summarization control. The\nreward function encourages the generation to resemble the human-written\nreference, while the constraints are used to explicitly prevent the generated\nsummaries from violating user-imposed requirements. Our framework can be\napplied to control important attributes of summarization, including length,\ncovered entities, and abstractiveness, as we devise specific constraints for\neach of these aspects. Extensive experiments on popular benchmarks show that\nour CMDP framework helps generate informative summaries while complying with a\ngiven attribute's requirement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An empirical assessment of deep learning approaches to task-oriented dialog management. (arXiv:2108.03478v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03478","description":"<p>Deep learning is providing very positive results in areas related to\nconversational interfaces, such as speech recognition, but its potential\nbenefit for dialog management has still not been fully studied. In this paper,\nwe perform an assessment of different configurations for deep-learned dialog\nmanagement with three dialog corpora from different application domains and\nvarying in size, dimensionality and possible system responses. Our results have\nallowed us to identify several aspects that can have an impact on accuracy,\nincluding the approaches used for feature extraction, input representation,\ncontext consideration and the hyper-parameters of the deep neural networks\nemployed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matej%5Cr%7Bu%7D_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Mat&#x11b;j&#x16f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griol_D/0/1/0/all/0/1\">David Griol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callejas_Z/0/1/0/all/0/1\">Zoraida Callejas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molina_J/0/1/0/all/0/1\">Jos&#xe9; Manuel Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchis_A/0/1/0/all/0/1\">Araceli Sanchis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning GPT-3 for Russian Text Summarization. (arXiv:2108.03502v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03502","description":"<p>Automatic summarization techniques aim to shorten and generalize information\ngiven in the text while preserving its core message and the most relevant\nideas. This task can be approached and treated with a variety of methods,\nhowever, not many attempts have been made to produce solutions specifically for\nthe Russian language despite existing localizations of the state-of-the-art\nmodels. In this paper, we aim to showcase ruGPT3 ability to summarize texts,\nfine-tuning it on the corpora of Russian news with their corresponding\nhuman-generated summaries. Additionally, we employ hyperparameter tuning so\nthat the model's output becomes less random and more tied to the original text.\nWe evaluate the resulting texts with a set of metrics, showing that our\nsolution can surpass the state-of-the-art model's performance without\nadditional changes in architecture or loss function. Despite being able to\nproduce sensible summaries, our model still suffers from a number of flaws,\nnamely, it is prone to altering Named Entities present in the original text\n(such as surnames, places, dates), deviating from facts stated in the given\ndocument, and repeating the information in the summary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolich_A/0/1/0/all/0/1\">Alexandr Nikolich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puchkova_A/0/1/0/all/0/1\">Arina Puchkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Compositional Wikidata Questions. (arXiv:2108.03509v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03509","description":"<p>Semantic parsing allows humans to leverage vast knowledge resources through\nnatural interaction. However, parsers are mostly designed for and evaluated on\nEnglish resources, such as CFQ (Keysers et al., 2020), the current standard\nbenchmark based on English data generated from grammar rules and oriented\ntowards Freebase, an outdated knowledge base. We propose a method for creating\na multilingual, parallel dataset of question-query pairs, grounded in Wikidata,\nand introduce such a dataset called Compositional Wikidata Questions (CWQ). We\nutilize this data to train and evaluate semantic parsers for Hebrew, Kannada,\nChinese and English, to better understand the current strengths and weaknesses\nof multilingual semantic parsing. Experiments on zero-shot cross-lingual\ntransfer demonstrate that models fail to generate valid queries even with\npretrained multilingual encoders. Our methodology, dataset and results will\nfacilitate future research on semantic parsing in more realistic and diverse\nsettings than has been possible with existing resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1\">Rahul Aralikatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lent_H/0/1/0/all/0/1\">Heather Lent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Similar Language Translation With Transfer Learning. (arXiv:2108.03533v1 [cs.AI])","link":"http://arxiv.org/abs/2108.03533","description":"<p>We investigate transfer learning based on pre-trained neural machine\ntranslation models to translate between (low-resource) similar languages. This\nwork is part of our contribution to the WMT 2021 Similar Languages Translation\nShared Task where we submitted models for different language pairs, including\nFrench-Bambara, Spanish-Catalan, and Spanish-Portuguese in both directions. Our\nmodels for Catalan-Spanish ($82.79$ BLEU) and Portuguese-Spanish ($87.11$ BLEU)\nrank top 1 in the official shared task evaluation, and we are the only team to\nsubmit models for the French-Bambara pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Evaluation in Open-ended Text Generation. (arXiv:2108.03578v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03578","description":"<p>Although current state-of-the-art language models have achieved impressive\nresults in numerous natural language processing tasks, still they could not\nsolve the problem of producing repetitive, dull and sometimes inconsistent text\nin open-ended text generation. Studies often attribute this problem to the\nmaximum likelihood training objective, and propose alternative approaches by\nusing stochastic decoding methods or altering the training objective. However,\nthere is still a lack of consistent evaluation metrics to directly compare the\nefficacy of these solutions. In this work, we study different evaluation\nmetrics that have been proposed to evaluate quality, diversity and consistency\nof machine-generated text. From there, we propose a practical pipeline to\nevaluate language models in open-ended generation task, and research on how to\nimprove the model's performance in all dimensions by leveraging different\nauxiliary training objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">An Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"#StayHome or #Marathon? Social Media Enhanced Pandemic Surveillance on Spatial-temporal Dynamic Graphs. (arXiv:2108.03670v1 [cs.SI])","link":"http://arxiv.org/abs/2108.03670","description":"<p>COVID-19 has caused lasting damage to almost every domain in public health,\nsociety, and economy. To monitor the pandemic trend, existing studies rely on\nthe aggregation of traditional statistical models and epidemic spread theory.\nIn other words, historical statistics of COVID-19, as well as the population\nmobility data, become the essential knowledge for monitoring the pandemic\ntrend. However, these solutions can barely provide precise prediction and\nsatisfactory explanations on the long-term disease surveillance while the\nubiquitous social media resources can be the key enabler for solving this\nproblem. For example, serious discussions may occur on social media before and\nafter some breaking events take place. These events, such as marathon and\nparade, may impact the spread of the virus. To take advantage of the social\nmedia data, we propose a novel framework, Social Media enhAnced pandemic\nsuRveillance Technique (SMART), which is composed of two modules: (i)\ninformation extraction module to construct heterogeneous knowledge graphs based\non the extracted events and relationships among them; (ii) time series\nprediction module to provide both short-term and long-term forecasts of the\nconfirmed cases and fatality at the state-level in the United States and to\ndiscover risk factors for COVID-19 interventions. Extensive experiments show\nthat our method largely outperforms the state-of-the-art baselines by 7.3% and\n7.4% in confirmed case/fatality prediction, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jyun-yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiusi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Commonsense Knowledge on Classifying False News and Determining Checkworthiness of Claims. (arXiv:2108.03731v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03731","description":"<p>Widespread and rapid dissemination of false news has made fact-checking an\nindispensable requirement. Given its time-consuming and labor-intensive nature,\nthe task calls for an automated support to meet the demand. In this paper, we\npropose to leverage commonsense knowledge for the tasks of false news\nclassification and check-worthy claim detection. Arguing that commonsense\nknowledge is a factor in human believability, we fine-tune the BERT language\nmodel with a commonsense question answering task and the aforementioned tasks\nin a multi-task learning environment. For predicting fine-grained false news\ntypes, we compare the proposed fine-tuned model's performance with the false\nnews classification models on a public dataset as well as a newly collected\ndataset. We compare the model's performance with the single-task BERT model and\na state-of-the-art check-worthy claim detection tool to evaluate the\ncheck-worthy claim detection. Our experimental analysis demonstrates that\ncommonsense knowledge can improve performance in both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek Baris Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezerer_E/0/1/0/all/0/1\">Erhan Sezerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekir_S/0/1/0/all/0/1\">Selma Tekir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_O/0/1/0/all/0/1\">Oul Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Translation of Low-Resource Indo-European Languages. (arXiv:2108.03739v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03739","description":"<p>Transfer learning has been an important technique for low-resource neural\nmachine translation. In this work, we build two systems to study how\nrelatedness can benefit the translation performance. The primary system adopts\nmachine translation model pre-trained on related language pair and the\ncontrastive system adopts that pre-trained on unrelated language pair. We show\nthat relatedness is not required for transfer learning to work but does benefit\nthe performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Rui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The HW-TSC's Offline Speech Translation Systems for IWSLT 2021 Evaluation. (arXiv:2108.03845v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03845","description":"<p>This paper describes our work in participation of the IWSLT-2021 offline\nspeech translation task. Our system was built in a cascade form, including a\nspeaker diarization module, an Automatic Speech Recognition (ASR) module and a\nMachine Translation (MT) module. We directly use the LIUM SpkDiarization tool\nas the diarization module. The ASR module is trained with three ASR datasets\nfrom different sources, by multi-source training, using a modified Transformer\nencoder. The MT module is pretrained on the large-scale WMT news translation\ndataset and fine-tuned on the TED corpus. Our method achieves 24.6 BLEU score\non the 2021 test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Minghan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingtao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yujia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Shimin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Ying Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])","link":"http://arxiv.org/abs/2108.03857","description":"<p>\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03861","description":"<p>Identifying political perspective in news media has become an important task\ndue to the rapid growth of political commentary and the increasingly polarized\nideologies. Previous approaches only focus on leveraging the semantic\ninformation and leaves out the rich social and political context that helps\nindividuals understand political stances. In this paper, we propose a\nperspective detection method that incorporates external knowledge of real-world\npolitics. Specifically, we construct a contemporary political knowledge graph\nwith 1,071 entities and 10,703 triples. We then build a heterogeneous\ninformation network for each news document that jointly models article\nsemantics and external knowledge in knowledge graphs. Finally, we apply gated\nrelational graph convolutional networks and conduct political perspective\ndetection as graph-level classification. Extensive experiments show that our\nmethod achieves the best performance and outperforms state-of-the-art methods\nby 5.49\\%. Numerous ablation studies further bear out the necessity of external\nknowledge and the effectiveness of our graph-based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03881","description":"<p>Political stance detection has become an important task due to the\nincreasingly polarized political ideologies. Most existing works focus on\nidentifying perspectives in news articles or social media posts, while social\nentities, such as individuals and organizations, produce these texts and\nactually take stances. In this paper, we propose the novel task of entity\nstance prediction, which aims to predict entities' stances given their social\nand political context. Specifically, we retrieve facts from Wikipedia about\nsocial entities regarding contemporary U.S. politics. We then annotate social\nentities' stances towards political ideologies with the help of domain experts.\nAfter defining the task of entity stance prediction, we propose a graph-based\nsolution, which constructs a heterogeneous information network from collected\nfacts and adopts gated relational graph convolutional networks for\nrepresentation learning. Our model is then trained with a combination of\nsupervised, self-supervised and unsupervised loss functions, which are\nmotivated by multiple social and political phenomenons. We conduct extensive\nexperiments to compare our method with existing text and graph analysis\nbaselines. Our model achieves highest stance detection accuracy and yields\ninspiring insights regarding social entity stances. We further conduct ablation\nstudy and parameter analysis to study the mechanism and effectiveness of our\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peisheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Images really do the Talking? Analysing the significance of Images in Tamil Troll meme classification. (arXiv:2108.03886v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03886","description":"<p>A meme is an part of media created to share an opinion or emotion across the\ninternet. Due to its popularity, memes have become the new forms of\ncommunication on social media. However, due to its nature, they are being used\nin harmful ways such as trolling and cyberbullying progressively. Various data\nmodelling methods create different possibilities in feature extraction and\nturning them into beneficial information. The variety of modalities included in\ndata plays a significant part in predicting the results. We try to explore the\nsignificance of visual features of images in classifying memes. Memes are a\nblend of both image and text, where the text is embedded into the image. We try\nto incorporate the memes as troll and non-trolling memes based on the images\nand the text on them. However, the images are to be analysed and combined with\nthe text to increase performance. Our work illustrates different textual\nanalysis methods and contrasting multimodal methods ranging from simple merging\nto cross attention to utilising both worlds' - best visual and textual\nfeatures. The fine-tuned cross-lingual language model, XLM, performed the best\nin textual analysis, and the multimodal transformer performs the best in\nmultimodal analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Siddhanth U Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thavareesan_S/0/1/0/all/0/1\">Sajeetha Thavareesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakuntharaj_R/0/1/0/all/0/1\">Ratnasingam Sakuntharaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangasamy_S/0/1/0/all/0/1\">Sathiyaraj Thangasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharathi_B/0/1/0/all/0/1\">B Bharathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Transferability of Neural Models of Morphological Analogies. (arXiv:2108.03938v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03938","description":"<p>Analogical proportions are statements expressed in the form \"A is to B as C\nis to D\" and are used for several reasoning and classification tasks in\nartificial intelligence and natural language processing (NLP). In this paper,\nwe focus on morphological tasks and we propose a deep learning approach to\ndetect morphological analogies. We present an empirical study to see how our\nframework transfers across languages, and that highlights interesting\nsimilarities and differences between these languages. In view of these results,\nwe also discuss the possibility of building a multilingual morphological model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1\">Safa Alsaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1\">Amandine Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1\">Puthineath Lay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1\">Esteban Marquer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1\">Pierre-Alexandre Murena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Approach for Detecting Morphological Analogies. (arXiv:2108.03945v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03945","description":"<p>Analogical proportions are statements of the form \"A is to B as C is to D\"\nthat are used for several reasoning and classification tasks in artificial\nintelligence and natural language processing (NLP). For instance, there are\nanalogy based approaches to semantics as well as to morphology. In fact,\nsymbolic approaches were developed to solve or to detect analogies between\ncharacter strings, e.g., the axiomatic approach as well as that based on\nKolmogorov complexity. In this paper, we propose a deep learning approach to\ndetect morphological analogies, for instance, with reinflexion or conjugation.\nWe present empirical results that show that our framework is competitive with\nthe above-mentioned state of the art symbolic approaches. We also explore\nempirically its transferability capacity across languages, which highlights\ninteresting similarities between them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1\">Safa Alsaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1\">Amandine Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1\">Puthineath Lay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1\">Esteban Marquer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1\">Pierre-Alexandre Murena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not quite there yet: Combining analogical patterns and encoder-decoder networks for cognitively plausible inflection. (arXiv:2108.03968v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03968","description":"<p>The paper presents four models submitted to Part 2 of the SIGMORPHON 2021\nShared Task 0, which aims at replicating human judgements on the inflection of\nnonce lexemes. Our goal is to explore the usefulness of combining pre-compiled\nanalogical patterns with an encoder-decoder architecture. Two models are\ndesigned using such patterns either in the input or the output of the network.\nTwo extra models controlled for the role of raw similarity of nonce inflected\nforms to existing inflected forms in the same paradigm cell, and the role of\nthe type frequency of analogical patterns. Our strategy is entirely endogenous\nin the sense that the models appealing solely to the data provided by the\nSIGMORPHON organisers, without using external resources. Our model 2 ranks\nsecond among all submitted systems, suggesting that the inclusion of analogical\npatterns in the network architecture is useful in mimicking speakers'\npredictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderone_B/0/1/0/all/0/1\">Basilio Calderone</a> (CLLE), <a href=\"http://arxiv.org/find/cs/1/au:+Hathout_N/0/1/0/all/0/1\">Nabil Hathout</a> (CLLE), <a href=\"http://arxiv.org/find/cs/1/au:+Bonami_O/0/1/0/all/0/1\">Olivier Bonami</a> (LLF UMR7110)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT-based distractor generation for Swedish reading comprehension questions using a small-scale dataset. (arXiv:2108.03973v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03973","description":"<p>An important part when constructing multiple-choice questions (MCQs) for\nreading comprehension assessment are the distractors, the incorrect but\npreferably plausible answer options. In this paper, we present a new BERT-based\nmethod for automatically generating distractors using only a small-scale\ndataset. We also release a new such dataset of Swedish MCQs (used for training\nthe model), and propose a methodology for assessing the generated distractors.\nEvaluation shows that from a student's perspective, our method generated one or\nmore plausible distractors for more than 50% of the MCQs in our test set. From\na teacher's perspective, about 50% of the generated distractors were deemed\nappropriate. We also do a thorough analysis of the results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalpakchi_D/0/1/0/all/0/1\">Dmytro Kalpakchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boye_J/0/1/0/all/0/1\">Johan Boye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04024","description":"<p>We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&amp;L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1\">Cristian Rodriguez-Opazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models. (arXiv:2108.04049v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04049","description":"<p>Open-domain extractive question answering works well on textual data by first\nretrieving candidate texts and then extracting the answer from those\ncandidates. However, some questions cannot be answered by text alone but\nrequire information stored in tables. In this paper, we present an approach for\nretrieving both texts and tables relevant to a question by jointly encoding\ntexts, tables and questions into a single vector space. To this end, we create\na new multi-modal dataset based on text and table datasets from related work\nand compare the retrieval performance of different encoding schemata. We find\nthat dense vector embeddings of transformer models outperform sparse embeddings\non four out of six evaluation datasets. Comparing different dense embedding\nmodels, tri-encoders, with one encoder for each question, text and table,\nincrease retrieval performance compared to bi-encoders with one encoder for the\nquestion and one for both text and tables. We release the newly created\nmulti-modal dataset to the community so that it can be used for training and\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_B/0/1/0/all/0/1\">Bogdan Kosti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting and categorising the reactions to COVID-19 by the South African public -- A social media study. (arXiv:2006.06336v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2006.06336","description":"<p>Social Media can be used to extract discussion topics during a disaster. With\nthe COVID-19 pandemic impact on South Africa, we need to understand how the law\nand regulation promulgated by the government in response to the pandemic\ncontrasts with discussion topics social media users have been engaging in. In\nthis work, we expand on traditional media analysis by using Social Media\ndiscussions driven by or directed to South African government officials. We\nfind topics that are similar as well as different in some cases. The findings\ncan inform further study into social media during disaster settings in South\nAfrica and beyond. This paper sets a framework for future analysis in\nunderstanding the opinions of the public during a pandemic and how these\nopinions can be distilled [in a semi-automated approach] to inform government\ncommunication in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moodley_A/0/1/0/all/0/1\">Avashlin Moodley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saba_A/0/1/0/all/0/1\">Athandiwe Saba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation-Augmented Retrieval for Open-domain Question Answering. (arXiv:2009.08553v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.08553","description":"<p>We propose Generation-Augmented Retrieval (GAR) for answering open-domain\nquestions, which augments a query through text generation of heuristically\ndiscovered relevant contexts without external resources as supervision. We\ndemonstrate that the generated contexts substantially enrich the semantics of\nthe queries and GAR with sparse representations (BM25) achieves comparable or\nbetter performance than state-of-the-art dense retrieval methods such as DPR.\nWe show that generating diverse contexts for a query is beneficial as fusing\ntheir results consistently yields better retrieval accuracy. Moreover, as\nsparse and dense representations are often complementary, GAR can be easily\ncombined with DPR to achieve even better performance. GAR achieves\nstate-of-the-art performance on Natural Questions and TriviaQA datasets under\nthe extractive QA setup when equipped with an extractive reader, and\nconsistently outperforms other retrieval methods when the same generative\nreader is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!. (arXiv:2009.10684v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.10684","description":"<p>Despite efforts to distinguish three different evaluation setups (Bekoulis et\nal., 2018), numerous end-to-end Relation Extraction (RE) articles present\nunreliable performance comparison to previous work. In this paper, we first\nidentify several patterns of invalid comparisons in published papers and\ndescribe them to avoid their propagation. We then propose a small empirical\nstudy to quantify the impact of the most common mistake and evaluate it leads\nto overestimating the final RE performance by around 5% on ACE05. We also seize\nthis opportunity to study the unexplored ablations of two recent developments:\nthe use of language model pretraining (specifically BERT) and span-level NER.\nThis meta-analysis emphasizes the need for rigor in the report of both the\nevaluation setting and the datasets statistics and we call for unifying the\nevaluation setting in end-to-end RE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taille_B/0/1/0/all/0/1\">Bruno Taill&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1\">Vincent Guigue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Notes on Coalgebras in Stylometry. (arXiv:2010.02733v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.02733","description":"<p>The syntactic behaviour of texts can highly vary depending on their contexts\n(e.g. author, genre, etc.). From the standpoint of stylometry, it can be\nhelpful to objectively measure this behaviour. In this paper, we discuss how\ncoalgebras are used to formalise the notion of behaviour by embedding syntactic\nfeatures of a given text into probabilistic transition systems. By introducing\nthe behavioural distance, we are then able to quantitatively measure\ndifferences between points in these systems and thus, comparing features of\ndifferent texts. Furthermore, the behavioural distance of points can be\napproximated by a polynomial-time algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doat_J/0/1/0/all/0/1\">Jo&#xeb;l A. Doat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14353","description":"<p>The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behaviour like online\nharassment, cyberbullying, and hate speech. Numerous works have been proposed\nto utilize textual data for social and anti-social behaviour analysis, by\npredicting the contexts mostly for highly-resourced languages like English.\nHowever, some languages are under-resourced, e.g., South Asian languages like\nBengali, that lack computational resources for accurate natural language\nprocessing (NLP). In this paper, we propose an explainable approach for hate\nspeech detection from the under-resourced Bengali language, which we called\nDeepHateExplainer. Bengali texts are first comprehensively preprocessed, before\nclassifying them into political, personal, geopolitical, and religious hates\nusing a neural ensemble method of transformer-based neural architectures (i.e.,\nmonolingual Bangla BERT-base, multilingual BERT-cased/uncased, and\nXLM-RoBERTa). Important(most and least) terms are then identified using\nsensitivity analysis and layer-wise relevance propagation(LRP), before\nproviding human-interpretable explanations. Finally, we compute\ncomprehensiveness and sufficiency scores to measure the quality of explanations\nw.r.t faithfulness. Evaluations against machine learning~(linear and tree-based\nmodels) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word\nembeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political,\npersonal, geopolitical, and religious hates, respectively, outperforming both\nML and DNN baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Sagor Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mehadi Hasan Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Kabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Azam Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering. (arXiv:2101.00294v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00294","description":"<p>Current open-domain question answering systems often follow a\nRetriever-Reader architecture, where the retriever first retrieves relevant\npassages and the reader then reads the retrieved passages to form an answer. In\nthis paper, we propose a simple and effective passage reranking method, named\nReader-guIDEd Reranker (RIDER), which does not involve training and reranks the\nretrieved passages solely based on the top predictions of the reader before\nreranking. We show that RIDER, despite its simplicity, achieves 10 to 20\nabsolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains\nwithout refining the retriever or reader. In addition, RIDER, without any\ntraining, outperforms state-of-the-art transformer-based supervised rerankers.\nRemarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM\non the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are\nused as the reader input after passage reranking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands. (arXiv:2103.02523v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.02523","description":"<p>The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of\nnatural language processing to the command line. Participants were tasked with\nbuilding models that can transform descriptions of command line tasks in\nEnglish to their Bash syntax. This is a report on the competition with details\nof the task, metrics, data, attempted solutions, and lessons learned.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mayank Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborti_T/0/1/0/all/0/1\">Tathagata Chakraborti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Quchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gros_D/0/1/0/all/0/1\">David Gros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xi Victoria Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maene_J/0/1/0/all/0/1\">Jaron Maene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talamadupula_K/0/1/0/all/0/1\">Kartik Talamadupula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhongwei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jules White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExKaldi-RT: A Real-Time Automatic Speech Recognition Extension Toolkit of Kaldi. (arXiv:2104.01384v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2104.01384","description":"<p>This paper describes the ExKaldi-RT online automatic speech recognition (ASR)\ntoolkit that is implemented based on the Kaldi ASR toolkit and Python language.\nExKaldi-RT provides tools for building online recognition pipelines. While\nsimilar tools are available built on Kaldi, a key feature of ExKaldi-RT that it\nworks on Python, which has an easy-to-use interface that allows online ASR\nsystem developers to develop original research, such as by applying neural\nnetwork-based signal processing and by decoding model trained with deep\nlearning frameworks. We performed benchmark experiments on the minimum\nLibriSpeech corpus, and it showed that ExKaldi-RT could achieve competitive ASR\nperformance in real-time recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leow_C/0/1/0/all/0/1\">Chee Siang Leow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kobayashi_A/0/1/0/all/0/1\">Akio Kobayashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Utsuro_T/0/1/0/all/0/1\">Takehito Utsuro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nishizaki_H/0/1/0/all/0/1\">Hiromitsu Nishizaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.12227","description":"<p>Classic information extraction techniques consist in building questions and\nanswers about the facts. Indeed, it is still a challenge to subjective\ninformation extraction systems to identify opinions and feelings in context. In\nsentiment-based NLP tasks, there are few resources to information extraction,\nabove all offensive or hateful opinions in context. To fill this important gap,\nthis short paper provides a new cross-lingual and contextual offensive lexicon,\nwhich consists of explicit and implicit offensive and swearing expressions of\nopinion, which were annotated in two different classes: context dependent and\ncontext-independent offensive. In addition, we provide markers to identify hate\nspeech. Annotation approach was evaluated at the expression-level and achieves\nhigh human inter-annotator agreement. The provided offensive lexicon is\navailable in Portuguese and English languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francielle Alves Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_I/0/1/0/all/0/1\">Isabelle Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_F/0/1/0/all/0/1\">Fabiana Rodrigues de G&#xf3;es</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06977","description":"<p>Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-T: Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.15082","description":"<p>Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiamang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TellMeWhy: A Dataset for Answering Why-Questions in Narratives. (arXiv:2106.06132v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06132","description":"<p>Answering questions about why characters perform certain actions is central\nto understanding and reasoning about narratives. Despite recent progress in QA,\nit is not clear if existing models have the ability to answer \"why\" questions\nthat may require commonsense knowledge external to the input narrative. In this\nwork, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more\nthan 30k questions and free-form answers concerning why characters in short\nnarratives perform the actions described. For a third of this dataset, the\nanswers are not present within the narrative. Given the limitations of\nautomated evaluation for this task, we also present a systematized human\nevaluation interface for this dataset. Our evaluation of state-of-the-art\nmodels show that they are far below human performance on answering such\nquestions. They are especially worse on questions whose answers are external to\nthe narrative, thus providing a challenge for future QA and narrative\nunderstanding research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1\">Yash Kumar Lal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1\">Nathanael Chambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1\">Raymond Mooney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14463","description":"<p>Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15078","description":"<p>Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence not perfect, e.g.,\nwhen the target sequence is corrupted with noises, or when only weak sequence\nsupervision is available. To address this challenge, we propose a novel\nEdit-Invariant Sequence Loss (EISL), which computes the matching loss of a\ntarget n-gram with all n-grams in the generated sequence. EISL draws\ninspirations from convolutional networks (ConvNets) which are shift-invariant\nto images, hence is robust to the shift of n-grams to tolerate edits in the\ntarget sequences. Moreover, the computation of EISL is essentially a\nconvolution operation with target n-grams as kernels, which is easy to\nimplement with existing libraries. To demonstrate the effectiveness of EISL, we\nconduct experiments on three tasks: machine translation with noisy target\nsequences, unsupervised text style transfer, and non-autoregressive machine\ntranslation. Experimental results show our method significantly outperforms\ncross entropy loss on these three tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision. (arXiv:2107.09852v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09852","description":"<p>Recent work has shown success in incorporating pre-trained models like BERT\nto improve NLP systems. However, existing pre-trained models lack of causal\nknowledge which prevents today's NLP systems from thinking like humans. In this\npaper, we investigate the problem of injecting causal knowledge into\npre-trained models. There are two fundamental problems: 1) how to collect\nvarious granularities of causal pairs from unstructured texts; 2) how to\neffectively inject causal knowledge into pre-trained models. To address these\nissues, we extend the idea of CausalBERT from previous studies, and conduct\nexperiments on various datasets to evaluate its effectiveness. In addition, we\nadopt a regularization-based method to preserve the already learned knowledge\nwith an extra regularization term while injecting causal knowledge. Extensive\nexperiments on 7 datasets, including four causal pair classification tasks, two\ncausal QA tasks and a causal inference task, demonstrate that CausalBERT\ncaptures rich causal knowledge and outperforms all pre-trained models-based\nstate-of-the-art methods, achieving a new causal inference benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kuo Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1\">Bing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12651","description":"<p>Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Underreporting of errors in NLG output, and what to do about it. (arXiv:2108.01182v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01182","description":"<p>We observe a severe under-reporting of the different kinds of errors that\nNatural Language Generation systems make. This is a problem, because mistakes\nare an important indicator of where systems should still be improved. If\nauthors only report overall performance metrics, the research community is left\nin the dark about the specific weaknesses that are exhibited by\n`state-of-the-art' research. Next to quantifying the extent of error\nunder-reporting, this position paper provides recommendations for error\nidentification, analysis and reporting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1\">Emiel van Miltenburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinciu_M/0/1/0/all/0/1\">Miruna-Adriana Clinciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inglis_S/0/1/0/all/0/1\">Stephanie Inglis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leppanen_L/0/1/0/all/0/1\">Leo Lepp&#xe4;nen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1\">Saad Mahamood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_E/0/1/0/all/0/1\">Emma Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoch_S/0/1/0/all/0/1\">Stephanie Schoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1\">Craig Thomson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Luou Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02941","description":"<p>Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1\">Harm de Wet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.IR updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.IR","description":"Computer Science -- Information Retrieval (cs.IR) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Profiling Web Archival Voids for Memento Routing. (arXiv:2108.03311v1 [cs.DL])","link":"http://arxiv.org/abs/2108.03311","description":"<p>Prior work on web archive profiling were focused on Archival Holdings to\ndescribe what is present in an archive. This work defines and explores Archival\nVoids to establish a means to represent portions of URI spaces that are not\npresent in a web archive. Archival Holdings and Archival Voids profiles can\nwork independently or as complements to each other to maximize the Accuracy of\nMemento Aggregators. We discuss various sources of truth that can be used to\ncreate Archival Voids profiles. We use access logs from Arquivo.pt to create\nvarious Archival Voids profiles and analyze them against our MemGator access\nlogs for evaluation. We find that we could have avoided more than 8% of\nadditional False Positives on top of the 60% Accuracy we got from profiling\nArchival Holdings in our prior work, if Arquivo.pt were to provide an Archival\nVoids profile based on URIs that were requested hundreds of times and never\nreturned any success responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1\">Sawood Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weigle_M/0/1/0/all/0/1\">Michele C. Weigle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nelson_M/0/1/0/all/0/1\">Michael L. Nelson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Transformers for Neural Cross-Domain Search. (arXiv:2108.03322v1 [cs.IR])","link":"http://arxiv.org/abs/2108.03322","description":"<p>Pre-trained transformers have recently clinched top spots in the gamut of\nnatural language tasks and pioneered solutions to software engineering tasks.\nEven information retrieval has not been immune to the charm of the transformer,\nthough their large size and cost is generally a barrier to deployment. While\nthere has been much work in streamlining, caching, and modifying transformer\narchitectures for production, here we explore a new direction: distilling a\nlarge pre-trained translation model into a lightweight bi-encoder which can be\nefficiently cached and queried. We argue from a probabilistic perspective that\nsequence-to-sequence models are a conceptually ideal---albeit highly\nimpractical---retriever. We derive a new distillation objective, implementing\nit as a data augmentation scheme. Using natural language source code search as\na case study for cross-domain search, we demonstrate the validity of this idea\nby significantly improving upon the current leader of the CodeSearchNet\nchallenge, a recent natural language code search benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin B. Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Represent Human Motives for Goal-directed Web Browsing. (arXiv:2108.03350v1 [cs.IR])","link":"http://arxiv.org/abs/2108.03350","description":"<p>Motives or goals are recognized in psychology literature as the most\nfundamental drive that explains and predicts why people do what they do,\nincluding when they browse the web. Although providing enormous value, these\nhigher-ordered goals are often unobserved, and little is known about how to\nleverage such goals to assist people's browsing activities. This paper proposes\nto take a new approach to address this problem, which is fulfilled through a\nnovel neural framework, Goal-directed Web Browsing (GoWeB). We adopt a\npsychologically-sound taxonomy of higher-ordered goals and learn to build their\nrepresentations in a structure-preserving manner. Then we incorporate the\nresulting representations for enhancing the experiences of common activities\npeople perform on the web. Experiments on large-scale data from Microsoft Edge\nweb browser show that GoWeB significantly outperforms competitive baselines for\nin-session web page recommendation, re-visitation classification, and\ngoal-based web page grouping. A follow-up analysis further characterizes how\nthe variety of human motives can affect the difference observed in human\nbehavioral patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jyun-Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chia-Jung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarrafzadeh_B/0/1/0/all/0/1\">Bahareh Sarrafzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hecht_B/0/1/0/all/0/1\">Brent Hecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teevan_J/0/1/0/all/0/1\">Jaime Teevan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions. (arXiv:2108.03357v1 [cs.IR])","link":"http://arxiv.org/abs/2108.03357","description":"<p>Traditional recommendation systems are faced with two long-standing\nobstacles, namely, data sparsity and cold-start problems, which promote the\nemergence and development of Cross-Domain Recommendation (CDR). The core idea\nof CDR is to leverage information collected from other domains to alleviate the\ntwo problems in one domain. Over the last decade, many efforts have been\nengaged for cross-domain recommendation. Recently, with the development of deep\nlearning and neural networks, a large number of methods have emerged. However,\nthere is a limited number of systematic surveys on CDR, especially regarding\nthe latest proposed methods as well as the recommendation scenarios and\nrecommendation tasks they address. In this survey paper, we first proposed a\ntwo-level taxonomy of cross-domain recommendation which classifies different\nrecommendation scenarios and recommendation tasks. We then introduce and\nsummarize existing cross-domain recommendation approaches under different\nrecommendation scenarios in a structured manner. We also organize datasets\ncommonly used. We conclude this survey by providing several potential research\ndirections about this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_T/0/1/0/all/0/1\">Tianzi Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanmin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haobing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiadi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What a million Indian farmers say?: A crowdsourcing-based method for pest surveillance. (arXiv:2108.03374v1 [cs.AI])","link":"http://arxiv.org/abs/2108.03374","description":"<p>Many different technologies are used to detect pests in the crops, such as\nmanual sampling, sensors, and radar. However, these methods have scalability\nissues as they fail to cover large areas, are uneconomical and complex. This\npaper proposes a crowdsourced based method utilising the real-time farmer\nqueries gathered over telephones for pest surveillance. We developed\ndata-driven strategies by aggregating and analyzing historical data to find\npatterns and get future insights into pest occurrence. We showed that it can be\nan accurate and economical method for pest surveillance capable of enveloping a\nlarge area with high spatio-temporal granularity. Forecasting the pest\npopulation will help farmers in making informed decisions at the right time.\nThis will also help the government and policymakers to make the necessary\npreparations as and when required and may also ensure food security.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adhikari_P/0/1/0/all/0/1\">Poonam Adhikari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ritesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyengar_S/0/1/0/all/0/1\">S.R.S Iyengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rishemjit Kaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased Cascade Bandits: Mitigating Exposure Bias in Online Learning to Rank Recommendation. (arXiv:2108.03440v1 [cs.IR])","link":"http://arxiv.org/abs/2108.03440","description":"<p>Exposure bias is a well-known issue in recommender systems where items and\nsuppliers are not equally represented in the recommendation results. This is\nespecially problematic when bias is amplified over time as a few popular items\nare repeatedly over-represented in recommendation lists. This phenomenon can be\nviewed as a recommendation feedback loop: the system repeatedly recommends\ncertain items at different time points and interactions of users with those\nitems will amplify bias towards those items over time. This issue has been\nextensively studied in the literature on model-based or neighborhood-based\nrecommendation algorithms, but less work has been done on online recommendation\nmodels such as those based on multi-armed Bandit algorithms. In this paper, we\nstudy exposure bias in a class of well-known bandit algorithms known as Linear\nCascade Bandits. We analyze these algorithms on their ability to handle\nexposure bias and provide a fair representation for items and suppliers in the\nrecommendation results. Our analysis reveals that these algorithms fail to\ntreat items and suppliers fairly and do not sufficiently explore the item space\nfor each user. To mitigate this bias, we propose a discounting factor and\nincorporate it into these algorithms that controls the exposure of items at\neach time step. To show the effectiveness of the proposed discounting factor on\nmitigating exposure bias, we perform experiments on two datasets using three\ncascading bandit algorithms and our experimental results show that the proposed\nmethod improves the exposure fairness for items and suppliers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansoury_M/0/1/0/all/0/1\">Masoud Mansoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdollahpouri_H/0/1/0/all/0/1\">Himan Abdollahpouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1\">Bamshad Mobasher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burke_R/0/1/0/all/0/1\">Robin Burke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabouri_M/0/1/0/all/0/1\">Milad Sabouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and Meter Tracking. (arXiv:2108.03576v1 [eess.AS])","link":"http://arxiv.org/abs/2108.03576","description":"<p>The online estimation of rhythmic information, such as beat positions,\ndownbeat positions, and meter, is critical for many real-time music\napplications. Musical rhythm comprises complex hierarchical relationships\nacross time, rendering its analysis intrinsically challenging and at times\nsubjective. Furthermore, systems which attempt to estimate rhythmic information\nin real-time must be causal and must produce estimates quickly and efficiently.\nIn this work, we introduce an online system for joint beat, downbeat, and meter\ntracking, which utilizes causal convolutional and recurrent layers, followed by\na pair of sequential Monte Carlo particle filters applied during inference. The\nproposed system does not need to be primed with a time signature in order to\nperform downbeat tracking, and is instead able to estimate meter and adjust the\npredictions over time. Additionally, we propose an information gate strategy to\nsignificantly decrease the computational cost of particle filtering during the\ninference step, making the system much faster than previous sampling-based\nmethods. Experiments on the GTZAN dataset, which is unseen during training,\nshow that the system outperforms various online beat and downbeat tracking\nsystems and achieves comparable performance to a baseline offline joint method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Heydari_M/0/1/0/all/0/1\">Mojtaba Heydari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cwitkowitz_F/0/1/0/all/0/1\">Frank Cwitkowitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoolRank: Max/Min Pooling-based Ranking Loss for Listwise Learning & Ranking Balance. (arXiv:2108.03586v1 [cs.IR])","link":"http://arxiv.org/abs/2108.03586","description":"<p>Numerous neural retrieval models have been proposed in recent years. These\nmodels learn to compute a ranking score between the given query and document.\nThe majority of existing models are trained in pairwise fashion using\nhuman-judged labels directly without further calibration. The traditional\npairwise schemes can be time-consuming and require pre-defined\npositive-negative document pairs for training, potentially leading to learning\nbias due to document distribution mismatch between training and test\nconditions. Some popular existing listwise schemes rely on the strong\npre-defined probabilistic assumptions and stark difference between relevant and\nnon-relevant documents for the given query, which may limit the model potential\ndue to the low-quality or ambiguous relevance labels. To address these\nconcerns, we turn to a physics-inspired ranking balance scheme and propose\nPoolRank, a pooling-based listwise learning framework. The proposed scheme has\nfour major advantages: (1) PoolRank extracts training information from the best\ncandidates at the local level based on model performance and relative ranking\namong abundant document candidates. (2) By combining four pooling-based loss\ncomponents in a multi-task learning fashion, PoolRank calibrates the ranking\nbalance for the partially relevant and the highly non-relevant documents\nautomatically without costly human inspection. (3) PoolRank can be easily\ngeneralized to any neural retrieval model without requiring additional\nlearnable parameters or model structure modifications. (4) Compared to pairwise\nlearning and existing listwise learning schemes, PoolRank yields better ranking\nperformance for all studied retrieval models while retaining efficient\nconvergence rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03788","description":"<p>This paper presents a three-tier modality alignment approach to learning\ntext-image joint embedding, coined as JEMA, for cross-modal retrieval of\ncooking recipes and food images. The first tier improves recipe text embedding\nby optimizing the LSTM networks with term extraction and ranking enhanced\nsequence patterns, and optimizes the image embedding by combining the\nResNeXt-101 image encoder with the category embedding using wideResNet-50 with\nword2vec. The second tier modality alignment optimizes the textual-visual joint\nembedding loss function using a double batch-hard triplet loss with soft-margin\noptimization. The third modality alignment incorporates two types of\ncross-modality alignments as the auxiliary loss regularizations to further\nreduce the alignment errors in the joint learning of the two modality-specific\nembedding functions. The category-based cross-modal alignment aims to align the\nimage category with the recipe category as a loss regularization to the joint\nembedding. The cross-modal discriminator-based alignment aims to add the\nvisual-textual embedding distribution alignment to further regularize the joint\nembedding loss. Extensive experiments with the one-million recipes benchmark\ndataset Recipe1M demonstrate that the proposed JEMA approach outperforms the\nstate-of-the-art cross-modal embedding methods for both image-to-recipe and\nrecipe-to-image retrievals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DoSSIER@COLIEE 2021: Leveraging dense retrieval and summarization-based re-ranking for case law retrieval. (arXiv:2108.03937v1 [cs.IR])","link":"http://arxiv.org/abs/2108.03937","description":"<p>In this paper, we present our approaches for the case law retrieval and the\nlegal case entailment task in the Competition on Legal Information\nExtraction/Entailment (COLIEE) 2021. As first stage retrieval methods combined\nwith neural re-ranking methods using contextualized language models like BERT\nachieved great performance improvements for information retrieval in the web\nand news domain, we evaluate these methods for the legal domain. A distinct\ncharacteristic of legal case retrieval is that the query case and case\ndescription in the corpus tend to be long documents and therefore exceed the\ninput length of BERT. We address this challenge by combining lexical and dense\nretrieval methods on the paragraph-level of the cases for the first stage\nretrieval. Here we demonstrate that the retrieval on the paragraph-level\noutperforms the retrieval on the document-level. Furthermore the experiments\nsuggest that dense retrieval methods outperform lexical retrieval. For\nre-ranking we address the problem of long documents by summarizing the cases\nand fine-tuning a BERT-based re-ranker with the summaries. Overall, our best\nresults were obtained with a combination of BM25 and dense passage retrieval\nusing domain-specific embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Althammer_S/0/1/0/all/0/1\">Sophia Althammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askari_A/0/1/0/all/0/1\">Arian Askari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanbury_A/0/1/0/all/0/1\">Allan Hanbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04024","description":"<p>We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&amp;L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1\">Cristian Rodriguez-Opazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IntenT5: Search Result Diversification using Causal Language Models. (arXiv:2108.04026v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04026","description":"<p>Search result diversification is a beneficial approach to overcome\nunder-specified queries, such as those that are ambiguous or multi-faceted.\nExisting approaches often rely on massive query logs and interaction data to\ngenerate a variety of possible query intents, which then can be used to re-rank\ndocuments. However, relying on user interaction data is problematic because one\nfirst needs a massive user base to build a sufficient log; public query logs\nare insufficient on their own. Given the recent success of causal language\nmodels (such as the Text-To-Text Transformer (T5) model) at text generation\ntasks, we explore the capacity of these models to generate potential query\nintents. We find that to encourage diversity in the generated queries, it is\nbeneficial to adapt the model by including a new Distributional Causal Language\nModeling (DCLM) objective during fine-tuning and a representation replacement\nduring inference. Across six standard evaluation benchmarks, we find that our\nmethod (which we call IntenT5) improves search result diversity and attains\n(and sometimes exceeds) the diversity obtained when using query suggestions\nbased on a proprietary query log. Our analysis shows that our approach is most\neffective for multi-faceted queries and is able to generalize effectively to\nqueries that were unseen in training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+MacAvaney_S/0/1/0/all/0/1\">Sean MacAvaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DGEM: A New Dual-modal Graph Embedding Method in Recommendation System. (arXiv:2108.04031v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04031","description":"<p>In the current deep learning based recommendation system, the embedding\nmethod is generally employed to complete the conversion from the\nhigh-dimensional sparse feature vector to the low-dimensional dense feature\nvector. However, as the dimension of the input vector of the embedding layer is\ntoo large, the addition of the embedding layer significantly slows down the\nconvergence speed of the entire neural network, which is not acceptable in\nreal-world scenarios. In addition, as the interaction between users and items\nincreases and the relationship between items becomes more complicated, the\nembedding method proposed for sequence data is no longer suitable for graphic\ndata in the current real environment. Therefore, in this paper, we propose the\nDual-modal Graph Embedding Method (DGEM) to solve these problems. DGEM includes\ntwo modes, static and dynamic. We first construct the item graph to extract the\ngraph structure and use random walk of unequal probability to capture the\nhigh-order proximity between the items. Then we generate the graph embedding\nvector through the Skip-Gram model, and finally feed the downstream deep neural\nnetwork for the recommendation task. The experimental results show that DGEM\ncan mine the high-order proximity between items and enhance the expression\nability of the recommendation model. Meanwhile it also improves the\nrecommendation performance by utilizing the time dependent relationship between\nitems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rongwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhuyun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models. (arXiv:2108.04049v1 [cs.CL])","link":"http://arxiv.org/abs/2108.04049","description":"<p>Open-domain extractive question answering works well on textual data by first\nretrieving candidate texts and then extracting the answer from those\ncandidates. However, some questions cannot be answered by text alone but\nrequire information stored in tables. In this paper, we present an approach for\nretrieving both texts and tables relevant to a question by jointly encoding\ntexts, tables and questions into a single vector space. To this end, we create\na new multi-modal dataset based on text and table datasets from related work\nand compare the retrieval performance of different encoding schemata. We find\nthat dense vector embeddings of transformer models outperform sparse embeddings\non four out of six evaluation datasets. Comparing different dense embedding\nmodels, tri-encoders, with one encoder for each question, text and table,\nincrease retrieval performance compared to bi-encoders with one encoder for the\nquestion and one for both text and tables. We release the newly created\nmulti-modal dataset to the community so that it can be used for training and\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kostic_B/0/1/0/all/0/1\">Bogdan Kosti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risch_J/0/1/0/all/0/1\">Julian Risch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1\">Timo M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions. (arXiv:1811.00414v3 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/1811.00414","description":"<p>A central roadblock to analyzing quantum algorithms on quantum states is the\nlack of a comparable input model for classical algorithms. Inspired by recent\nwork of the author [E. Tang, STOC'19], we introduce such a model, where we\nassume we can efficiently perform $\\ell^2$-norm samples of input data, a\nnatural analogue to quantum algorithms that assume efficient state preparation\nof classical data. Though this model produces less practical algorithms than\nthe (stronger) standard model of classical computation, it captures versions of\nmany of the features and nuances of quantum linear algebra algorithms. With\nthis model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's\nquantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)]\nand nearest-centroid clustering [<a href=\"/abs/1307.0411\">arXiv:1307.0411</a>]. Since they are only\npolynomially slower, these algorithms suggest that the exponential speedups of\ntheir quantum counterparts are simply an artifact of state preparation\nassumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_E/0/1/0/all/0/1\">Ewin Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tag Embedding Based Personalized Point Of Interest Recommendation System. (arXiv:2004.06389v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2004.06389","description":"<p>Personalized Point of Interest recommendation is very helpful for satisfying\nusers' needs at new places. In this article, we propose a tag embedding based\nmethod for Personalized Recommendation of Point Of Interest. We model the\nrelationship between tags corresponding to Point Of Interest. The model\nprovides representative embedding corresponds to a tag in a way that related\ntags will be closer. We model Point of Interest-based on tag embedding and also\nmodel the users (user profile) based on the Point Of Interest rated by them.\nfinally, we rank the user's candidate Point Of Interest based on cosine\nsimilarity between user's embedding and Point of Interest's embedding. Further,\nwe find the parameters required to model user by discrete optimizing over\ndifferent measures (like ndcg@5, MRR, ...). We also analyze the result while\nconsidering the same parameters for all users and individual parameters for\neach user. Along with it we also analyze the effect on the result while\nchanging the dataset to model the relationship between tags. Our method also\nminimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase\n2 dataset and have significant improvement over all the measures on the state\nof the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,\nwhich shows the effectiveness of the method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Suraj Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Dwaipayan Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_M/0/1/0/all/0/1\">Mandar Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation-Augmented Retrieval for Open-domain Question Answering. (arXiv:2009.08553v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.08553","description":"<p>We propose Generation-Augmented Retrieval (GAR) for answering open-domain\nquestions, which augments a query through text generation of heuristically\ndiscovered relevant contexts without external resources as supervision. We\ndemonstrate that the generated contexts substantially enrich the semantics of\nthe queries and GAR with sparse representations (BM25) achieves comparable or\nbetter performance than state-of-the-art dense retrieval methods such as DPR.\nWe show that generating diverse contexts for a query is beneficial as fusing\ntheir results consistently yields better retrieval accuracy. Moreover, as\nsparse and dense representations are often complementary, GAR can be easily\ncombined with DPR to achieve even better performance. GAR achieves\nstate-of-the-art performance on Natural Questions and TriviaQA datasets under\nthe extractive QA setup when equipped with an extractive reader, and\nconsistently outperforms other retrieval methods when the same generative\nreader is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rider: Reader-Guided Passage Reranking for Open-Domain Question Answering. (arXiv:2101.00294v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00294","description":"<p>Current open-domain question answering systems often follow a\nRetriever-Reader architecture, where the retriever first retrieves relevant\npassages and the reader then reads the retrieved passages to form an answer. In\nthis paper, we propose a simple and effective passage reranking method, named\nReader-guIDEd Reranker (RIDER), which does not involve training and reranks the\nretrieved passages solely based on the top predictions of the reader before\nreranking. We show that RIDER, despite its simplicity, achieves 10 to 20\nabsolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains\nwithout refining the retriever or reader. In addition, RIDER, without any\ntraining, outperforms state-of-the-art transformer-based supervised rerankers.\nRemarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM\non the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are\nused as the reader input after passage reranking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuning Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Models for the First-stage Retrieval: A Comprehensive Review. (arXiv:2103.04831v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2103.04831","description":"<p>Multi-stage ranking pipelines have been a practical solution in modern search\nsystems, where the first-stage retrieval is to return a subset of candidate\ndocuments, and latter stages attempt to re-rank those candidates. Unlike\nre-ranking stages going through quick technique shifts during past decades, the\nfirst-stage retrieval has long been dominated by classical term-based models.\nUnfortunately, these models suffer from the vocabulary mismatch problem, which\nmay block re-ranking stages from relevant documents at the very beginning.\nTherefore, it has been a long-term desire to build semantic models for the\nfirst-stage retrieval that can achieve high recall efficiently. Recently, we\nhave witnessed an explosive growth of research interests on the first-stage\nsemantic retrieval models. We believe it is the right time to survey current\nstatus, learn from existing methods, and gain some insights for future\ndevelopment. In this paper, we describe the current landscape of the\nfirst-stage retrieval models under a unified framework to clarify the\nconnection between classical term-based retrieval methods, early semantic\nretrieval methods and neural semantic retrieval methods. Moreover, we identify\nsome open challenges and envision some future directions, with the hope of\ninspiring more researches on these important yet less investigated topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yinqiong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.05248","description":"<p>Recent studies unveil the vulnerabilities of deep ranking models, where an\nimperceptible perturbation can trigger dramatic changes in the ranking result.\nWhile previous attempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order remains\nunder-explored. In this paper, we formulate a new adversarial attack against\ndeep ranking systems, i.e., the Order Attack, which covertly alters the\nrelative order among a selected set of candidates according to an\nattacker-specified permutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style loss imposing an\ninequality chain reflecting the specified permutation. However, direct\noptimization of such white-box objective is infeasible in a real-world attack\nscenario due to various black-box limitations. To cope with them, we propose a\nShort-range Ranking Correlation metric as a surrogate objective for black-box\nOrder Attack to approximate the white-box method. The Order Attack is evaluated\non the Fashion-MNIST and Stanford-Online-Products datasets under both white-box\nand black-box threat models. The black-box attack is also successfully\nimplemented on a major e-commerce platform. Comprehensive experimental\nevaluations demonstrate the effectiveness of the proposed methods, revealing a\nnew type of ranking model vulnerability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.08649","description":"<p>User response prediction, which aims to predict the probability that a user\nwill provide a predefined positive response in a given context such as clicking\non an ad or purchasing an item, is crucial to many industrial applications such\nas online advertising, recommender systems, and search ranking. However, due to\nthe high dimensionality and super sparsity of the data collected in these\ntasks, handcrafting cross features is inevitably time expensive. Prior studies\nin predicting user response leveraged the feature interactions by enhancing\nfeature vectors with products of features to model second-order or high-order\ncross features, either explicitly or implicitly. Nevertheless, these existing\nmethods can be hindered by not learning sufficient cross features due to model\narchitecture limitations or modeling all high-order feature interactions with\nequal weights. This work aims to fill this gap by proposing a novel\narchitecture Deep Cross Attentional Product Network (DCAP), which keeps cross\nnetwork's benefits in modeling high-order feature interactions explicitly at\nthe vector-wise level. Beyond that, it can differentiate the importance of\ndifferent cross features in each network layer inspired by the multi-head\nattention mechanism and Product Neural Network (PNN), allowing practitioners to\nperform a more in-depth analysis of user behaviors. Additionally, our proposed\nmodel can be easily implemented and train in parallel. We conduct comprehensive\nexperiments on three real-world datasets. The results have robustly\ndemonstrated that our proposed model DCAP achieves superior prediction\nperformance compared with the state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangtian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1\">Robert Pless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuzhen Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14463","description":"<p>Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate Localization by Instance Search. (arXiv:2107.05005v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05005","description":"<p>Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi-Geng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hui-Chu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Information Retrieval"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.MM updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.MM","description":"Computer Science -- Multimedia (cs.MM) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition. (arXiv:2108.03354v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03354","description":"<p>The research on human emotion under multimedia stimulation based on\nphysiological signals is an emerging field, and important progress has been\nachieved for emotion recognition based on multi-modal signals. However, it is\nchallenging to make full use of the complementarity among\nspatial-spectral-temporal domain features for emotion recognition, as well as\nmodel the heterogeneity and correlation among multi-modal signals. In this\npaper, we propose a novel two-stream heterogeneous graph recurrent neural\nnetwork, named HetEmotionNet, fusing multi-modal physiological signals for\nemotion recognition. Specifically, HetEmotionNet consists of the\nspatial-temporal stream and the spatial-spectral stream, which can fuse\nspatial-spectral-temporal domain features in a unified framework. Each stream\nis composed of the graph transformer network for modeling the heterogeneity,\nthe graph convolutional network for modeling the correlation, and the gated\nrecurrent unit for capturing the temporal domain or spectral domain dependency.\nExtensive experiments on two real-world datasets demonstrate that our proposed\nmodel achieves better performance than state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Ziyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiangheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Caijie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Action Localization Using Gated Recurrent Units. (arXiv:2108.03375v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03375","description":"<p>Temporal Action Localization (TAL) task in which the aim is to predict the\nstart and end of each action and its class label has many applications in the\nreal world. But due to its complexity, researchers have not reached great\nresults compared to the action recognition task. The complexity is related to\npredicting precise start and end times for different actions in any video. In\nthis paper, we propose a new network based on Gated Recurrent Unit (GRU) and\ntwo novel post-processing ideas for TAL task. Specifically, we propose a new\ndesign for the output layer of the GRU resulting in the so-called GRU-Splitted\nmodel. Moreover, linear interpolation is used to generate the action proposals\nwith precise start and end times. Finally, to rank the generated proposals\nappropriately, we use a Learn to Rank (LTR) approach. We evaluated the\nperformance of the proposed method on Thumos14 dataset. Results show the\nsuperiority of the performance of the proposed method compared to\nstate-of-the-art. Especially in the mean Average Precision (mAP) metric at\nIntersection over Union (IoU) 0.7, we get 27.52% which is 5.12% better than\nthat of state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khojasteh_H/0/1/0/all/0/1\">Hassan Keshvari Khojasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzade_H/0/1/0/all/0/1\">Hoda Mohammadzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behroozi_H/0/1/0/all/0/1\">Hamid Behroozi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cough Detection Using Selected Informative Features from Audio Signals. (arXiv:2108.03538v1 [cs.SD])","link":"http://arxiv.org/abs/2108.03538","description":"<p>Cough is a common symptom of respiratory and lung diseases. Cough detection\nis important to prevent, assess and control epidemic, such as COVID-19. This\npaper proposes a model to detect cough events from cough audio signals. The\nmodels are trained by the dataset combined ESC-50 dataset with self-recorded\ncough recordings. The test dataset contains inpatient cough recordings\ncollected from inpatients of the respiratory disease department in Ruijin\nHospital. We totally build 15 cough detection models based on different feature\nnumbers selected by Random Frog, Uninformative Variable Elimination (UVE), and\nVariable influence on projection (VIP) algorithms respectively. The optimal\nmodel is based on 20 features selected from Mel Frequency Cepstral Coefficients\n(MFCC) features by UVE algorithm and classified with Support Vector Machine\n(SVM) linear two-class classifier. The best cough detection model realizes the\naccuracy, recall, precision and F1-score with 94.9%, 97.1%, 93.1% and 0.95\nrespectively. Its excellent performance with fewer dimensionality of the\nfeature vector shows the potential of being applied to mobile devices, such as\nsmartphones, thus making cough detection remote and non-contact.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinru Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Menghan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-pronged Strategy: Lightweight Augmented Graph Network Hashing for Scalable Image Retrieval. (arXiv:2108.03914v1 [cs.MM])","link":"http://arxiv.org/abs/2108.03914","description":"<p>Hashing learns compact binary codes to store and retrieve massive data\nefficiently. Particularly, unsupervised deep hashing is supported by powerful\ndeep neural networks and has the desirable advantage of label independence. It\nis a promising technique for scalable image retrieval. However, deep models\nintroduce a large number of parameters, which is hard to optimize due to the\nlack of explicit semantic labels and brings considerable training cost. As a\nresult, the retrieval accuracy and training efficiency of existing unsupervised\ndeep hashing are still limited. To tackle the problems, in this paper, we\npropose a simple and efficient \\emph{Lightweight Augmented Graph Network\nHashing} (LAGNH) method with a two-pronged strategy. For one thing, we extract\nthe inner structure of the image as the auxiliary semantics to enhance the\nsemantic supervision of the unsupervised hash learning process. For another, we\ndesign a lightweight network structure with the assistance of the auxiliary\nsemantics, which greatly reduces the number of network parameters that needs to\nbe optimized and thus greatly accelerates the training process. Specifically,\nwe design a cross-modal attention module based on the auxiliary semantic\ninformation to adaptively mitigate the adverse effects in the deep image\nfeatures. Besides, the hash codes are learned by multi-layer message passing\nwithin an adversarial regularized graph convolutional network. Simultaneously,\nthe semantic representation capability of hash codes is further enhanced by\nreconstructing the similarity graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1\">Hui Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingjing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.08649","description":"<p>User response prediction, which aims to predict the probability that a user\nwill provide a predefined positive response in a given context such as clicking\non an ad or purchasing an item, is crucial to many industrial applications such\nas online advertising, recommender systems, and search ranking. However, due to\nthe high dimensionality and super sparsity of the data collected in these\ntasks, handcrafting cross features is inevitably time expensive. Prior studies\nin predicting user response leveraged the feature interactions by enhancing\nfeature vectors with products of features to model second-order or high-order\ncross features, either explicitly or implicitly. Nevertheless, these existing\nmethods can be hindered by not learning sufficient cross features due to model\narchitecture limitations or modeling all high-order feature interactions with\nequal weights. This work aims to fill this gap by proposing a novel\narchitecture Deep Cross Attentional Product Network (DCAP), which keeps cross\nnetwork's benefits in modeling high-order feature interactions explicitly at\nthe vector-wise level. Beyond that, it can differentiate the importance of\ndifferent cross features in each network layer inspired by the multi-head\nattention mechanism and Product Neural Network (PNN), allowing practitioners to\nperform a more in-depth analysis of user behaviors. Additionally, our proposed\nmodel can be easily implemented and train in parallel. We conduct comprehensive\nexperiments on three real-world datasets. The results have robustly\ndemonstrated that our proposed model DCAP achieves superior prediction\nperformance compared with the state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangtian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1\">Robert Pless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuzhen Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00705","description":"<p>This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Multimedia"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging. (arXiv:2108.03233v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03233","description":"<p>Incorporating boundaries of the imaging object as a priori information to\nimaging algorithms can significantly improve the performance of electromagnetic\nmedical imaging systems. To avoid overly complicating the system by using\ndifferent sensors and the adverse effect of the subject's movement, a\nlearning-based method is proposed to estimate the boundary (external contour)\nof the imaged object using the same electromagnetic imaging data. While imaging\ntechniques may discard the reflection coefficients for being dominant and\nuninformative for imaging, these parameters are made use of for boundary\ndetection. The learned model is verified through independent clinical human\ntrials by using a head imaging system with a 16-element antenna array that\nworks across the band 0.7-1.6 GHz. The evaluation demonstrated that the model\nachieves average dissimilarity of 0.012 in Hu-moment while detecting head\nboundary. The model enables fast scan and image creation while eliminating the\nneed for additional devices for accurate boundary estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1\">A. Al-Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stancombe_A/0/1/0/all/0/1\">A. Stancombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_A/0/1/0/all/0/1\">A. Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1\">A. Abbosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Right to Talk: An Audio-Visual Transformer Approach. (arXiv:2108.03256v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03256","description":"<p>Turn-taking has played an essential role in structuring the regulation of a\nconversation. The task of identifying the main speaker (who is properly taking\nhis/her turn of speaking) and the interrupters (who are interrupting or\nreacting to the main speaker's utterances) remains a challenging task. Although\nsome prior methods have partially addressed this task, there still remain some\nlimitations. Firstly, a direct association of Audio and Visual features may\nlimit the correlations to be extracted due to different modalities. Secondly,\nthe relationship across temporal segments helping to maintain the consistency\nof localization, separation, and conversation contexts is not effectively\nexploited. Finally, the interactions between speakers that usually contain the\ntracking and anticipatory decisions about the transition to a new speaker are\nusually ignored. Therefore, this work introduces a new Audio-Visual Transformer\napproach to the problem of localization and highlighting the main speaker in\nboth audio and visual channels of a multi-speaker conversation video in the\nwild. The proposed method exploits different types of correlations presented in\nboth visual and audio signals. The temporal audio-visual relationships across\nspatial-temporal space are anticipated and optimized via the self-attention\nmechanism in a Transformerstructure. Moreover, a newly collected dataset is\nintroduced for the main speaker detection. To the best of our knowledge, it is\none of the first studies that is able to automatically localize and highlight\nthe main speaker in both visual and audio channels in multi-speaker\nconversation videos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, The <a href=\"http://arxiv.org/find/cs/1/au:+Vu_D/0/1/0/all/0/1\">De Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hoang Anh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"(Just) A Spoonful of Refinements Helps the Registration Error Go Down. (arXiv:2108.03257v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03257","description":"<p>We tackle data-driven 3D point cloud registration. Given point\ncorrespondences, the standard Kabsch algorithm provides an optimal rotation\nestimate. This allows to train registration models in an end-to-end manner by\ndifferentiating the SVD operation. However, given the initial rotation estimate\nsupplied by Kabsch, we show we can improve point correspondence learning during\nmodel training by extending the original optimization problem. In particular,\nwe linearize the governing constraints of the rotation matrix and solve the\nresulting linear system of equations. We then iteratively produce new solutions\nby updating the initial estimate. Our experiments show that, by plugging our\ndifferentiable layer to existing learning-based registration methods, we\nimprove the correspondence matching quality. This yields up to a 7% decrease in\nrotation error for correspondence-based data-driven registration methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agostinho_S/0/1/0/all/0/1\">S&#xe9;rgio Agostinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljo&#x161;a O&#x161;ep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation. (arXiv:2108.03267v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03267","description":"<p>Semantic segmentation aims to predict pixel-level labels. It has become a\npopular task in various computer vision applications. While fully supervised\nsegmentation methods have achieved high accuracy on large-scale vision\ndatasets, they are unable to generalize on a new test environment or a new\ndomain well. In this work, we first introduce a new Un-aligned Domain Score to\nmeasure the efficiency of a learned model on a new target domain in\nunsupervised manner. Then, we present the new Bijective Maximum\nLikelihood(BiMaL) loss that is a generalized form of the Adversarial Entropy\nMinimization without any assumption about pixel independence. We have evaluated\nthe proposed BiMaL on two domains. The proposed BiMaL approach consistently\noutperforms the SOTA methods on empirical experiments on \"SYNTHIA to\nCityscapes\", \"GTA5 to Cityscapes\", and \"SYNTHIA to Vistas\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1\">Thanh-Dat Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1\">Chi Nhan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_S/0/1/0/all/0/1\">Son Lam Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainwater_C/0/1/0/all/0/1\">Chase Rainwater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Khoa Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03272","description":"<p>Recent research in embodied AI has been boosted by the use of simulation\nenvironments to develop and train robot learning approaches. However, the use\nof simulation has skewed the attention to tasks that only require what robotics\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\nan open-source simulation environment that supports the simulation of a more\ndiverse set of household tasks through three key innovations. First, iGibson\n2.0 supports object states, including temperature, wetness level, cleanliness\nlevel, and toggled and sliced states, necessary to cover a wider range of\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\nit. This functionality can generate potentially infinite instances of tasks\nwith minimal effort from the users. The sampling mechanism allows our scenes to\nbe more densely populated with small objects in semantically meaningful\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\nimmerse humans in its scenes to collect demonstrations. As a result, we can\ncollect demonstrations from humans on these new types of tasks, and use them\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\nenable robot learning of novel tasks, in the hope of demonstrating the\npotential of this new simulator to support new research in embodied AI. iGibson\n2.0 and its new dataset will be publicly available at\n<a href=\"http://svl.stanford.edu/igibson/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1\">Gokul Dharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation and Object Detection Towards Instance Segmentation: Breast Tumor Identification. (arXiv:2108.03287v1 [cs.AI])","link":"http://arxiv.org/abs/2108.03287","description":"<p>Breast cancer is one of the factors that cause the increase of mortality of\nwomen. The most widely used method for diagnosing this geological disease i.e.\nbreast cancer is the ultrasound scan. Several key features such as the\nsmoothness and the texture of the tumor captured through ultrasound scans\nencode the abnormality of the breast tumors (malignant from benign). However,\nultrasound scans are often noisy and include irrelevant parts of the breast\nthat may bias the segmentation of eventual tumors. In this paper, we are going\nto extract the region of interest ( i.e, bounding boxes of the tumors) and\nfeed-forward them to one semantic segmentation encoder-decoder structure based\non its classification (i.e, malignant or benign). the whole process aims to\nbuild an instance-based segmenter from a semantic segmenter and an object\ndetector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mejri_M/0/1/0/all/0/1\">Mohamed Mejri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mejri_A/0/1/0/all/0/1\">Aymen Mejri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mejri_O/0/1/0/all/0/1\">Oumayma Mejri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fekih_C/0/1/0/all/0/1\">Chiraz Fekih</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical image segmentation with imperfect 3D bounding boxes. (arXiv:2108.03300v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03300","description":"<p>The development of high quality medical image segmentation algorithms depends\non the availability of large datasets with pixel-level labels. The challenges\nof collecting such datasets, especially in case of 3D volumes, motivate to\ndevelop approaches that can learn from other types of labels that are cheap to\nobtain, e.g. bounding boxes. We focus on 3D medical images with their\ncorresponding 3D bounding boxes which are considered as series of per-slice\nnon-tight 2D bounding boxes. While current weakly-supervised approaches that\nuse 2D bounding boxes as weak labels can be applied to medical image\nsegmentation, we show that their success is limited in cases when the\nassumption about the tightness of the bounding boxes breaks. We propose a new\nbounding box correction framework which is trained on a small set of\npixel-level annotations to improve the tightness of a larger set of non-tight\nbounding box annotations. The effectiveness of our solution is demonstrated by\nevaluating a known weakly-supervised segmentation approach with and without the\nproposed bounding box correction algorithm. When the tightness is improved by\nour solution, the results of the weakly-supervised segmentation become much\ncloser to those of the fully-supervised one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Redekop_E/0/1/0/all/0/1\">Ekaterina Redekop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Alexey Chernyavskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-Supervised Action Modality Transfer. (arXiv:2108.03329v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03329","description":"<p>This paper strives for action recognition and detection in video modalities\nlike RGB, depth maps or 3D-skeleton sequences when only limited\nmodality-specific labeled examples are available. For the RGB, and derived\noptical-flow, modality many large-scale labeled datasets have been made\navailable. They have become the de facto pre-training choice when recognizing\nor detecting new actions from RGB datasets that have limited amounts of labeled\nexamples available. Unfortunately, large-scale labeled action datasets for\nother modalities are unavailable for pre-training. In this paper, our goal is\nto recognize actions from limited examples in non-RGB video modalities, by\nlearning from large-scale labeled RGB data. To this end, we propose a two-step\ntraining process: (i) we extract action representation knowledge from an\nRGB-trained teacher network and adapt it to a non-RGB student network. (ii) we\nthen fine-tune the transfer model with available labeled examples of the target\nmodality. For the knowledge transfer we introduce feature-supervision\nstrategies, which rely on unlabeled pairs of two modalities (the RGB and the\ntarget modality) to transfer feature level representations from the teacher to\nthe student network. Ablations and generalizations with two RGB source datasets\nand two non-RGB target datasets demonstrate that an optical-flow teacher\nprovides better action transfer features than RGB for both depth maps and\n3D-skeletons, even when evaluated on a different target domain, or for a\ndifferent task. Compared to alternative cross-modal action transfer methods we\nshow a good improvement in performance especially when labeled non-RGB examples\nto learn from are scarce\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thoker_F/0/1/0/all/0/1\">Fida Mohammad Thoker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments. (arXiv:2108.03332v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03332","description":"<p>We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in\nsimulation, spanning a range of everyday household chores such as cleaning,\nmaintenance, and food preparation. These activities are designed to be\nrealistic, diverse, and complex, aiming to reproduce the challenges that agents\nmust face in the real world. Building such a benchmark poses three fundamental\ndifficulties for each activity: definition (it can differ by time, place, or\nperson), instantiation in a simulator, and evaluation. BEHAVIOR addresses these\nwith three innovations. First, we propose an object-centric, predicate\nlogic-based description language for expressing an activity's initial and goal\nconditions, enabling generation of diverse instances for any activity. Second,\nwe identify the simulator-agnostic features required by an underlying\nenvironment to support BEHAVIOR, and demonstrate its realization in one such\nsimulator. Third, we introduce a set of metrics to measure task progress and\nefficiency, absolute and relative to human demonstrators. We include 500 human\ndemonstrations in virtual reality (VR) to serve as the human ground truth. Our\nexperiments demonstrate that even state of the art embodied AI solutions\nstruggle with the level of realism, diversity, and complexity imposed by the\nactivities in our benchmark. We make BEHAVIOR publicly available at\nbehavior.stanford.edu to facilitate and calibrate the development of new\nembodied AI solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zheng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buch_S/0/1/0/all/0/1\">Shyamal Buch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Geo-localization Using Satellite Imagery and Topography for Unmanned Aerial Vehicles. (arXiv:2108.03344v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03344","description":"<p>The capabilities of autonomous flight with unmanned aerial vehicles (UAVs)\nhave significantly increased in recent times. However, basic problems such as\nfast and robust geo-localization in GPS-denied environments still remain\nunsolved. Existing research has primarily concentrated on improving the\naccuracy of localization at the cost of long and varying computation time in\nvarious situations, which often necessitates the use of powerful ground station\nmachines. In order to make image-based geo-localization online and pragmatic\nfor lightweight embedded systems on UAVs, we propose a framework that is\nreliable in changing scenes, flexible about computing resource allocation and\nadaptable to common camera placements. The framework is comprised of two\nstages: offline database preparation and online inference. At the first stage,\ncolor images and depth maps are rendered as seen from potential vehicle poses\nquantized over the satellite and topography maps of anticipated flying areas. A\ndatabase is then populated with the global and local descriptors of the\nrendered images. At the second stage, for each captured real-world query image,\ntop global matches are retrieved from the database and the vehicle pose is\nfurther refined via local descriptor matching. We present field experiments of\nimage-based localization on two different UAV platforms to validate our\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1\">Mark W. Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sreenath_K/0/1/0/all/0/1\">Koushil Sreenath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation. (arXiv:2108.03372v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03372","description":"<p>In object re-identification (ReID), the development of deep learning\ntechniques often involves model update and deployment. It is unbearable to\nre-extract image features of the large-scale gallery when deploying new models.\nTherefore, backward-compatible representation is proposed to enable the \"new\"\nfeatures compatible with \"old\"' features, free from the re-extracting process.\nThe existing backward-compatible methods simply conduct constraints in the\nembedding space or discriminative space and ignore the intra-class variance of\nthe old embeddings, resulting in a risk of damaging the discriminability of new\nembeddings.\n</p>\n<p>In this work, we propose a Neighborhood Consensus Contrastive Learning (NCCL)\nmethod, which learns backward-compatible representation from a neighborhood\nconsensus perspective with both embedding structures and discriminative\nknowledge. With NCCL, the new embeddings are aligned and improved with old\nembeddings in a multi-cluster view. Besides, we also propose a scheme to filter\nthe old embeddings with low credibility, which can further improve the\ncompatibility robustness. Our method ensures backward compatibility without\nimpairing the accuracy of the new model. And it can even improve the new\nmodel's accuracy in most scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YanBai/0/1/0/all/0/1\">YanBai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lingyu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Action Localization Using Gated Recurrent Units. (arXiv:2108.03375v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03375","description":"<p>Temporal Action Localization (TAL) task in which the aim is to predict the\nstart and end of each action and its class label has many applications in the\nreal world. But due to its complexity, researchers have not reached great\nresults compared to the action recognition task. The complexity is related to\npredicting precise start and end times for different actions in any video. In\nthis paper, we propose a new network based on Gated Recurrent Unit (GRU) and\ntwo novel post-processing ideas for TAL task. Specifically, we propose a new\ndesign for the output layer of the GRU resulting in the so-called GRU-Splitted\nmodel. Moreover, linear interpolation is used to generate the action proposals\nwith precise start and end times. Finally, to rank the generated proposals\nappropriately, we use a Learn to Rank (LTR) approach. We evaluated the\nperformance of the proposed method on Thumos14 dataset. Results show the\nsuperiority of the performance of the proposed method compared to\nstate-of-the-art. Especially in the mean Average Precision (mAP) metric at\nIntersection over Union (IoU) 0.7, we get 27.52% which is 5.12% better than\nthat of state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khojasteh_H/0/1/0/all/0/1\">Hassan Keshvari Khojasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzade_H/0/1/0/all/0/1\">Hoda Mohammadzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behroozi_H/0/1/0/all/0/1\">Hamid Behroozi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Indoor Layouts from Simple Point-Clouds. (arXiv:2108.03378v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03378","description":"<p>Reconstructing a layout of indoor spaces has been a crucial part of growing\nindoor location based services. One of the key challenges in the proliferation\nof indoor location based services is the unavailability of indoor spatial maps\ndue to the complex nature of capturing an indoor space model (e.g., floor plan)\nof an existing building. In this paper, we propose a system to automatically\ngenerate floor plans that can recognize rooms from the point-clouds obtained\nthrough smartphones like Google's Tango. In particular, we propose two\napproaches - a Recurrent Neural Network based approach using Pointer Network\nand a Convolutional Neural Network based approach using Mask-RCNN to identify\nrooms (and thereby floor plans) from point-clouds. Experimental results on\ndifferent datasets demonstrate approximately 0.80-0.90 Intersection-over-Union\nscores, which show that our models can effectively identify the rooms and\nregenerate the shapes of the rooms in heterogeneous environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_M/0/1/0/all/0/1\">Md. Tareq Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohammed Eunus Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Categorized Reflection Removal Dataset with Diverse Real-world Scenes. (arXiv:2108.03380v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03380","description":"<p>Due to the lack of a large-scale reflection removal dataset with diverse\nreal-world scenes, many existing reflection removal methods are trained on\nsynthetic data plus a small amount of real-world data, which makes it difficult\nto evaluate the strengths or weaknesses of different reflection removal methods\nthoroughly. Furthermore, existing real-world benchmarks and datasets do not\ncategorize image data based on the types and appearances of reflection (e.g.,\nsmoothness, intensity), making it hard to analyze reflection removal methods.\nHence, we construct a new reflection removal dataset that is categorized,\ndiverse, and real-world (CDR). A pipeline based on RAW data is used to capture\nperfectly aligned input images and transmission images. The dataset is\nconstructed using diverse glass types under various environments to ensure\ndiversity. By analyzing several reflection removal methods and conducting\nextensive experiments on our dataset, we show that state-of-the-art reflection\nremoval methods generally perform well on blurry reflection but fail in\nobtaining satisfying performance on other types of real-world reflection. We\nbelieve our dataset can help develop novel methods to remove real-world\nreflection better. Our dataset is available at\nhttps://alexzhao-hugga.github.io/Real-World-Reflection-Removal/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuhua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chenyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yankun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Bottleneck Approach to Spatial Attention Learning. (arXiv:2108.03418v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03418","description":"<p>The selective visual attention mechanism in the human visual system (HVS)\nrestricts the amount of information to reach visual awareness for perceiving\nnatural scenes, allowing near real-time information processing with limited\ncomputational capacity [Koch and Ullman, 1987]. This kind of selectivity acts\nas an 'Information Bottleneck (IB)', which seeks a trade-off between\ninformation compression and predictive accuracy. However, such information\nconstraints are rarely explored in the attention mechanism for deep neural\nnetworks (DNNs). In this paper, we propose an IB-inspired spatial attention\nmodule for DNN structures built for visual recognition. The module takes as\ninput an intermediate representation of the input image, and outputs a\nvariational 2D attention map that minimizes the mutual information (MI) between\nthe attention-modulated representation and the input, while maximizing the MI\nbetween the attention-modulated representation and the task label. To further\nrestrict the information bypassed by the attention map, we quantize the\ncontinuous attention scores to a set of learnable anchor values during\ntraining. Extensive experiments show that the proposed IB-inspired spatial\nattention mechanism can yield attention maps that neatly highlight the regions\nof interest while suppressing backgrounds, and bootstrap standard DNN\nstructures for visual recognition tasks (e.g., image classification,\nfine-grained recognition, cross-domain classification). The attention maps are\ninterpretable for the decision making of the DNNs as verified in the\nexperiments. Our code is available at https://github.com/ashleylqx/AIB.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_Q/0/1/0/all/0/1\">Qiuxia Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Ailing Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hanqiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Facial Representations from the Cycle-consistency of Face. (arXiv:2108.03427v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03427","description":"<p>Faces manifest large variations in many aspects, such as identity,\nexpression, pose, and face styling. Therefore, it is a great challenge to\ndisentangle and extract these characteristics from facial images, especially in\nan unsupervised manner. In this work, we introduce cycle-consistency in facial\ncharacteristics as free supervisory signal to learn facial representations from\nunlabeled facial images. The learning is realized by superimposing the facial\nmotion cycle-consistency and identity cycle-consistency constraints. The main\nidea of the facial motion cycle-consistency is that, given a face with\nexpression, we can perform de-expression to a neutral face via the removal of\nfacial motion and further perform re-expression to reconstruct back to the\noriginal face. The main idea of the identity cycle-consistency is to exploit\nboth de-identity into mean face by depriving the given neutral face of its\nidentity via feature re-normalization and re-identity into neutral face by\nadding the personal attributes to the mean face. At training time, our model\nlearns to disentangle two distinct facial representations to be useful for\nperforming cycle-consistent face reconstruction. At test time, we use the\nlinear protocol scheme for evaluating facial representations on various tasks,\nincluding facial expression recognition and head pose regression. We also can\ndirectly apply the learnt facial representations to person recognition,\nfrontalization and image-to-image translation. Our experiments show that the\nresults of our approach is competitive with those of existing methods,\ndemonstrating the rich and unique information embedded in the disentangled\nrepresentations. Code is available at https://github.com/JiaRenChang/FaceCycle .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jia-Ren Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong-Sheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSViT: Better Vision Transformer via Token Pooling and Attention Sharing. (arXiv:2108.03428v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03428","description":"<p>In this paper, we observe two levels of redundancies when applying vision\ntransformers (ViT) for image recognition. First, fixing the number of tokens\nthrough the whole network produces redundant features at the spatial level.\nSecond, the attention maps among different transformer layers are redundant.\nBased on the observations above, we propose a PSViT: a ViT with token Pooling\nand attention Sharing to reduce the redundancy, effectively enhancing the\nfeature representation ability, and achieving a better speed-accuracy\ntrade-off. Specifically, in our PSViT, token pooling can be defined as the\noperation that decreases the number of tokens at the spatial level. Besides,\nattention sharing will be built between the neighboring transformer layers for\nreusing the attention maps having a strong correlation among adjacent layers.\nThen, a compact set of the possible combinations for different token pooling\nand attention sharing mechanisms are constructed. Based on the proposed compact\nset, the number of tokens in each layer and the choices of layers sharing\nattention can be treated as hyper-parameters that are learned from data\nautomatically. Experimental results show that the proposed scheme can achieve\nup to 6.6% accuracy improvement in ImageNet classification compared with the\nDeiT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing MR Image Segmentation with Realistic Adversarial Data Augmentation. (arXiv:2108.03429v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03429","description":"<p>The success of neural networks on medical image segmentation tasks typically\nrelies on large labeled datasets for model training. However, acquiring and\nmanually labeling a large medical image set is resource-intensive, expensive,\nand sometimes impractical due to data sharing and privacy issues. To address\nthis challenge, we propose an adversarial data augmentation approach to improve\nthe efficiency in utilizing training data and to enlarge the dataset via\nsimulated but realistic transformations. Specifically, we present a generic\ntask-driven learning framework, which jointly optimizes a data augmentation\nmodel and a segmentation network during training, generating informative\nexamples to enhance network generalizability for the downstream task. The data\naugmentation model utilizes a set of photometric and geometric image\ntransformations and chains them to simulate realistic complex imaging\nvariations that could exist in magnetic resonance (MR) imaging. The proposed\nadversarial data augmentation does not rely on generative networks and can be\nused as a plug-in module in general segmentation networks. It is\ncomputationally efficient and applicable for both supervised and\nsemi-supervised learning. We analyze and evaluate the method on two MR image\nsegmentation tasks: cardiac segmentation and prostate segmentation. Results\nshow that the proposed approach can alleviate the need for labeled data while\nimproving model generalization ability, indicating its practical value in\nmedical imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Huaqi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarroni_G/0/1/0/all/0/1\">Giacomo Tarroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models. (arXiv:2108.03434v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03434","description":"<p>Fine-tuning from pre-trained ImageNet models has been a simple, effective,\nand popular approach for various computer vision tasks. The common practice of\nfine-tuning is to adopt a default hyperparameter setting with a fixed\npre-trained model, while both of them are not optimized for specific tasks and\ntime constraints. Moreover, in cloud computing or GPU clusters where the tasks\narrive sequentially in a stream, faster online fine-tuning is a more desired\nand realistic strategy for saving money, energy consumption, and CO2 emission.\nIn this paper, we propose a joint Neural Architecture Search and Online\nAdaption framework named NASOA towards a faster task-oriented fine-tuning upon\nthe request of users. Specifically, NASOA first adopts an offline NAS to\nidentify a group of training-efficient networks to form a pretrained model zoo.\nWe propose a novel joint block and macro-level search space to enable a\nflexible and efficient search. Then, by estimating fine-tuning performance via\nan adaptive model by accumulating experience from the past tasks, an online\nschedule generator is proposed to pick up the most suitable model and generate\na personalized training regime with respect to each desired task in a one-shot\nfashion. The resulting model zoo is more training efficient than SOTA models,\ne.g. 6x faster than RegNetY-16GF, and 1.7x faster than EfficientNetB3.\nExperiments on multiple datasets also show that NASOA achieves much better\nfine-tuning results, i.e. improving around 2.1% accuracy than the best\nperformance in RegNet series under various constraints and tasks; 40x faster\ncompared to the BOHB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_N/0/1/0/all/0/1\">Ning Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gengwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chuanlong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Discriminative Representation Learning for Unsupervised Person Re-identification. (arXiv:2108.03439v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03439","description":"<p>In this work, we address the problem of unsupervised domain adaptation for\nperson re-ID where annotations are available for the source domain but not for\ntarget. Previous methods typically follow a two-stage optimization pipeline,\nwhere the network is first pre-trained on source and then fine-tuned on target\nwith pseudo labels created by feature clustering. Such methods sustain two main\nlimitations. (1) The label noise may hinder the learning of discriminative\nfeatures for recognizing target classes. (2) The domain gap may hinder\nknowledge transferring from source to target. We propose three types of\ntechnical schemes to alleviate these issues. First, we propose a cluster-wise\ncontrastive learning algorithm (CCL) by iterative optimization of feature\nlearning and cluster refinery to learn noise-tolerant representations in the\nunsupervised manner. Second, we adopt a progressive domain adaptation (PDA)\nstrategy to gradually mitigate the domain gap between source and target data.\nThird, we propose Fourier augmentation (FA) for further maximizing the class\nseparability of re-ID models by imposing extra constraints in the Fourier\nspace. We observe that these proposed schemes are capable of facilitating the\nlearning of discriminative feature representations. Experiments demonstrate\nthat our method consistently achieves notable improvements over the\nstate-of-the-art unsupervised re-ID methods on multiple benchmarks, e.g.,\nsurpassing MMT largely by 8.1\\%, 9.9\\%, 11.4\\% and 11.1\\% mAP on the\nMarket-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT tasks,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable Image Registration using Neural ODEs. (arXiv:2108.03443v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03443","description":"<p>Deformable image registration, aiming to find spatial correspondence between\na given image pair, is one of the most critical problems in the domain of\nmedical image analysis. In this paper, we present a generic, fast, and accurate\ndiffeomorphic image registration framework that leverages neural ordinary\ndifferential equations (NODEs). We model each voxel as a moving particle and\nconsider the set of all voxels in a 3D image as a high-dimensional dynamical\nsystem whose trajectory determines the targeted deformation field. Compared\nwith traditional optimization-based methods, our framework reduces the running\ntime from tens of minutes to tens of seconds. Compared with recent data-driven\ndeep learning methods, our framework is more accessible since it does not\nrequire large amounts of training data. Our experiments show that the\nregistration results of our method outperform state-of-the-arts under various\nmetrics, indicating that our modeling approach is well fitted for the task of\ndeformable image registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yifan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z.Jiahao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiancong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A.Yushkevich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">James C.Gee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">M.Ani Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Waterdrop Removal with Row-wise Dilated Attention. (arXiv:2108.03457v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03457","description":"<p>Existing vision systems for autonomous driving or robots are sensitive to\nwaterdrops adhered to windows or camera lenses. Most recent waterdrop removal\napproaches take a single image as input and often fail to recover the missing\ncontent behind waterdrops faithfully. Thus, we propose a learning-based model\nfor waterdrop removal with stereo images. To better detect and remove\nwaterdrops from stereo images, we propose a novel row-wise dilated attention\nmodule to enlarge attention's receptive field for effective information\npropagation between the two stereo images. In addition, we propose an attention\nconsistency loss between the ground-truth disparity map and attention scores to\nenhance the left-right consistency in stereo images. Because of related\ndatasets' unavailability, we collect a real-world dataset that contains stereo\nimages with and without waterdrops. Extensive experiments on our dataset\nsuggest that our model outperforms state-of-the-art methods both quantitatively\nand qualitatively. Our source code and the stereo waterdrop dataset are\navailable at\n\\href{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zifan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Portrait Shadow Removal via Generative Priors. (arXiv:2108.03466v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03466","description":"<p>Portrait images often suffer from undesirable shadows cast by casual objects\nor even the face itself. While existing methods for portrait shadow removal\nrequire training on a large-scale synthetic dataset, we propose the first\nunsupervised method for portrait shadow removal without any training data. Our\nkey idea is to leverage the generative facial priors embedded in the\noff-the-shelf pretrained StyleGAN2. To achieve this, we formulate the shadow\nremoval task as a layer decomposition problem: a shadowed portrait image is\nconstructed by the blending of a shadow image and a shadow-free image. We\npropose an effective progressive optimization algorithm to learn the\ndecomposition process. Our approach can also be extended to portrait tattoo\nremoval and watermark removal. Qualitative and quantitative experiments on a\nreal-world portrait shadow dataset demonstrate that our approach achieves\ncomparable performance with supervised shadow removal methods. Our source code\nis available at\nhttps://github.com/YingqingHe/Shadow-Removal-via-Generative-Priors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yingqing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yazhou Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A distillation based approach for the diagnosis of diseases. (arXiv:2108.03470v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03470","description":"<p>Presently, Covid-19 is a serious threat to the world at large. Efforts are\nbeing made to reduce disease screening times and in the development of a\nvaccine to resist this disease, even as thousands succumb to it everyday. We\npropose a novel method of automated screening of diseases like Covid-19 and\npneumonia from Chest X-Ray images with the help of Computer Vision. Unlike\ncomputer vision classification algorithms which come with heavy computational\ncosts, we propose a knowledge distillation based approach which allows us to\nbring down the model depth, while preserving the accuracy. We make use of an\naugmentation of the standard distillation module with an auxiliary intermediate\nassistant network that aids in the continuity of the flow of information.\nFollowing this approach, we are able to build an extremely light student\nnetwork, consisting of just 3 convolutional blocks without any compromise on\naccuracy. We thus propose a method of classification of diseases which can not\nonly lead to faster screening, but can also operate seamlessly on low-end\ndevices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_H/0/1/0/all/0/1\">Hmrishav Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dastidar_S/0/1/0/all/0/1\">Shuvayan Ghosh Dastidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_B/0/1/0/all/0/1\">Bisakh Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1\">Biplab Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1\">Nibaran Das</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Aliasing on Generalization in Deep Convolutional Networks. (arXiv:2108.03489v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03489","description":"<p>We investigate the impact of aliasing on generalization in Deep Convolutional\nNetworks and show that data augmentation schemes alone are unable to prevent it\ndue to structural limitations in widely used architectures. Drawing insights\nfrom frequency analysis theory, we take a closer look at ResNet and\nEfficientNet architectures and review the trade-off between aliasing and\ninformation loss in each of their major components. We show how to mitigate\naliasing by inserting non-trainable low-pass filters at key locations,\nparticularly where networks lack the capacity to learn them. These simple\narchitectural changes lead to substantial improvements in generalization on\ni.i.d. and even more on out-of-distribution conditions, such as image\nclassification under natural corruptions on ImageNet-C [11] and few-shot\nlearning on Meta-Dataset [26]. State-of-the art results are achieved on both\ndatasets without introducing additional trainable parameters and using the\ndefault hyper-parameters of open source codebases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1\">Rob Romijnders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1\">Ross Goroshin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Foveated Reconstruction to Preserve Perceived Image Statistics. (arXiv:2108.03499v1 [cs.GR])","link":"http://arxiv.org/abs/2108.03499","description":"<p>Foveated image reconstruction recovers full image from a sparse set of\nsamples distributed according to the human visual system's retinal sensitivity\nthat rapidly drops with eccentricity. Recently, the use of Generative\nAdversarial Networks was shown to be a promising solution for such a task as\nthey can successfully hallucinate missing image information. Like for other\nsupervised learning approaches, also for this one, the definition of the loss\nfunction and training strategy heavily influences the output quality. In this\nwork, we pose the question of how to efficiently guide the training of foveated\nreconstruction techniques such that they are fully aware of the human visual\nsystem's capabilities and limitations, and therefore, reconstruct visually\nimportant image features. Due to the nature of GAN-based solutions, we\nconcentrate on the human's sensitivity to hallucination for different input\nsample densities. We present new psychophysical experiments, a dataset, and a\nprocedure for training foveated image reconstruction. The strategy provides\nflexibility to the generator network by penalizing only perceptually important\ndeviations in the output. As a result, the method aims to preserve perceived\nimage statistics rather than natural image statistics. We evaluate our strategy\nand compare it to alternative solutions using a newly trained objective metric\nand user experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Surace_L/0/1/0/all/0/1\">Luca Surace</a> (Universit&#xe0; della Svizzera italiana), <a href=\"http://arxiv.org/find/cs/1/au:+Wernikowski_M/0/1/0/all/0/1\">Marek Wernikowski</a> (West Pomeranian University of Technology), <a href=\"http://arxiv.org/find/cs/1/au:+Tursun_O/0/1/0/all/0/1\">Okan Tursun</a> (Universit&#xe0; della Svizzera italiana), <a href=\"http://arxiv.org/find/cs/1/au:+Myszkowski_K/0/1/0/all/0/1\">Karol Myszkowski</a> (Max Planck Institute for Informatics), <a href=\"http://arxiv.org/find/cs/1/au:+Mantiuk_R/0/1/0/all/0/1\">Rados&#x142;aw Mantiuk</a> (West Pomeranian University of Technology), <a href=\"http://arxiv.org/find/cs/1/au:+Didyk_P/0/1/0/all/0/1\">Piotr Didyk</a> (Universit&#xe0; della Svizzera italiana)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepFH Segmentations for Superpixel-based Object Proposal Refinement. (arXiv:2108.03503v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03503","description":"<p>Class-agnostic object proposal generation is an important first step in many\nobject detection pipelines. However, object proposals of modern systems are\nrather inaccurate in terms of segmentation and only roughly adhere to object\nboundaries. Since typical refinement steps are usually not applicable to\nthousands of proposals, we propose a superpixel-based refinement system for\nobject proposal generation systems. Utilizing precise superpixels and\nsuperpixel pooling on deep features, we refine initial coarse proposals in an\nend-to-end learned system. Furthermore, we propose a novel DeepFH segmentation,\nwhich enriches the classic Felzenszwalb and Huttenlocher (FH) segmentation with\ndeep features leading to improved segmentation results and better object\nproposal refinements. On the COCO dataset with LVIS annotations, we show that\nour refinement based on DeepFH superpixels outperforms state-of-the-art methods\nand leads to more precise object proposals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilms_C/0/1/0/all/0/1\">Christian Wilms</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frintrop_S/0/1/0/all/0/1\">Simone Frintrop</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Membership Inference Attacks on Lottery Ticket Networks. (arXiv:2108.03506v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03506","description":"<p>The vulnerability of the Lottery Ticket Hypothesis has not been studied from\nthe purview of Membership Inference Attacks. Through this work, we are the\nfirst to empirically show that the lottery ticket networks are equally\nvulnerable to membership inference attacks. A Membership Inference Attack (MIA)\nis the process of determining whether a data sample belongs to a training set\nof a trained model or not. Membership Inference Attacks could leak critical\ninformation about the training data that can be used for targeted attacks.\nRecent deep learning models often have very large memory footprints and a high\ncomputational cost associated with training and drawing inferences. Lottery\nTicket Hypothesis is used to prune the networks to find smaller sub-networks\nthat at least match the performance of the original model in terms of test\naccuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and\nImageNet datasets to perform image classification tasks and observe that the\nattack accuracies are similar. We also see that the attack accuracy varies\ndirectly according to the number of classes in the dataset and the sparsity of\nthe network. We demonstrate that these attacks are transferable across models\nwith high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagmar_A/0/1/0/all/0/1\">Aadesh Bagmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiya_S/0/1/0/all/0/1\">Shishira R Maiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidwalka_S/0/1/0/all/0/1\">Shruti Bidwalka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Amol Deshpande</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContinuityLearner: Geometric Continuity Feature Learning for Lane Segmentation. (arXiv:2108.03507v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03507","description":"<p>Lane segmentation is a challenging issue in autonomous driving system\ndesigning because lane marks show weak textural consistency due to occlusion or\nextreme illumination but strong geometric continuity in traffic images, from\nwhich general convolution neural networks (CNNs) are not capable of learning\nsemantic objects. To empower conventional CNNs in learning geometric clues of\nlanes, we propose a deep network named ContinuityLearner to better learn\ngeometric prior within lane. Specifically, our proposed CNN-based paradigm\ninvolves a novel Context-encoding image feature learning network to generate\nclass-dependent image feature maps and a new encoding layer to exploit the\ngeometric continuity feature representation by fusing both spatial and visual\ninformation of lane together. The ContinuityLearner, performing on the\ngeometric continuity feature of lanes, is trained to directly predict the lane\nin traffic scenarios with integrated and continuous instance semantic. The\nexperimental results on the CULane dataset and the Tusimple benchmark\ndemonstrate that our ContinuityLearner has superior performance over other\nstate-of-the-art techniques in lane segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Haoyu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Annotating Load: Active Learning with Synthetic Images in Surgical Instrument Segmentation. (arXiv:2108.03534v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03534","description":"<p>Accurate instrument segmentation in endoscopic vision of robot-assisted\nsurgery is challenging due to reflection on the instruments and frequent\ncontacts with tissue. Deep neural networks (DNN) show competitive performance\nand are in favor in recent years. However, the hunger of DNN for labeled data\nposes a huge workload of annotation. Motivated by alleviating this workload, we\npropose a general embeddable method to decrease the usage of labeled real\nimages, using active generated synthetic images. In each active learning\niteration, the most informative unlabeled images are first queried by active\nlearning and then labeled. Next, synthetic images are generated based on these\nselected images. The instruments and backgrounds are cropped out and randomly\ncombined with each other with blending and fusion near the boundary. The\neffectiveness of the proposed method is validated on 2 sinus surgery datasets\nand 1 intraabdominal surgery dataset. The results indicate a considerable\nimprovement in performance, especially when the budget for annotation is small.\nThe effectiveness of different types of synthetic images, blending methods, and\nexternal background are also studied. All the code is open-sourced at:\nhttps://github.com/HaonanPeng/active_syn_generator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Haonan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_D/0/1/0/all/0/1\">Daniel King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yun-Hsuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bly_R/0/1/0/all/0/1\">Randall A. Bly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moe_K/0/1/0/all/0/1\">Kris S. Moe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hannaford_B/0/1/0/all/0/1\">Blake Hannaford</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution. (arXiv:2108.03541v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03541","description":"<p>Images tell powerful stories but cannot always be trusted. Matching images\nback to trusted sources (attribution) enables users to make a more informed\njudgment of the images they encounter online. We propose a robust image hashing\nalgorithm to perform such matching. Our hash is sensitive to manipulation of\nsubtle, salient visual details that can substantially change the story told by\nan image. Yet the hash is invariant to benign transformations (changes in\nquality, codecs, sizes, shapes, etc.) experienced by images during online\nredistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph\nAttention for Image Attribution Network); a robust image hashing model inspired\nby recent successes of Transformers in the visual domain. OSCAR-Net constructs\na scene graph representation that attends to fine-grained changes of every\nobject's visual appearance and their spatial relationships. The network is\ntrained via contrastive learning on a dataset of original and manipulated\nimages yielding a state of the art image hash for content fingerprinting that\nscales to millions of images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1\">Eric Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Vishy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Attention Mechanism and Knowledge Distillation for Lip Reading. (arXiv:2108.03543v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03543","description":"<p>Despite the advancement in the domain of audio and audio-visual speech\nrecognition, visual speech recognition systems are still quite under-explored\ndue to the visual ambiguity of some phonemes. In this work, we propose a new\nlip-reading model that combines three contributions. First, the model front-end\nadopts a spatio-temporal attention mechanism to help extract the informative\ndata from the input visual frames. Second, the model back-end utilizes a\nsequence-level and frame-level Knowledge Distillation (KD) techniques that\nallow leveraging audio data during the visual model training. Third, a data\npreprocessing pipeline is adopted that includes facial landmarks\ndetection-based lip-alignment. On LRW lip-reading dataset benchmark, a\nnoticeable accuracy improvement is demonstrated; the spatio-temporal attention,\nKnowledge Distillation, and lip-alignment contributions achieved 88.43%,\n88.64%, and 88.37% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elashmawy_S/0/1/0/all/0/1\">Shahd Elashmawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramsis_M/0/1/0/all/0/1\">Marian Ramsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1\">Hesham M. Eraqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldeshnawy_F/0/1/0/all/0/1\">Farah Eldeshnawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mabrouk_H/0/1/0/all/0/1\">Hadeel Mabrouk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abugabal_O/0/1/0/all/0/1\">Omar Abugabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakr_N/0/1/0/all/0/1\">Nourhan Sakr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled High Quality Salient Object Detection. (arXiv:2108.03551v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03551","description":"<p>Aiming at discovering and locating most distinctive objects from visual\nscenes, salient object detection (SOD) plays an essential role in various\ncomputer vision systems. Coming to the era of high resolution, SOD methods are\nfacing new challenges. The major limitation of previous methods is that they\ntry to identify the salient regions and estimate the accurate objects\nboundaries simultaneously with a single regression task at low-resolution. This\npractice ignores the inherent difference between the two difficult problems,\nresulting in poor detection quality. In this paper, we propose a novel deep\nlearning framework for high-resolution SOD task, which disentangles the task\ninto a low-resolution saliency classification network (LRSCN) and a\nhigh-resolution refinement network (HRRN). As a pixel-wise classification task,\nLRSCN is designed to capture sufficient semantics at low-resolution to identify\nthe definite salient, background and uncertain image regions. HRRN is a\nregression task, which aims at accurately refining the saliency value of pixels\nin the uncertain region to preserve a clear object boundary at high-resolution\nwith limited GPU memory. It is worth noting that by introducing uncertainty\ninto the training process, our HRRN can well address the high-resolution\nrefinement task without using any high-resolution training data. Extensive\nexperiments on high-resolution saliency datasets as well as some widely used\nsaliency benchmarks show that the proposed method achieves superior performance\ncompared to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lv Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mofei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adversarial Disentangling for Specific Domain Adaptation. (arXiv:2108.03553v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03553","description":"<p>Domain adaptation aims to bridge the domain shifts between the source and\ntarget domains. These shifts may span different dimensions such as fog,\nrainfall, etc. However, recent methods typically do not consider explicit prior\nknowledge on a specific dimension, thus leading to less desired adaptation\nperformance. In this paper, we study a practical setting called Specific Domain\nAdaptation (SDA) that aligns the source and target domains in a\ndemanded-specific dimension. Within this setting, we observe the intra-domain\ngap induced by different domainness (i.e., numerical magnitudes of this\ndimension) is crucial when adapting to a specific domain. To address the\nproblem, we propose a novel Self-Adversarial Disentangling (SAD) framework. In\nparticular, given a specific dimension, we first enrich the source domain by\nintroducing a domainness creator with providing additional supervisory signals.\nGuided by the created domainness, we design a self-adversarial regularizer and\ntwo loss functions to jointly disentangle the latent representations into\ndomainness-specific and domainness-invariant features, thus mitigating the\nintra-domain gap. Our method can be easily taken as a plug-and-play framework\nand does not introduce any extra costs in the inference time. We achieve\nconsistent improvements over state-of-the-art methods in both object detection\nand semantic segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Representation Learning for Rapid Intraoperative Diagnosis of Skull Base Tumors Imaged Using Stimulated Raman Histology. (arXiv:2108.03555v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03555","description":"<p>Background: Accurate diagnosis of skull base tumors is essential for\nproviding personalized surgical treatment strategies. Intraoperative diagnosis\ncan be challenging due to tumor diversity and lack of intraoperative pathology\nresources.\n</p>\n<p>Objective: To develop an independent and parallel intraoperative pathology\nworkflow that can provide rapid and accurate skull base tumor diagnoses using\nlabel-free optical imaging and artificial intelligence (AI).\n</p>\n<p>Method: We used a fiber laser-based, label-free, non-consumptive,\nhigh-resolution microscopy method ($&lt;$ 60 sec per 1 $\\times$ 1 mm$^\\text{2}$),\ncalled stimulated Raman histology (SRH), to image a consecutive, multicenter\ncohort of skull base tumor patients. SRH images were then used to train a\nconvolutional neural network (CNN) model using three representation learning\nstrategies: cross-entropy, self-supervised contrastive learning, and supervised\ncontrastive learning. Our trained CNN models were tested on a held-out,\nmulticenter SRH dataset.\n</p>\n<p>Results: SRH was able to image the diagnostic features of both benign and\nmalignant skull base tumors. Of the three representation learning strategies,\nsupervised contrastive learning most effectively learned the distinctive and\ndiagnostic SRH image features for each of the skull base tumor types. In our\nmulticenter testing set, cross-entropy achieved an overall diagnostic accuracy\nof 91.5%, self-supervised contrastive learning 83.9%, and supervised\ncontrastive learning 96.6%. Our trained model was able to identify tumor-normal\nmargins and detect regions of microscopic tumor infiltration in whole-slide SRH\nimages.\n</p>\n<p>Conclusion: SRH with AI models trained using contrastive representation\nlearning can provide rapid and accurate intraoperative diagnosis of skull base\ntumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Abhishek Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzey_J/0/1/0/all/0/1\">Joseph Linzey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rushikesh Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sung Jik Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudharsan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alber_D/0/1/0/all/0/1\">Daniel Alber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondepudi_A/0/1/0/all/0/1\">Akhil Kondepudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urias_E/0/1/0/all/0/1\">Esteban Urias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandian_B/0/1/0/all/0/1\">Balaji Pandian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Holou_W/0/1/0/all/0/1\">Wajd Al-Holou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_S/0/1/0/all/0/1\">Steve Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">B. Gregory Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heth_J/0/1/0/all/0/1\">Jason Heth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freudiger_C/0/1/0/all/0/1\">Chris Freudiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalsa_S/0/1/0/all/0/1\">Siri Khalsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacione_D/0/1/0/all/0/1\">Donato Pacione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golfinos_J/0/1/0/all/0/1\">John G. Golfinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camelo_Piragua_S/0/1/0/all/0/1\">Sandra Camelo-Piragua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orringer_D/0/1/0/all/0/1\">Daniel A. Orringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollon_T/0/1/0/all/0/1\">Todd Hollon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Mixup for Domain Adaptive Semantic Segmentation. (arXiv:2108.03557v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03557","description":"<p>Unsupervised domain adaptation (UDA) aims to adapt a model of the labeled\nsource domain to an unlabeled target domain. Although the domain shifts may\nexist in various dimensions such as appearance, textures, etc, the contextual\ndependency, which is generally shared across different domains, is neglected by\nrecent methods. In this paper, we utilize this important clue as explicit prior\nknowledge and propose end-to-end Context-Aware Mixup (CAMix) for domain\nadaptive semantic segmentation. Firstly, we design a contextual mask generation\nstrategy by leveraging accumulated spatial distributions and contextual\nrelationships. The generated contextual mask is critical in this work and will\nguide the domain mixup. In addition, we define the significance mask to\nindicate where the pixels are credible. To alleviate the over-alignment (e.g.,\nearly performance degradation), the source and target significance masks are\nmixed based on the contextual mask into the mixed significance mask, and we\nintroduce a significance-reweighted consistency loss on it. Experimental\nresults show that the proposed method outperforms the state-of-the-art methods\nby a large margin on two widely-used domain adaptation benchmarks, i.e., GTAV\n$\\rightarrow $ Cityscapes and SYNTHIA $\\rightarrow $ Cityscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhengyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guangliang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianping Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LeafMask: Towards Greater Accuracy on Leaf Segmentation. (arXiv:2108.03568v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03568","description":"<p>Leaf segmentation is the most direct and effective way for high-throughput\nplant phenotype data analysis and quantitative researches of complex traits.\nCurrently, the primary goal of plant phenotyping is to raise the accuracy of\nthe autonomous phenotypic measurement. In this work, we present the LeafMask\nneural network, a new end-to-end model to delineate each leaf region and count\nthe number of leaves, with two main components: 1) the mask assembly module\nmerging position-sensitive bases of each predicted box after non-maximum\nsuppression (NMS) and corresponding coefficients to generate original masks; 2)\nthe mask refining module elaborating leaf boundaries from the mask assembly\nmodule by the point selection strategy and predictor. In addition, we also\ndesign a novel and flexible multi-scale attention module for the dual\nattention-guided mask (DAG-Mask) branch to effectively enhance information\nexpression and produce more accurate bases. Our main contribution is to\ngenerate the final improved masks by combining the mask assembly module with\nthe mask refining module under the anchor-free instance segmentation paradigm.\nWe validate our LeafMask through extensive experiments on Leaf Segmentation\nChallenge (LSC) dataset. Our proposed model achieves the 90.09% BestDice score\noutperforming other state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruohao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Dantong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_J/0/1/0/all/0/1\">Jun Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expressive Power and Loss Surfaces of Deep Learning Models. (arXiv:2108.03579v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03579","description":"<p>The goals of this paper are two-fold. The first goal is to serve as an\nexpository tutorial on the working of deep learning models which emphasizes\ngeometrical intuition about the reasons for success of deep learning. The\nsecond goal is to complement the current results on the expressive power of\ndeep learning models and their loss surfaces with novel insights and results.\nIn particular, we describe how deep neural networks carve out manifolds\nespecially when the multiplication neurons are introduced. Multiplication is\nused in dot products and the attention mechanism and it is employed in capsule\nnetworks and self-attention based transformers. We also describe how random\npolynomial, random matrix, spin glass and computational complexity perspectives\non the loss surfaces are interconnected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1\">Simant Dube</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visible Watermark Removal via Self-calibrated Localization and Background Refinement. (arXiv:2108.03581v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03581","description":"<p>Superimposing visible watermarks on images provides a powerful weapon to cope\nwith the copyright issue. Watermark removal techniques, which can strengthen\nthe robustness of visible watermarks in an adversarial way, have attracted\nincreasing research interest. Modern watermark removal methods perform\nwatermark localization and background restoration simultaneously, which could\nbe viewed as a multi-task learning problem. However, existing approaches suffer\nfrom incomplete detected watermark and degraded texture quality of restored\nbackground. Therefore, we design a two-stage multi-task network to address the\nabove issues. The coarse stage consists of a watermark branch and a background\nbranch, in which the watermark branch self-calibrates the roughly estimated\nmask and passes the calibrated mask to background branch to reconstruct the\nwatermarked area. In the refinement stage, we integrate multi-level features to\nimprove the texture quality of watermarked area. Extensive experiments on two\ndatasets demonstrate the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jing Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fengjun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_T/0/1/0/all/0/1\">Teng Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ZiGAN: Fine-grained Chinese Calligraphy Font Generation via a Few-shot Style Transfer Approach. (arXiv:2108.03596v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03596","description":"<p>Chinese character style transfer is a very challenging problem because of the\ncomplexity of the glyph shapes or underlying structures and large numbers of\nexisted characters, when comparing with English letters. Moreover, the\nhandwriting of calligraphy masters has a more irregular stroke and is difficult\nto obtain in real-world scenarios. Recently, several GAN-based methods have\nbeen proposed for font synthesis, but some of them require numerous reference\ndata and the other part of them have cumbersome preprocessing steps to divide\nthe character into different parts to be learned and transferred separately. In\nthis paper, we propose a simple but powerful end-to-end Chinese calligraphy\nfont generation framework ZiGAN, which does not require any manual operation or\nredundant preprocessing to generate fine-grained target-style characters with\nfew-shot references. To be specific, a few paired samples from different\ncharacter styles are leveraged to attain a fine-grained correlation between\nstructures underlying different glyphs. To capture valuable style knowledge in\ntarget and strengthen the coarse-grained understanding of character content, we\nutilize multiple unpaired samples to align the feature distributions belonging\nto different character styles. By doing so, only a few target Chinese\ncalligraphy characters are needed to generated expected style transferred\ncharacters. Experiments demonstrate that our method has a state-of-the-art\ngeneralization ability in few-shot Chinese character style transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1\">Qi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bingfeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yi Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the computational demands underlying visual reasoning. (arXiv:2108.03603v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03603","description":"<p>Visual understanding requires comprehending complex visual relations between\nobjects within a scene. Here, we seek to characterize the computational demands\nfor abstract visual reasoning. We do this by systematically assessing the\nability of modern deep convolutional neural networks (CNNs) to learn to solve\nthe Synthetic Visual Reasoning Test (SVRT) challenge, a collection of\ntwenty-three visual reasoning problems. Our analysis leads to a novel taxonomy\nof visual reasoning tasks, which can be primarily explained by both the type of\nrelations (same-different vs. spatial-relation judgments) and the number of\nrelations used to compose the underlying rules. Prior cognitive neuroscience\nwork suggests that attention plays a key role in human's visual reasoning\nability. To test this, we extended the CNNs with spatial and feature-based\nattention mechanisms. In a second series of experiments, we evaluated the\nability of these attention networks to learn to solve the SVRT challenge and\nfound the resulting architectures to be much more efficient at solving the\nhardest of these visual reasoning tasks. Most importantly, the corresponding\nimprovements on individual tasks partially explained the taxonomy. Overall,\nthis work advances our understanding of visual reasoning and yields testable\nNeuroscience predictions regarding the need for feature-based vs. spatial\nattention in visual reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaishnav_M/0/1/0/all/0/1\">Mohit Vaishnav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">Remi Cadene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1\">Andrea Alamia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linsley_D/0/1/0/all/0/1\">Drew Linsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triplet Contrastive Learning for Brain Tumor Classification. (arXiv:2108.03611v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03611","description":"<p>Brain tumor is a common and fatal form of cancer which affects both adults\nand children. The classification of brain tumors into different types is hence\na crucial task, as it greatly influences the treatment that physicians will\nprescribe. In light of this, medical imaging techniques, especially those\napplying deep convolutional networks followed by a classification layer, have\nbeen developed to make possible computer-aided classification of brain tumor\ntypes. In this paper, we present a novel approach of directly learning deep\nembeddings for brain tumor types, which can be used for downstream tasks such\nas classification. Along with using triplet loss variants, our approach applies\ncontrastive learning to performing unsupervised pre-training, combined with a\nrare-case data augmentation module to effectively ameliorate the lack of data\nproblem in the brain tumor imaging analysis domain. We evaluate our method on\nan extensive brain tumor dataset which consists of 27 different tumor classes,\nout of which 13 are defined as rare. With a common encoder during all the\nexperiments, we compare our approach with a baseline classification-layer based\nmodel, and the results well prove the effectiveness of our approach across all\nmeasured metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tian Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An EM Framework for Online Incremental Learning of Semantic Segmentation. (arXiv:2108.03613v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03613","description":"<p>Incremental learning of semantic segmentation has emerged as a promising\nstrategy for visual scene interpretation in the open- world setting. However,\nit remains challenging to acquire novel classes in an online fashion for the\nsegmentation task, mainly due to its continuously-evolving semantic label\nspace, partial pixelwise ground-truth annotations, and constrained data\navailability. To ad- dress this, we propose an incremental learning strategy\nthat can fast adapt deep segmentation models without catastrophic forgetting,\nusing a streaming input data with pixel annotations on the novel classes only.\nTo this end, we develop a uni ed learning strategy based on the\nExpectation-Maximization (EM) framework, which integrates an iterative\nrelabeling strategy that lls in the missing labels and a rehearsal-based\nincremental learning step that balances the stability-plasticity of the model.\nMoreover, our EM algorithm adopts an adaptive sampling method to select\ninformative train- ing data and a class-balancing training strategy in the\nincremental model updates, both improving the e cacy of model learning. We\nvalidate our approach on the PASCAL VOC 2012 and ADE20K datasets, and the\nresults demonstrate its superior performance over the existing incremental\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiale Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monte Carlo DropBlock for Modelling Uncertainty in Object Detection. (arXiv:2108.03614v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03614","description":"<p>With the advancements made in deep learning, computer vision problems like\nobject detection and segmentation have seen a great improvement in performance.\nHowever, in many real-world applications such as autonomous driving vehicles,\nthe risk associated with incorrect predictions of objects is very high.\nStandard deep learning models for object detection such as YOLO models are\noften overconfident in their predictions and do not take into account the\nuncertainty in predictions on out-of-distribution data. In this work, we\npropose an efficient and effective approach to model uncertainty in object\ndetection and segmentation tasks using Monte-Carlo DropBlock (MC-DropBlock)\nbased inference. The proposed approach applies drop-block during training time\nand test time on the convolutional layer of the deep learning models such as\nYOLO. We show that this leads to a Bayesian convolutional neural network\ncapable of capturing the epistemic uncertainty in the model. Additionally, we\ncapture the aleatoric uncertainty using a Gaussian likelihood. We demonstrate\nthe effectiveness of the proposed approach on modeling uncertainty in object\ndetection and segmentation tasks using out-of-distribution experiments.\nExperimental results show that MC-DropBlock improves the generalization,\ncalibration, and uncertainty modeling capabilities of YOLO models in object\ndetection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deepshikha_K/0/1/0/all/0/1\">Kumari Deepshikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yelleni_S/0/1/0/all/0/1\">Sai Harsha Yelleni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1\">P.K. Srijith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1\">C Krishna Mohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPI: Multi-receptive and Parallel Integration for Salient Object Detection. (arXiv:2108.03618v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03618","description":"<p>The semantic representation of deep features is essential for image context\nunderstanding, and effective fusion of features with different semantic\nrepresentations can significantly improve the model's performance on salient\nobject detection. In this paper, a novel method called MPI is proposed for\nsalient object detection. Firstly, a multi-receptive enhancement module (MRE)\nis designed to effectively expand the receptive fields of features from\ndifferent layers and generate features with different receptive fields. MRE can\nenhance the semantic representation and improve the model's perception of the\nimage context, which enables the model to locate the salient object accurately.\nSecondly, in order to reduce the reuse of redundant information in the complex\ntop-down fusion method and weaken the differences between semantic features, a\nrelatively simple but effective parallel fusion strategy (PFS) is proposed. It\nallows multi-scale features to better interact with each other, thus improving\nthe overall performance of the model. Experimental results on multiple datasets\ndemonstrate that the proposed method outperforms state-of-the-art methods under\ndifferent evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Han Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ningzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning an Augmented RGB Representation with Cross-Modal Knowledge Distillation for Action Detection. (arXiv:2108.03619v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03619","description":"<p>In video understanding, most cross-modal knowledge distillation (KD) methods\nare tailored for classification tasks, focusing on the discriminative\nrepresentation of the trimmed videos. However, action detection requires not\nonly categorizing actions, but also localizing them in untrimmed videos.\nTherefore, transferring knowledge pertaining to temporal relations is critical\nfor this task which is missing in the previous cross-modal KD frameworks. To\nthis end, we aim at learning an augmented RGB representation for action\ndetection, taking advantage of additional modalities at training time through\nKD. We propose a KD framework consisting of two levels of distillation. On one\nhand, atomic-level distillation encourages the RGB student to learn the\nsub-representation of the actions from the teacher in a contrastive manner. On\nthe other hand, sequence-level distillation encourages the student to learn the\ntemporal knowledge from the teacher, which consists of transferring the Global\nContextual Relations and the Action Boundary Saliency. The result is an\nAugmented-RGB stream that can achieve competitive performance as the two-stream\nnetwork while using only RGB at inference time. Extensive experimental analysis\nshows that our proposed distillation framework is generic and outperforms other\npopular cross-modal distillation methods in action detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WideCaps: A Wide Attention based Capsule Network for Image Classification. (arXiv:2108.03627v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03627","description":"<p>The capsule network is a distinct and promising segment of the neural network\nfamily that drew attention due to its unique ability to maintain the\nequivariance property by preserving the spatial relationship amongst the\nfeatures. The capsule network has attained unprecedented success over image\nclassification tasks with datasets such as MNIST and affNIST by encoding the\ncharacteristic features into the capsules and building the parse-tree\nstructure. However, on the datasets involving complex foreground and background\nregions such as CIFAR-10, the performance of the capsule network is sub-optimal\ndue to its naive data routing policy and incompetence towards extracting\ncomplex features. This paper proposes a new design strategy for capsule network\narchitecture for efficiently dealing with complex images. The proposed method\nincorporates wide bottleneck residual modules and the Squeeze and Excitation\nattention blocks upheld by the modified FM routing algorithm to address the\ndefined problem. A wide bottleneck residual module facilitates extracting\ncomplex features followed by the squeeze and excitation attention block to\nenable channel-wise attention by suppressing the trivial features. This setup\nallows channel inter-dependencies at almost no computational cost, thereby\nenhancing the representation ability of capsules on complex images. We\nextensively evaluate the performance of the proposed model on three publicly\navailable datasets, namely CIFAR-10, Fashion MNIST, and SVHN, to outperform the\ntop-5 performance on CIFAR-10 and Fashion MNIST with highly competitive\nperformance on the SVHN dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+J_P/0/1/0/all/0/1\">Pawan S J</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_H/0/1/0/all/0/1\">Hemanth Sai Ram Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vani_M/0/1/0/all/0/1\">M Vani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_J/0/1/0/all/0/1\">Jeny Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor-free 3D Single Stage Detector with Mask-Guided Attention for Point Cloud. (arXiv:2108.03634v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03634","description":"<p>Most of the existing single-stage and two-stage 3D object detectors are\nanchor-based methods, while the efficient but challenging anchor-free\nsingle-stage 3D object detection is not well investigated. Recent studies on 2D\nobject detection show that the anchor-free methods also are of great potential.\nHowever, the unordered and sparse properties of point clouds prevent us from\ndirectly leveraging the advanced 2D methods on 3D point clouds. We overcome\nthis by converting the voxel-based sparse 3D feature volumes into the sparse 2D\nfeature maps. We propose an attentive module to fit the sparse feature maps to\ndense mostly on the object regions through the deformable convolution tower and\nthe supervised mask-guided attention. By directly regressing the 3D bounding\nbox from the enhanced and dense feature maps, we construct a novel single-stage\n3D detector for point clouds in an anchor-free manner. We propose an IoU-based\ndetection confidence re-calibration scheme to improve the correlation between\nthe detection confidence score and the accuracy of the bounding box regression.\nOur code is publicly available at \\url{https://github.com/jialeli1/MGAF-3DSSD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiale Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yong Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Light Field Reconstruction via Spatio-Angular Dense Network. (arXiv:2108.03635v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03635","description":"<p>As an image sensing instrument, light field images can supply extra angular\ninformation compared with monocular images and have facilitated a wide range of\nmeasurement applications. Light field image capturing devices usually suffer\nfrom the inherent trade-off between the angular and spatial resolutions. To\ntackle this problem, several methods, such as light field reconstruction and\nlight field super-resolution, have been proposed but leaving two problems\nunaddressed, namely domain asymmetry and efficient information flow. In this\npaper, we propose an end-to-end Spatio-Angular Dense Network (SADenseNet) for\nlight field reconstruction with two novel components, namely correlation blocks\nand spatio-angular dense skip connections to address them. The former performs\neffective modeling of the correlation information in a way that conforms with\nthe domain asymmetry. And the latter consists of three kinds of connections\nenhancing the information flow within two domains. Extensive experiments on\nboth real-world and synthetic datasets have been conducted to demonstrate that\nthe proposed SADenseNet's state-of-the-art performance at significantly reduced\ncosts in memory and computation. The qualitative results show that the\nreconstructed light field images are sharp with correct details and can serve\nas pre-processing to improve the accuracy of related measurement applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zexi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_H/0/1/0/all/0/1\">Henry Wing Fung Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yuk Ying Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haisheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency-Associated Object Tracking. (arXiv:2108.03637v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03637","description":"<p>Most existing trackers based on deep learning perform tracking in a holistic\nstrategy, which aims to learn deep representations of the whole target for\nlocalizing the target. It is arduous for such methods to track targets with\nvarious appearance variations. To address this limitation, another type of\nmethods adopts a part-based tracking strategy which divides the target into\nequal patches and tracks all these patches in parallel. The target state is\ninferred by summarizing the tracking results of these patches. A potential\nlimitation of such trackers is that not all patches are equally informative for\ntracking. Some patches that are not discriminative may have adverse effects. In\nthis paper, we propose to track the salient local parts of the target that are\ndiscriminative for tracking. In particular, we propose a fine-grained saliency\nmining module to capture the local saliencies. Further, we design a\nsaliency-association modeling module to associate the captured saliencies\ntogether to learn effective correlation representations between the exemplar\nand the search image for state estimation. Extensive experiments on five\ndiverse datasets demonstrate that the proposed method performs favorably\nagainst state-of-the-art trackers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zikun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenyu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image reconstruction in light-sheet microscopy: spatially varying deconvolution and mixed noise. (arXiv:2108.03642v1 [math.NA])","link":"http://arxiv.org/abs/2108.03642","description":"<p>We study the problem of deconvolution for light-sheet microscopy, where the\ndata is corrupted by spatially varying blur and a combination of Poisson and\nGaussian noise. The spatial variation of the point spread function (PSF) of a\nlight-sheet microscope is determined by the interaction between the excitation\nsheet and the detection objective PSF. First, we introduce a model of the image\nformation process that incorporates this interaction, therefore capturing the\nmain characteristics of this imaging modality. Then, we formulate a variational\nmodel that accounts for the combination of Poisson and Gaussian noise through a\ndata fidelity term consisting of the infimal convolution of the single noise\nfidelities, first introduced in L. Calatroni et al. \"Infimal convolution of\ndata discrepancies for mixed noise removal\", SIAM Journal on Imaging Sciences\n10.3 (2017), 1196-1233. We establish convergence rates in a Bregman distance\nunder a source condition for the infimal convolution fidelity and a discrepancy\nprinciple for choosing the value of the regularisation parameter. The inverse\nproblem is solved by applying the primal-dual hybrid gradient (PDHG) algorithm\nin a novel way. Finally, numerical experiments performed on both simulated and\nreal data show superior reconstruction results in comparison with other\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Toader_B/0/1/0/all/0/1\">Bogdan Toader</a>, <a href=\"http://arxiv.org/find/math/1/au:+Boulanger_J/0/1/0/all/0/1\">Jerome Boulanger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Korolev_Y/0/1/0/all/0/1\">Yury Korolev</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lenz_M/0/1/0/all/0/1\">Martin O. Lenz</a>, <a href=\"http://arxiv.org/find/math/1/au:+Manton_J/0/1/0/all/0/1\">James Manton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Schonlieb</a>, <a href=\"http://arxiv.org/find/math/1/au:+Muresan_L/0/1/0/all/0/1\">Leila Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer. (arXiv:2108.03647v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03647","description":"<p>Fast arbitrary neural style transfer has attracted widespread attention from\nacademic, industrial and art communities due to its flexibility in enabling\nvarious applications. Existing solutions either attentively fuse deep style\nfeature into deep content feature without considering feature distributions, or\nadaptively normalize deep content feature according to the style such that\ntheir global statistics are matched. Although effective, leaving shallow\nfeature unexplored and without locally considering feature statistics, they are\nprone to unnatural output with unpleasing local distortions. To alleviate this\nproblem, in this paper, we propose a novel attention and normalization module,\nnamed Adaptive Attention Normalization (AdaAttN), to adaptively perform\nattentive normalization on per-point basis. Specifically, spatial attention\nscore is learnt from both shallow and deep features of content and style\nimages. Then per-point weighted statistics are calculated by regarding a style\nfeature point as a distribution of attention-weighted output of all style\nfeature points. Finally, the content feature is normalized so that they\ndemonstrate the same local feature statistics as the calculated per-point\nweighted style feature statistics. Besides, a novel local feature loss is\nderived based on AdaAttN to enhance local visual quality. We also extend\nAdaAttN to be ready for video style transfer with slight modifications.\nExperiments demonstrate that our method achieves state-of-the-art arbitrary\nimage/video style transfer. Codes and models are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meiling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhengxing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Voxel to Point: IoU-guided 3D Object Detection for Point Cloud with Voxel-to-Point Decoder. (arXiv:2108.03648v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03648","description":"<p>In this paper, we present an Intersection-over-Union (IoU) guided two-stage\n3D object detector with a voxel-to-point decoder. To preserve the necessary\ninformation from all raw points and maintain the high box recall in voxel based\nRegion Proposal Network (RPN), we propose a residual voxel-to-point decoder to\nextract the point features in addition to the map-view features from the voxel\nbased RPN. We use a 3D Region of Interest (RoI) alignment to crop and align the\nfeatures with the proposal boxes for accurately perceiving the object position.\nThe RoI-Aligned features are finally aggregated with the corner geometry\nembeddings that can provide the potentially missing corner information in the\nbox refinement stage. We propose a simple and efficient method to align the\nestimated IoUs to the refined proposal boxes as a more relevant localization\nconfidence. The comprehensive experiments on KITTI and Waymo Open Dataset\ndemonstrate that our method achieves significant improvements with novel\narchitectures against the existing methods. The code is available on Github\nURL\\footnote{\\url{https://github.com/jialeli1/From-Voxel-to-Point}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiale Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yong Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Depth and Normal Estimation from Real-world Time-of-flight Raw Data. (arXiv:2108.03649v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03649","description":"<p>We present a novel approach to joint depth and normal estimation for\ntime-of-flight (ToF) sensors. Our model learns to predict the high-quality\ndepth and normal maps jointly from ToF raw sensor data. To achieve this, we\nmeticulously constructed the first large-scale dataset (named ToF-100) with\npaired raw ToF data and ground-truth high-resolution depth maps provided by an\nindustrial depth camera. In addition, we also design a simple but effective\nframework for joint depth and normal estimation, applying a robust Chamfer loss\nvia jittering to improve the performance of our model. Our experiments\ndemonstrate that our proposed method can efficiently reconstruct\nhigh-resolution depth and normal maps and significantly outperforms\nstate-of-the-art approaches. Our code and data will be available at\n\\url{https://github.com/hkustVisionRr/JointlyDepthNormalEstimation}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rongrong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-Contrastive 3D Action Representation Learning. (arXiv:2108.03656v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03656","description":"<p>This paper strives for self-supervised learning of a feature space suitable\nfor skeleton-based action recognition. Our proposal is built upon learning\ninvariances to input skeleton representations and various skeleton\naugmentations via a noise contrastive estimation. In particular, we propose\ninter-skeleton contrastive learning, which learns from multiple different input\nskeleton representations in a cross-contrastive manner. In addition, we\ncontribute several skeleton-specific spatial and temporal augmentations which\nfurther encourage the model to learn the spatio-temporal dynamics of skeleton\ndata. By learning similarities between different skeleton representations as\nwell as augmented views of the same sequence, the network is encouraged to\nlearn higher-level semantics of the skeleton data than when only using the\naugmented views. Our approach achieves state-of-the-art performance for\nself-supervised learning from skeleton data on the challenging PKU and NTU\ndatasets with multiple downstream tasks, including action recognition, action\nretrieval and semi-supervised learning. Code is available at\nhttps://github.com/fmthoker/skeleton-contrast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thoker_F/0/1/0/all/0/1\">Fida Mohammad Thoker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1\">Hazel Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G.M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Shot Object Affordance Detection in the Wild. (arXiv:2108.03658v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03658","description":"<p>Affordance detection refers to identifying the potential action possibilities\nof objects in an image, which is a crucial ability for robot perception and\nmanipulation. To empower robots with this ability in unseen scenarios, we first\nstudy the challenging one-shot affordance detection problem in this paper,\ni.e., given a support image that depicts the action purpose, all objects in a\nscene with the common affordance should be detected. To this end, we devise a\nOne-Shot Affordance Detection Network (OSAD-Net) that firstly estimates the\nhuman action purpose and then transfers it to help detect the common affordance\nfrom all candidate images. Through collaboration learning, OSAD-Net can capture\nthe common characteristics between objects having the same underlying\naffordance and learn a good adaptation capability for perceiving unseen\naffordances. Besides, we build a large-scale Purpose-driven Affordance Dataset\nv2 (PADv2) by collecting and labeling 30k images from 39 affordance and 103\nobject categories. With complex scenes and rich annotations, our PADv2 dataset\ncan be used as a test bed to benchmark affordance detection methods and may\nalso facilitate downstream vision tasks, such as scene understanding, action\nrecognition, and robot manipulation. Specifically, we conducted comprehensive\nexperiments on PADv2 dataset by including 11 advanced models from several\nrelated research fields. Experimental results demonstrate the superiority of\nour model over previous representative ones in terms of both objective metrics\nand visual quality. The benchmark suite is available at\nhttps://github.com/lhc1224/OSAD Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Latent Semantic Graph for Video Captioning. (arXiv:2108.03662v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03662","description":"<p>Video captioning aims to automatically generate natural language sentences\nthat can describe the visual contents of a given video. Existing generative\nmodels like encoder-decoder frameworks cannot explicitly explore the\nobject-level interactions and frame-level information from complex\nspatio-temporal data to generate semantic-rich captions. Our main contribution\nis to identify three key problems in a joint framework for future video\nsummarization tasks. 1) Enhanced Object Proposal: we propose a novel\nConditional Graph that can fuse spatio-temporal information into latent object\nproposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to\ndynamically extract visual words with higher semantic levels. 3) Sentence\nValidation: A novel Discriminative Language Validator is proposed to verify\ngenerated captions so that key semantic concepts can be effectively preserved.\nOur experiments on two public datasets (MVSD and MSR-VTT) manifest significant\nimprovements over state-of-the-art approaches on all metrics, especially for\nBLEU-4 and CIDEr. Our code is available at\nhttps://github.com/baiyang4/D-LSG-Video-Caption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bingzhang Hul Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1\">Maurice Pagnucco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RECALL: Replay-based Continual Learning in Semantic Segmentation. (arXiv:2108.03673v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03673","description":"<p>Deep networks allow to obtain outstanding results in semantic segmentation,\nhowever they need to be trained in a single shot with a large amount of data.\nContinual learning settings where new classes are learned in incremental steps\nand previous training data is no longer available are challenging due to the\ncatastrophic forgetting phenomenon. Existing approaches typically fail when\nseveral incremental steps are performed or in presence of a distribution shift\nof the background class. We tackle these issues by recreating no longer\navailable data for the old classes and outlining a content inpainting scheme on\nthe background class. We propose two sources for replay data. The first resorts\nto a generative adversarial network to sample from the class space of past\nlearning steps. The second relies on web-crawled data to retrieve images\ncontaining examples of old classes from online databases. In both scenarios no\nsamples of past steps are stored, thus avoiding privacy concerns. Replay data\nare then blended with new samples during the incremental steps. Our approach,\nRECALL, outperforms state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maracani_A/0/1/0/all/0/1\">Andrea Maracani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMDet: A Tool for Mitotic Cell Detection in Histopathology Slides. (arXiv:2108.03676v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03676","description":"<p>Breast Cancer is the most prevalent cancer in the world. The World Health\nOrganization reports that the disease still affects a significant portion of\nthe developing world citing increased mortality rates in the majority of low to\nmiddle income countries. The most popular protocol pathologists use for\ndiagnosing breast cancer is the Nottingham grading system which grades the\nproliferation of tumors based on 3 major criteria, the most important of them\nbeing mitotic cell count. The way in which pathologists evaluate mitotic cell\ncount is to subjectively and qualitatively analyze cells present in stained\nslides of tissue and make a decision on its mitotic state i.e. is it mitotic or\nnot?This process is extremely inefficient and tiring for pathologists and so an\nefficient, accurate, and fully automated tool to aid with the diagnosis is\nextremely desirable. Fortunately, creating such a tool is made significantly\neasier with the AutoML tool available from Microsoft Azure, however to the best\nof our knowledge the AutoML tool has never been formally evaluated for use in\nmitotic cell detection in histopathology images. This paper serves as an\nevaluation of the AutoML tool for this purpose and will provide a first look on\nhow the tool handles this challenging problem. All code is available\nathttps://github.com/WaltAFWilliams/AMDet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Williams_W/0/1/0/all/0/1\">Walt Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_J/0/1/0/all/0/1\">Jimmy Hall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Inductive and Transductive Learning for Video Object Segmentation. (arXiv:2108.03679v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03679","description":"<p>Semi-supervised video object segmentation is a task of segmenting the target\nobject in a video sequence given only a mask annotation in the first frame. The\nlimited information available makes it an extremely challenging task. Most\nprevious best-performing methods adopt matching-based transductive reasoning or\nonline inductive learning. Nevertheless, they are either less discriminative\nfor similar instances or insufficient in the utilization of spatio-temporal\ninformation. In this work, we propose to integrate transductive and inductive\nlearning into a unified framework to exploit the complementarity between them\nfor accurate and robust video object segmentation. The proposed approach\nconsists of two functional branches. The transduction branch adopts a\nlightweight transformer architecture to aggregate rich spatio-temporal cues\nwhile the induction branch performs online inductive learning to obtain\ndiscriminative target information. To bridge these two diverse branches, a\ntwo-head label encoder is introduced to learn the suitable target prior for\neach of them. The generated mask encodings are further forced to be\ndisentangled to better retain their complementarity. Extensive experiments on\nseveral prevalent benchmarks show that, without the need of synthetic training\ndata, the proposed approach sets a series of new state-of-the-art records. Code\nis available at https://github.com/maoyunyao/JOINT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yunyao Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Invertible Encoding for Learned Image Compression. (arXiv:2108.03690v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03690","description":"<p>Although deep learning based image compression methods have achieved\npromising progress these days, the performance of these methods still cannot\nmatch the latest compression standard Versatile Video Coding (VVC). Most of the\nrecent developments focus on designing a more accurate and flexible entropy\nmodel that can better parameterize the distributions of the latent features.\nHowever, few efforts are devoted to structuring a better transformation between\nthe image space and the latent feature space. In this paper, instead of\nemploying previous autoencoder style networks to build this transformation, we\npropose an enhanced Invertible Encoding Network with invertible neural networks\n(INNs) to largely mitigate the information loss problem for better compression.\nExperimental results on the Kodak, CLIC, and Tecnick datasets show that our\nmethod outperforms the existing learned image compression methods and\ncompression standards, including VVC (VTM 12.1), especially for high-resolution\nimages. Our source code is available at https://github.com/xyq7/InvCompress.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yueqi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Ka Leong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alignment of Tractography Streamlines using Deformation Transfer via Parallel Transport. (arXiv:2108.03697v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03697","description":"<p>We present a geometric framework for aligning white matter fiber tracts. By\nregistering fiber tracts between brains, one expects to see overlap of\nanatomical structures that often provide meaningful comparisons across\nsubjects. However, the geometry of white matter tracts is highly heterogeneous,\nand finding direct tract-correspondence across multiple individuals remains a\nchallenging problem. We present a novel deformation metric between tracts that\nallows one to compare tracts while simultaneously obtaining a registration. To\naccomplish this, fiber tracts are represented by an intrinsic mean along with\nthe deformation fields represented by tangent vectors from the mean. In this\nsetting, one can determine a parallel transport between tracts and then\nregister corresponding tangent vectors. We present the results of bundle\nalignment on a population of 43 healthy adult subjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lizarraga_A/0/1/0/all/0/1\">Andrew Lizarraga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">David Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubicki_A/0/1/0/all/0/1\">Antoni Kubicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahib_A/0/1/0/all/0/1\">Ashish Sahib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunez_E/0/1/0/all/0/1\">Elvis Nunez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narr_K/0/1/0/all/0/1\">Katherine Narr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Shantanu H. Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIGRoC: Boosting Image Generation via a Robust Classifier. (arXiv:2108.03702v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03702","description":"<p>The interest of the machine learning community in image synthesis has grown\nsignificantly in recent years, with the introduction of a wide range of deep\ngenerative models and means for training them. Such machines' ultimate goal is\nto match the distributions of the given training images and the synthesized\nones. In this work, we propose a general model-agnostic technique for improving\nthe image quality and the distribution fidelity of generated images, obtained\nby any generative model. Our method, termed BIGRoC (boosting image generation\nvia a robust classifier), is based on a post-processing procedure via the\nguidance of a given robust classifier and without a need for additional\ntraining of the generative model. Given a synthesized image, we propose to\nupdate it through projected gradient steps over the robust classifier, in an\nattempt to refine its recognition. We demonstrate this post-processing\nalgorithm on various image synthesis methods and show a significant improvement\nof the generated images, both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OVIS: Open-Vocabulary Visual Instance Search via Visual-Semantic Aligned Representation Learning. (arXiv:2108.03704v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03704","description":"<p>We introduce the task of open-vocabulary visual instance search (OVIS). Given\nan arbitrary textual search query, Open-vocabulary Visual Instance Search\n(OVIS) aims to return a ranked list of visual instances, i.e., image patches,\nthat satisfies the search intent from an image database. The term \"open\nvocabulary\" means that there are neither restrictions to the visual instance to\nbe searched nor restrictions to the word that can be used to compose the\ntextual search query. We propose to address such a search challenge via\nvisual-semantic aligned representation learning (ViSA). ViSA leverages massive\nimage-caption pairs as weak image-level (not instance-level) supervision to\nlearn a rich cross-modal semantic space where the representations of visual\ninstances (not images) and those of textual queries are aligned, thus allowing\nus to measure the similarities between any visual instance and an arbitrary\ntextual query. To evaluate the performance of ViSA, we build two datasets named\nOVIS40 and OVIS1600 and also introduce a pipeline for error analysis. Through\nextensive experiments on the two datasets, we demonstrate ViSA's ability to\nsearch for visual instances in images not available during training given a\nwide range of textual queries including those composed of uncommon words.\nExperimental results show that ViSA achieves an mAP@50 of 21.9% on OVIS40 under\nthe most challenging setting and achieves an mAP@6 of 14.9% on OVIS1600\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junsong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical View Predictor: Unsupervised 3D Global Feature Learning through Hierarchical Prediction among Unordered Views. (arXiv:2108.03743v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03743","description":"<p>Unsupervised learning of global features for 3D shape analysis is an\nimportant research challenge because it avoids manual effort for supervised\ninformation collection. In this paper, we propose a view-based deep learning\nmodel called Hierarchical View Predictor (HVP) to learn 3D shape features from\nunordered views in an unsupervised manner. To mine highly discriminative\ninformation from unordered views, HVP performs a novel hierarchical view\nprediction over a view pair, and aggregates the knowledge learned from the\npredictions in all view pairs into a global feature. In a view pair, we pose\nhierarchical view prediction as the task of hierarchically predicting a set of\nimage patches in a current view from its complementary set of patches, and in\naddition, completing the current view and its opposite from any one of the two\nsets of patches. Hierarchical prediction, in patches to patches, patches to\nview and view to view, facilitates HVP to effectively learn the structure of 3D\nshapes from the correlation between patches in the same view and the\ncorrelation between a pair of complementary views. In addition, the employed\nimplicit aggregation over all view pairs enables HVP to learn global features\nfrom unordered views. Our results show that HVP can outperform state-of-the-art\nmethods under large-scale 3D shape benchmarks in shape classification and\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching. (arXiv:2108.03746v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03746","description":"<p>Learning to generate 3D point clouds without 3D supervision is an important\nbut challenging problem. Current solutions leverage various differentiable\nrenderers to project the generated 3D point clouds onto a 2D image plane, and\ntrain deep neural networks using the per-pixel difference with 2D ground truth\nimages. However, these solutions are still struggling to fully recover fine\nstructures of 3D shapes, such as thin tubes or planes. To resolve this issue,\nwe propose an unsupervised approach for 3D point cloud generation with fine\nstructures. Specifically, we cast 3D point cloud learning as a 2D projection\nmatching problem. Rather than using entire 2D silhouette images as a regular\npixel supervision, we introduce structure adaptive sampling to randomly sample\n2D points within the silhouettes as an irregular point supervision, which\nalleviates the consistency issue of sampling from different view angles. Our\nmethod pushes the neural network to generate a 3D point cloud whose 2D\nprojections match the irregular point supervision from different view angles.\nOur 2D projection matching approach enables the neural network to learn more\naccurate structure information than using the per-pixel difference, especially\nfor fine and thin 3D structures. Our method can recover fine 3D structures from\n2D silhouette images at different resolutions, and is robust to different\nsampling methods and point number in irregular point supervision. Our method\noutperforms others under widely used benchmarks. Our code, data and models are\navailable at https://github.com/chenchao15/2D\\_projection\\_matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chao_C/0/1/0/all/0/1\">Chen Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition. (arXiv:2108.03764v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03764","description":"<p>Face recognition networks encode information about sensitive attributes while\nbeing trained for identity classification. Such encoding has two major issues:\n(a) it makes the face representations susceptible to privacy leakage (b) it\nappears to contribute to bias in face recognition. However, existing bias\nmitigation approaches generally require end-to-end training and are unable to\nachieve high verification accuracy. Therefore, we present a descriptor-based\nadversarial de-biasing approach called `Protected Attribute Suppression System\n(PASS)'. PASS can be trained on top of descriptors obtained from any previously\ntrained high-performing network to classify identities and simultaneously\nreduce encoding of sensitive attributes. This eliminates the need for\nend-to-end training. As a component of PASS, we present a novel discriminator\ntraining strategy that discourages a network from encoding protected attribute\ninformation. We show the efficacy of PASS to reduce gender and skintone\ninformation in descriptors from SOTA face recognition networks like Arcface. As\na result, PASS descriptors outperform existing baselines in reducing gender and\nskintone bias on the IJB-C dataset, while maintaining a high verification\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_P/0/1/0/all/0/1\">Prithviraj Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Joshua Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Aniket Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Slice Net: A novel light weight framework for COVID-19 Diagnosis. (arXiv:2108.03786v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03786","description":"<p>This paper presents a novel lightweight COVID-19 diagnosis framework using CT\nscans. Our system utilises a novel two-stage approach to generate robust and\nefficient diagnoses across heterogeneous patient level inputs. We use a\npowerful backbone network as a feature extractor to capture discriminative\nslice-level features. These features are aggregated by a lightweight network to\nobtain a patient level diagnosis. The aggregation network is carefully designed\nto have a small number of trainable parameters while also possessing sufficient\ncapacity to generalise to diverse variations within different CT volumes and to\nadapt to noise introduced during the data acquisition. We achieve a significant\nperformance increase over the baselines when benchmarked on the SPGC COVID-19\nRadiomics Dataset, despite having only 2.5 million trainable parameters and\nrequiring only 0.623 seconds on average to process a single patient's CT volume\nusing an Nvidia-GeForce RTX 2080 GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gammulle_H/0/1/0/all/0/1\">Harshala Gammulle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1\">Tharindu Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Joint Embedding with Modality Alignments for Cross-Modal Retrieval of Recipes and Food Images. (arXiv:2108.03788v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03788","description":"<p>This paper presents a three-tier modality alignment approach to learning\ntext-image joint embedding, coined as JEMA, for cross-modal retrieval of\ncooking recipes and food images. The first tier improves recipe text embedding\nby optimizing the LSTM networks with term extraction and ranking enhanced\nsequence patterns, and optimizes the image embedding by combining the\nResNeXt-101 image encoder with the category embedding using wideResNet-50 with\nword2vec. The second tier modality alignment optimizes the textual-visual joint\nembedding loss function using a double batch-hard triplet loss with soft-margin\noptimization. The third modality alignment incorporates two types of\ncross-modality alignments as the auxiliary loss regularizations to further\nreduce the alignment errors in the joint learning of the two modality-specific\nembedding functions. The category-based cross-modal alignment aims to align the\nimage category with the recipe category as a loss regularization to the joint\nembedding. The cross-modal discriminator-based alignment aims to add the\nvisual-textual embedding distribution alignment to further regularize the joint\nembedding loss. Extensive experiments with the one-million recipes benchmark\ndataset Recipe1M demonstrate that the proposed JEMA approach outperforms the\nstate-of-the-art cross-modal embedding methods for both image-to-recipe and\nrecipe-to-image retrievals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary-aware Graph Reasoning for Semantic Segmentation. (arXiv:2108.03791v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03791","description":"<p>In this paper, we propose a Boundary-aware Graph Reasoning (BGR) module to\nlearn long-range contextual features for semantic segmentation. Rather than\ndirectly construct the graph based on the backbone features, our BGR module\nexplores a reasonable way to combine segmentation erroneous regions with the\ngraph construction scenario. Motivated by the fact that most hard-to-segment\npixels broadly distribute on boundary regions, our BGR module uses the boundary\nscore map as prior knowledge to intensify the graph node connections and\nthereby guide the graph reasoning focus on boundary regions. In addition, we\nemploy an efficient graph convolution implementation to reduce the\ncomputational cost, which benefits the integration of our BGR module into\ncurrent segmentation backbones. Extensive experiments on three challenging\nsegmentation benchmarks demonstrate the effectiveness of our proposed BGR\nmodule for semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoteng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Haozhe Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liang Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paint Transformer: Feed Forward Neural Painting with Stroke Prediction. (arXiv:2108.03798v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03798","description":"<p>Neural painting refers to the procedure of producing a series of strokes for\na given image and non-photo-realistically recreating it using neural networks.\nWhile reinforcement learning (RL) based agents can generate a stroke sequence\nstep by step for this task, it is not easy to train a stable RL agent. On the\nother hand, stroke optimization methods search for a set of stroke parameters\niteratively in a large search space; such low efficiency significantly limits\ntheir prevalence and practicality. Different from previous methods, in this\npaper, we formulate the task as a set prediction problem and propose a novel\nTransformer-based framework, dubbed Paint Transformer, to predict the\nparameters of a stroke set with a feed forward network. This way, our model can\ngenerate a set of strokes in parallel and obtain the final painting of size 512\n* 512 in near real time. More importantly, since there is no dataset available\nfor training the Paint Transformer, we devise a self-training pipeline such\nthat it can be trained without any off-the-shelf dataset while still achieving\nexcellent generalization capability. Experiments demonstrate that our method\nachieves better painting performance than previous ones with cheaper training\nand inference costs. Codes and models are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tianwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruifeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-view: Diagnosis of COVID-19 using Chest CT. (arXiv:2108.03799v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03799","description":"<p>Significant work has been done towards deep learning (DL) models for\nautomatic lung and lesion segmentation and classification of COVID-19 on chest\nCT data. However, comprehensive visualization systems focused on supporting the\ndual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a\nvisualization application specially tailored for radiologists to diagnose\nCOVID-19 from chest CT data. The system incorporates a complete pipeline of\nautomatic lungs segmentation, localization/ isolation of lung abnormalities,\nfollowed by visualization, visual and DL analysis, and\nmeasurement/quantification tools. Our system combines the traditional 2D\nworkflow of radiologists with newer 2D and 3D visualization techniques with DL\nsupport for a more comprehensive diagnosis. COVID-view incorporates a novel DL\nmodel for classifying the patients into positive/negative COVID-19 cases, which\nacts as a reading aid for the radiologist using COVID-view and provides the\nattention heatmap as an explainable DL for the model output. We designed and\nevaluated COVID-view through suggestions, close feedback and conducting case\nstudies of real-world patient data by expert radiologists who have substantial\nexperience diagnosing chest CT scans for COVID-19, pulmonary embolism, and\nother forms of lung infections. We present requirements and task analysis for\nthe diagnosis of COVID-19 that motivate our design choices and results in a\npractical system which is capable of handling real-world patient cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jadhav_S/0/1/0/all/0/1\">Shreeraj Jadhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_G/0/1/0/all/0/1\">Gaofeng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zawin_M/0/1/0/all/0/1\">Marlene Zawin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufman_A/0/1/0/all/0/1\">Arie E. Kaufman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSGR: Pixel-wise Sparse Graph Reasoning for COVID-19 Pneumonia Segmentation in CT Images. (arXiv:2108.03809v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03809","description":"<p>Automated and accurate segmentation of the infected regions in computed\ntomography (CT) images is critical for the prediction of the pathological stage\nand treatment response of COVID-19. Several deep convolutional neural networks\n(DCNNs) have been designed for this task, whose performance, however, tends to\nbe suppressed by their limited local receptive fields and insufficient global\nreasoning ability. In this paper, we propose a pixel-wise sparse graph\nreasoning (PSGR) module and insert it into a segmentation network to enhance\nthe modeling of long-range dependencies for COVID-19 infected region\nsegmentation in CT images. In the PSGR module, a graph is first constructed by\nprojecting each pixel on a node based on the features produced by the\nsegmentation backbone, and then converted into a sparsely-connected graph by\nkeeping only K strongest connections to each uncertain pixel. The long-range\ninformation reasoning is performed on the sparsely-connected graph to generate\nenhanced features. The advantages of this module are two-fold: (1) the\npixel-wise mapping strategy not only avoids imprecise pixel-to-node projections\nbut also preserves the inherent information of each pixel for global reasoning;\nand (2) the sparsely-connected graph construction results in effective\ninformation retrieval and reduction of the noise propagation. The proposed\nsolution has been evaluated against four widely-used segmentation models on\nthree public datasets. The results show that the segmentation model equipped\nwith our PSGR module can effectively segment COVID-19 infected regions in CT\nimages, outperforming all other competing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1\">Haozhe Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoteng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guixiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1\">Liang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-WAE: Generalized Patch-Wasserstein Autoencoder for Anomaly Screening. (arXiv:2108.03815v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03815","description":"<p>To mitigate the inspector's workload and improve the quality of the product,\ncomputer vision-based anomaly detection (AD) techniques are gradually deployed\nin real-world industrial scenarios. Recent anomaly analysis benchmarks progress\nto generative models. The aim is to model the defect-free distribution so that\nanomalies can be classified as out-of-distribution samples. Nevertheless, there\nare two disturbing factors that need researchers and deployers to prioritize:\n(i) the simplistic prior latent distribution inducing limited expressive\ncapability; (ii) the collapsed mutual-dependent features resulting in poor\ngeneralization. In this paper, we propose a novel Patch-wise Wasserstein\nAutoEncoder (P-WAE) architecture to alleviate those challenges. In particular,\na patch-wise variational inference model coupled with solving the jigsaw puzzle\nis designed, which is a simple yet effective way to increase the expressiveness\nand complexity of the latent manifold. This alleviates the blurry\nreconstruction problem. In addition, the Hilbert-Schmidt Independence Criterion\n(HSIC) bottleneck is introduced to constrain the over-regularization\nrepresentation. Comprehensive experiments, conducted on the MVTec AD dataset,\ndemonstrate the superior performance of our propo\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistillPose: Lightweight Camera Localization Using Auxiliary Learning. (arXiv:2108.03819v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03819","description":"<p>We propose a lightweight retrieval-based pipeline to predict 6DOF camera\nposes from RGB images. Our pipeline uses a convolutional neural network (CNN)\nto encode a query image as a feature vector. A nearest neighbor lookup finds\nthe pose-wise nearest database image. A siamese convolutional neural network\nregresses the relative pose from the nearest neighboring database image to the\nquery image. The relative pose is then applied to the nearest neighboring\nabsolute pose to obtain the query image's final absolute pose prediction. Our\nmodel is a distilled version of NN-Net that reduces its parameters by 98.87%,\ninformation retrieval feature vector size by 87.5%, and inference time by\n89.18% without a significant decrease in localization accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abouelnaga_Y/0/1/0/all/0/1\">Yehya Abouelnaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Mai Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Annotation for Visual Tracking via Selection and Refinement. (arXiv:2108.03821v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03821","description":"<p>Deep learning based visual trackers entail offline pre-training on large\nvolumes of video datasets with accurate bounding box annotations that are\nlabor-expensive to achieve. We present a new framework to facilitate bounding\nbox annotations for video sequences, which investigates a\nselection-and-refinement strategy to automatically improve the preliminary\nannotations generated by tracking algorithms. A temporal assessment network\n(T-Assess Net) is proposed which is able to capture the temporal coherence of\ntarget locations and select reliable tracking results by measuring their\nquality. Meanwhile, a visual-geometry refinement network (VG-Refine Net) is\nalso designed to further enhance the selected tracking results by considering\nboth target appearance and temporal geometry constraints, allowing inaccurate\ntracking results to be corrected. The combination of the above two networks\nprovides a principled approach to ensure the quality of automatic video\nannotation. Experiments on large scale tracking benchmarks demonstrate that our\nmethod can deliver highly accurate bounding box annotations and significantly\nreduce human labor by 94.0%, yielding an effective means to further boost\ntracking performance with augmented training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_K/0/1/0/all/0/1\">Kenan Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xuesheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyun Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards to Robust and Generalized Medical Image Segmentation Framework. (arXiv:2108.03823v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03823","description":"<p>To mitigate the radiologist's workload, computer-aided diagnosis with the\ncapability to review and analyze medical images is gradually deployed. Deep\nlearning-based region of interest segmentation is among the most exciting use\ncases. However, this paradigm is restricted in real-world clinical applications\ndue to poor robustness and generalization. The issue is more sinister with a\nlack of training data. In this paper, we address the challenge from the\nrepresentation learning point of view. We investigate that the collapsed\nrepresentations, as one of the main reasons which caused poor robustness and\ngeneralization, could be avoided through transfer learning. Therefore, we\npropose a novel two-stage framework for robust generalized segmentation. In\nparticular, an unsupervised Tile-wise AutoEncoder (T-AE) pretraining\narchitecture is coined to learn meaningful representation for improving the\ngeneralization and robustness of the downstream tasks. Furthermore, the learned\nknowledge is transferred to the segmentation benchmark. Coupled with an image\nreconstruction network, the representation keeps to be decoded, encouraging the\nmodel to capture more semantic features. Experiments of lung segmentation on\nmulti chest X-ray datasets are conducted. Empirically, the related experimental\nresults demonstrate the superior generalization capability of the proposed\nframework on unseen domains in terms of high performance and robustness to\ncorruption, especially under the scenario of the limited training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AA-RMVSNet: Adaptive Aggregation Recurrent Multi-view Stereo Network. (arXiv:2108.03824v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03824","description":"<p>In this paper, we present a novel recurrent multi-view stereo network based\non long short-term memory (LSTM) with adaptive aggregation, namely AA-RMVSNet.\nWe firstly introduce an intra-view aggregation module to adaptively extract\nimage features by using context-aware convolution and multi-scale aggregation,\nwhich efficiently improves the performance on challenging regions, such as thin\nobjects and large low-textured surfaces. To overcome the difficulty of varying\nocclusion in complex scenes, we propose an inter-view cost volume aggregation\nmodule for adaptive pixel-wise view aggregation, which is able to preserve\nbetter-matched pairs among all views. The two proposed adaptive aggregation\nmodules are lightweight, effective and complementary regarding improving the\naccuracy and completeness of 3D reconstruction. Instead of conventional 3D\nCNNs, we utilize a hybrid network with recurrent structure for cost volume\nregularization, which allows high-resolution reconstruction and finer\nhypothetical plane sweep. The proposed network is trained end-to-end and\nachieves excellent performance on various datasets. It ranks $1^{st}$ among all\nsubmissions on Tanks and Temples benchmark and achieves competitive results on\nDTU dataset, which exhibits strong generalizability and robustness.\nImplementation of our method is available at\nhttps://github.com/QT-Zhu/AA-RMVSNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zizhuang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chen Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yisong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video. (arXiv:2108.03825v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03825","description":"<p>In this paper, we introduce a novel task, referred to as Weakly-Supervised\nSpatio-Temporal Anomaly Detection (WSSTAD) in surveillance video. Specifically,\ngiven an untrimmed video, WSSTAD aims to localize a spatio-temporal tube (i.e.,\na sequence of bounding boxes at consecutive times) that encloses the abnormal\nevent, with only coarse video-level annotations as supervision during training.\nTo address this challenging task, we propose a dual-branch network which takes\nas input the proposals with multi-granularities in both spatial-temporal\ndomains. Each branch employs a relationship reasoning module to capture the\ncorrelation between tubes/videolets, which can provide rich contextual\ninformation and complex entity relationships for the concept learning of\nabnormal behaviors. Mutually-guided Progressive Refinement framework is set up\nto employ dual-path mutual guidance in a recurrent manner, iteratively sharing\nauxiliary supervision information across branches. It impels the learned\nconcepts of each branch to serve as a guide for its counterpart, which\nprogressively refines the corresponding branch and the whole framework.\nFurthermore, we contribute two datasets, i.e., ST-UCF-Crime and STRA,\nconsisting of videos containing spatio-temporal abnormal annotations to serve\nas the benchmarks for WSSTAD. We conduct extensive qualitative and quantitative\nevaluations to demonstrate the effectiveness of the proposed approach and\nanalyze the key factors that contribute more to handle this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xiao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yingying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark. (arXiv:2108.03830v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03830","description":"<p>Monocular depth estimation aims at predicting depth from a single image or\nvideo. Recently, self-supervised methods draw much attention, due to their free\nof depth annotations and impressive performance on several daytime benchmarks,\nsuch as KITTI and Cityscapes. However, they produce weird outputs in more\nchallenging nighttime scenarios because of low visibility and varying\nilluminations, which bring weak textures and break brightness-consistency\nassumption, respectively. To address these problems, in this paper we propose a\nnovel framework with several improvements: (1) we introduce Priors-Based\nRegularization to learn distribution knowledge from unpaired depth maps and\nprevent model from being incorrectly trained; (2) we leverage\nMapping-Consistent Image Enhancement module to enhance image visibility and\ncontrast while maintaining brightness consistency; and (3) we present\nStatistics-Based Mask strategy to tune the number of removed pixels within\ntextureless regions, using dynamic statistics. Experimental results demonstrate\nthe effectiveness of each component. Meanwhile, our framework achieves\nremarkable improvements and state-of-the-art results on two nighttime datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Baobei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementary Patch for Weakly Supervised Semantic Segmentation. (arXiv:2108.03852v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03852","description":"<p>Weakly Supervised Semantic Segmentation (WSSS) based on image-level labels\nhas been greatly advanced by exploiting the outputs of Class Activation Map\n(CAM) to generate the pseudo labels for semantic segmentation. However, CAM\nmerely discovers seeds from a small number of regions, which may be\ninsufficient to serve as pseudo masks for semantic segmentation. In this paper,\nwe formulate the expansion of object regions in CAM as an increase in\ninformation. From the perspective of information theory, we propose a novel\nComplementary Patch (CP) Representation and prove that the information of the\nsum of the CAMs by a pair of input images with complementary hidden (patched)\nparts, namely CP Pair, is greater than or equal to the information of the\nbaseline CAM. Therefore, a CAM with more information related to object seeds\ncan be obtained by narrowing down the gap between the sum of CAMs generated by\nthe CP Pair and the original CAM. We propose a CP Network (CPN) implemented by\na triplet network and three regularization functions. To further improve the\nquality of the CAMs, we propose a Pixel-Region Correlation Module (PRCM) to\naugment the contextual information by using object-region relations between the\nfeature maps and the CAMs. Experimental results on the PASCAL VOC 2012 datasets\nshow that our proposed method achieves a new state-of-the-art in WSSS,\nvalidating the effectiveness of our CP Representation and CPN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chaochen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])","link":"http://arxiv.org/abs/2108.03857","description":"<p>\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safe Vessel Navigation Visually Aided by Autonomous Unmanned Aerial Vehicles in Congested Harbors and Waterways. (arXiv:2108.03862v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03862","description":"<p>In the maritime sector, safe vessel navigation is of great importance,\nparticularly in congested harbors and waterways. The focus of this work is to\nestimate the distance between an object of interest and potential obstacles\nusing a companion UAV. The proposed approach fuses GPS data with long-range\naerial images. First, we employ semantic segmentation DNN for discriminating\nthe vessel of interest, water, and potential solid objects using raw image\ndata. The network is trained with both real and images generated and\nautomatically labeled from a realistic AirSim simulation environment. Then, the\ndistances between the extracted vessel and non-water obstacle blobs are\ncomputed using a novel GSD estimation algorithm. To the best of our knowledge,\nthis work is the first attempt to detect and estimate distances to unknown\nobjects from long-range visual data captured with conventional RGB cameras and\nauxiliary absolute positioning systems (e.g. GPS). The simulation results\nillustrate the accuracy and efficacy of the proposed method for visually aided\nnavigation of vessels assisted by UAV.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sejersen_J/0/1/0/all/0/1\">Jonas le Fevre Sejersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_R/0/1/0/all/0/1\">Rui Pimentel de Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kayacan_E/0/1/0/all/0/1\">Erdal Kayacan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransForensics: Image Forgery Localization with Dense Self-Attention. (arXiv:2108.03871v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03871","description":"<p>Nowadays advanced image editing tools and technical skills produce tampered\nimages more realistically, which can easily evade image forensic systems and\nmake authenticity verification of images more difficult. To tackle this\nchallenging problem, we introduce TransForensics, a novel image forgery\nlocalization method inspired by Transformers. The two major components in our\nframework are dense self-attention encoders and dense correction modules. The\nformer is to model global context and all pairwise interactions between local\npatches at different scales, while the latter is used for improving the\ntransparency of the hidden layers and correcting the outputs from different\nbranches. Compared to previous traditional and deep learning methods,\nTransForensics not only can capture discriminative representations and obtain\nhigh-quality mask predictions but is also not limited by tampering types and\npatch sequence orders. By conducting experiments on main benchmarks, we show\nthat TransForensics outperforms the stateof-the-art methods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jing Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shicai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rain Removal and Illumination Enhancement Done in One Go. (arXiv:2108.03873v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03873","description":"<p>Rain removal plays an important role in the restoration of degraded images.\nRecently, data-driven methods have achieved remarkable success. However, these\napproaches neglect that the appearance of rain is often accompanied by low\nlight conditions, which will further degrade the image quality. Therefore, it\nis very indispensable to jointly remove the rain and enhance the light for\nreal-world rain image restoration. In this paper, we aim to address this\nproblem from two aspects. First, we proposed a novel entangled network, namely\nEMNet, which can remove the rain and enhance illumination in one go.\nSpecifically, two encoder-decoder networks interact complementary information\nthrough entanglement structure, and parallel rain removal and illumination\nenhancement. Considering that the encoder-decoder structure is unreliable in\npreserving spatial details, we employ a detail recovery network to restore the\ndesired fine texture. Second, we present a new synthetic dataset, namely\nDarkRain, to boost the development of rain image restoration algorithms in\npractical scenarios. DarkRain not only contains different degrees of rain, but\nalso considers different lighting conditions, and more realistically simulates\nthe rainfall in the real world. EMNet is extensively evaluated on the proposed\nbenchmark and achieves state-of-the-art results. In addition, after a simple\ntransformation, our method outshines existing methods in both rain removal and\nlow-light image enhancement. The source code and dataset will be made publicly\navailable later.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yecong Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuanshuo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingwen Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralMVS: Bridging Multi-View Stereo and Novel View Synthesis. (arXiv:2108.03880v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03880","description":"<p>Multi-View Stereo (MVS) is a core task in 3D computer vision. With the surge\nof novel deep learning methods, learned MVS has surpassed the accuracy of\nclassical approaches, but still relies on building a memory intensive dense\ncost volume. Novel View Synthesis (NVS) is a parallel line of research and has\nrecently seen an increase in popularity with Neural Radiance Field (NeRF)\nmodels, which optimize a per scene radiance field. However, NeRF methods do not\ngeneralize to novel scenes and are slow to train and test. We propose to bridge\nthe gap between these two methodologies with a novel network that can recover\n3D scene geometry as a distance function, together with high-resolution color\nimages. Our method uses only a sparse set of images as input and can generalize\nwell to novel scenes. Additionally, we propose a coarse-to-fine sphere tracing\napproach in order to significantly increase speed. We show on various datasets\nthat our method reaches comparable accuracy to per-scene optimized methods\nwhile being able to generalize and running significantly faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning of Occlusion Aware Flow Guided 3D Geometry Perception with Adaptive Cross Weighted Loss from Monocular Videos. (arXiv:2108.03893v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03893","description":"<p>Self-supervised deep learning-based 3D scene understanding methods can\novercome the difficulty of acquiring the densely labeled ground-truth and have\nmade a lot of advances. However, occlusions and moving objects are still some\nof the major limitations. In this paper, we explore the learnable occlusion\naware optical flow guided self-supervised depth and camera pose estimation by\nan adaptive cross weighted loss to address the above limitations. Firstly, we\nexplore to train the learnable occlusion mask fused optical flow network by an\nocclusion-aware photometric loss with the temporally supplemental information\nand backward-forward consistency of adjacent views. And then, we design an\nadaptive cross-weighted loss between the depth-pose and optical flow loss of\nthe geometric and photometric error to distinguish the moving objects which\nviolate the static scene assumption. Our method shows promising results on\nKITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good\ngeneralization ability under a variety of challenging scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Jiaojiao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIFA: Fast Inference Approximation for Action Segmentation. (arXiv:2108.03894v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03894","description":"<p>We introduce FIFA, a fast approximate inference method for action\nsegmentation and alignment. Unlike previous approaches, FIFA does not rely on\nexpensive dynamic programming for inference. Instead, it uses an approximate\ndifferentiable energy function that can be minimized using gradient-descent.\nFIFA is a general approach that can replace exact inference improving its speed\nby more than 5 times while maintaining its performance. FIFA is an anytime\ninference algorithm that provides a better speed vs. accuracy trade-off\ncompared to exact inference. We apply FIFA on top of state-of-the-art\napproaches for weakly supervised action segmentation and alignment as well as\nfully supervised action segmentation. FIFA achieves state-of-the-art results on\nmost metrics on two action segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1\">Yaser Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1\">Yazan Abu Farha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Despinoy_F/0/1/0/all/0/1\">Fabien Despinoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1\">Gianpiero Francesca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Regularity Measures for Sample-wise Learning and Generalization. (arXiv:2108.03913v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03913","description":"<p>Fundamental machine learning theory shows that different samples contribute\nunequally both in learning and testing processes. Contemporary studies on DNN\nimply that such sample di?erence is rooted on the distribution of intrinsic\npattern information, namely sample regularity. Motivated by the recent\ndiscovery on network memorization and generalization, we proposed a pair of\nsample regularity measures for both processes with a formulation-consistent\nrepresentation. Specifically, cumulative binary training/generalizing loss\n(CBTL/CBGL), the cumulative number of correct classi?cations of the\ntraining/testing sample within training stage, is proposed to quantize the\nstability in memorization-generalization process; while\nforgetting/mal-generalizing events, i.e., the mis-classification of previously\nlearned or generalized sample, are utilized to represent the uncertainty of\nsample regularity with respect to optimization dynamics. Experiments validated\nthe effectiveness and robustness of the proposed approaches for mini-batch SGD\noptimization. Further applications on training/testing sample selection show\nthe proposed measures sharing the uni?ed computing procedure could benefit for\nboth tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuanqi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuehu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices. (arXiv:2108.03917v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03917","description":"<p>Deep convolutional neural networks (CNNs) have shown outstanding performance\nin the task of semantically segmenting images. Applying the same methods on 3D\ndata still poses challenges due to the heavy memory requirements and the lack\nof structured data. Here, we propose LatticeNet, a novel approach for 3D\nsemantic segmentation, which takes raw point clouds as input. A PointNet\ndescribes the local geometry which we embed into a sparse permutohedral\nlattice. The lattice allows for fast convolutions while keeping a low memory\nfootprint. Further, we introduce DeformSlice, a novel learned data-dependent\ninterpolation for projecting lattice features back onto the point cloud. We\npresent results of 3D segmentation on multiple datasets where our method\nachieves state-of-the-art performance. We also extend and evaluate our network\nfor instance and dynamic object segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutt_P/0/1/0/all/0/1\">Peer Sch&#xfc;tt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1\">Jan Quenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Light Field Refocusing for Camera Arrays Using Bokeh Rendering and Superresolution. (arXiv:2108.03918v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03918","description":"<p>Camera arrays provide spatial and angular information within a single\nsnapshot. With refocusing methods, focal planes can be altered after exposure.\nIn this letter, we propose a light field refocusing method to improve the\nimaging quality of camera arrays. In our method, the disparity is first\nestimated. Then, the unfocused region (bokeh) is rendered by using a\ndepth-based anisotropic filter. Finally, the refocused image is produced by a\nreconstruction-based superresolution approach where the bokeh image is used as\na regularization term. Our method can selectively refocus images with focused\nregion being superresolved and bokeh being aesthetically rendered. Our method\nalso enables postadjustment of depth of field. We conduct experiments on both\npublic and self-developed datasets. Our method achieves superior visual\nperformance with acceptable computational cost as compared to other\nstate-of-the-art methods. Code is available at\nhttps://github.com/YingqianWang/Selective-LF-Refocusing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FA-GAN: Fused Attentive Generative Adversarial Networks for MRI Image Super-Resolution. (arXiv:2108.03920v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03920","description":"<p>High-resolution magnetic resonance images can provide fine-grained anatomical\ninformation, but acquiring such data requires a long scanning time. In this\npaper, a framework called the Fused Attentive Generative Adversarial\nNetworks(FA-GAN) is proposed to generate the super-resolution MR image from\nlow-resolution magnetic resonance images, which can reduce the scanning time\neffectively but with high resolution MR images. In the framework of the FA-GAN,\nthe local fusion feature block, consisting of different three-pass networks by\nusing different convolution kernels, is proposed to extract image features at\ndifferent scales. And the global feature fusion module, including the channel\nattention module, the self-attention module, and the fusion operation, is\ndesigned to enhance the important features of the MR image. Moreover, the\nspectral normalization process is introduced to make the discriminator network\nstable. 40 sets of 3D magnetic resonance images (each set of images contains\n256 slices) are used to train the network, and 10 sets of images are used to\ntest the proposed method. The experimental results show that the PSNR and SSIM\nvalues of the super-resolution magnetic resonance image generated by the\nproposed FA-GAN method are higher than the state-of-the-art reconstruction\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mingfeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_M/0/1/0/all/0/1\">Minghao Zhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Liying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaocheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jucheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Human Reconstruction in the Wild with Collaborative Aerial Cameras. (arXiv:2108.03936v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03936","description":"<p>Aerial vehicles are revolutionizing applications that require capturing the\n3D structure of dynamic targets in the wild, such as sports, medicine, and\nentertainment. The core challenges in developing a motion-capture system that\noperates in outdoors environments are: (1) 3D inference requires multiple\nsimultaneous viewpoints of the target, (2) occlusion caused by obstacles is\nfrequent when tracking moving targets, and (3) the camera and vehicle state\nestimation is noisy. We present a real-time aerial system for multi-camera\ncontrol that can reconstruct human motions in natural environments without the\nuse of special-purpose markers. We develop a multi-robot coordination scheme\nthat maintains the optimal flight formation for target reconstruction quality\namongst obstacles. We provide studies evaluating system performance in\nsimulation, and validate real-world performance using two drones while a target\nperforms activities such as jogging and playing soccer. Supplementary video:\nhttps://youtu.be/jxt91vx0cns\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_C/0/1/0/all/0/1\">Cherie Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jong_A/0/1/0/all/0/1\">Andrew Jong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_H/0/1/0/all/0/1\">Harry Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1\">Rohan Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonatti_R/0/1/0/all/0/1\">Rogerio Bonatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TriTransNet: RGB-D Salient Object Detection with a Triplet Transformer Embedding Network. (arXiv:2108.03990v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03990","description":"<p>Salient object detection is the pixel-level dense prediction task which can\nhighlight the prominent object in the scene. Recently U-Net framework is widely\nused, and continuous convolution and pooling operations generate multi-level\nfeatures which are complementary with each other. In view of the more\ncontribution of high-level features for the performance, we propose a triplet\ntransformer embedding module to enhance them by learning long-range\ndependencies across layers. It is the first to use three transformer encoders\nwith shared weights to enhance multi-level features. By further designing scale\nadjustment module to process the input, devising three-stream decoder to\nprocess the output and attaching depth features to color features for the\nmulti-modal fusion, the proposed triplet transformer embedding network\n(TriTransNet) achieves the state-of-the-art performance in RGB-D salient object\ndetection, and pushes the performance to a new level. Experimental results\ndemonstrate the effectiveness of the proposed modules and the competition of\nTriTransNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Bin Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transductive Few-Shot Classification on the Oblique Manifold. (arXiv:2108.04009v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04009","description":"<p>Few-shot learning (FSL) attempts to learn with limited data. In this work, we\nperform the feature extraction in the Euclidean space and the geodesic distance\nmetric on the Oblique Manifold (OM). Specially, for better feature extraction,\nwe propose a non-parametric Region Self-attention with Spatial Pyramid Pooling\n(RSSPP), which realizes a trade-off between the generalization and the\ndiscriminative ability of the single image feature. Then, we embed the feature\nto OM as a point. Furthermore, we design an Oblique Distance-based Classifier\n(ODC) that achieves classification in the tangent spaces which better\napproximate OM locally by learnable tangency points. Finally, we introduce a\nnew method for parameters initialization and a novel loss function in the\ntransductive settings. Extensive experiments demonstrate the effectiveness of\nour algorithm and it outperforms state-of-the-art methods on the popular\nbenchmarks: mini-ImageNet, tiered-ImageNet, and Caltech-UCSD Birds-200-2011\n(CUB).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guodong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huimin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhaohui Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuzhao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Multi-Scale Loss Optimization for Object Detection. (arXiv:2108.04014v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04014","description":"<p>With the continuous improvement of the performance of object detectors via\nadvanced model architectures, imbalance problems in the training process have\nreceived more attention. It is a common paradigm in object detection frameworks\nto perform multi-scale detection. However, each scale is treated equally during\ntraining. In this paper, we carefully study the objective imbalance of\nmulti-scale detector training. We argue that the loss in each scale level is\nneither equally important nor independent. Different from the existing\nsolutions of setting multi-task weights, we dynamically optimize the loss\nweight of each scale level in the training process. Specifically, we propose an\nAdaptive Variance Weighting (AVW) to balance multi-scale loss according to the\nstatistical variance. Then we develop a novel Reinforcement Learning\nOptimization (RLO) to decide the weighting scheme probabilistically during\ntraining. The proposed dynamic methods make better utilization of multi-scale\ntraining loss without extra computational complexity and learnable parameters\nfor backpropagation. Experiments show that our approaches can consistently\nboost the performance over various baseline detectors on Pascal VOC and MS COCO\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yihao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juntao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning methods for automatic evaluation of delayed enhancement-MRI. The results of the EMIDEC challenge. (arXiv:2108.04016v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04016","description":"<p>A key factor for assessing the state of the heart after myocardial infarction\n(MI) is to measure whether the myocardium segment is viable after reperfusion\nor revascularization therapy. Delayed enhancement-MRI or DE-MRI, which is\nperformed several minutes after injection of the contrast agent, provides high\ncontrast between viable and nonviable myocardium and is therefore a method of\nchoice to evaluate the extent of MI. To automatically assess myocardial status,\nthe results of the EMIDEC challenge that focused on this task are presented in\nthis paper. The challenge's main objectives were twofold. First, to evaluate if\ndeep learning methods can distinguish between normal and pathological cases.\nSecond, to automatically calculate the extent of myocardial infarction. The\npublicly available database consists of 150 exams divided into 50 cases with\nnormal MRI after injection of a contrast agent and 100 cases with myocardial\ninfarction (and then with a hyperenhanced area on DE-MRI), whatever their\ninclusion in the cardiac emergency department. Along with MRI, clinical\ncharacteristics are also provided. The obtained results issued from several\nworks show that the automatic classification of an exam is a reachable task\n(the best method providing an accuracy of 0.92), and the automatic segmentation\nof the myocardium is possible. However, the segmentation of the diseased area\nneeds to be improved, mainly due to the small size of these areas and the lack\nof contrast with the surrounding structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lalande_A/0/1/0/all/0/1\">Alain Lalande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pommier_T/0/1/0/all/0/1\">Thibaut Pommier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decourselle_T/0/1/0/all/0/1\">Thomas Decourselle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qayyum_A/0/1/0/all/0/1\">Abdul Qayyum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salomon_M/0/1/0/all/0/1\">Michel Salomon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginhac_D/0/1/0/all/0/1\">Dominique Ginhac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skandarani_Y/0/1/0/all/0/1\">Youssef Skandarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boucher_A/0/1/0/all/0/1\">Arnaud Boucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahim_K/0/1/0/all/0/1\">Khawla Brahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruijne_M/0/1/0/all/0/1\">Marleen de Bruijne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camarasa_R/0/1/0/all/0/1\">Robin Camarasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_T/0/1/0/all/0/1\">Teresa M. Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girum_K/0/1/0/all/0/1\">Kibrom B. Girum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennemuth_A/0/1/0/all/0/1\">Anja Hennemuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huellebrand_M/0/1/0/all/0/1\">Markus Huellebrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1\">Raabid Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivantsits_M/0/1/0/all/0/1\">Matthias Ivantsits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1\">Craig Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rishabh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jixi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsekos_N/0/1/0/all/0/1\">Nikolaos V. Tsekos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varela_M/0/1/0/all/0/1\">Marta Varela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hannu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuncheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couturier_R/0/1/0/all/0/1\">Raphael Couturier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meriaudeau_F/0/1/0/all/0/1\">Fabrice Meriaudeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation. (arXiv:2108.04023v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04023","description":"<p>We present a novel and flexible architecture for point cloud segmentation\nwith dual-representation iterative learning. In point cloud processing,\ndifferent representations have their own pros and cons. Thus, finding suitable\nways to represent point cloud data structure while keeping its own internal\nphysical property such as permutation and scale-invariant is a fundamental\nproblem. Therefore, we propose our work, DRINet, which serves as the basic\nnetwork structure for dual-representation learning with great flexibility at\nfeature transferring and less computation cost, especially for large-scale\npoint clouds. DRINet mainly consists of two modules called Sparse Point-Voxel\nFeature Extraction and Sparse Voxel-Point Feature Extraction. By utilizing\nthese two modules iteratively, features can be propagated between two different\nrepresentations. We further propose a novel multi-scale pooling layer for\npointwise locality learning to improve context information propagation. Our\nnetwork achieves state-of-the-art results for point cloud classification and\nsegmentation tasks on several datasets while maintaining high runtime\nefficiency. For large-scale outdoor scenarios, our method outperforms\nstate-of-the-art methods with a real-time inference speed of 62ms per frame.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Maosheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuangjie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tongyi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models. (arXiv:2108.04024v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04024","description":"<p>We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&amp;L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1\">Cristian Rodriguez-Opazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1\">Stephen Gould</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Yard: One-Shot Algorithm of Hardware-Friendly Tensor-Train Decomposition for Convolutional Neural Networks. (arXiv:2108.04029v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04029","description":"<p>Nowadays Deep Learning became widely used in many economic, technical and\nscientific areas of human interest. It is clear that efficiency of solutions\nbased on Deep Neural Networks should consider not only quality metric for the\ntarget task, but also latency and constraints of target platform design should\nbe taken into account. In this paper we present novel hardware-friendly\nTensor-Train decomposition implementation for Convolutional Neural Networks\ntogether with Tensor Yard - one-shot training algorithm which optimizes an\norder of decomposition of network layers. These ideas allow to accelerate\nResNet models on Ascend 310 NPU devices without significant loss of accuracy.\nFor example we accelerate ResNet-101 by 14.6% with drop by 0.5 of top-1\nImageNet accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taskynov_A/0/1/0/all/0/1\">Anuar Taskynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korviakov_V/0/1/0/all/0/1\">Vladimir Korviakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurenko_I/0/1/0/all/0/1\">Ivan Mazurenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yepan Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stream Convolutional Networks for Multi-frame Face Anti-spoofing. (arXiv:2108.04032v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04032","description":"<p>Face anti-spoofing is an important task to protect the security of face\nrecognition. Most of previous work either struggle to capture discriminative\nand generalizable feature or rely on auxiliary information which is unavailable\nfor most of industrial product. Inspired by the video classification work, we\npropose an efficient two-stream model to capture the key differences between\nlive and spoof faces, which takes multi-frames and RGB difference as input\nrespectively. Feature pyramid modules with two opposite fusion directions and\npyramid pooling modules are applied to enhance feature representation. We\nevaluate the proposed method on the datasets of Siw, Oulu-NPU, CASIA-MFSD and\nReplay-Attack. The results show that our model achieves the state-of-the-art\nresults on most of datasets' protocol with much less parameter size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xiya Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Visual Design Principles in Art and Architecture through Deep Convolutional Neural Networks. (arXiv:2108.04048v1 [cs.CV])","link":"http://arxiv.org/abs/2108.04048","description":"<p>Visual design is associated with the use of some basic design elements and\nprinciples. Those are applied by the designers in the various disciplines for\naesthetic purposes, relying on an intuitive and subjective process. Thus,\nnumerical analysis of design visuals and disclosure of the aesthetic value\nembedded in them are considered as hard. However, it has become possible with\nemerging artificial intelligence technologies. This research aims at a neural\nnetwork model, which recognizes and classifies the design principles over\ndifferent domains. The domains include artwork produced since the late 20th\ncentury; professional photos; and facade pictures of contemporary buildings.\nThe data collection and curation processes, including the production of\ncomputationally-based synthetic dataset, is genuine. The proposed model learns\nfrom the knowledge of myriads of original designs, by capturing the underlying\nshared patterns. It is expected to consolidate design processes by providing an\naesthetic evaluation of the visual compositions with objectivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demir_G/0/1/0/all/0/1\">Gozdenur Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cekmis_A/0/1/0/all/0/1\">Asli Cekmis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yesilkaynak_V/0/1/0/all/0/1\">Vahit Bugra Yesilkaynak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Image Transformation for Inducing Affect. (arXiv:1707.08148v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1707.08148","description":"<p>Current image transformation and recoloring algorithms try to introduce\nartistic effects in the photographed images, based on user input of target\nimage(s) or selection of pre-designed filters. These manipulations, although\nintended to enhance the impact of an image on the viewer, do not include the\noption of image transformation by specifying the affect information. In this\npaper we present an automatic image-transformation method that transforms the\nsource image such that it can induce an emotional affect on the viewer, as\ndesired by the user. Our proposed novel image emotion transfer algorithm does\nnot require a user-specified target image. The proposed algorithm uses features\nextracted from top layers of deep convolutional neural network and the\nuser-specified emotion distribution to select multiple target images from an\nimage database for color transformation, such that the resultant image has\ndesired emotional impact. Our method can handle more diverse set of photographs\nthan the previous methods. We conducted a detailed user study showing the\neffectiveness of our proposed method. A discussion and reasoning of failure\ncases has also been provided, indicating inherent limitation of color-transfer\nbased methods in the use of emotion assignment.\n</p>\n<p>Project Page: <a href=\"http://im.itu.edu.pk/affective-image-transfer/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Afsheen Rafaqat Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BAOD: Budget-Aware Object Detection. (arXiv:1904.05443v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1904.05443","description":"<p>We study the problem of object detection from a novel perspective in which\nannotation budget constraints are taken into consideration, appropriately\ncoined Budget Aware Object Detection (BAOD). When provided with a fixed budget,\nwe propose a strategy for building a diverse and informative dataset that can\nbe used to optimally train a robust detector. We investigate both optimization\nand learning-based methods to sample which images to annotate and what type of\nannotation (strongly or weakly supervised) to annotate them with. We adopt a\nhybrid supervised learning framework to train the object detector from both\nthese types of annotation. We conduct a comprehensive empirical study showing\nthat a handcrafted optimization method outperforms other selection techniques\nincluding random sampling, uncertainty sampling and active learning. By\ncombining an optimal image/annotation selection scheme with hybrid supervised\nlearning to solve the BAOD problem, we show that one can achieve the\nperformance of a strongly supervised detector on PASCAL-VOC 2007 while saving\n12.8% of its original annotation budget. Furthermore, when $100\\%$ of the\nbudget is used, it surpasses this performance by 2.0 mAP percentage points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbelaez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persistence Curves: A canonical framework for summarizing persistence diagrams. (arXiv:1904.07768v4 [cs.CG] UPDATED)","link":"http://arxiv.org/abs/1904.07768","description":"<p>Persistence diagrams are one of the main tools in the field of Topological\nData Analysis (TDA). They contain fruitful information about the shape of data.\nThe use of machine learning algorithms on the space of persistence diagrams\nproves to be challenging as the space lacks an inner product. For that reason,\ntransforming these diagrams in a way that is compatible with machine learning\nis an important topic currently researched in TDA. In this paper, our main\ncontribution consists of three components. First, we develop a general and\nunifying framework of vectorizing diagrams that we call the \\textit{Persistence\nCurves} (PCs), and show that several well-known summaries, such as Persistence\nLandscapes, fall under the PC framework. Second, we propose several new\nsummaries based on PC framework and provide a theoretical foundation for their\nstability analysis. Finally, we apply proposed PCs to two\napplications---texture classification and determining the parameters of a\ndiscrete dynamical system; their performances are competitive with other TDA\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yu-Min Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawson_A/0/1/0/all/0/1\">Austin Lawson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Nonnegative Matrix Factorization for Blind Hyperspectral Unmixing incorporating Endmember Independence. (arXiv:2003.01041v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2003.01041","description":"<p>Hyperspectral unmixing (HU) has become an important technique in exploiting\nhyperspectral data since it decomposes a mixed pixel into a collection of\nendmembers weighted by fractional abundances. The endmembers of a hyperspectral\nimage (HSI) are more likely to be generated by independent sources and be mixed\nin a macroscopic degree before arriving at the sensor element of the imaging\nspectrometer as mixed spectra. Over the past few decades, many attempts have\nfocused on imposing auxiliary constraints on the conventional nonnegative\nmatrix factorization (NMF) framework in order to effectively unmix these mixed\nspectra. As a promising step toward finding an optimum constraint to extract\nendmembers, this paper presents a novel blind HU algorithm, referred to as\nKurtosis-based Smooth Nonnegative Matrix Factorization (KbSNMF) which\nincorporates a novel constraint based on the statistical independence of the\nprobability density functions of endmember spectra. Imposing this constraint on\nthe conventional NMF framework promotes the extraction of independent\nendmembers while further enhancing the parts-based representation of data.\nExperiments conducted on diverse synthetic HSI datasets (with numerous numbers\nof endmembers, spectral bands, pixels, and noise levels) and three standard\nreal HSI datasets demonstrate the validity of the proposed KbSNMF algorithm\ncompared to several state-of-the-art NMF-based HU baselines. The proposed\nalgorithm exhibits superior performance especially in terms of extracting\nendmember spectra from hyperspectral data; therefore, it could uplift the\nperformance of recent deep learning HU methods which utilize the endmember\nspectra as supervisory input data for abundance extraction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ekanayake_E/0/1/0/all/0/1\">E.M.M.B. Ekanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weerasooriya_H/0/1/0/all/0/1\">H.M.H.K. Weerasooriya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ranasinghe_D/0/1/0/all/0/1\">D.Y.L. Ranasinghe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herath_S/0/1/0/all/0/1\">S. Herath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathnayake_B/0/1/0/all/0/1\">B. Rathnayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Godaliyadda_G/0/1/0/all/0/1\">G.M.R.I. Godaliyadda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ekanayake_M/0/1/0/all/0/1\">M.P.B. Ekanayake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herath_H/0/1/0/all/0/1\">H.M.V.R. Herath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective. (arXiv:2005.00959v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2005.00959","description":"<p>Ill-posed linear inverse problems appear in many scientific setups, and are\ntypically addressed by solving optimization problems, which are composed of\ndata fidelity and prior terms. Recently, several works have considered a\nback-projection (BP) based fidelity term as an alternative to the common least\nsquares (LS), and demonstrated excellent results for popular inverse problems.\nThese works have also empirically shown that using the BP term, rather than the\nLS term, requires fewer iterations of optimization algorithms. In this paper,\nwe examine the convergence rate of the projected gradient descent (PGD)\nalgorithm for the BP objective. Our analysis allows to identify an inherent\nsource for its faster convergence compared to using the LS objective, while\nmaking only mild assumptions. We also analyze the more general proximal\ngradient method under a relaxed contraction condition on the proximal mapping\nof the prior. This analysis further highlights the advantage of BP when the\nlinear measurement operator is badly conditioned. Numerical experiments with\nboth $\\ell_1$-norm and GAN-based priors corroborate our theoretical results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Tirer_T/0/1/0/all/0/1\">Tom Tirer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks. (arXiv:2006.09134v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.09134","description":"<p>Generative Adversarial Networks (GANs) are formulated as minimax game\nproblems, whereby generators attempt to approach real data distributions by\nvirtue of adversarial learning against discriminators. The intrinsic problem\ncomplexity poses the challenge to enhance the performance of generative\nnetworks. In this work, we aim to boost model learning from the perspective of\nnetwork architectures, by incorporating recent progress on automated\narchitecture search into GANs. To this end, we propose a fully differentiable\nsearch framework for generative adversarial networks, dubbed alphaGAN. The\nsearching process is formalized as solving a bi-level minimax optimization\nproblem, in which the outer-level objective aims for seeking a suitable network\narchitecture towards pure Nash Equilibrium conditioned on the generator and the\ndiscriminator network parameters optimized with a traditional GAN loss in the\ninner level. The entire optimization performs a first-order method by\nalternately minimizing the two-level objective in a fully differentiable\nmanner, enabling architecture search to be completed in an enormous search\nspace. Extensive experiments on CIFAR-10 and STL-10 datasets show that our\nalgorithm can obtain high-performing architectures only with 3-GPU hours on a\nsingle GPU in the search space comprised of approximate 2 ? 1011 possible\nconfigurations. We also provide a comprehensive analysis on the behavior of the\nsearching process and the properties of searched architectures, which would\nbenefit further research on architectures for generative models. Pretrained\nmodels and codes are available at https://github.com/yuesongtian/AlphaGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuesong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guinan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Negative Transfer. (arXiv:2009.00909v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.00909","description":"<p>Transfer learning (TL) utilizes data or knowledge from one or more source\ndomains to facilitate the learning in a target domain. It is particularly\nuseful when the target domain has very few or no labeled data, due to\nannotation expense, privacy concerns, etc. Unfortunately, the effectiveness of\nTL is not always guaranteed. Negative transfer (NT), i.e., leveraging source\ndomain data/knowledge undesirably reduces the learning performance in the\ntarget domain, has been a long-standing and challenging problem in TL. Various\napproaches have been proposed in the literature to handle it. However, there\ndoes not exist a systematic survey on the formulation of NT, the factors\nleading to NT, and the algorithms that mitigate NT. This paper fills this gap,\nby first introducing the definition of NT and its factors, then reviewing about\nfifty representative approaches for overcoming NT, according to four\ncategories: secure transfer, domain similarity estimation, distant transfer,\nand NT mitigation. NT in related fields, e.g., multi-task learning, lifelong\nlearning, and adversarial attacks, are also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lingfei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DanceIt: Music-inspired Dancing Video Synthesis. (arXiv:2009.08027v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.08027","description":"<p>Close your eyes and listen to music, one can easily imagine an actor dancing\nrhythmically along with the music. These dance movements are usually made up of\ndance movements you have seen before. In this paper, we propose to reproduce\nsuch an inherent capability of the human-being within a computer vision system.\nThe proposed system consists of three modules. To explore the relationship\nbetween music and dance movements, we propose a cross-modal alignment module\nthat focuses on dancing video clips, accompanied on pre-designed music, to\nlearn a system that can judge the consistency between the visual features of\npose sequences and the acoustic features of music. The learned model is then\nused in the imagination module to select a pose sequence for the given music.\nSuch pose sequence selected from the music, however, is usually discontinuous.\nTo solve this problem, in the spatial-temporal alignment module we develop a\nspatial alignment algorithm based on the tendency and periodicity of dance\nmovements to predict dance movements between discontinuous fragments. In\naddition, the selected pose sequence is often misaligned with the music beat.\nTo solve this problem, we further develop a temporal alignment algorithm to\nalign the rhythm of music and dance. Finally, the processed pose sequence is\nused to synthesize realistic dancing videos in the imagination module. The\ngenerated dancing videos match the content and rhythm of the music.\nExperimental results and subjective evaluations show that the proposed approach\ncan perform the function of generating promising dancing videos by inputting\nmusic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Densely Guided Knowledge Distillation using Multiple Teacher Assistants. (arXiv:2009.08825v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.08825","description":"<p>With the success of deep neural networks, knowledge distillation which guides\nthe learning of a small student network from a large teacher network is being\nactively studied for model compression and transfer learning. However, few\nstudies have been performed to resolve the poor learning issue of the student\nnetwork when the student and teacher model sizes significantly differ. In this\npaper, we propose a densely guided knowledge distillation using multiple\nteacher assistants that gradually decreases the model size to efficiently\nbridge the large gap between the teacher and student networks. To stimulate\nmore efficient learning of the student network, we guide each teacher assistant\nto every other smaller teacher assistants iteratively. Specifically, when\nteaching a smaller teacher assistant at the next step, the existing larger\nteacher assistants from the previous step are used as well as the teacher\nnetwork. Moreover, we design stochastic teaching where, for each mini-batch, a\nteacher or teacher assistants are randomly dropped. This acts as a regularizer\nto improve the efficiency of teaching of the student network. Thus, the student\ncan always learn salient distilled knowledge from the multiple sources. We\nverified the effectiveness of the proposed method for a classification task\nusing CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant\nperformance improvements with various backbone architectures such as ResNet,\nWideResNet, and VGG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Son_W/0/1/0/all/0/1\">Wonchul Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_J/0/1/0/all/0/1\">Jaemin Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Junyong Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonjun Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.05690","description":"<p>The issue of COVID-19, increasing with a massive mortality rate. This led to\nthe WHO declaring it as a pandemic. In this situation, it is crucial to perform\nefficient and fast diagnosis. The reverse transcript polymerase chain reaction\n(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is\ntime-consuming and instead chest CT (or Chest X-ray) can be used for a fast and\naccurate diagnosis. Automated diagnosis is considered to be important as it\nreduces human effort and provides accurate and low-cost tests. The\ncontributions of our research are three-fold. First, it is aimed to analyse the\nbehaviour and performance of variant vision models ranging from Inception to\nNAS networks with the appropriate fine-tuning procedure. Second, the behaviour\nof these models is visually analysed by plotting CAMs for individual networks\nand determining classification performance with AUCROC curves. Thirdly, stacked\nensembles techniques are imparted to provide higher generalisation on combining\nthe fine-tuned models, in which six ensemble neural networks are designed by\ncombining the existing fine-tuned networks. Implying these stacked ensembles\nprovides a great generalization to the models. The ensemble model designed by\ncombining all the fine-tuned networks obtained a state-of-the-art accuracy\nscore of 99.17%. The precision and recall for the COVID-19 class are 99.99% and\n89.79% respectively, which resembles the robustness of the stacked ensembles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1\">Lalith Bharadwaj B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1\">Rohit Boddeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Sai Vardhan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1\">Madhu G</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimizing Labeling Effort for Tree Skeleton Segmentation using an Automated Iterative Training Methodology. (arXiv:2010.08296v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.08296","description":"<p>Training of convolutional neural networks for semantic segmentation requires\naccurate pixel-wise labeling which requires large amounts of human effort. The\nhuman-in-the-loop method reduces labeling effort; however, it requires human\nintervention for each image. This paper describes a general iterative training\nmethodology for semantic segmentation, Automating-the-Loop. This aims to\nreplicate the manual adjustments of the human-in-the-loop method with an\nautomated process, hence, drastically reducing labeling effort. Using the\napplication of detecting partially occluded apple tree segmentation, we compare\nmanually labeled annotations, self-training, human-in-the-loop, and\nAutomating-the-Loop methods in both the quality of the trained convolutional\nneural networks, and the effort needed to create them. The convolutional neural\nnetwork (U-Net) performance is analyzed using traditional metrics and a new\nmetric, Complete Grid Scan, which promotes connectivity and low noise. It is\nshown that in our application, the new Automating-the-Loop method greatly\nreduces the labeling effort while producing comparable performance to both\nhuman-in-the-loop and complete manual labeling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Granland_K/0/1/0/all/0/1\">Keenan Granland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newbury_R/0/1/0/all/0/1\">Rhys Newbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ting_D/0/1/0/all/0/1\">David Ting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Refinement Network for Point Cloud Completion with Self-supervision. (arXiv:2010.08719v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.08719","description":"<p>Point clouds are often sparse and incomplete, which imposes difficulties for\nreal-world applications. Existing shape completion methods tend to generate\nrough shapes without fine-grained details. Considering this, we introduce a\ntwo-branch network for shape completion. The first branch is a cascaded shape\ncompletion sub-network to synthesize complete objects, where we propose to use\nthe partial input together with the coarse output to preserve the object\ndetails during the dense point reconstruction. The second branch is an\nauto-encoder to reconstruct the original partial input. The two branches share\na same feature extractor to learn an accurate global feature for shape\ncompletion. Furthermore, we propose two strategies to enable the training of\nour network when ground truth data are not available. This is to mitigate the\ndependence of existing approaches on large amounts of ground truth training\ndata that are often difficult to obtain in real-world applications.\nAdditionally, our proposed strategies are also able to improve the\nreconstruction quality for fully supervised learning. We verify our approach in\nself-supervised, semi-supervised and fully supervised settings with superior\nperformances. Quantitative and qualitative results on different datasets\ndemonstrate that our method achieves more realistic outputs than\nstate-of-the-art approaches on the point cloud completion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGIC: Multigrid-in-Channels Neural Network Architectures. (arXiv:2011.09128v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09128","description":"<p>We present a multigrid-in-channels (MGIC) approach that tackles the quadratic\ngrowth of the number of parameters with respect to the number of channels in\nstandard convolutional neural networks (CNNs). Thereby our approach addresses\nthe redundancy in CNNs that is also exposed by the recent success of\nlightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard\nCNNs with fewer parameters; however, the number of weights still scales\nquadratically with the CNN's width. Our MGIC architectures replace each CNN\nblock with an MGIC counterpart that utilizes a hierarchy of nested grouped\nconvolutions of small group size to address this.\n</p>\n<p>Hence, our proposed architectures scale linearly with respect to the\nnetwork's width while retaining full coupling of the channels as in standard\nCNNs.\n</p>\n<p>Our extensive experiments on image classification, segmentation, and point\ncloud classification show that applying this strategy to different\narchitectures like ResNet and MobileNetV3 reduces the number of parameters\nwhile obtaining similar or better accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1\">Moshe Eliasof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ephrath_J/0/1/0/all/0/1\">Jonathan Ephrath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruthotto_L/0/1/0/all/0/1\">Lars Ruthotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedestrian Trajectory Prediction using Context-Augmented Transformer Networks. (arXiv:2012.01757v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01757","description":"<p>Forecasting the trajectory of pedestrians in shared urban traffic\nenvironments is still considered one of the challenging problems facing the\ndevelopment of autonomous vehicles (AVs). In the literature, this problem is\noften tackled using recurrent neural networks (RNNs). Despite the powerful\ncapabilities of RNNs in capturing the temporal dependency in the pedestrians'\nmotion trajectories, they were argued to be challenged when dealing with longer\nsequential data. Thus, in this work, we are introducing a framework based on\nthe transformer networks that were shown recently to be more efficient and\noutperformed RNNs in many sequential-based tasks. We relied on a fusion of the\npast positional information, agent interactions information and scene physical\nsemantics information as an input to our framework in order to provide a robust\ntrajectory prediction of pedestrians. We have evaluated our framework on two\nreal-life datasets of pedestrians in shared urban traffic environments and it\nhas outperformed the compared baseline approaches in both short-term and\nlong-term prediction horizons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_K/0/1/0/all/0/1\">Khaled Saleh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChartPointFlow for Topology-Aware 3D Point Cloud Generation. (arXiv:2012.02346v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02346","description":"<p>A point cloud serves as a representation of the surface of a\nthree-dimensional (3D) shape. Deep generative models have been adapted to model\ntheir variations typically using a map from a ball-like set of latent\nvariables. However, previous approaches did not pay much attention to the\ntopological structure of a point cloud, despite that a continuous map cannot\nexpress the varying numbers of holes and intersections. Moreover, a point cloud\nis often composed of multiple subparts, and it is also difficult to express. In\nthis study, we propose ChartPointFlow, a flow-based generative model with\nmultiple latent labels for 3D point clouds. Each label is assigned to points in\nan unsupervised manner. Then, a map conditioned on a label is assigned to a\ncontinuous subset of a point cloud, similar to a chart of a manifold. This\nenables our proposed model to preserve the topological structure with clear\nboundaries, whereas previous approaches tend to generate blurry point clouds\nand fail to generate holes. The experimental results demonstrate that\nChartPointFlow achieves state-of-the-art performance in terms of generation and\nreconstruction compared with other point cloud generators. Moreover,\nChartPointFlow divides an object into semantic subparts using charts, and it\ndemonstrates superior performance in case of unsupervised segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kimura_T/0/1/0/all/0/1\">Takumi Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takashi Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kuniaki Uehara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bifold and Semantic Reasoning for Pedestrian Behavior Prediction. (arXiv:2012.03298v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.03298","description":"<p>Pedestrian behavior prediction is one of the major challenges for intelligent\ndriving systems. Pedestrians often exhibit complex behaviors influenced by\nvarious contextual elements. To address this problem, we propose BiPed, a\nmultitask learning framework that simultaneously predicts trajectories and\nactions of pedestrians by relying on multimodal data. Our method benefits from\n1) a bifold encoding approach where different data modalities are processed\nindependently allowing them to develop their own representations, and jointly\nto produce a representation for all modalities using shared parameters; 2) a\nnovel interaction modeling technique that relies on categorical semantic\nparsing of the scenes to capture interactions between target pedestrians and\ntheir surroundings; and 3) a bifold prediction mechanism that uses both\nindependent and shared decoding of multimodal representations. Using public\npedestrian behavior benchmark datasets for driving, PIE and JAAD, we highlight\nthe benefits of the proposed method for behavior prediction and show that our\nmodel achieves state-of-the-art performance and improves trajectory and action\nprediction by up to 22% and 9% respectively. We further investigate the\ncontributions of the proposed reasoning techniques via extensive ablation\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1\">Amir Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohani_M/0/1/0/all/0/1\">Mohsen Rohani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jun Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViNet: Pushing the limits of Visual Modality for Audio-Visual Saliency Prediction. (arXiv:2012.06170v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.06170","description":"<p>We propose the ViNet architecture for audio-visual saliency prediction. ViNet\nis a fully convolutional encoder-decoder architecture. The encoder uses visual\nfeatures from a network trained for action recognition, and the decoder infers\na saliency map via trilinear interpolation and 3D convolutions, combining\nfeatures from multiple hierarchies. The overall architecture of ViNet is\nconceptually simple; it is causal and runs in real-time (60 fps). ViNet does\nnot use audio as input and still outperforms the state-of-the-art audio-visual\nsaliency prediction models on nine different datasets (three visual-only and\nsix audio-visual datasets). ViNet also surpasses human performance on the CC,\nSIM and AUC metrics for the AVE dataset, and to our knowledge, it is the first\nnetwork to do so. We also explore a variation of ViNet architecture by\naugmenting audio features into the decoder. To our surprise, upon sufficient\ntraining, the network becomes agnostic to the input audio and provides the same\noutput irrespective of the input. Interestingly, we also observe similar\nbehaviour in the previous state-of-the-art models \\cite{tsiami2020stavis} for\naudio-visual saliency prediction. Our findings contrast with previous works on\ndeep learning-based audio-visual saliency prediction, suggesting a clear avenue\nfor future explorations incorporating audio in a more effective manner. The\ncode and pre-trained models are available at\nhttps://github.com/samyak0210/ViNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Samyak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarlagadda_P/0/1/0/all/0/1\">Pradeep Yarlagadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyoti_S/0/1/0/all/0/1\">Shreyank Jyoti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_R/0/1/0/all/0/1\">Ramanathan Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Monocular Hand Pose Estimation on Embedded Systems. (arXiv:2102.07067v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2102.07067","description":"<p>Hand pose estimation is a fundamental task in many human-robot\ninteraction-related applications. However, previous approaches suffer from\nunsatisfying hand landmark predictions in real-world scenes and high\ncomputation burden. In this paper, we propose a fast and accurate framework for\nhand pose estimation, dubbed as \"FastHand\". Using a lightweight encoder-decoder\nnetwork architecture, FastHand fulfills the requirements of practical\napplications running on embedded devices. The encoder consists of deep layers\nwith a small number of parameters, while the decoder makes use of spatial\nlocation information to obtain more accurate results. The evaluation took place\non two publicly available datasets demonstrating the improved performance of\nthe proposed pipeline compared to other state-of-the-art approaches. FastHand\noffers high accuracy scores while reaching a speed of 25 frames per second on\nan NVIDIA Jetson TX2 graphics processing unit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haogang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsintotas_K/0/1/0/all/0/1\">Konstantinos A. Tsintotas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiconNet: An Edge-preserved Connectivity-based Approach for Salient Object Detection. (arXiv:2103.00334v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.00334","description":"<p>Salient object detection (SOD) is viewed as a pixel-wise saliency modeling\ntask by traditional deep learning-based methods. A limitation of current SOD\nmodels is insufficient utilization of inter-pixel information, which usually\nresults in imperfect segmentation near edge regions and low spatial coherence.\nAs we demonstrate, using a saliency mask as the only label is suboptimal. To\naddress this limitation, we propose a connectivity-based approach called\nbilateral connectivity network (BiconNet), which uses connectivity masks\ntogether with saliency masks as labels for effective modeling of inter-pixel\nrelationships and object saliency. Moreover, we propose a bilateral voting\nmodule to enhance the output connectivity map, and a novel edge feature\nenhancement method that efficiently utilizes edge-specific features. Through\ncomprehensive experiments on five benchmark datasets, we demonstrate that our\nproposed method can be plugged into any existing state-of-the-art\nsaliency-based SOD framework to improve its performance with negligible\nparameter increase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanian_Zadeh_S/0/1/0/all/0/1\">Somayyeh Soltanian-Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farsiu_S/0/1/0/all/0/1\">Sina Farsiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Consistency Regularization for Semi-Supervised Transfer Learning. (arXiv:2103.02193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02193","description":"<p>While recent studies on semi-supervised learning have shown remarkable\nprogress in leveraging both labeled and unlabeled data, most of them presume a\nbasic setting of the model is randomly initialized. In this work, we consider\nsemi-supervised learning and transfer learning jointly, leading to a more\npractical and competitive paradigm that can utilize both powerful pre-trained\nmodels from source domain as well as labeled/unlabeled data in the target\ndomain. To better exploit the value of both pre-trained weights and unlabeled\ntarget examples, we introduce adaptive consistency regularization that consists\nof two complementary components: Adaptive Knowledge Consistency (AKC) on the\nexamples between the source and target model, and Adaptive Representation\nConsistency (ARC) on the target model between labeled and unlabeled examples.\nExamples involved in the consistency regularization are adaptively selected\naccording to their potential contributions to the target task. We conduct\nextensive experiments on popular benchmarks including CIFAR-10, CUB-200, and\nMURA, by fine-tuning the ImageNet pre-trained ResNet-50 model. Results show\nthat our proposed adaptive consistency regularization outperforms\nstate-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean\nTeacher, and FixMatch. Moreover, our algorithm is orthogonal to existing\nmethods and thus able to gain additional improvements on top of MixMatch and\nFixMatch. Our code is available at\nhttps://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolution Neural Network Hyperparameter Optimization Using Simplified Swarm Optimization. (arXiv:2103.03995v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2103.03995","description":"<p>Convolutional neural networks (CNNs) are widely used in image recognition.\nNumerous CNN models, such as LeNet, AlexNet, VGG, ResNet, and GoogLeNet, have\nbeen proposed by increasing the number of layers, to improve the performance of\nCNNs. However, performance deteriorates beyond a certain number of layers.\nHence, hyperparameter optimisation is a more efficient way to improve CNNs. To\nvalidate this concept, a new algorithm based on simplified swarm optimisation\nis proposed to optimise the hyperparameters of the simplest CNN model, which is\nLeNet. The results of experiments conducted on the MNIST, Fashion MNIST, and\nCifar10 datasets showed that the accuracy of the proposed algorithm is higher\nthan the original LeNet model and PSO-LeNet and that it has a high potential to\nbe extended to more complicated models, such as AlexNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_W/0/1/0/all/0/1\">Wei-Chang Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi-Ping Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yun-Chia Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chyh-Ming Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.05248","description":"<p>Recent studies unveil the vulnerabilities of deep ranking models, where an\nimperceptible perturbation can trigger dramatic changes in the ranking result.\nWhile previous attempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order remains\nunder-explored. In this paper, we formulate a new adversarial attack against\ndeep ranking systems, i.e., the Order Attack, which covertly alters the\nrelative order among a selected set of candidates according to an\nattacker-specified permutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style loss imposing an\ninequality chain reflecting the specified permutation. However, direct\noptimization of such white-box objective is infeasible in a real-world attack\nscenario due to various black-box limitations. To cope with them, we propose a\nShort-range Ranking Correlation metric as a surrogate objective for black-box\nOrder Attack to approximate the white-box method. The Order Attack is evaluated\non the Fashion-MNIST and Stanford-Online-Products datasets under both white-box\nand black-box threat models. The black-box attack is also successfully\nimplemented on a major e-commerce platform. Comprehensive experimental\nevaluations demonstrate the effectiveness of the proposed methods, revealing a\nnew type of ranking model vulnerability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-energizing Domain Discriminator with Sample Relabeling for Adversarial Domain Adaptation. (arXiv:2103.11661v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11661","description":"<p>Many unsupervised domain adaptation (UDA) methods exploit domain adversarial\ntraining to align the features to reduce domain gap, where a feature extractor\nis trained to fool a domain discriminator in order to have aligned feature\ndistributions. The discrimination capability of the domain classifier w.r.t the\nincreasingly aligned feature distributions deteriorates as training goes on,\nthus cannot effectively further drive the training of feature extractor. In\nthis work, we propose an efficient optimization strategy named Re-enforceable\nAdversarial Domain Adaptation (RADA) which aims to re-energize the domain\ndiscriminator during the training by using dynamic domain labels. Particularly,\nwe relabel the well aligned target domain samples as source domain samples on\nthe fly. Such relabeling makes the less separable distributions more separable,\nand thus leads to a more powerful domain classifier w.r.t. the new data\ndistributions, which in turn further drives feature alignment. Extensive\nexperiments on multiple UDA benchmarks demonstrate the effectiveness and\nsuperiority of our RADA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Exposing the Challenging Long Tail in Future Prediction of Traffic Actors. (arXiv:2103.12474v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12474","description":"<p>Predicting the states of dynamic traffic actors into the future is important\nfor autonomous systems to operate safelyand efficiently. Remarkably, the most\ncritical scenarios aremuch less frequent and more complex than the\nuncriticalones. Therefore, uncritical cases dominate the prediction. In this\npaper, we address specifically the challenging scenarios at the long tail of\nthe dataset distribution. Our analysis shows that the common losses tend to\nplace challenging cases suboptimally in the embedding space. As a consequence,\nwe propose to supplement the usual loss with aloss that places challenging\ncases closer to each other. This triggers sharing information among challenging\ncases andlearning specific predictive features. We show on four public datasets\nthat this leads to improved performance on the challenging scenarios while the\noverall performance stays stable. The approach is agnostic w.r.t. the used\nnetwork architecture, input modality or viewpoint, and can be integrated into\nexisting solutions easily. Code is available at\nhttps://github.com/lmb-freiburg/Contrastive-Future-Trajectory-Prediction\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makansi_O/0/1/0/all/0/1\">Osama Makansi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicek_O/0/1/0/all/0/1\">&#xd6;zg&#xfc;n Cicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrakchi_Y/0/1/0/all/0/1\">Yassine Marrakchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Latent Classes for Few-shot Segmentation. (arXiv:2103.15402v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15402","description":"<p>Few-shot segmentation (FSS) aims to segment unseen classes given only a few\nannotated samples. Existing methods suffer the problem of feature undermining,\ni.e. potential novel classes are treated as background during training phase.\nOur method aims to alleviate this problem and enhance the feature embedding on\nlatent novel classes. In our work, we propose a novel joint-training framework.\nBased on conventional episodic training on support-query pairs, we add an\nadditional mining branch that exploits latent novel classes via transferable\nsub-clusters, and a new rectification technique on both background and\nforeground categories to enforce more stable prototypes. Over and above that,\nour transferable sub-cluster has the ability to leverage extra unlabeled data\nfor further feature enhancement. Extensive experiments on two FSS benchmarks\ndemonstrate that our method outperforms previous state-of-the-art by a large\nmargin of 3.7% mIOU on PASCAL-5i and 7.0% mIOU on COCO-20i at the cost of 74%\nfewer parameters and 2.5x faster inference speed. The source code is available\nat https://github.com/LiheYoung/MiningFSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lihe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-based Differentiable Depth Sensor Simulation. (arXiv:2103.16563v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16563","description":"<p>Gradient-based algorithms are crucial to modern computer-vision and graphics\napplications, enabling learning-based optimization and inverse problems. For\nexample, photorealistic differentiable rendering pipelines for color images\nhave been proven highly valuable to applications aiming to map 2D and 3D\ndomains. However, to the best of our knowledge, no effort has been made so far\ntowards extending these gradient-based methods to the generation of depth\n(2.5D) images, as simulating structured-light depth sensors implies solving\ncomplex light transport and stereo-matching problems. In this paper, we\nintroduce a novel end-to-end differentiable simulation pipeline for the\ngeneration of realistic 2.5D scans, built on physics-based 3D rendering and\ncustom block-matching algorithms. Each module can be differentiated w.r.t\nsensor and scene parameters; e.g., to automatically tune the simulation for new\ndevices over some provided scans or to leverage the pipeline as a 3D-to-2.5D\ntransformer within larger computer-vision applications. Applied to the training\nof deep-learning methods for various depth-based recognition tasks\n(classification, pose estimation, semantic segmentation), our simulation\ngreatly improves the performance of the resulting models on real scans, thereby\ndemonstrating the fidelity and value of its synthetic depth data compared to\nprevious static simulations and learning-based domain adaptation schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Planche_B/0/1/0/all/0/1\">Benjamin Planche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajat Vikram Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Camera Simulators. (arXiv:2104.05237v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05237","description":"<p>We present a controllable camera simulator based on deep neural networks to\nsynthesize raw image data under different camera settings, including exposure\ntime, ISO, and aperture. The proposed simulator includes an exposure module\nthat utilizes the principle of modern lens designs for correcting the luminance\nlevel. It also contains a noise module using the noise level function and an\naperture module with adaptive attention to simulate the side effects on noise\nand defocus blur. To facilitate the learning of a simulator model, we collect a\ndataset of the 10,000 raw images of 450 scenes with different exposure\nsettings. Quantitative experiments and qualitative comparisons show that our\napproach outperforms relevant baselines in raw data synthesize on multiple\ncameras. Furthermore, the camera simulator enables various applications,\nincluding large-aperture enhancement, HDR, auto exposure, and data augmentation\nfor training local feature detectors. Our work represents the first attempt to\nsimulate a camera sensor's behavior leveraging both the advantage of\ntraditional raw sensor features and the power of data-driven deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1\">Hao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zifan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_K/0/1/0/all/0/1\">Ka Lung Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN. (arXiv:2104.06534v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06534","description":"<p>Existing thermal-to-visible face verification approaches expect the thermal\nand visible face images to be of similar resolution. This is unlikely in\nreal-world long-range surveillance systems, since humans are distant from the\ncameras. To address this issue, we introduce the task of thermal-to-visible\nface verification from low-resolution thermal images. Furthermore, we propose\nAxial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution\nvisible images for matching. In the proposed approach we augment the GAN\nframework with axial-attention layers which leverage the recent advances in\ntransformers for modelling long-range dependencies. We demonstrate the\neffectiveness of the proposed method by evaluating on two different\nthermal-visible face datasets. When compared to related state-of-the-art works,\nour results show significant improvements in both image quality and face\nverification performance, and are also much more efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Immidisetti_R/0/1/0/all/0/1\">Rakhil Immidisetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent Monitoring of Stress Induced by Water Deficiency in Plants using Deep Learning. (arXiv:2104.07911v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07911","description":"<p>In the recent decade, high-throughput plant phenotyping techniques, which\ncombine non-invasive image analysis and machine learning, have been\nsuccessfully applied to identify and quantify plant health and diseases.\nHowever, these techniques usually do not consider the progressive nature of\nplant stress and often require images showing severe signs of stress to ensure\nhigh confidence detection, thereby reducing the feasibility for early detection\nand recovery of plants under stress. To overcome the problem mentioned above,\nwe propose a deep learning pipeline for the temporal analysis of the visual\nchanges induced in the plant due to stress and apply it for the specific case\nof water stress identification in Chickpea plant shoot images. For this, we\nhave considered an image dataset of two chickpea varieties JG-62 and Pusa-372,\nunder three water stress conditions; control, young seedling, and before\nflowering, captured over five months. We have employed a variant of the\nLong-term Recurrent Convolutional Network (LRCN) to learn spatio-temporal\npatterns from the chickpea plant dataset and use them for water stress\nclassification. Our model has achieved ceiling level classification performance\nof 98.52% on JG-62 and 97.78% on Pusa-372 chickpea plant data and has\noutperformed the state-of-the-art time-invariant technique by at least 14% for\nboth JG-62 and Pusa-372 species, to the best of our knowledge. Furthermore, our\nLRCN model has demonstrated robustness to noisy input, with a less than 2.5%\ndip in average model accuracy and a small standard deviation about the mean for\nboth species. Lastly, we have performed an ablation study to analyze the\nperformance of the LRCN model by decreasing the number of temporal session data\nused for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azimi_S/0/1/0/all/0/1\">Shiva Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1\">Rohan Wadhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan K. Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining a Convolutional Neural Network with Autoencoders to Predict the Survival Chance of COVID-19 Patients. (arXiv:2104.08954v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.08954","description":"<p>COVID-19 has caused many deaths worldwide. The automation of the diagnosis of\nthis virus is highly desired. Convolutional neural networks (CNNs) have shown\noutstanding classification performance on image datasets. To date, it appears\nthat COVID computer-aided diagnosis systems based on CNNs and clinical\ninformation have not yet been analysed or explored. We propose a novel method,\nnamed the CNN-AE, to predict the survival chance of COVID-19 patients using a\nCNN trained with clinical information. Notably, the required resources to\nprepare CT images are expensive and limited compared to those required to\ncollect clinical data, such as blood pressure, liver disease, etc. We evaluated\nour method using a publicly available clinical dataset that we collected. The\ndataset properties were carefully analysed to extract important features and\ncompute the correlations of features. A data augmentation procedure based on\nautoencoders (AEs) was proposed to balance the dataset. The experimental\nresults revealed that the average accuracy of the CNN-AE (96.05%) was higher\nthan that of the CNN (92.49%). To demonstrate the generality of our\naugmentation method, we trained some existing mortality risk prediction methods\non our dataset (with and without data augmentation) and compared their\nperformances. We also evaluated our method using another dataset for further\ngenerality verification. To show that clinical data can be used for COVID-19\nsurvival chance prediction, the CNN-AE was compared with multiple pre-trained\ndeep models that were tuned based on CT images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khozeimeh_F/0/1/0/all/0/1\">Fahime Khozeimeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izadi_N/0/1/0/all/0/1\">Navid Hoseini Izadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1\">Sadiq Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sani_Z/0/1/0/all/0/1\">Zahra Alizadeh Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1\">Hossein Moosaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sheikh Mohammed Shariful Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DisCo: Remedy Self-supervised Learning on Lightweight Models with Distilled Contrastive Learning. (arXiv:2104.09124v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09124","description":"<p>While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4\\%/16.3\\% of\nResNet-101/ResNet-50. Code is available at https://github.\ncom/Yuting-Gao/DisCo-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jia-Xin Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaowei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EigenGAN: Layer-Wise Eigen-Learning for GANs. (arXiv:2104.12476v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12476","description":"<p>Recent studies on Generative Adversarial Network (GAN) reveal that different\nlayers of a generative CNN hold different semantics of the synthesized images.\nHowever, few GAN models have explicit dimensions to control the semantic\nattributes represented in a specific layer. This paper proposes EigenGAN which\nis able to unsupervisedly mine interpretable and controllable dimensions from\ndifferent generator layers. Specifically, EigenGAN embeds one linear subspace\nwith orthogonal basis into each generator layer. Via generative adversarial\ntraining to learn a target distribution, these layer-wise subspaces\nautomatically discover a set of \"eigen-dimensions\" at each layer corresponding\nto a set of semantic attributes or interpretable variations. By traversing the\ncoefficient of a specific eigen-dimension, the generator can produce samples\nwith continuous changes corresponding to a specific semantic attribute. Taking\nthe human face for example, EigenGAN can discover controllable dimensions for\nhigh-level concepts such as pose and gender in the subspace of deep layers, as\nwell as low-level concepts such as hue and color in the subspace of shallow\nlayers. Moreover, in the linear case, we theoretically prove that our algorithm\nderives the principal components as PCA does. Codes can be found in\nhttps://github.com/LynnHo/EigenGAN-Tensorflow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhenliang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Meina Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Series Forecasting of New Cases and New Deaths Rate for COVID-19 using Deep Learning Methods. (arXiv:2104.15007v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.15007","description":"<p>The first known case of Coronavirus disease 2019 (COVID-19) was identified in\nDecember 2019. It has spread worldwide, leading to an ongoing pandemic, imposed\nrestrictions and costs to many countries. Predicting the number of new cases\nand deaths during this period can be a useful step in predicting the costs and\nfacilities required in the future. The purpose of this study is to predict new\ncases and deaths rate one, three and seven-day ahead during the next 100 days.\nThe motivation for predicting every n days (instead of just every day) is the\ninvestigation of the possibility of computational cost reduction and still\nachieving reasonable performance. Such a scenario may be encountered real-time\nforecasting of time series. Six different deep learning methods are examined on\nthe data adopted from the WHO website. Three methods are LSTM, Convolutional\nLSTM, and GRU. The bidirectional extension is then considered for each method\nto forecast the rate of new cases and new deaths in Australia and Iran\ncountries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayoobi_N/0/1/0/all/0/1\">Nooshin Ayoobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1\">Hossein Moosaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chofreh_A/0/1/0/all/0/1\">Abdoulmohammad Gholamzadeh Chofreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goni_F/0/1/0/all/0/1\">Feybi Ariani Goni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemes_J/0/1/0/all/0/1\">Jiri Jaromir Klemes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation. (arXiv:2105.00591v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00591","description":"<p>In the past few years, mobile deep-learning deployment progressed by leaps\nand bounds, but solutions still struggle to accommodate its severe and\nfluctuating operational restrictions, which include bandwidth, latency,\ncomputation, and energy. In this work, we help to bridge that gap, introducing\nthe first configurable solution for object detection that manages the triple\ncommunication-computation-accuracy trade-off with a single set of weights. Our\nsolution shows state-of-the-art results on COCO-2017, adding only a minor\npenalty on the base EfficientDet-D2 architecture. Our design is robust to the\nchoice of base architecture and compressor and should adapt well for future\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Assine_J/0/1/0/all/0/1\">Juliano S. Assine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filho_J/0/1/0/all/0/1\">J. C. S. Santos Filho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Novel Target Discovery Through Open-Set Domain Adaptation. (arXiv:2105.02432v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.02432","description":"<p>Open-set domain adaptation (OSDA) considers that the target domain contains\nsamples from novel categories unobserved in external source domain.\nUnfortunately, existing OSDA methods always ignore the demand for the\ninformation of unseen categories and simply recognize them as \"unknown\" set\nwithout further explanation. This motivates us to understand the unknown\ncategories more specifically by exploring the underlying structures and\nrecovering their interpretable semantic attributes. In this paper, we propose a\nnovel framework to accurately identify the seen categories in target domain,\nand effectively recover the semantic attributes for unseen categories.\nSpecifically, structure preserving partial alignment is developed to recognize\nthe seen categories through domain-invariant feature learning. Attribute\npropagation over visual graph is designed to smoothly transit attributes from\nseen to unseen categories via visual-semantic mapping. Moreover, two new\ncross-main benchmarks are constructed to evaluate the proposed framework in the\nnovel and practical challenge. Experimental results on open-set recognition and\nsemantic recovery demonstrate the superiority of the proposed method over other\ncompared baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_T/0/1/0/all/0/1\">Taotao Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adaptive Transfer Learning for Multicenter Glaucoma Classification in Fundus Retina Images. (arXiv:2105.03068v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.03068","description":"<p>The early diagnosis and screening of glaucoma are important for patients to\nreceive treatment in time and maintain eyesight. Nowadays, deep learning (DL)\nbased models have been successfully used for computer-aided diagnosis (CAD) of\nglaucoma from retina fundus images. However, a DL model pre-trained using a\ndataset from one hospital center may have poor performance on a dataset from\nanother new hospital center and therefore its applications in the real scene\nare limited. In this paper, we propose a self-adaptive transfer learning (SATL)\nstrategy to fill the domain gap between multicenter datasets. Specifically, the\nencoder of a DL model that is pre-trained on the source domain is used to\ninitialize the encoder of a reconstruction model. Then, the reconstruction\nmodel is trained using only unlabeled image data from the target domain, which\nmakes the encoder in the model adapt itself to extract useful high-level\nfeatures both for target domain images encoding and glaucoma classification,\nsimultaneously. Experimental results demonstrate that the proposed SATL\nstrategy is effective in the domain adaptation task between one private and two\npublic glaucoma diagnosis datasets, i.e. pri-RFG, REFUGE, and LAG. Moreover,\nthe proposed strategy is completely independent of the source domain data,\nwhich meets the real scene application and the privacy protection policy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bao_Y/0/1/0/all/0/1\">Yiming Bao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Linyan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jianwei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Juan Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_D/0/1/0/all/0/1\">Dahong Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applications of Deep Learning Techniques for Automated Multiple Sclerosis Detection Using Magnetic Resonance Imaging: A Review. (arXiv:2105.04881v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.04881","description":"<p>Multiple Sclerosis (MS) is a type of brain disease which causes visual,\nsensory, and motor problems for people with a detrimental effect on the\nfunctioning of the nervous system. In order to diagnose MS, multiple screening\nmethods have been proposed so far; among them, magnetic resonance imaging (MRI)\nhas received considerable attention among physicians. MRI modalities provide\nphysicians with fundamental information about the structure and function of the\nbrain, which is crucial for the rapid diagnosis of MS lesions. Diagnosing MS\nusing MRI is time-consuming, tedious, and prone to manual errors. Hence,\ncomputer aided diagnosis systems (CADS) based on artificial intelligence (AI)\nmethods have been proposed in recent years for accurate diagnosis of MS using\nMRI neuroimaging modalities. In the AI field, automated MS diagnosis is being\nconducted using (i) conventional machine learning and (ii) deep learning (DL)\ntechniques. The conventional machine learning approach is based on feature\nextraction and selection by trial and error. In DL, these steps are performed\nby the DL model itself. In this paper, a complete review of automated MS\ndiagnosis methods performed using DL techniques with MRI neuroimaging\nmodalities are discussed. Also, each work is thoroughly reviewed and discussed.\nFinally, the most important challenges and future directions in the automated\nMS diagnosis using DL techniques coupled with MRI modalities are presented in\ndetail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khodatars_M/0/1/0/all/0/1\">Marjane Khodatars</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jafari_M/0/1/0/all/0/1\">Mahboobeh Jafari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moridian_P/0/1/0/all/0/1\">Parisa Moridian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rezaei_M/0/1/0/all/0/1\">Mitra Rezaei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khozeimeh_F/0/1/0/all/0/1\">Fahime Khozeimeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan Manuel Gorriz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heras_J/0/1/0/all/0/1\">J&#xf3;nathan Heras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panahiazar_M/0/1/0/all/0/1\">Maryam Panahiazar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1\">U. Rajendra Acharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Perspective Anomaly Detection. (arXiv:2105.09903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09903","description":"<p>Anomaly detection is a critical problem in the manufacturing industry. In\nmany applications, images of objects to be analyzed are captured from multiple\nperspectives which can be exploited to improve the robustness of anomaly\ndetection. In this work, we build upon the deep support vector data description\nalgorithm and address multi-perspective anomaly detection using three different\nfusion techniques, i.e., early fusion, late fusion, and late fusion with\nmultiple decoders. We employ different augmentation techniques with a denoising\nprocess to deal with scarce one-class data, which further improves the\nperformance (ROC AUC $= 80\\%$). Furthermore, we introduce the dices dataset,\nwhich consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g., drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed\n{multi-perspective} approach exceeds the state-of-the-art {single-perspective\nanomaly detection on both the MNIST and dices datasets}. To the best of our\nknowledge, this is the first work that focuses on addressing multi-perspective\nanomaly detection in images by jointly using different perspectives together\nwith one single objective function for anomaly detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistenty. (arXiv:2106.05616v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05616","description":"<p>Recovering 3D human pose from 2D joints is still a challenging problem,\nespecially without any 3D annotation, video information, or multi-view\ninformation. In this paper, we present an unsupervised GAN-based model\nconsisting of multiple weight-sharing generators to estimate a 3D human pose\nfrom a single image without 3D annotations. In our model, we introduce\nsingle-view-multi-angle consistency (SVMAC) to significantly improve the\nestimation performance. With 2D joint locations as input, our model estimates a\n3D pose and a camera simultaneously. During training, the estimated 3D pose is\nrotated by random angles and the estimated camera projects the rotated 3D poses\nback to 2D. The 2D reprojections will be fed into weight-sharing generators to\nestimate the corresponding 3D poses and cameras, which are then mixed to impose\nSVMAC constraints to self-supervise the training process. The experimental\nresults show that our method outperforms the state-of-the-art unsupervised\nmethods by 2.6% on Human 3.6M and 15.0% on MPI-INF-3DHP. Moreover, qualitative\nresults on MPII and LSP show that our method can generalize well to unknown\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yicheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiahui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yongqi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Deepfake Detection. (arXiv:2106.10705v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10705","description":"<p>In this paper, we propose to utilize Automated Machine Learning to adaptively\nsearch a neural architecture for deepfake detection. This is the first time to\nemploy automated machine learning for deepfake detection. Based on our explored\nsearch space, our proposed method achieves competitive prediction accuracy\ncompared to previous methods. To improve the generalizability of our method,\nespecially when training data and testing data are manipulated by different\nmethods, we propose a simple yet effective strategy in our network learning\nprocess: making it to estimate potential manipulation regions besides\npredicting the real/fake labels. Unlike previous works manually design neural\nnetworks, our method can relieve us from the high labor cost in network\nconstruction. More than that, compared to previous works, our method depends\nmuch less on prior knowledge, e.g., which manipulation method is utilized or\nwhere exactly the fake image is manipulated. Extensive experimental results on\ntwo benchmark datasets demonstrate the effectiveness of our proposed method for\ndeepfake detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuewei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_L/0/1/0/all/0/1\">Liangli Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team PyKale (xy9) Submission to the EPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2106.12023v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12023","description":"<p>This report describes the technical details of our submission to the\nEPIC-Kitchens 2021 Unsupervised Domain Adaptation Challenge for Action\nRecognition. The EPIC-Kitchens dataset is more difficult than other video\ndomain adaptation datasets due to multi-tasks with more modalities. Firstly, to\nparticipate in the challenge, we employ a transformer to capture the spatial\ninformation from each modality. Secondly, we employ a temporal attention module\nto model temporal-wise inter-dependency. Thirdly, we employ the adversarial\ndomain adaptation network to learn the general features between labeled source\nand unlabeled target domain. Finally, we incorporate multiple modalities to\nimprove the performance by a three-stream network with late fusion. Our network\nachieves the comparable performance with the state-of-the-art baseline T$A^3$N\nand outperforms the baseline on top-1 accuracy for verb class and top-5\naccuracies for all three tasks which are verb, noun and action. Under the team\nname xy9, our submission achieved 5th place in terms of top-1 accuracy for verb\nclass and all top-5 accuracies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Tao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching. (arXiv:2107.00689v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00689","description":"<p>This paper proposes a novel approach to map-based navigation system for\nunmanned aircraft. The proposed system attempts label-to-label matching, not\nimage-to-image matching, between aerial images and a map database. By using\nsemantic segmentation, the ground objects are labelled and the configuration of\nthe objects is used to find the corresponding location in the map database. The\nuse of the deep learning technique as a tool for extracting high-level features\nreduces the image-based localization problem to a pattern matching problem.\nThis paper proposes a pattern matching algorithm which does not require\naltitude information or a camera model to estimate the absolute horizontal\nposition. The feasibility analysis with simulated images shows the proposed\nmap-based navigation can be realized with the proposed pattern matching\nalgorithm and it is able to provide positions given the labelled objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection using Edge Computing in Video Surveillance System: Review. (arXiv:2107.02778v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02778","description":"<p>The current concept of Smart Cities influences urban planners and researchers\nto provide modern, secured and sustainable infrastructure and give a decent\nquality of life to its residents. To fulfill this need video surveillance\ncameras have been deployed to enhance the safety and well-being of the\ncitizens. Despite technical developments in modern science, abnormal event\ndetection in surveillance video systems is challenging and requires exhaustive\nhuman efforts. In this paper, we surveyed various methodologies developed to\ndetect anomalies in intelligent video surveillance. Firstly, we revisit the\nsurveys on anomaly detection in the last decade. We then present a systematic\ncategorization of methodologies developed for ease of understanding.\nConsidering the notion of anomaly depends on context, we identify different\nobjects-of-interest and publicly available datasets in anomaly detection. Since\nanomaly detection is considered a time-critical application of computer vision,\nour emphasis is on anomaly detection using edge devices and approaches\nexplicitly designed for them. Further, we discuss the challenges and\nopportunities involved in anomaly detection at the edge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patrikar_D/0/1/0/all/0/1\">Devashree R. Patrikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1\">Mayur Rajram Parate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLiT: Neural Architecture Search for Global and Local Image Transformer. (arXiv:2107.02960v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02960","description":"<p>We introduce the first Neural Architecture Search (NAS) method to find a\nbetter transformer architecture for image recognition. Recently, transformers\nwithout CNN-based backbones are found to achieve impressive performance for\nimage recognition. However, the transformer is designed for NLP tasks and thus\ncould be sub-optimal when directly used for image recognition. In order to\nimprove the visual representation ability for transformers, we propose a new\nsearch space and searching algorithm. Specifically, we introduce a locality\nmodule that models the local correlations in images explicitly with fewer\ncomputational cost. With the locality module, our search space is defined to\nlet the search algorithm freely trade off between global and local information\nas well as optimizing the low-level design choice in each module. To tackle the\nproblem caused by huge search space, a hierarchical neural architecture search\nmethod is proposed to search the optimal vision transformer from two levels\nseparately with the evolutionary algorithm. Extensive experiments on the\nImageNet dataset demonstrate that our method can find more discriminative and\nefficient transformer variants than the ResNet family (e.g., ResNet101) and the\nbaseline ViT for image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chuming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Ming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yan_J/0/1/0/all/0/1\">Junjie yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Detection in Residential Video Surveillance on Edge Devices in IoT Framework. (arXiv:2107.04767v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04767","description":"<p>Intelligent resident surveillance is one of the most essential smart\ncommunity services. The increasing demand for security needs surveillance\nsystems to be able to detect anomalies in surveillance scenes. Employing\nhigh-capacity computational devices for intelligent surveillance in residential\nsocieties is costly and not feasible. Therefore, we propose anomaly detection\nfor intelligent surveillance using CPU-only edge devices. A modular framework\nto capture object-level inferences and tracking is developed. To cope with\npartial occlusions, posture deformations, and complex scenes, we employed\nfeature encoding and trajectory association governed by two metrices\ncomplementing to each other. The elements of an anomaly detection framework are\noptimized to run on CPU-only edge devices with sufficient frames per second\n(FPS). The experimental results indicate the proposed method is feasible and\nachieves satisfactory results in real-life scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parate_M/0/1/0/all/0/1\">Mayur R. Parate</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhurchandi_K/0/1/0/all/0/1\">Kishor M. Bhurchandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothari_A/0/1/0/all/0/1\">Ashwin G. Kothari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate Localization by Instance Search. (arXiv:2107.05005v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.05005","description":"<p>Visual object localization is the key step in a series of object detection\ntasks. In the literature, high localization accuracy is achieved with the\nmainstream strongly supervised frameworks. However, such methods require\nobject-level annotations and are unable to detect objects of unknown\ncategories. Weakly supervised methods face similar difficulties. In this paper,\na self-paced learning framework is proposed to achieve accurate object\nlocalization on the rank list returned by instance search. The proposed\nframework mines the target instance gradually from the queries and their\ncorresponding top-ranked search results. Since a common instance is shared\nbetween the query and the images in the rank list, the target visual instance\ncan be accurately localized even without knowing what the object category is.\nIn addition to performing localization on instance search, the issue of\nfew-shot object detection is also addressed under the same framework. Superior\nperformance over state-of-the-art methods is observed on both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi-Geng Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hui-Chu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2107.08142","description":"<p>Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11769","description":"<p>Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Hierarchical Graph Reasoning with Semantic Coherence for Video-and-Language Inference. (arXiv:2107.12270v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12270","description":"<p>Video-and-Language Inference is a recently proposed task for joint\nvideo-and-language understanding. This new task requires a model to draw\ninference on whether a natural language statement entails or contradicts a\ngiven video clip. In this paper, we study how to address three critical\nchallenges for this task: judging the global correctness of the statement\ninvolved multiple semantic meanings, joint reasoning over video and subtitles,\nand modeling long-range relationships and complex social interactions. First,\nwe propose an adaptive hierarchical graph network that achieves in-depth\nunderstanding of the video over complex interactions. Specifically, it performs\njoint reasoning over video and subtitles in three hierarchies, where the graph\nstructure is adaptively adjusted according to the semantic structures of the\nstatement. Secondly, we introduce semantic coherence learning to explicitly\nencourage the semantic coherence of the adaptive hierarchical graph network\nfrom three hierarchies. The semantic coherence learning can further improve the\nalignment between vision and linguistics, and the coherence across a sequence\nof video segments. Experimental results show that our method significantly\noutperforms the baseline by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanwen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Transformer for Dynamic Scene Graph Generation. (arXiv:2107.12309v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12309","description":"<p>Dynamic scene graph generation aims at generating a scene graph of the given\nvideo. Compared to the task of scene graph generation from images, it is more\nchallenging because of the dynamic relationships between objects and the\ntemporal dependencies between frames allowing for a richer semantic\ninterpretation. In this paper, we propose Spatial-temporal Transformer\n(STTran), a neural network that consists of two core modules: (1) a spatial\nencoder that takes an input frame to extract spatial context and reason about\nthe visual relationships within a frame, and (2) a temporal decoder which takes\nthe output of the spatial encoder as input in order to capture the temporal\ndependencies between frames and infer the dynamic relationships. Furthermore,\nSTTran is flexible to take varying lengths of videos as input without clipping,\nwhich is especially important for long videos. Our method is validated on the\nbenchmark dataset Action Genome (AG). The experimental results demonstrate the\nsuperior performance of our method in terms of dynamic scene graphs. Moreover,\na set of ablative studies is conducted and the effect of each proposed module\nis justified. Code available at: https://github.com/yrcong/STTran.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yuren Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">Wentong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ackermann_H/0/1/0/all/0/1\">Hanno Ackermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1\">Bodo Rosenhahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Ying Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12619","description":"<p>Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12651","description":"<p>Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Self-Aligned Network for Text-to-Image Part-aware Person Re-identification. (arXiv:2107.12666v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12666","description":"<p>Text-to-image person re-identification (ReID) aims to search for images\ncontaining a person of interest using textual descriptions. However, due to the\nsignificant modality gap and the large intra-class variance in textual\ndescriptions, text-to-image ReID remains a challenging problem. Accordingly, in\nthis paper, we propose a Semantically Self-Aligned Network (SSAN) to handle the\nabove problems. First, we propose a novel method that automatically extracts\nsemantically aligned part-level features from the two modalities. Second, we\ndesign a multi-view non-local network that captures the relationships between\nbody parts, thereby establishing better correspondences between body parts and\nnoun phrases. Third, we introduce a Compound Ranking (CR) loss that makes use\nof textual descriptions for other images of the same identity to provide extra\nsupervision, thereby effectively reducing the intra-class variance in textual\nfeatures. Finally, to expedite future research in text-to-image ReID, we build\na new database named ICFG-PEDES. Extensive experiments demonstrate that SSAN\noutperforms state-of-the-art approaches by significant margins. Both the new\nICFG-PEDES database and the SSAN code are available at\nhttps://github.com/zifyloo/SSAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zefeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhiyin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework. (arXiv:2107.12746v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12746","description":"<p>Localizing individuals in crowds is more in accordance with the practical\ndemands of subsequent high-level crowd analysis tasks than simply counting.\nHowever, existing localization based methods relying on intermediate\nrepresentations (\\textit{i.e.}, density maps or pseudo boxes) serving as\nlearning targets are counter-intuitive and error-prone. In this paper, we\npropose a purely point-based framework for joint crowd counting and individual\nlocalization. For this framework, instead of merely reporting the absolute\ncounting error at image level, we propose a new metric, called density\nNormalized Average Precision (nAP), to provide more comprehensive and more\nprecise performance evaluation. Moreover, we design an intuitive solution under\nthis framework, which is called Point to Point Network (P2PNet). P2PNet\ndiscards superfluous steps and directly predicts a set of point proposals to\nrepresent heads in an image, being consistent with the human annotation\nresults. By thorough analysis, we reveal the key step towards implementing such\na novel idea is to assign optimal learning targets for these proposals.\nTherefore, we propose to conduct this crucial association in an one-to-one\nmatching manner using the Hungarian algorithm. The P2PNet not only\nsignificantly surpasses state-of-the-art methods on popular counting\nbenchmarks, but also achieves promising localization accuracy. The codes will\nbe available at: https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhengkai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching Local and Global Contexts for Temporal Action Localization. (arXiv:2107.12960v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12960","description":"<p>Effectively tackling the problem of temporal action localization (TAL)\nnecessitates a visual representation that jointly pursues two confounding\ngoals, i.e., fine-grained discrimination for temporal localization and\nsufficient visual invariance for action classification. We address this\nchallenge by enriching both the local and global contexts in the popular\ntwo-stage temporal localization framework, where action proposals are first\ngenerated followed by action classification and temporal boundary regression.\nOur proposed model, dubbed ContextLoc, can be divided into three sub-networks:\nL-Net, G-Net and P-Net. L-Net enriches the local context via fine-grained\nmodeling of snippet-level features, which is formulated as a\nquery-and-retrieval process. G-Net enriches the global context via higher-level\nmodeling of the video-level representation. In addition, we introduce a novel\ncontext adaptation module to adapt the global context to different proposals.\nP-Net further models the context-aware inter-proposal relations. We explore two\nexisting models to be the P-Net in our experiments. The efficacy of our\nproposed method is validated by experimental results on the THUMOS14 (54.3\\% at\ntIoU@0.5) and ActivityNet v1.3 (56.01\\% at tIoU@0.5) datasets, which\noutperforms recent states of the art. Code is available at\nhttps://github.com/buxiangzhiren/ContextLoc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zixin Zhu</a> (Xi&#x27;an jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a> (University of Illinois at Chicago), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a> (Xi&#x27;an Jiaotong University), <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a> (Wormpex AI Research)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14483","description":"<p>Learning generalizable manipulation skills is central for robots to achieve\ntask automation in environments with endless scene and object variations.\nHowever, existing robot learning environments are limited in both scale and\ndiversity of 3D assets (especially of articulated objects), making it difficult\nto train and evaluate the generalization ability of agents over novel objects.\nIn this work, we focus on object-level generalization and propose SAPIEN\nManipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale\nlearning-from-demonstrations benchmark for articulated object manipulation with\n3D visual input (point cloud and RGB-D image). ManiSkill supports object-level\nvariations by utilizing a rich and diverse set of articulated objects, and each\ntask is carefully designed for learning manipulations on a single category of\nobjects. We equip ManiSkill with a large number of high-quality demonstrations\nto facilitate learning-from-demonstrations approaches and perform evaluations\non baseline algorithms. We believe that ManiSkill can encourage the robot\nlearning community to explore more on learning generalizable object\nmanipulation skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining. (arXiv:2107.14572v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14572","description":"<p>Nowadays, customer's demands for E-commerce are more diversified, which\nintroduces more complications to the product retrieval industry. Previous\nmethods are either subject to single-modal input or perform supervised\nimage-level product retrieval, thus fail to accommodate real-life scenarios\nwhere enormous weakly annotated multi-modal data are present. In this paper, we\ninvestigate a more realistic setting that aims to perform weakly-supervised\nmulti-modal instance-level product retrieval among fine-grained product\ncategories. To promote the study of this challenging task, we contribute\nProduct1M, one of the largest multi-modal cosmetic datasets for real-world\ninstance-level retrieval. Notably, Product1M contains over 1 million\nimage-caption pairs and consists of two sample types, i.e., single-product and\nmulti-product samples, which encompass a wide variety of cosmetics brands. In\naddition to the great diversity, Product1M enjoys several appealing\ncharacteristics including fine-grained categories, complex combinations, and\nfuzzy correspondence that well mimic the real-world scenes. Moreover, we\npropose a novel model named Cross-modal contrAstive Product Transformer for\ninstance-level prodUct REtrieval (CAPTURE), that excels in capturing the\npotential synergy between multi-modal inputs via a hybrid-stream transformer in\na self-supervised manner.CAPTURE generates discriminative instance features via\nmasked multi-modal learning as well as cross-modal contrastive pretraining and\nit outperforms several SOTA cross-modal baselines. Extensive ablation studies\nwell demonstrate the effectiveness and the generalization capacity of our\nmodel. Dataset and codes are available at https:\n//github.com/zhanxlin/Product1M.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse-to-dense Feature Matching: Intra and Inter domain Cross-modal Learning in Domain Adaptation for 3D Semantic Segmentation. (arXiv:2107.14724v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14724","description":"<p>Domain adaptation is critical for success when confronting with the lack of\nannotations in a new domain. As the huge time consumption of labeling process\non 3D point cloud, domain adaptation for 3D semantic segmentation is of great\nexpectation. With the rise of multi-modal datasets, large amount of 2D images\nare accessible besides 3D point clouds. In light of this, we propose to further\nleverage 2D data for 3D domain adaptation by intra and inter domain cross modal\nlearning. As for intra-domain cross modal learning, most existing works sample\nthe dense 2D pixel-wise features into the same size with sparse 3D point-wise\nfeatures, resulting in the abandon of numerous useful 2D features. To address\nthis problem, we propose Dynamic sparse-to-dense Cross Modal Learning (DsCML)\nto increase the sufficiency of multi-modality information interaction for\ndomain adaptation. For inter-domain cross modal learning, we further advance\nCross Modal Adversarial Learning (CMAL) on 2D and 3D data which contains\ndifferent semantic content aiming to promote high-level modal complementarity.\nWe evaluate our model under various multi-modality domain adaptation settings\nincluding day-to-night, country-to-country and dataset-to-dataset, brings large\nimprovements over both uni-modal and multi-modal domain adaptation methods on\nall settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Duo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1\">Yinjie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pingping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00602","description":"<p>In this paper, we study the task of hallucinating an authentic\nhigh-resolution (HR) face from an occluded thumbnail. We propose a multi-stage\nProgressive Upsampling and Inpainting Generative Adversarial Network, dubbed\nPro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)\nthe occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates\nfacial geometry priors for low-resolution (LR) faces and (2) acquires\nnon-occluded HR face images under the guidance of the estimated priors. Our\nmulti-stage hallucination network super-resolves and inpaints occluded LR faces\nin a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts\nsignificantly. Specifically, we design a novel cross-modal transformer module\nfor facial priors estimation, in which an input face and its landmark features\nare formulated as queries and keys, respectively. Such a design encourages\njoint feature learning across the input facial and landmark features, and deep\nfeature correspondences will be discovered by attention. Thus, facial\nappearance features and facial geometry priors are learned in a mutual\npromotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves\nvisually pleasing HR faces, reaching superior performance in downstream tasks,\ni.e., face alignment, face parsing, face recognition and expression\nclassification, compared with other state-of-the-art (SotA) methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaobo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00705","description":"<p>This paper introduces a two-phase deep feature calibration framework for\nefficient learning of semantics enhanced text-image cross-modal joint\nembedding, which clearly separates the deep feature calibration in data\npreprocessing from training the joint embedding model. We use the Recipe1M\ndataset for the technical description and empirical validation. In\npreprocessing, we perform deep feature calibration by combining deep feature\nengineering with semantic context features derived from raw text-image input\ndata. We leverage LSTM to identify key terms, NLP methods to produce ranking\nscores for key terms before generating the key term feature. We leverage\nwideResNet50 to extract and encode the image category semantics to help\nsemantic alignment of the learned recipe and image embeddings in the joint\nlatent space. In joint embedding learning, we perform deep feature calibration\nby optimizing the batch-hard triplet loss function with soft-margin and double\nnegative sampling, also utilizing the category-based alignment loss and\ndiscriminator-based alignment loss. Extensive experiments demonstrate that our\nSEJE approach with the deep feature calibration significantly outperforms the\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhongwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Luo Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning. (arXiv:2108.01959v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01959","description":"<p>Skeleton-based human action recognition has attracted increasing attention in\nrecent years. However, most of the existing works focus on supervised learning\nwhich requiring a large number of annotated action sequences that are often\nexpensive to collect. We investigate unsupervised representation learning for\nskeleton action recognition, and design a novel skeleton cloud colorization\ntechnique that is capable of learning skeleton representations from unlabeled\nskeleton sequence data. Specifically, we represent a skeleton action sequence\nas a 3D skeleton cloud and colorize each point in the cloud according to its\ntemporal and spatial orders in the original (unannotated) skeleton sequence.\nLeveraging the colorized skeleton point cloud, we design an auto-encoder\nframework that can learn spatial-temporal features from the artificial color\nlabels of skeleton joints effectively. We evaluate our skeleton cloud\ncolorization approach with action classifiers trained under different\nconfigurations, including unsupervised, semi-supervised and fully-supervised\nsettings. Extensive experiments on NTU RGB+D and NW-UCLA datasets show that the\nproposed method outperforms existing unsupervised and semi-supervised 3D action\nrecognition methods by large margins, and it achieves competitive performance\nin supervised 3D action recognition as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Er_M/0/1/0/all/0/1\">Meng Hwa Er</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex C. Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02574","description":"<p>Recently, much progress has been made in unsupervised restoration learning.\nHowever, existing methods more or less rely on some assumptions on the signal\nand/or degradation model, which limits their practical performance. How to\nconstruct an optimal criterion for unsupervised restoration learning without\nany prior knowledge on the degradation model is still an open question. Toward\nanswering this question, this work proposes a criterion for unsupervised\nrestoration learning based on the optimal transport theory. This criterion has\nfavorable properties, e.g., approximately maximal preservation of the\ninformation of the signal, whilst achieving perceptual reconstruction.\nFurthermore, though a relaxed unconstrained formulation is used in practical\nimplementation, we show that the relaxed formulation in theory has the same\nsolution as the original constrained formulation. Experiments on synthetic and\nreal-world data, including realistic photographic, microscopy, depth, and raw\ndepth images, demonstrate that the proposed method even compares favorably with\nsupervised methods, e.g., approaching the PSNR of supervised methods while\nhaving better perceptual quality. Particularly, for spatially correlated noise\nand realistic microscopy images, the proposed method not only achieves better\nperceptual quality but also has higher PSNR than supervised methods. Besides,\nit shows remarkable superiority in harsh practical conditions with complex\nnoise, e.g., raw depth images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zeyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1\">Rendong Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer. (arXiv:2108.03032v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03032","description":"<p>A few-shot semantic segmentation model is typically composed of a CNN\nencoder, a CNN decoder and a simple classifier (separating foreground and\nbackground pixels). Most existing methods meta-learn all three model components\nfor fast adaptation to a new class. However, given that as few as a single\nsupport set image is available, effective model adaption of all three\ncomponents to the new class is extremely challenging. In this work we propose\nto simplify the meta-learning task by focusing solely on the simplest\ncomponent, the classifier, whilst leaving the encoder and decoder to\npre-training. We hypothesize that if we pre-train an off-the-shelf segmentation\nmodel over a set of diverse training classes with sufficient annotations, the\nencoder and decoder can capture rich discriminative features applicable for any\nunseen classes, rendering the subsequent meta-learning stage unnecessary. For\nthe classifier meta-learning, we introduce a Classifier Weight Transformer\n(CWT) designed to dynamically adapt the supportset trained classifier's weights\nto each query image in an inductive way. Extensive experiments on two standard\nbenchmarks show that despite its simplicity, our method outperforms the\nstate-of-the-art alternatives, often by a large margin.Code is available on\nhttps://github.com/zhiheLu/CWT-for-FSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+lu_Z/0/1/0/all/0/1\">Zhihe lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLASS: Geometric Latent Augmentation for Shape Spaces. (arXiv:2108.03225v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03225","description":"<p>We investigate the problem of training generative models on a very sparse\ncollection of 3D models. We use geometrically motivated energies to augment and\nthus boost a sparse collection of example (training) models. We analyze the\nHessian of the as-rigid-as-possible (ARAP) energy to sample from and project to\nthe underlying (local) shape space, and use the augmented dataset to train a\nvariational autoencoder (VAE). We iterate the process of building latent spaces\nof VAE and augmenting the associated dataset, to progressively reveal a richer\nand more expressive generative space for creating geometrically and\nsemantically valid samples. Our framework allows us to train generative 3D\nmodels even with a small set of good quality 3D models, which are typically\nhard to curate. We extensively evaluate our method against a set of strong\nbaselines, provide ablation studies and demonstrate application towards\nestablishing shape correspondences. We present multiple examples of interesting\nand meaningful shape variations even when starting from as few as 3-10 training\nshapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muralikrishnan_S/0/1/0/all/0/1\">Sanjeev Muralikrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.LG updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.LG","description":"Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Operational Learning-based Boundary Estimation in Electromagnetic Medical Imaging. (arXiv:2108.03233v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03233","description":"<p>Incorporating boundaries of the imaging object as a priori information to\nimaging algorithms can significantly improve the performance of electromagnetic\nmedical imaging systems. To avoid overly complicating the system by using\ndifferent sensors and the adverse effect of the subject's movement, a\nlearning-based method is proposed to estimate the boundary (external contour)\nof the imaged object using the same electromagnetic imaging data. While imaging\ntechniques may discard the reflection coefficients for being dominant and\nuninformative for imaging, these parameters are made use of for boundary\ndetection. The learned model is verified through independent clinical human\ntrials by using a head imaging system with a 16-element antenna array that\nworks across the band 0.7-1.6 GHz. The evaluation demonstrated that the model\nachieves average dissimilarity of 0.012 in Hu-moment while detecting head\nboundary. The model enables fast scan and image creation while eliminating the\nneed for additional devices for accurate boundary estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1\">A. Al-Saffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stancombe_A/0/1/0/all/0/1\">A. Stancombe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamani_A/0/1/0/all/0/1\">A. Zamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1\">A. Abbosh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMOTified-GAN for class imbalanced pattern classification problems. (arXiv:2108.03235v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03235","description":"<p>Class imbalance in a dataset is a major problem for classifiers that results\nin poor prediction with a high true positive rate (TPR) but a low true negative\nrate (TNR) for a majority positive training dataset. Generally, the\npre-processing technique of oversampling of minority class(es) are used to\novercome this deficiency. Our focus is on using the hybridization of Generative\nAdversarial Network (GAN) and Synthetic Minority Over-Sampling Technique\n(SMOTE) to address class imbalanced problems. We propose a novel two-phase\noversampling approach that has the synergy of SMOTE and GAN. The initial data\nof minority class(es) generated by SMOTE is further enhanced by GAN that\nproduces better quality samples. We named it SMOTified-GAN as GAN works on\npre-sampled minority data produced by SMOTE rather than randomly generating the\nsamples itself. The experimental results prove the sample quality of minority\nclass(es) has been improved in a variety of tested benchmark datasets. Its\nperformance is improved by up to 9\\% from the next best algorithm tested on\nF1-score measurements. Its time complexity is also reasonable which is around\n$O(N^2d^2T)$ for a sequential algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuraganand Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Prabhat Kumar Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohitash Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Representation for Electric Vehicle Charging Station Operations using Reinforcement Learning. (arXiv:2108.03236v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03236","description":"<p>Effectively operating electrical vehicle charging station (EVCS) is crucial\nfor enabling the rapid transition of electrified transportation. To solve this\nproblem using reinforcement learning (RL), the dimension of state/action spaces\nscales with the number of EVs and is thus very large and time-varying. This\ndimensionality issue affects the efficiency and convergence properties of\ngeneric RL algorithms. We develop aggregation schemes that are based on the\nemergency of EV charging, namely the laxity value. A least-laxity first (LLF)\nrule is adopted to consider only the total charging power of the EVCS which\nensures the feasibility of individual EV schedules. In addition, we propose an\nequivalent state aggregation that can guarantee to attain the same optimal\npolicy. Based on the proposed representation, policy gradient method is used to\nfind the best parameters for the linear Gaussian policy . Numerical results\nhave validated the performance improvement of the proposed representation\napproaches in attaining higher rewards and more effective policies as compared\nto existing approximation based approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_K/0/1/0/all/0/1\">Kyung-bin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks. (arXiv:2108.03272v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03272","description":"<p>Recent research in embodied AI has been boosted by the use of simulation\nenvironments to develop and train robot learning approaches. However, the use\nof simulation has skewed the attention to tasks that only require what robotics\nsimulators can simulate: motion and physical contact. We present iGibson 2.0,\nan open-source simulation environment that supports the simulation of a more\ndiverse set of household tasks through three key innovations. First, iGibson\n2.0 supports object states, including temperature, wetness level, cleanliness\nlevel, and toggled and sliced states, necessary to cover a wider range of\ntasks. Second, iGibson 2.0 implements a set of predicate logic functions that\nmap the simulator states to logic states like Cooked or Soaked. Additionally,\ngiven a logic state, iGibson 2.0 can sample valid physical states that satisfy\nit. This functionality can generate potentially infinite instances of tasks\nwith minimal effort from the users. The sampling mechanism allows our scenes to\nbe more densely populated with small objects in semantically meaningful\nlocations. Third, iGibson 2.0 includes a virtual reality (VR) interface to\nimmerse humans in its scenes to collect demonstrations. As a result, we can\ncollect demonstrations from humans on these new types of tasks, and use them\nfor imitation learning. We evaluate the new capabilities of iGibson 2.0 to\nenable robot learning of novel tasks, in the hope of demonstrating the\npotential of this new simulator to support new research in embodied AI. iGibson\n2.0 and its new dataset will be publicly available at\n<a href=\"http://svl.stanford.edu/igibson/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingelbach_M/0/1/0/all/0/1\">Michael Lingelbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sanjana Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bokui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vainio_K/0/1/0/all/0/1\">Kent Vainio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokmen_C/0/1/0/all/0/1\">Cem Gokmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharan_G/0/1/0/all/0/1\">Gokul Dharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_T/0/1/0/all/0/1\">Tanish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurenkov_A/0/1/0/all/0/1\">Andrey Kurenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Karen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gweon_H/0/1/0/all/0/1\">Hyowon Gweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Drift Detection with Variable Interaction Networks. (arXiv:2108.03273v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03273","description":"<p>The current development of today's production industry towards seamless\nsensor-based monitoring is paving the way for concepts such as Predictive\nMaintenance. By this means, the condition of plants and products in future\nproduction lines will be continuously analyzed with the objective to predict\nany kind of breakdown and trigger preventing actions proactively. Such\nambitious predictions are commonly performed with support of machine learning\nalgorithms. In this work, we utilize these algorithms to model complex systems,\nsuch as production plants, by focusing on their variable interactions. The core\nof this contribution is a sliding window based algorithm, designed to detect\nchanges of the identified interactions, which might indicate beginning\nmalfunctions in the context of a monitored production plant. Besides a detailed\ndescription of the algorithm, we present results from experiments with a\nsynthetic dynamical system, simulating stable and drifting system behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zenisek_J/0/1/0/all/0/1\">Jan Zenisek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfartsberger_J/0/1/0/all/0/1\">Josef Wolfartsberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wild_N/0/1/0/all/0/1\">Norbert Wild</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Affenzeller_M/0/1/0/all/0/1\">Michael Affenzeller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smooth Symbolic Regression: Transformation of Symbolic Regression into a Real-valued Optimization Problem. (arXiv:2108.03274v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03274","description":"<p>The typical methods for symbolic regression produce rather abrupt changes in\nsolution candidates. In this work, we have tried to transform symbolic\nregression from an optimization problem, with a landscape that is so rugged\nthat typical analysis methods do not produce meaningful results, to one that\ncan be compared to typical and very smooth real-valued problems. While the\nruggedness might not interfere with the performance of optimization, it\nrestricts the possibilities of analysis. Here, we have explored different\naspects of a transformation and propose a simple procedure to create\nreal-valued optimization problems from symbolic regression problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pitzer_E/0/1/0/all/0/1\">Erik Pitzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kronberger_G/0/1/0/all/0/1\">Gabriel Kronberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble Augmentation for Deep Neural Networks Using 1-D Time Series Vibration Data. (arXiv:2108.03288v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03288","description":"<p>Time-series data are one of the fundamental types of raw data representation\nused in data-driven techniques. In machine condition monitoring, time-series\nvibration data are overly used in data mining for deep neural networks.\nTypically, vibration data is converted into images for classification using\nDeep Neural Networks (DNNs), and scalograms are the most effective form of\nimage representation. However, the DNN classifiers require huge labeled\ntraining samples to reach their optimum performance. So, many forms of data\naugmentation techniques are applied to the classifiers to compensate for the\nlack of training samples. However, the scalograms are graphical representations\nwhere the existing augmentation techniques suffer because they either change\nthe graphical meaning or have too much noise in the samples that change the\nphysical meaning. In this study, a data augmentation technique named ensemble\naugmentation is proposed to overcome this limitation. This augmentation method\nuses the power of white noise added in ensembles to the original samples to\ngenerate real-like samples. After averaging the signal with ensembles, a new\nsignal is obtained that contains the characteristics of the original signal.\nThe parameters for the ensemble augmentation are validated using a simulated\nsignal. The proposed method is evaluated using 10 class bearing vibration data\nusing three state-of-the-art Transfer Learning (TL) models, namely,\nInception-V3, MobileNet-V2, and ResNet50. Augmented samples are generated in\ntwo increments: the first increment generates the same number of fake samples\nas the training samples, and in the second increment, the number of samples is\nincreased gradually. The outputs from the proposed method are compared with no\naugmentation, augmentations using deep convolution generative adversarial\nnetwork (DCGAN), and several geometric transformation-based augmentations...\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faysal_A/0/1/0/all/0/1\">Atik Faysal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keng_N/0/1/0/all/0/1\">Ngui Wai Keng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1\">M. H. Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint AP Probing and Scheduling: A Contextual Bandit Approach. (arXiv:2108.03297v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03297","description":"<p>We consider a set of APs with unknown data rates that cooperatively serve a\nmobile client. The data rate of each link is i.i.d. sampled from a distribution\nthat is unknown a priori. In contrast to traditional link scheduling problems\nunder uncertainty, we assume that in each time step, the device can probe a\nsubset of links before deciding which one to use. We model this problem as a\ncontextual bandit problem with probing (CBwP) and present an efficient\nalgorithm. We further establish the regret of our algorithm for links with\nBernoulli data rates. Our CBwP model is a novel extension of the classic\ncontextual bandit model and can potentially be applied to a large class of\nsequential decision-making problems that involve joint probing and play under\nuncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Ding Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_P/0/1/0/all/0/1\">Parth H. Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zizhan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Matters in Learning from Offline Human Demonstrations for Robot Manipulation. (arXiv:2108.03298v1 [cs.RO])","link":"http://arxiv.org/abs/2108.03298","description":"<p>Imitating human demonstrations is a promising approach to endow robots with\nvarious manipulation capabilities. While recent advances have been made in\nimitation learning and batch (offline) reinforcement learning, a lack of\nopen-source human datasets and reproducible learning methods make assessing the\nstate of the field difficult. In this paper, we conduct an extensive study of\nsix offline learning algorithms for robot manipulation on five simulated and\nthree real-world multi-stage manipulation tasks of varying complexity, and with\ndatasets of varying quality. Our study analyzes the most critical challenges\nwhen learning from offline human data for manipulation. Based on the study, we\nderive a series of lessons including the sensitivity to different algorithmic\ndesign choices, the dependence on the quality of the demonstrations, and the\nvariability based on the stopping criteria due to the different objectives in\ntraining and evaluation. We also highlight opportunities for learning from\nhuman datasets, such as the ability to learn proficient policies on\nchallenging, multi-stage tasks beyond the scope of current reinforcement\nlearning methods, and the ability to easily scale to natural, real-world\nmanipulation scenarios where only raw sensory signals are available. We have\nopen-sourced our datasets and all algorithm implementations to facilitate\nfuture research and fair comparisons in learning from human demonstration data.\nCodebase, datasets, trained models, and more available at\nhttps://arise-initiative.github.io/robomimic-web/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandlekar_A/0/1/0/all/0/1\">Ajay Mandlekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Danfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Josiah Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasiriany_S/0/1/0/all/0/1\">Soroush Nasiriany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1\">Rohun Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1\">Roberto Mart&#xed;n-Mart&#xed;n</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Neural Network Approach for Crop Selection and Yield Prediction in Bangladesh. (arXiv:2108.03320v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03320","description":"<p>Agriculture is the essential ingredients to mankind which is a major source\nof livelihood. Agriculture work in Bangladesh is mostly done in old ways which\ndirectly affects our economy. In addition, institutions of agriculture are\nworking with manual data which cannot provide a proper solution for crop\nselection and yield prediction. This paper shows the best way of crop selection\nand yield prediction in minimum cost and effort. Artificial Neural Network is\nconsidered robust tools for modeling and prediction. This algorithm aims to get\nbetter output and prediction, as well as, support vector machine, Logistic\nRegression, and random forest algorithm is also considered in this study for\ncomparing the accuracy and error rate. Moreover, all of these algorithms used\nhere are just to see how well they performed for a dataset which is over 0.3\nmillion. We have collected 46 parameters such as maximum and minimum\ntemperature, average rainfall, humidity, climate, weather, and types of land,\ntypes of chemical fertilizer, types of soil, soil structure, soil composition,\nsoil moisture, soil consistency, soil reaction and soil texture for applying\ninto this prediction process. In this paper, we have suggested using the deep\nneural network for agricultural crop selection and yield prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chisty_T/0/1/0/all/0/1\">Tanjir Alam Chisty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_A/0/1/0/all/0/1\">Amitabha Chakrabarty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Transformers for Neural Cross-Domain Search. (arXiv:2108.03322v1 [cs.IR])","link":"http://arxiv.org/abs/2108.03322","description":"<p>Pre-trained transformers have recently clinched top spots in the gamut of\nnatural language tasks and pioneered solutions to software engineering tasks.\nEven information retrieval has not been immune to the charm of the transformer,\nthough their large size and cost is generally a barrier to deployment. While\nthere has been much work in streamlining, caching, and modifying transformer\narchitectures for production, here we explore a new direction: distilling a\nlarge pre-trained translation model into a lightweight bi-encoder which can be\nefficiently cached and queried. We argue from a probabilistic perspective that\nsequence-to-sequence models are a conceptually ideal---albeit highly\nimpractical---retriever. We derive a new distillation objective, implementing\nit as a data augmentation scheme. Using natural language source code search as\na case study for cross-domain search, we demonstrate the validity of this idea\nby significantly improving upon the current leader of the CodeSearchNet\nchallenge, a recent natural language code search benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin B. Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Graph Dimension with Cross-validated Eigenvalues. (arXiv:2108.03336v1 [stat.ME])","link":"http://arxiv.org/abs/2108.03336","description":"<p>In applied multivariate statistics, estimating the number of latent\ndimensions or the number of clusters is a fundamental and recurring problem.\nOne common diagnostic is the scree plot, which shows the largest eigenvalues of\nthe data matrix; the user searches for a \"gap\" or \"elbow\" in the decreasing\neigenvalues; unfortunately, these patterns can hide beneath the bias of the\nsample eigenvalues. This methodological problem is conceptually difficult\nbecause, in many situations, there is only enough signal to detect a subset of\nthe $k$ population dimensions/eigenvectors. In this situation, one could argue\nthat the correct choice of $k$ is the number of detectable dimensions. We\nalleviate these problems with cross-validated eigenvalues. Under a large class\nof random graph models, without any parametric assumptions, we provide a\np-value for each sample eigenvector. It tests the null hypothesis that this\nsample eigenvector is orthogonal to (i.e., uncorrelated with) the true latent\ndimensions. This approach naturally adapts to problems where some dimensions\nare not statistically detectable. In scenarios where all $k$ dimensions can be\nestimated, we prove that our procedure consistently estimates $k$. In\nsimulations and a data example, the proposed estimator compares favorably to\nalternative approaches in both computational and statistical performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roch_S/0/1/0/all/0/1\">Sebastien Roch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rohe_K/0/1/0/all/0/1\">Karl Rohe</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yu_S/0/1/0/all/0/1\">Shuqi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs. (arXiv:2108.03348v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03348","description":"<p>Transformer neural networks have achieved state-of-the-art results for\nunstructured data such as text and images but their adoption for\ngraph-structured data has been limited. This is partly due to the difficulty in\nincorporating complex structural information in the basic transformer\nframework. We propose a simple yet powerful extension to the transformer -\nresidual edge channels. The resultant framework, which we call Edge-augmented\nGraph Transformer (EGT), can directly accept, process and output structural\ninformation as well as node information. This simple addition allows us to use\nglobal self-attention, the key element of transformers, directly for graphs and\ncomes with the benefit of long-range interaction among nodes. Moreover, the\nedge channels allow the structural information to evolve from layer to layer,\nand prediction tasks on edges can be derived directly from these channels. In\naddition to that, we introduce positional encodings based on Singular Value\nDecomposition which can improve the performance of EGT. Our framework, which\nrelies on global node feature aggregation, achieves better performance compared\nto Graph Convolutional Networks (GCN), which rely on local feature aggregation\nwithin a neighborhood. We verify the performance of EGT in a supervised\nlearning setting on a wide range of experiments on benchmark datasets. Our\nfindings indicate that convolutional aggregation is not an essential inductive\nbias for graphs and global self-attention can serve as a flexible and adaptive\nalternative to graph convolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1\">Md Shamim Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammed J. Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_D/0/1/0/all/0/1\">Dharmashankar Subramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning. (arXiv:2108.03353v1 [cs.HC])","link":"http://arxiv.org/abs/2108.03353","description":"<p>Mobile User Interface Summarization generates succinct language descriptions\nof mobile screens for conveying important contents and functionalities of the\nscreen, which can be useful for many language-based application scenarios. We\npresent Screen2Words, a novel screen summarization approach that automatically\nencapsulates essential information of a UI screen into a coherent language\nphrase. Summarizing mobile screens requires a holistic understanding of the\nmulti-modal data of mobile UIs, including text, image, structures as well as UI\nsemantics, motivating our multi-modal learning approach. We collected and\nanalyzed a large-scale screen summarization dataset annotated by human workers.\nOur dataset contains more than 112k language summarization across $\\sim$22k\nunique UI screens. We then experimented with a set of deep models with\ndifferent configurations. Our evaluation of these models with both automatic\naccuracy metrics and human rating shows that our approach can generate\nhigh-quality summaries for mobile screens. We demonstrate potential use cases\nof Screen2Words and open-source our dataset and model to lay the foundations\nfor further bridging language and user interfaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bryan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhourong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grossman_T/0/1/0/all/0/1\">Tovi Grossman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition. (arXiv:2108.03354v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03354","description":"<p>The research on human emotion under multimedia stimulation based on\nphysiological signals is an emerging field, and important progress has been\nachieved for emotion recognition based on multi-modal signals. However, it is\nchallenging to make full use of the complementarity among\nspatial-spectral-temporal domain features for emotion recognition, as well as\nmodel the heterogeneity and correlation among multi-modal signals. In this\npaper, we propose a novel two-stream heterogeneous graph recurrent neural\nnetwork, named HetEmotionNet, fusing multi-modal physiological signals for\nemotion recognition. Specifically, HetEmotionNet consists of the\nspatial-temporal stream and the spatial-spectral stream, which can fuse\nspatial-spectral-temporal domain features in a unified framework. Each stream\nis composed of the graph transformer network for modeling the heterogeneity,\nthe graph convolutional network for modeling the correlation, and the gated\nrecurrent unit for capturing the temporal domain or spectral domain dependency.\nExtensive experiments on two real-world datasets demonstrate that our proposed\nmodel achieves better performance than state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Ziyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiangheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Caijie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Cross-domain Recommendation: Taxonomies, Methods, and Future Directions. (arXiv:2108.03357v1 [cs.IR])","link":"http://arxiv.org/abs/2108.03357","description":"<p>Traditional recommendation systems are faced with two long-standing\nobstacles, namely, data sparsity and cold-start problems, which promote the\nemergence and development of Cross-Domain Recommendation (CDR). The core idea\nof CDR is to leverage information collected from other domains to alleviate the\ntwo problems in one domain. Over the last decade, many efforts have been\nengaged for cross-domain recommendation. Recently, with the development of deep\nlearning and neural networks, a large number of methods have emerged. However,\nthere is a limited number of systematic surveys on CDR, especially regarding\nthe latest proposed methods as well as the recommendation scenarios and\nrecommendation tasks they address. In this survey paper, we first proposed a\ntwo-level taxonomy of cross-domain recommendation which classifies different\nrecommendation scenarios and recommendation tasks. We then introduce and\nsummarize existing cross-domain recommendation approaches under different\nrecommendation scenarios in a structured manner. We also organize datasets\ncommonly used. We conclude this survey by providing several potential research\ndirections about this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zang_T/0/1/0/all/0/1\">Tianzi Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanmin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haobing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruohan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiadi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Indoor Layouts from Simple Point-Clouds. (arXiv:2108.03378v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03378","description":"<p>Reconstructing a layout of indoor spaces has been a crucial part of growing\nindoor location based services. One of the key challenges in the proliferation\nof indoor location based services is the unavailability of indoor spatial maps\ndue to the complex nature of capturing an indoor space model (e.g., floor plan)\nof an existing building. In this paper, we propose a system to automatically\ngenerate floor plans that can recognize rooms from the point-clouds obtained\nthrough smartphones like Google's Tango. In particular, we propose two\napproaches - a Recurrent Neural Network based approach using Pointer Network\nand a Convolutional Neural Network based approach using Mask-RCNN to identify\nrooms (and thereby floor plans) from point-clouds. Experimental results on\ndifferent datasets demonstrate approximately 0.80-0.90 Intersection-over-Union\nscores, which show that our models can effectively identify the rooms and\nregenerate the shapes of the rooms in heterogeneous environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_M/0/1/0/all/0/1\">Md. Tareq Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohammed Eunus Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Attacking Graph Neural Network and its Explanations. (arXiv:2108.03388v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03388","description":"<p>Graph Neural Networks (GNNs) have boosted the performance for many\ngraph-related tasks. Despite the great success, recent studies have shown that\nGNNs are highly vulnerable to adversarial attacks, where adversaries can\nmislead the GNNs' prediction by modifying graphs. On the other hand, the\nexplanation of GNNs (GNNExplainer) provides a better understanding of a trained\nGNN model by generating a small subgraph and features that are most influential\nfor its prediction. In this paper, we first perform empirical studies to\nvalidate that GNNExplainer can act as an inspection tool and have the potential\nto detect the adversarial perturbations for graphs. This finding motivates us\nto further initiate a new problem investigation: Whether a graph neural network\nand its explanations can be jointly attacked by modifying graphs with malicious\ndesires? It is challenging to answer this question since the goals of\nadversarial attacks and bypassing the GNNExplainer essentially contradict each\nother. In this work, we give a confirmative answer to this question by\nproposing a novel attack framework (GEAttack), which can attack both a GNN\nmodel and its explanations by simultaneously exploiting their vulnerabilities.\nExtensive experiments on two explainers (GNNExplainer and PGExplainer) under\nvarious real-world datasets demonstrate the effectiveness of the proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Wei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaorui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Han Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianfeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Suhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Missing Data Estimation in Temporal Multilayer Position-aware Graph Neural Network (TMP-GNN). (arXiv:2108.03400v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03400","description":"<p>GNNs have been proven to perform highly effective in various node-level,\nedge-level, and graph-level prediction tasks in several domains. Existing\napproaches mainly focus on static graphs. However, many graphs change over time\nwith their edge may disappear, or node or edge attribute may alter from one\ntime to the other. It is essential to consider such evolution in representation\nlearning of nodes in time varying graphs. In this paper, we propose a Temporal\nMultilayered Position-aware Graph Neural Network (TMP-GNN), a node embedding\napproach for dynamic graph that incorporates the interdependence of temporal\nrelations into embedding computation. We evaluate the performance of TMP-GNN on\ntwo different representations of temporal multilayered graphs. The performance\nis assessed against the most popular GNNs on node-level prediction tasks. Then,\nwe incorporate TMP-GNN into a deep learning framework to estimate missing data\nand compare the performance with their corresponding competent GNNs from our\nformer experiment, and a baseline method. Experimental results on four\nreal-world datasets yield up to 58% of lower ROC AUC for pairwise node\nclassification task, and 96% of lower MAE in missing feature estimation,\nparticularly for graphs with a relatively high number of nodes and lower mean\ndegree of connectivity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Najafi_B/0/1/0/all/0/1\">Bahareh Najafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsaeefard_S/0/1/0/all/0/1\">Saeedeh Parsaeefard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_Garcia_A/0/1/0/all/0/1\">Alberto Leon-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised learning of anomalous diffusion data. (arXiv:2108.03411v1 [cond-mat.stat-mech])","link":"http://arxiv.org/abs/2108.03411","description":"<p>The characterization of diffusion processes is a keystone in our\nunderstanding of a variety of physical phenomena. Many of these deviate from\nBrownian motion, giving rise to anomalous diffusion. Various theoretical models\nexists nowadays to describe such processes, but their application to\nexperimental setups is often challenging, due to the stochastic nature of the\nphenomena and the difficulty to harness reliable data. The latter often\nconsists on short and noisy trajectories, which are hard to characterize with\nusual statistical approaches. In recent years, we have witnessed an impressive\neffort to bridge theory and experiments by means of supervised machine learning\ntechniques, with astonishing results. In this work, we explore the use of\nunsupervised methods in anomalous diffusion data. We show that the main\ndiffusion characteristics can be learnt without the need of any labelling of\nthe data. We use such method to discriminate between anomalous diffusion models\nand extract their physical parameters. Moreover, we explore the feasibility of\nfinding novel types of diffusion, in this case represented by compositions of\nexisting diffusion models. At last, we showcase the use of the method in\nexperimental data and demonstrate its advantages for cases where supervised\nlearning is not applicable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Munoz_Gil_G/0/1/0/all/0/1\">Gorka Mu&#xf1;oz-Gil</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Corominas_G/0/1/0/all/0/1\">Guillem Guig&#xf3; i Corominas</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lewenstein_M/0/1/0/all/0/1\">Maciej Lewenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Secure Neuroimaging Analysis using Federated Learning with Homomorphic Encryption. (arXiv:2108.03437v1 [cs.CR])","link":"http://arxiv.org/abs/2108.03437","description":"<p>Federated learning (FL) enables distributed computation of machine learning\nmodels over various disparate, remote data sources, without requiring to\ntransfer any individual data to a centralized location. This results in an\nimproved generalizability of models and efficient scaling of computation as\nmore sources and larger datasets are added to the federation. Nevertheless,\nrecent membership attacks show that private or sensitive personal data can\nsometimes be leaked or inferred when model parameters or summary statistics are\nshared with a central site, requiring improved security solutions. In this\nwork, we propose a framework for secure FL using fully-homomorphic encryption\n(FHE). Specifically, we use the CKKS construction, an approximate, floating\npoint compatible scheme that benefits from ciphertext packing and rescaling. In\nour evaluation on large-scale brain MRI datasets, we use our proposed secure FL\nframework to train a deep learning model to predict a person's age from\ndistributed MRI scans, a common benchmarking task, and demonstrate that there\nis no degradation in the learning performance between the encrypted and\nnon-encrypted federated models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stripelis_D/0/1/0/all/0/1\">Dimitris Stripelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleem_H/0/1/0/all/0/1\">Hamza Saleem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghai_T/0/1/0/all/0/1\">Tanmay Ghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhinagar_N/0/1/0/all/0/1\">Nikhil Dhinagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1\">Umang Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiou_C/0/1/0/all/0/1\">Chrysovalantis Anastasiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Srivatsan Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naveed_M/0/1/0/all/0/1\">Muhammad Naveed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_P/0/1/0/all/0/1\">Paul M. Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambite_J/0/1/0/all/0/1\">Jose Luis Ambite</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering Large Data Sets with Incremental Estimation of Low-density Separating Hyperplanes. (arXiv:2108.03442v1 [stat.ML])","link":"http://arxiv.org/abs/2108.03442","description":"<p>An efficient method for obtaining low-density hyperplane separators in the\nunsupervised context is proposed. Low density separators can be used to obtain\na partition of a set of data based on their allocations to the different sides\nof the separators. The proposed method is based on applying stochastic gradient\ndescent to the integrated density on the hyperplane with respect to a\nconvolution of the underlying distribution and a smoothing kernel. In the case\nwhere the bandwidth of the smoothing kernel is decreased towards zero, the bias\nof these updates with respect to the true underlying density tends to zero, and\nconvergence to a minimiser of the density on the hyperplane can be obtained. A\npost-processing of the partition induced by a collection of low-density\nhyperplanes yields an efficient and accurate clustering method which is capable\nof automatically selecting an appropriate number of clusters. Experiments with\nthe proposed approach show that it is highly competitive in terms of both speed\nand accuracy when compared with relevant benchmarks. Code to implement the\nproposed approach is available in the form of an R package from\nhttps://github.com/DavidHofmeyr/iMDH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Hofmeyr_D/0/1/0/all/0/1\">David P. Hofmeyr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine Learning Tool to Determine State of Mind and Emotion. (arXiv:2108.03444v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03444","description":"<p>This paper investigates the possibility of creating a machine learning tool\nthat automatically determines the state of mind and emotion of an individual\nthrough a questionnaire, without the aid of a human expert. The state of mind\nand emotion is defined in this work as pertaining to preference, feelings, or\nopinion that is not based on logic or reason. It is the case when a person\ngives out an answer to start by saying, \"I feel...\". The tool is designed to\nmimic the expertise of a psychologist and is built without any formal knowledge\nof psychology. The idea is to build the expertise by purely computational\nmethods through thousands of questions collected from users. It is aimed\ntowards possibly diagnosing substance addiction, alcoholism, sexual attraction,\nHIV status, degree of commitment, activity inclination, etc. First, the paper\npresents the related literature and classifies them according to data gathering\nmethods. Another classification is created according to preference, emotion,\ngrouping, and rules to achieve a deeper interpretation and better understanding\nof the state of mind and emotion. Second, the proposed tool is developed using\nan online addiction questionnaire with 10 questions and 292 respondents. In\naddition, an initial investigation on the dimension of addiction is presented\nthrough the built machine learning model. Machine learning methods, namely,\nartificial neural network (ANN) and support vector machine (SVM), are used to\ndetermine a true or false or degree of state of a respondent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jamisola_R/0/1/0/all/0/1\">Rodrigo S. Jamisola Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-learning sparse PCA for multimode process monitoring. (arXiv:2108.03449v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03449","description":"<p>This paper proposes a novel sparse principal component analysis algorithm\nwith self-learning ability for successive modes, where synaptic intelligence is\nemployed to measure the importance of variables and a regularization term is\nadded to preserve the learned knowledge of previous modes. Different from\ntraditional multimode monitoring methods, the monitoring model is updated based\non the current model and new data when a new mode arrives, thus delivering\nprominent performance for sequential modes. Besides, the computation and\nstorage resources are saved in the long run, because it is not necessary to\nretrain the model from scratch frequently and store data from previous modes.\nMore importantly, the model furnishes excellent interpretability owing to the\nsparsity of parameters. Finally, a numerical case and a practical pulverizing\nsystem are adopted to illustrate the effectiveness of the proposed algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Donghua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Maoyin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Waterdrop Removal with Row-wise Dilated Attention. (arXiv:2108.03457v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03457","description":"<p>Existing vision systems for autonomous driving or robots are sensitive to\nwaterdrops adhered to windows or camera lenses. Most recent waterdrop removal\napproaches take a single image as input and often fail to recover the missing\ncontent behind waterdrops faithfully. Thus, we propose a learning-based model\nfor waterdrop removal with stereo images. To better detect and remove\nwaterdrops from stereo images, we propose a novel row-wise dilated attention\nmodule to enlarge attention's receptive field for effective information\npropagation between the two stereo images. In addition, we propose an attention\nconsistency loss between the ground-truth disparity map and attention scores to\nenhance the left-right consistency in stereo images. Because of related\ndatasets' unavailability, we collect a real-world dataset that contains stereo\nimages with and without waterdrops. Extensive experiments on our dataset\nsuggest that our model outperforms state-of-the-art methods both quantitatively\nand qualitatively. Our source code and the stereo waterdrop dataset are\navailable at\n\\href{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}{https://github.com/VivianSZF/Stereo-Waterdrop-Removal}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zifan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A k-mer Based Approach for SARS-CoV-2 Variant Identification. (arXiv:2108.03465v1 [q-bio.QM])","link":"http://arxiv.org/abs/2108.03465","description":"<p>With the rapid spread of the novel coronavirus (COVID-19) across the globe\nand its continuous mutation, it is of pivotal importance to design a system to\nidentify different known (and unknown) variants of SARS-CoV-2. Identifying\nparticular variants helps to understand and model their spread patterns, design\neffective mitigation strategies, and prevent future outbreaks. It also plays a\ncrucial role in studying the efficacy of known vaccines against each variant\nand modeling the likelihood of breakthrough infections. It is well known that\nthe spike protein contains most of the information/variation pertaining to\ncoronavirus variants.\n</p>\n<p>In this paper, we use spike sequences to classify different variants of the\ncoronavirus in humans. We show that preserving the order of the amino acids\nhelps the underlying classifiers to achieve better performance. We also show\nthat we can train our model to outperform the baseline algorithms using only a\nsmall number of training samples ($1\\%$ of the data). Finally, we show the\nimportance of the different amino acids which play a key role in identifying\nvariants and how they coincide with those reported by the USA's Centers for\nDisease Control and Prevention (CDC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sahoo_B/0/1/0/all/0/1\">Bikram Sahoo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ullah_N/0/1/0/all/0/1\">Naimat Ullah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zelikovskiy_A/0/1/0/all/0/1\">Alexander Zelikovskiy</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1\">Murray Patterson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1\">Imdadullah Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An empirical assessment of deep learning approaches to task-oriented dialog management. (arXiv:2108.03478v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03478","description":"<p>Deep learning is providing very positive results in areas related to\nconversational interfaces, such as speech recognition, but its potential\nbenefit for dialog management has still not been fully studied. In this paper,\nwe perform an assessment of different configurations for deep-learned dialog\nmanagement with three dialog corpora from different application domains and\nvarying in size, dimensionality and possible system responses. Our results have\nallowed us to identify several aspects that can have an impact on accuracy,\nincluding the approaches used for feature extraction, input representation,\ncontext consideration and the hyper-parameters of the deep neural networks\nemployed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matej%5Cr%7Bu%7D_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Mat&#x11b;j&#x16f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griol_D/0/1/0/all/0/1\">David Griol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callejas_Z/0/1/0/all/0/1\">Zoraida Callejas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molina_J/0/1/0/all/0/1\">Jos&#xe9; Manuel Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchis_A/0/1/0/all/0/1\">Araceli Sanchis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Aliasing on Generalization in Deep Convolutional Networks. (arXiv:2108.03489v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03489","description":"<p>We investigate the impact of aliasing on generalization in Deep Convolutional\nNetworks and show that data augmentation schemes alone are unable to prevent it\ndue to structural limitations in widely used architectures. Drawing insights\nfrom frequency analysis theory, we take a closer look at ResNet and\nEfficientNet architectures and review the trade-off between aliasing and\ninformation loss in each of their major components. We show how to mitigate\naliasing by inserting non-trainable low-pass filters at key locations,\nparticularly where networks lack the capacity to learn them. These simple\narchitectural changes lead to substantial improvements in generalization on\ni.i.d. and even more on out-of-distribution conditions, such as image\nclassification under natural corruptions on ImageNet-C [11] and few-shot\nlearning on Meta-Dataset [26]. State-of-the art results are achieved on both\ndatasets without introducing additional trainable parameters and using the\ndefault hyper-parameters of open source codebases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Vasconcelos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1\">Hugo Larochelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romijnders_R/0/1/0/all/0/1\">Rob Romijnders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goroshin_R/0/1/0/all/0/1\">Ross Goroshin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering Algorithms to Analyze the Road Traffic Crashes. (arXiv:2108.03490v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03490","description":"<p>Selecting an appropriate clustering method as well as an optimal number of\nclusters in road accident data is at times confusing and difficult. This paper\nanalyzes shortcomings of different existing techniques applied to cluster\naccident-prone areas and recommends using Density-Based Spatial Clustering of\nApplications with Noise (DBSCAN) and Ordering Points To Identify the Clustering\nStructure (OPTICS) to overcome them. Comparative performance analysis based on\nreal-life data on the recorded cases of road accidents in North Carolina also\nshow more effectiveness and efficiency achieved by these algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mahnaz Rafia Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenny_I/0/1/0/all/0/1\">Israt Jahan Jenny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayon_M/0/1/0/all/0/1\">Moniruzzaman Nayon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Rajibul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiruzzaman_M/0/1/0/all/0/1\">Md Amiruzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullah_Al_Wadud_M/0/1/0/all/0/1\">M. Abdullah-Al-Wadud</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approximate Last Iterate Convergence in Overparameterized GANs. (arXiv:2108.03491v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03491","description":"<p>In this work, we showed that the Implicit Update and Predictive Methods\ndynamics introduced in prior work satisfy last iterate convergence to a\nneighborhood around the optimum in overparameterized GANs, where the size of\nthe neighborhood shrinks with the width of the neural network. This is in\ncontrast to prior results, which only guaranteed average iterate convergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_E/0/1/0/all/0/1\">Elbert Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kinematics clustering enables head impact subtyping for better traumatic brain injury prediction. (arXiv:2108.03498v1 [stat.AP])","link":"http://arxiv.org/abs/2108.03498","description":"<p>Traumatic brain injury can be caused by various types of head impacts.\nHowever, due to different kinematic characteristics, many brain injury risk\nestimation models are not generalizable across the variety of impacts that\nhumans may sustain. The current definitions of head impact subtypes are based\non impact sources (e.g., football, traffic accident), which may not reflect the\nintrinsic kinematic similarities of impacts across the impact sources. To\ninvestigate the potential new definitions of impact subtypes based on\nkinematics, 3,161 head impacts from various sources including simulation,\ncollege football, mixed martial arts, and car racing were collected. We applied\nthe K-means clustering to cluster the impacts on 16 standardized temporal\nfeatures from head rotation kinematics. Then, we developed subtype-specific\nridge regression models for cumulative strain damage (using the threshold of\n15%), which significantly improved the estimation accuracy compared with the\nbaseline method which mixed impacts from different sources and developed one\nmodel (R^2 from 0.7 to 0.9). To investigate the effect of kinematic features,\nwe presented the top three critical features (maximum resultant angular\nacceleration, maximum angular acceleration along the z-axis, maximum linear\nacceleration along the y-axis) based on regression accuracy and used logistic\nregression to find the critical points for each feature that partitioned the\nsubtypes. This study enables researchers to define head impact subtypes in a\ndata-driven manner, which leads to more generalizable brain injury risk\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Zhan_X/0/1/0/all/0/1\">Xianghao Zhan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yiheng Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1\">Yuzhe Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cecchi_N/0/1/0/all/0/1\">Nicholas J. Cecchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gevaert_O/0/1/0/all/0/1\">Olivier Gevaert</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeineh_M/0/1/0/all/0/1\">Michael M. Zeineh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Grant_G/0/1/0/all/0/1\">Gerald A. Grant</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Camarillo_D/0/1/0/all/0/1\">David B. Camarillo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Membership Inference Attacks on Lottery Ticket Networks. (arXiv:2108.03506v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03506","description":"<p>The vulnerability of the Lottery Ticket Hypothesis has not been studied from\nthe purview of Membership Inference Attacks. Through this work, we are the\nfirst to empirically show that the lottery ticket networks are equally\nvulnerable to membership inference attacks. A Membership Inference Attack (MIA)\nis the process of determining whether a data sample belongs to a training set\nof a trained model or not. Membership Inference Attacks could leak critical\ninformation about the training data that can be used for targeted attacks.\nRecent deep learning models often have very large memory footprints and a high\ncomputational cost associated with training and drawing inferences. Lottery\nTicket Hypothesis is used to prune the networks to find smaller sub-networks\nthat at least match the performance of the original model in terms of test\naccuracy in a similar number of iterations. We used CIFAR-10, CIFAR-100, and\nImageNet datasets to perform image classification tasks and observe that the\nattack accuracies are similar. We also see that the attack accuracy varies\ndirectly according to the number of classes in the dataset and the sparsity of\nthe network. We demonstrate that these attacks are transferable across models\nwith high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagmar_A/0/1/0/all/0/1\">Aadesh Bagmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiya_S/0/1/0/all/0/1\">Shishira R Maiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bidwalka_S/0/1/0/all/0/1\">Shruti Bidwalka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1\">Amol Deshpande</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Training Parameters and Mechanisms on Decentralized Federated Learning based on MNIST Dataset. (arXiv:2108.03508v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03508","description":"<p>Federated Learning is an algorithm suited for training models on\ndecentralized data, but the requirement of a central \"server\" node is a\nbottleneck. In this document, we first introduce the notion of Decentralized\nFederated Learning (DFL). We then perform various experiments on different\nsetups, such as changing model aggregation frequency, switching from\nindependent and identically distributed (IID) dataset partitioning to non-IID\npartitioning with partial global sharing, using different optimization methods\nacross clients, and breaking models into segments with partial sharing. All\nexperiments are run on the MNIST handwritten digits dataset. We observe that\nthose altered training procedures are generally robust, albeit non-optimal. We\nalso observe failures in training when the variance between model weights is\ntoo large. The open-source experiment code is accessible through\nGitHub\\footnote{Code was uploaded at\n\\url{https://github.com/zhzhang2018/DecentralizedFL}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuofan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_C/0/1/0/all/0/1\">Chaouki Abdallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Assisted Security Analysis of 5G-Network-Connected Systems. (arXiv:2108.03514v1 [cs.NI])","link":"http://arxiv.org/abs/2108.03514","description":"<p>The core network architecture of telecommunication systems has undergone a\nparadigm shift in the fifth-generation (5G)networks. 5G networks have\ntransitioned to software-defined infrastructures, thereby reducing their\ndependence on hardware-based network functions. New technologies, like network\nfunction virtualization and software-defined networking, have been incorporated\nin the 5G core network (5GCN) architecture to enable this transition. This has\nresulted in significant improvements in efficiency, performance, and robustness\nof the networks. However, this has also made the core network more vulnerable,\nas software systems are generally easier to compromise than hardware systems.\nIn this article, we present a comprehensive security analysis framework for the\n5GCN. The novelty of this approach lies in the creation and analysis of attack\ngraphs of the software-defined and virtualized 5GCN through machine learning.\nThis analysis points to 119 novel possible exploits in the 5GCN. We demonstrate\nthat these possible exploits of 5GCN vulnerabilities generate five novel\nattacks on the 5G Authentication and Key Agreement protocol. We combine the\nattacks at the network, protocol, and the application layers to generate\ncomplex attack vectors. In a case study, we use these attack vectors to find\nfour novel security loopholes in WhatsApp running on a 5G network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_T/0/1/0/all/0/1\">Tanujay Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aaraj_N/0/1/0/all/0/1\">Najwa Aaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1\">Niraj K. Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Transfer with von Neumann Conditional Divergence. (arXiv:2108.03531v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03531","description":"<p>The similarity of feature representations plays a pivotal role in the success\nof domain adaptation and generalization. Feature similarity includes both the\ninvariance of marginal distributions and the closeness of conditional\ndistributions given the desired response $y$ (e.g., class labels).\nUnfortunately, traditional methods always learn such features without fully\ntaking into consideration the information in $y$, which in turn may lead to a\nmismatch of the conditional distributions or the mix-up of discriminative\nstructures underlying data distributions. In this work, we introduce the\nrecently proposed von Neumann conditional divergence to improve the\ntransferability across multiple domains. We show that this new divergence is\ndifferentiable and eligible to easily quantify the functional dependence\nbetween features and $y$. Given multiple source tasks, we integrate this\ndivergence to capture discriminative information in $y$ and design novel\nlearning objectives assuming those source tasks are observed either\nsimultaneously or sequentially. In both scenarios, we obtain favorable\nperformance against state-of-the-art methods in terms of smaller generalization\nerror on new tasks and less catastrophic forgetting on source tasks (in the\nsequential setup).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1\">Ammar Shaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shujian Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSCAR-Net: Object-centric Scene Graph Attention for Image Attribution. (arXiv:2108.03541v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03541","description":"<p>Images tell powerful stories but cannot always be trusted. Matching images\nback to trusted sources (attribution) enables users to make a more informed\njudgment of the images they encounter online. We propose a robust image hashing\nalgorithm to perform such matching. Our hash is sensitive to manipulation of\nsubtle, salient visual details that can substantially change the story told by\nan image. Yet the hash is invariant to benign transformations (changes in\nquality, codecs, sizes, shapes, etc.) experienced by images during online\nredistribution. Our key contribution is OSCAR-Net (Object-centric Scene Graph\nAttention for Image Attribution Network); a robust image hashing model inspired\nby recent successes of Transformers in the visual domain. OSCAR-Net constructs\na scene graph representation that attends to fine-grained changes of every\nobject's visual appearance and their spatial relationships. The network is\ntrained via contrastive learning on a dataset of original and manipulated\nimages yielding a state of the art image hash for content fingerprinting that\nscales to millions of images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1\">Eric Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Vishy Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Graph Neural Networks for Rumor Detection in Online Forums. (arXiv:2108.03548v1 [cs.SI])","link":"http://arxiv.org/abs/2108.03548","description":"<p>The widespread adoption of online social networks in daily life has created a\npressing need for effectively classifying user-generated content. This work\npresents techniques for classifying linked content spread on forum websites --\nspecifically, links to news articles or blogs -- using user interaction signals\nalone. Importantly, online forums such as Reddit do not have a user-generated\nsocial graph, which is assumed in social network behavioral-based\nclassification settings. Using Reddit as a case-study, we show how to obtain a\nderived social graph, and use this graph, Reddit post sequences, and comment\ntrees as inputs to a Recurrent Graph Neural Network (R-GNN) encoder. We train\nthe R-GNN on news link categorization and rumor detection, showing superior\nresults to recent baselines. Our code is made publicly available at\nhttps://github.com/google-research/social_cascades.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartel_J/0/1/0/all/0/1\">Jacob Bartel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palowitch_J/0/1/0/all/0/1\">John Palowitch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Representation Learning for Rapid Intraoperative Diagnosis of Skull Base Tumors Imaged Using Stimulated Raman Histology. (arXiv:2108.03555v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03555","description":"<p>Background: Accurate diagnosis of skull base tumors is essential for\nproviding personalized surgical treatment strategies. Intraoperative diagnosis\ncan be challenging due to tumor diversity and lack of intraoperative pathology\nresources.\n</p>\n<p>Objective: To develop an independent and parallel intraoperative pathology\nworkflow that can provide rapid and accurate skull base tumor diagnoses using\nlabel-free optical imaging and artificial intelligence (AI).\n</p>\n<p>Method: We used a fiber laser-based, label-free, non-consumptive,\nhigh-resolution microscopy method ($&lt;$ 60 sec per 1 $\\times$ 1 mm$^\\text{2}$),\ncalled stimulated Raman histology (SRH), to image a consecutive, multicenter\ncohort of skull base tumor patients. SRH images were then used to train a\nconvolutional neural network (CNN) model using three representation learning\nstrategies: cross-entropy, self-supervised contrastive learning, and supervised\ncontrastive learning. Our trained CNN models were tested on a held-out,\nmulticenter SRH dataset.\n</p>\n<p>Results: SRH was able to image the diagnostic features of both benign and\nmalignant skull base tumors. Of the three representation learning strategies,\nsupervised contrastive learning most effectively learned the distinctive and\ndiagnostic SRH image features for each of the skull base tumor types. In our\nmulticenter testing set, cross-entropy achieved an overall diagnostic accuracy\nof 91.5%, self-supervised contrastive learning 83.9%, and supervised\ncontrastive learning 96.6%. Our trained model was able to identify tumor-normal\nmargins and detect regions of microscopic tumor infiltration in whole-slide SRH\nimages.\n</p>\n<p>Conclusion: SRH with AI models trained using contrastive representation\nlearning can provide rapid and accurate intraoperative diagnosis of skull base\ntumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Abhishek Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzey_J/0/1/0/all/0/1\">Joseph Linzey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rushikesh Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sung Jik Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1\">Sudharsan Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alber_D/0/1/0/all/0/1\">Daniel Alber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondepudi_A/0/1/0/all/0/1\">Akhil Kondepudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urias_E/0/1/0/all/0/1\">Esteban Urias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandian_B/0/1/0/all/0/1\">Balaji Pandian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Holou_W/0/1/0/all/0/1\">Wajd Al-Holou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_S/0/1/0/all/0/1\">Steve Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1\">B. Gregory Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heth_J/0/1/0/all/0/1\">Jason Heth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freudiger_C/0/1/0/all/0/1\">Chris Freudiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalsa_S/0/1/0/all/0/1\">Siri Khalsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacione_D/0/1/0/all/0/1\">Donato Pacione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golfinos_J/0/1/0/all/0/1\">John G. Golfinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camelo_Piragua_S/0/1/0/all/0/1\">Sandra Camelo-Piragua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orringer_D/0/1/0/all/0/1\">Daniel A. Orringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollon_T/0/1/0/all/0/1\">Todd Hollon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining machine learning and data assimilation to forecast dynamical systems from noisy partial observations. (arXiv:2108.03561v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03561","description":"<p>We present a supervised learning method to learn the propagator map of a\ndynamical system from partial and noisy observations. In our computationally\ncheap and easy-to-implement framework a neural network consisting of random\nfeature maps is trained sequentially by incoming observations within a data\nassimilation procedure. By employing Takens' embedding theorem, the network is\ntrained on delay coordinates. We show that the combination of random feature\nmaps and data assimilation, called RAFDA, outperforms standard random feature\nmaps for which the dynamics is learned using batch data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gottwald_G/0/1/0/all/0/1\">Georg A. Gottwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1\">Sebastian Reich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust 1-bit Compressive Sensing with Partial Gaussian Circulant Matrices and Generative Priors. (arXiv:2108.03570v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03570","description":"<p>In 1-bit compressive sensing, each measurement is quantized to a single bit,\nnamely the sign of a linear function of an unknown vector, and the goal is to\naccurately recover the vector. While it is most popular to assume a standard\nGaussian sensing matrix for 1-bit compressive sensing, using structured sensing\nmatrices such as partial Gaussian circulant matrices is of significant\npractical importance due to their faster matrix operations. In this paper, we\nprovide recovery guarantees for a correlation-based optimization algorithm for\nrobust 1-bit compressive sensing with randomly signed partial Gaussian\ncirculant matrices and generative models. Under suitable assumptions, we match\nguarantees that were previously only known to hold for i.i.d.~Gaussian matrices\nthat require significantly more computation. We make use of a practical\niterative algorithm, and perform numerical experiments on image datasets to\ncorroborate our theoretical results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhroshekhar Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarlett_J/0/1/0/all/0/1\">Jonathan Scarlett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BeatNet: CRNN and Particle Filtering for Online Joint Beat Downbeat and Meter Tracking. (arXiv:2108.03576v1 [eess.AS])","link":"http://arxiv.org/abs/2108.03576","description":"<p>The online estimation of rhythmic information, such as beat positions,\ndownbeat positions, and meter, is critical for many real-time music\napplications. Musical rhythm comprises complex hierarchical relationships\nacross time, rendering its analysis intrinsically challenging and at times\nsubjective. Furthermore, systems which attempt to estimate rhythmic information\nin real-time must be causal and must produce estimates quickly and efficiently.\nIn this work, we introduce an online system for joint beat, downbeat, and meter\ntracking, which utilizes causal convolutional and recurrent layers, followed by\na pair of sequential Monte Carlo particle filters applied during inference. The\nproposed system does not need to be primed with a time signature in order to\nperform downbeat tracking, and is instead able to estimate meter and adjust the\npredictions over time. Additionally, we propose an information gate strategy to\nsignificantly decrease the computational cost of particle filtering during the\ninference step, making the system much faster than previous sampling-based\nmethods. Experiments on the GTZAN dataset, which is unseen during training,\nshow that the system outperforms various online beat and downbeat tracking\nsystems and achieves comparable performance to a baseline offline joint method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Heydari_M/0/1/0/all/0/1\">Mojtaba Heydari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cwitkowitz_F/0/1/0/all/0/1\">Frank Cwitkowitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Evaluation in Open-ended Text Generation. (arXiv:2108.03578v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03578","description":"<p>Although current state-of-the-art language models have achieved impressive\nresults in numerous natural language processing tasks, still they could not\nsolve the problem of producing repetitive, dull and sometimes inconsistent text\nin open-ended text generation. Studies often attribute this problem to the\nmaximum likelihood training objective, and propose alternative approaches by\nusing stochastic decoding methods or altering the training objective. However,\nthere is still a lack of consistent evaluation metrics to directly compare the\nefficacy of these solutions. In this work, we study different evaluation\nmetrics that have been proposed to evaluate quality, diversity and consistency\nof machine-generated text. From there, we propose a practical pipeline to\nevaluate language models in open-ended generation task, and research on how to\nimprove the model's performance in all dimensions by leveraging different\nauxiliary training objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">An Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expressive Power and Loss Surfaces of Deep Learning Models. (arXiv:2108.03579v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03579","description":"<p>The goals of this paper are two-fold. The first goal is to serve as an\nexpository tutorial on the working of deep learning models which emphasizes\ngeometrical intuition about the reasons for success of deep learning. The\nsecond goal is to complement the current results on the expressive power of\ndeep learning models and their loss surfaces with novel insights and results.\nIn particular, we describe how deep neural networks carve out manifolds\nespecially when the multiplication neurons are introduced. Multiplication is\nused in dot products and the attention mechanism and it is employed in capsule\nnetworks and self-attention based transformers. We also describe how random\npolynomial, random matrix, spin glass and computational complexity perspectives\non the loss surfaces are interconnected.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dube_S/0/1/0/all/0/1\">Simant Dube</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble neuroevolution based approach for multivariate time series anomaly detection. (arXiv:2108.03585v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03585","description":"<p>Multivariate time series anomaly detection is a very common problem in the\nfield of failure prevention. Fast prevention means lower repair costs and\nlosses. The amount of sensors in novel industry systems makes the anomaly\ndetection process quite difficult for humans. Algorithms which automates the\nprocess of detecting anomalies are crucial in modern failure-prevention\nsystems. Therefore, many machine and deep learning models have been designed to\naddress this problem. Mostly, they are autoencoder-based architectures with\nsome generative adversarial elements. In this work, a framework is shown which\nincorporates neuroevolution methods to boost the anomaly-detection scores of\nnew and already known models. The presented approach adapts evolution\nstrategies for evolving ensemble model, in which every single model works on a\nsubgroup of data sensors. The next goal of neuroevolution is to optimise\narchitecture and hyperparameters like window size, the number of layers, layer\ndepths, etc. The proposed framework shows that it is possible to boost most of\nthe anomaly detection deep learning models in a reasonable time and a fully\nautomated mode. The tests were run on SWAT and WADI datasets. To our knowledge,\nthis is the first approach in which an ensemble deep learning anomaly detection\nmodel is built in a fully automatic way using a neuroevolution strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faber_K/0/1/0/all/0/1\">Kamil Faber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zurek_D/0/1/0/all/0/1\">Dominik &#x17b;urek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1\">Marcin Pietro&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietak_K/0/1/0/all/0/1\">Kamil Pi&#x119;tak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Look at the Evaluation Setup of the M5 Forecasting Competition. (arXiv:2108.03588v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03588","description":"<p>Forecast evaluation plays a key role in how empirical evidence shapes the\ndevelopment of the discipline. Domain experts are interested in error measures\nrelevant for their decision making needs. Such measures may produce unreliable\nresults. Although reliability properties of several metrics have already been\ndiscussed, it has hardly been quantified in an objective way. We propose a\nmeasure named Rank Stability, which evaluates how much the rankings of an\nexperiment differ in between similar datasets, when the models and errors are\nconstant. We use this to study the evaluation setup of the M5. We find that the\nevaluation setup of the M5 is less reliable than other measures. The main\ndrivers of instability are hierarchical aggregation and scaling.\nPrice-weighting reduces the stability of all tested error measures. Scale\nnormalization of the M5 error measure results in less stability than other\nscale-free errors. Hierarchical levels taken separately are less stable with\nmore aggregation, and their combination is even less stable than individual\nlevels. We also show positive tradeoffs of retaining aggregation importance\nwithout affecting stability. Aggregation and stability can be linked to the\ninfluence of much debated magic numbers. Many of our findings can be applied to\ngeneral hierarchical forecast benchmarking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hewamalage_H/0/1/0/all/0/1\">Hansika Hewamalage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montero_Manso_P/0/1/0/all/0/1\">Pablo Montero-Manso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmeir_C/0/1/0/all/0/1\">Christoph Bergmeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyndman_R/0/1/0/all/0/1\">Rob J Hyndman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FederatedNILM: A Distributed and Privacy-preserving Framework for Non-intrusive Load Monitoring based on Federated Deep Learning. (arXiv:2108.03591v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03591","description":"<p>Non-intrusive load monitoring (NILM), which usually utilizes machine learning\nmethods and is effective in disaggregating smart meter readings from the\nhousehold-level into appliance-level consumptions, can help to analyze\nelectricity consumption behaviours of users and enable practical smart energy\nand smart grid applications. However, smart meters are privately owned and\ndistributed, which make real-world applications of NILM challenging. To this\nend, this paper develops a distributed and privacy-preserving federated deep\nlearning framework for NILM (FederatedNILM), which combines federated learning\nwith a state-of-the-art deep learning architecture to conduct NILM for the\nclassification of typical states of household appliances. Through extensive\ncomparative experiments, the effectiveness of the proposed FederatedNILM\nframework is demonstrated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Shuang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fanlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xizhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAF-GNN: Multi-adaptive Spatiotemporal-flow Graph Neural Network for Traffic Speed Forecasting. (arXiv:2108.03594v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03594","description":"<p>Traffic forecasting is a core element of intelligent traffic monitoring\nsystem. Approaches based on graph neural networks have been widely used in this\ntask to effectively capture spatial and temporal dependencies of road networks.\nHowever, these approaches can not effectively define the complicated network\ntopology. Besides, their cascade network structures have limitations in\ntransmitting distinct features in the time and space dimensions. In this paper,\nwe propose a Multi-adaptive Spatiotemporal-flow Graph Neural Network (MAF-GNN)\nfor traffic speed forecasting. MAF-GNN introduces an effective Multi-adaptive\nAdjacency Matrices Mechanism to capture multiple latent spatial dependencies\nbetween traffic nodes. Additionally, we propose Spatiotemporal-flow Modules\naiming to further enhance feature propagation in both time and space\ndimensions. MAF-GNN achieves better performance than other models on two\nreal-world datasets of public traffic network, METR-LA and PeMS-Bay,\ndemonstrating the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yaobin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weitang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongyi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zixuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1\">Tingyun Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lili Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingwei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Biological Variables and Social Determinants to Predict Malaria and Anemia among Children in Senegal. (arXiv:2108.03601v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03601","description":"<p>Integrating machine learning techniques in healthcare becomes very common\nnowadays, and it contributes positively to improving clinical care and health\ndecisions planning. Anemia and malaria are two life-threatening diseases in\nAfrica that affect the red blood cells and reduce hemoglobin production. This\npaper focuses on analyzing child health data in Senegal using four machine\nlearning algorithms in Python: KNN, Random Forests, SVM, and Na\\\"ive Bayes. Our\ntask aims to investigate large-scale data from The Demographic and Health\nSurvey (DHS) and to find out hidden information for anemia and malaria. We\npresent two classification models for the two blood disorders using biological\nvariables and social determinants. The findings of this research will\ncontribute to improving child healthcare in Senegal by eradicating anemia and\nmalaria, and decreasing the child mortality rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sow_B/0/1/0/all/0/1\">Boubacar Sow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suguri_H/0/1/0/all/0/1\">Hiroki Suguri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhtar_H/0/1/0/all/0/1\">Hamid Mukhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_H/0/1/0/all/0/1\">Hafiz Farooq Ahmad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An EM Framework for Online Incremental Learning of Semantic Segmentation. (arXiv:2108.03613v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03613","description":"<p>Incremental learning of semantic segmentation has emerged as a promising\nstrategy for visual scene interpretation in the open- world setting. However,\nit remains challenging to acquire novel classes in an online fashion for the\nsegmentation task, mainly due to its continuously-evolving semantic label\nspace, partial pixelwise ground-truth annotations, and constrained data\navailability. To ad- dress this, we propose an incremental learning strategy\nthat can fast adapt deep segmentation models without catastrophic forgetting,\nusing a streaming input data with pixel annotations on the novel classes only.\nTo this end, we develop a uni ed learning strategy based on the\nExpectation-Maximization (EM) framework, which integrates an iterative\nrelabeling strategy that lls in the missing labels and a rehearsal-based\nincremental learning step that balances the stability-plasticity of the model.\nMoreover, our EM algorithm adopts an adaptive sampling method to select\ninformative train- ing data and a class-balancing training strategy in the\nincremental model updates, both improving the e cacy of model learning. We\nvalidate our approach on the PASCAL VOC 2012 and ADE20K datasets, and the\nresults demonstrate its superior performance over the existing incremental\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shipeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiale Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangwei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Heterogenous Electronic Health Records Systems via Text-Based Code Embedding. (arXiv:2108.03625v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03625","description":"<p>Substantial increase in the use of Electronic Health Records (EHRs) has\nopened new frontiers for predictive healthcare. However, while EHR systems are\nnearly ubiquitous, they lack a unified code system for representing medical\nconcepts. Heterogeneous formats of EHR present a substantial barrier for the\ntraining and deployment of state-of-the-art deep learning models at scale. To\novercome this problem, we introduce Description-based Embedding, DescEmb, a\ncode-agnostic description-based representation learning framework for\npredictive modeling on EHR. DescEmb takes advantage of the flexibility of\nneural language understanding models while maintaining a neutral approach that\ncan be combined with prior frameworks for task-specific representation learning\nor predictive modeling. We tested our model's capacity on various experiments\nincluding prediction tasks, transfer learning and pooled learning. DescEmb\nshows higher performance in overall experiments compared to code-based\napproach, opening the door to a text-based approach in predictive healthcare\nresearch that is not constrained by EHR structure nor special domain knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hur_K/0/1/0/all/0/1\">Kyunghoon Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jungwoo Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Wesley Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Hak Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Network for DrawiNg Networks, (DNN)^2. (arXiv:2108.03632v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03632","description":"<p>By leveraging recent progress of stochastic gradient descent methods, several\nworks have shown that graphs could be efficiently laid out through the\noptimization of a tailored objective function. In the meantime, Deep Learning\n(DL) techniques achieved great performances in many applications. We\ndemonstrate that it is possible to use DL techniques to learn a graph-to-layout\nsequence of operations thanks to a graph-related objective function. In this\npaper, we present a novel graph drawing framework called (DNN)^2: Deep Neural\nNetwork for DrawiNg Networks. Our method uses Graph Convolution Networks to\nlearn a model. Learning is achieved by optimizing a graph topology related loss\nfunction that evaluates (DNN)^2 generated layouts during training. Once\ntrained, the (DNN)^ model is able to quickly lay any input graph out. We\nexperiment (DNN)^2 and statistically compare it to optimization-based and\nregular graph layout algorithms. The results show that (DNN)^2 performs well\nand are encouraging as the Deep Learning approach to Graph Drawing is novel and\nmany leads for future works are identified.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giovannangeli_L/0/1/0/all/0/1\">Loann Giovannangeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalanne_F/0/1/0/all/0/1\">Frederic Lalanne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auber_D/0/1/0/all/0/1\">David Auber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giot_R/0/1/0/all/0/1\">Romain Giot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourqui_R/0/1/0/all/0/1\">Romain Bourqui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Evolutionary Batch Size Orchestration for Scheduling Deep Learning Workloads in GPU Clusters. (arXiv:2108.03645v1 [cs.DC])","link":"http://arxiv.org/abs/2108.03645","description":"<p>Efficient GPU resource scheduling is essential to maximize resource\nutilization and save training costs for the increasing amount of deep learning\nworkloads in shared GPU clusters. Existing GPU schedulers largely rely on\nstatic policies to leverage the performance characteristics of deep learning\njobs. However, they can hardly reach optimal efficiency due to the lack of\nelasticity. To address the problem, we propose ONES, an ONline Evolutionary\nScheduler for elastic batch size orchestration. ONES automatically manages the\nelasticity of each job based on the training batch size, so as to maximize GPU\nutilization and improve scheduling efficiency. It determines the batch size for\neach job through an online evolutionary search that can continuously optimize\nthe scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on\nTACC's Longhorn supercomputers. The results show that ONES can outperform the\nprior deep learning schedulers with a significantly shorter average job\ncompletion time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bian_Z/0/1/0/all/0/1\">Zhengda Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shenggui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Depth and Normal Estimation from Real-world Time-of-flight Raw Data. (arXiv:2108.03649v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03649","description":"<p>We present a novel approach to joint depth and normal estimation for\ntime-of-flight (ToF) sensors. Our model learns to predict the high-quality\ndepth and normal maps jointly from ToF raw sensor data. To achieve this, we\nmeticulously constructed the first large-scale dataset (named ToF-100) with\npaired raw ToF data and ground-truth high-resolution depth maps provided by an\nindustrial depth camera. In addition, we also design a simple but effective\nframework for joint depth and normal estimation, applying a robust Chamfer loss\nvia jittering to improve the performance of our model. Our experiments\ndemonstrate that our proposed method can efficiently reconstruct\nhigh-resolution depth and normal maps and significantly outperforms\nstate-of-the-art approaches. Our code and data will be available at\n\\url{https://github.com/hkustVisionRr/JointlyDepthNormalEstimation}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Rongrong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio Spectral Enhancement: Leveraging Autoencoders for Low Latency Reconstruction of Long, Lossy Audio Sequences. (arXiv:2108.03703v1 [cs.SD])","link":"http://arxiv.org/abs/2108.03703","description":"<p>With active research in audio compression techniques yielding substantial\nbreakthroughs, spectral reconstruction of low-quality audio waves remains a\nless indulged topic. In this paper, we propose a novel approach for\nreconstructing higher frequencies from considerably longer sequences of\nlow-quality MP3 audio waves. Our technique involves inpainting audio\nspectrograms with residually stacked autoencoder blocks by manipulating\nindividual amplitude and phase values in relation to perceptual differences.\nOur architecture presents several bottlenecks while preserving the spectral\nstructure of the audio wave via skip-connections. We also compare several task\nmetrics and demonstrate our visual guide to loss selection. Moreover, we show\nhow to leverage differential quantization techniques to reduce the initial\nmodel size by more than half while simultaneously reducing inference time,\nwhich is crucial in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_D/0/1/0/all/0/1\">Darshan Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abichandani_H/0/1/0/all/0/1\">Harshavardhan Abichandani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Bootstrap Inference For Policy Evaluation in Reinforcement Learning. (arXiv:2108.03706v1 [stat.ML])","link":"http://arxiv.org/abs/2108.03706","description":"<p>The recent emergence of reinforcement learning has created a demand for\nrobust statistical inference methods for the parameter estimates computed using\nthese algorithms. Existing methods for statistical inference in online learning\nare restricted to settings involving independently sampled observations, while\nexisting statistical inference methods in reinforcement learning (RL) are\nlimited to the batch setting. The online bootstrap is a flexible and efficient\napproach for statistical inference in linear stochastic approximation\nalgorithms, but its efficacy in settings involving Markov noise, such as RL,\nhas yet to be explored. In this paper, we study the use of the online bootstrap\nmethod for statistical inference in RL. In particular, we focus on the temporal\ndifference (TD) learning and Gradient TD (GTD) learning algorithms, which are\nthemselves special instances of linear stochastic approximation under Markov\nnoise. The method is shown to be distributionally consistent for statistical\ninference in policy evaluation, and numerical experiments are included to\ndemonstrate the effectiveness of this algorithm at statistical inference tasks\nacross a range of real RL environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Ramprasad_P/0/1/0/all/0/1\">Pratik Ramprasad</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yuantong Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sun_W/0/1/0/all/0/1\">Will Wei Sun</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cheng_G/0/1/0/all/0/1\">Guang Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Dynamic Mode Decomposition: Balancing Accuracy and Expressiveness in Koopman Approximations. (arXiv:2108.03712v1 [eess.SY])","link":"http://arxiv.org/abs/2108.03712","description":"<p>This paper tackles the data-driven approximation of unknown dynamical systems\nusing Koopman-operator methods. Given a dictionary of functions, these methods\napproximate the projection of the action of the operator on the\nfinite-dimensional subspace spanned by the dictionary. We propose the Tunable\nSymmetric Subspace Decomposition algorithm to refine the dictionary, balancing\nits expressiveness and accuracy. Expressiveness corresponds to the ability of\nthe dictionary to describe the evolution of as many observables as possible and\naccuracy corresponds to the ability to correctly predict their evolution. Based\non the observation that Koopman-invariant subspaces give rise to exact\npredictions, we reason that prediction accuracy is a function of the degree of\ninvariance of the subspace generated by the dictionary and provide a\ndata-driven measure to measure invariance proximity. The proposed algorithm\niteratively prunes the initial functional space to identify a refined\ndictionary of functions that satisfies the desired level of accuracy while\nretaining as much of the original expressiveness as possible. We provide a full\ncharacterization of the algorithm properties and show that it generalizes both\nExtended Dynamic Mode Decomposition and Symmetric Subspace Decomposition.\nSimulations on planar systems show the effectiveness of the proposed methods in\nproducing Koopman approximations of tunable accuracy that capture relevant\ninformation about the dynamical system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Haseli_M/0/1/0/all/0/1\">Masih Haseli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cortes_J/0/1/0/all/0/1\">Jorge Cort&#xe9;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Difficulty of Generalizing Reinforcement Learning Framework for Combinatorial Optimization. (arXiv:2108.03713v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03713","description":"<p>Combinatorial optimization problems (COPs) on the graph with real-life\napplications are canonical challenges in Computer Science. The difficulty of\nfinding quality labels for problem instances holds back leveraging supervised\nlearning across combinatorial problems. Reinforcement learning (RL) algorithms\nhave recently been adopted to solve this challenge automatically. The\nunderlying principle of this approach is to deploy a graph neural network (GNN)\nfor encoding both the local information of the nodes and the graph-structured\ndata in order to capture the current state of the environment. Then, it is\nfollowed by the actor to learn the problem-specific heuristics on its own and\nmake an informed decision at each state for finally reaching a good solution.\nRecent studies on this subject mainly focus on a family of combinatorial\nproblems on the graph, such as the travel salesman problem, where the proposed\nmodel aims to find an ordering of vertices that optimizes a given objective\nfunction. We use the security-aware phone clone allocation in the cloud as a\nclassical quadratic assignment problem (QAP) to investigate whether or not deep\nRL-based model is generally applicable to solve other classes of such hard\nproblems. Extensive empirical evaluation shows that existing RL-based model may\nnot generalize to QAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pashazadeh_M/0/1/0/all/0/1\">Mostafa Pashazadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Reinforcement Learning in Broad and Non-Parametric Environments. (arXiv:2108.03718v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03718","description":"<p>Recent state-of-the-art artificial agents lack the ability to adapt rapidly\nto new tasks, as they are trained exclusively for specific objectives and\nrequire massive amounts of interaction to learn new skills. Meta-reinforcement\nlearning (meta-RL) addresses this challenge by leveraging knowledge learned\nfrom training tasks to perform well in previously unseen tasks. However,\ncurrent meta-RL approaches limit themselves to narrow parametric task\ndistributions, ignoring qualitative differences between tasks that occur in the\nreal world. In this paper, we introduce TIGR, a Task-Inference-based meta-RL\nalgorithm using Gaussian mixture models (GMM) and gated Recurrent units,\ndesigned for tasks in non-parametric environments. We employ a generative model\ninvolving a GMM to capture the multi-modality of the tasks. We decouple the\npolicy training from the task-inference learning and efficiently train the\ninference mechanism on the basis of an unsupervised reconstruction objective.\nWe provide a benchmark with qualitatively distinct tasks based on the\nhalf-cheetah environment and demonstrate the superior performance of TIGR\ncompared to state-of-the-art meta-RL approaches in terms of sample efficiency\n(3-10 times faster), asymptotic performance, and applicability in\nnon-parametric environments with zero-shot adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bing_Z/0/1/0/all/0/1\">Zhenshan Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knak_L/0/1/0/all/0/1\">Lukas Knak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robin_F/0/1/0/all/0/1\">Fabrice Oliver Robin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1\">Alois Knoll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVGen: Adversarial Networks for Learning Electric Vehicle Charging Loads and Hidden Representations. (arXiv:2108.03762v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03762","description":"<p>The nexus between transportation, the power grid, and consumer behavior is\nmore pronounced than ever before as the race to decarbonize the transportation\nsector intensifies. Electrification in the transportation sector has led to\ntechnology shifts and rapid deployment of electric vehicles (EVs). The\npotential increase in stochastic and spatially heterogeneous charging load\npresents a unique challenge that is not well studied, and will have significant\nimpacts on grid operations, emissions, and system reliability if not managed\neffectively. Realistic scenario generators can help operators prepare, and\nmachine learning can be leveraged to this end. In this work, we develop\ngenerative adversarial networks (GANs) to learn distributions of electric\nvehicle (EV) charging sessions and disentangled representations. We show that\nthis model structure successfully parameterizes unlabeled temporal and power\npatterns without supervision and is able to generate synthetic data conditioned\non these parameters. We benchmark the generation capability of this model with\nGaussian Mixture Models (GMMs), and empirically show that our proposed model\nframework is better at capturing charging distributions and temporal dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buechler_R/0/1/0/all/0/1\">Robert Buechler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balogun_E/0/1/0/all/0/1\">Emmanuel Balogun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1\">Arun Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1\">Ram Rajagopal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pathfinder: Parallel quasi-Newton variational inference. (arXiv:2108.03782v1 [stat.ML])","link":"http://arxiv.org/abs/2108.03782","description":"<p>We introduce Pathfinder, a variational method for approximately sampling from\ndifferentiable log densities. Starting from a random initialization, Pathfinder\nlocates normal approximations to the target density along a quasi-Newton\noptimization path, with local covariance estimated using the inverse Hessian\nestimates produced by the optimizer. Pathfinder returns draws from the\napproximation with the lowest estimated Kullback-Leibler (KL) divergence to the\ntrue posterior. We evaluate Pathfinder on a wide range of posterior\ndistributions, demonstrating that its approximate draws are better than those\nfrom automatic differentiation variational inference (ADVI) and comparable to\nthose produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as\nmeasured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC\nruns, Pathfinder requires one to two orders of magnitude fewer log density and\ngradient evaluations, with greater reductions for more challenging posteriors.\nImportance resampling over multiple runs of Pathfinder improves the diversity\nof approximate draws, reducing 1-Wasserstein distance further and providing a\nmeasure of robustness to optimization failures on plateaus, saddle points, or\nin minor modes. The Monte Carlo KL-divergence estimates are embarrassingly\nparallelizable in the core Pathfinder algorithm, as are multiple runs in the\nresampling version, further increasing Pathfinder's speed advantage with\nmultiple cores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carpenter_B/0/1/0/all/0/1\">Bob Carpenter</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1\">Andrew Gelman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1\">Aki Vehtari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Slice Net: A novel light weight framework for COVID-19 Diagnosis. (arXiv:2108.03786v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03786","description":"<p>This paper presents a novel lightweight COVID-19 diagnosis framework using CT\nscans. Our system utilises a novel two-stage approach to generate robust and\nefficient diagnoses across heterogeneous patient level inputs. We use a\npowerful backbone network as a feature extractor to capture discriminative\nslice-level features. These features are aggregated by a lightweight network to\nobtain a patient level diagnosis. The aggregation network is carefully designed\nto have a small number of trainable parameters while also possessing sufficient\ncapacity to generalise to diverse variations within different CT volumes and to\nadapt to noise introduced during the data acquisition. We achieve a significant\nperformance increase over the baselines when benchmarked on the SPGC COVID-19\nRadiomics Dataset, despite having only 2.5 million trainable parameters and\nrequiring only 0.623 seconds on average to process a single patient's CT volume\nusing an Nvidia-GeForce RTX 2080 GPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gammulle_H/0/1/0/all/0/1\">Harshala Gammulle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1\">Tharindu Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mis-spoke or mis-lead: Achieving Robustness in Multi-Agent Communicative Reinforcement Learning. (arXiv:2108.03803v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03803","description":"<p>Recent studies in multi-agent communicative reinforcement learning (MACRL)\ndemonstrate that multi-agent coordination can be significantly improved when\ncommunication between agents is allowed. Meanwhile, advances in adversarial\nmachine learning (ML) have shown that ML and reinforcement learning (RL) models\nare vulnerable to a variety of attacks that significantly degrade the\nperformance of learned behaviours. However, despite the obvious and growing\nimportance, the combination of adversarial ML and MACRL remains largely\nuninvestigated. In this paper, we make the first step towards conducting\nmessage attacks on MACRL methods. In our formulation, one agent in the\ncooperating group is taken over by an adversary and can send malicious messages\nto disrupt a deployed MACRL-based coordinated strategy during the deployment\nphase. We further our study by developing a defence method via message\nreconstruction. Finally, we address the resulting arms race, i.e., we consider\nthe ability of the malicious agent to adapt to the changing and improving\ndefensive communicative policies of the benign agents. Specifically, we model\nthe adversarial MACRL problem as a two-player zero-sum game and then utilize\nPolicy-Space Response Oracle to achieve communication robustness. Empirically,\nwe demonstrate that MACRL methods are vulnerable to message attacks while our\ndefence method the game-theoretic framework can effectively improve the\nrobustness of MACRL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wanqi Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1\">Bo An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabinovich_Z/0/1/0/all/0/1\">Zinovi Rabinovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obraztsova_S/0/1/0/all/0/1\">Svetlana Obraztsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1\">Chai Kiat Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whittle Index for A Class of Restless Bandits with Imperfect Observations. (arXiv:2108.03812v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03812","description":"<p>We consider a class of restless bandit problems that finds a broad\napplication area in stochastic optimization, reinforcement learning and\noperations research. In our model, there are $N$ independent $2$-state Markov\nprocesses that may be observed and accessed for accruing rewards. The\nobservation is error-prone, i.e., both false alarm and miss detection may\nhappen. Furthermore, the user can only choose a subset of $M~(M&lt;N)$ processes\nto observe at each discrete time. If a process in state~$1$ is correctly\nobserved, then it will offer some reward. Due to the partial and imperfect\nobservation model, the system is formulated as a restless multi-armed bandit\nproblem with an information state space of uncountable cardinality. Restless\nbandit problems with finite state spaces are PSPACE-HARD in general. In this\npaper, we establish a low-complexity algorithm that achieves a strong\nperformance for this class of restless bandits. Under certain conditions, we\ntheoretically prove the existence (indexability) of Whittle index and its\nequivalence to our algorithm. When those conditions do not hold, we show by\nnumerical experiments the near-optimal performance of our algorithm in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Keqin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Ting Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bob and Alice Go to a Bar: Reasoning About Future With Probabilistic Programs. (arXiv:2108.03834v1 [cs.AI])","link":"http://arxiv.org/abs/2108.03834","description":"<p>Agent preferences should be specified stochastically rather than\ndeterministically. Planning as inference with stochastic preferences naturally\ndescribes agent behaviors, does not require introducing rewards and exponential\nweighing of behaviors, and allows to reason about agents using the solid\nfoundation of Bayesian statistics. Stochastic conditioning is the formalism\nbehind agents with stochastic preferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolpin_D/0/1/0/all/0/1\">David Tolpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobkin_T/0/1/0/all/0/1\">Tomer Dobkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Multiobjective Minimax Optimization and Applications. (arXiv:2108.03837v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03837","description":"<p>We introduce a simple but general online learning framework, in which at\nevery round, an adaptive adversary introduces a new game, consisting of an\naction space for the learner, an action space for the adversary, and a vector\nvalued objective function that is convex-concave in every coordinate. The\nlearner and the adversary then play in this game. The learner's goal is to play\nso as to minimize the maximum coordinate of the cumulative vector-valued loss.\nThe resulting one-shot game is not convex-concave, and so the minimax theorem\ndoes not apply. Nevertheless, we give a simple algorithm that can compete with\nthe setting in which the adversary must announce their action first, with\noptimally diminishing regret.\n</p>\n<p>We demonstrate the power of our simple framework by using it to derive\noptimal bounds and algorithms across a variety of domains. This includes no\nregret learning: we can recover optimal algorithms and bounds for minimizing\nexternal regret, internal regret, adaptive regret, multigroup regret,\nsubsequence regret, and a notion of regret in the sleeping experts setting.\nNext, we use it to derive a variant of Blackwell's Approachability Theorem,\nwhich we term \"Fast Polytope Approachability\". Finally, we are able to recover\nrecently derived algorithms and bounds for online adversarial multicalibration\nand related notions (mean-conditioned moment multicalibration, and prediction\ninterval multivalidity).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noarov_G/0/1/0/all/0/1\">Georgy Noarov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_M/0/1/0/all/0/1\">Mallesh Pai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAN Computers Generate Arts? A Survey on Visual Arts, Music, and Literary Text Generation using Generative Adversarial Network. (arXiv:2108.03857v1 [cs.AI])","link":"http://arxiv.org/abs/2108.03857","description":"<p>\"Art is the lie that enables us to realize the truth.\" - Pablo Picasso. For\ncenturies, humans have dedicated themselves to producing arts to convey their\nimagination. The advancement in technology and deep learning in particular, has\ncaught the attention of many researchers trying to investigate whether art\ngeneration is possible by computers and algorithms. Using generative\nadversarial networks (GANs), applications such as synthesizing photorealistic\nhuman faces and creating captions automatically from images were realized. This\nsurvey takes a comprehensive look at the recent works using GANs for generating\nvisual arts, music, and literary text. A performance comparison and description\nof the various GAN architecture are also presented. Finally, some of the key\nchallenges in art generation using GANs are highlighted along with\nrecommendations for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1\">Sakib Shahriar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Anomaly Detection for Internet of Things in Hierarchical Edge Computing: A Contextual-Bandit Approach. (arXiv:2108.03872v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03872","description":"<p>The advances in deep neural networks (DNN) have significantly enhanced\nreal-time detection of anomalous data in IoT applications. However, the\ncomplexity-accuracy-delay dilemma persists: complex DNN models offer higher\naccuracy, but typical IoT devices can barely afford the computation load, and\nthe remedy of offloading the load to the cloud incurs long delay. In this\npaper, we address this challenge by proposing an adaptive anomaly detection\nscheme with hierarchical edge computing (HEC). Specifically, we first construct\nmultiple anomaly detection DNN models with increasing complexity, and associate\neach of them to a corresponding HEC layer. Then, we design an adaptive model\nselection scheme that is formulated as a contextual-bandit problem and solved\nby using a reinforcement learning policy network. We also incorporate a\nparallelism policy training method to accelerate the training process by taking\nadvantage of distributed models. We build an HEC testbed using real IoT\ndevices, implement and evaluate our contextual-bandit approach with both\nunivariate and multivariate IoT datasets. In comparison with both baseline and\nstate-of-the-art schemes, our adaptive approach strikes the best accuracy-delay\ntradeoff on the univariate dataset, and achieves the best accuracy and F1-score\non the multivariate dataset with only negligibly longer delay than the best\n(but inflexible) scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_M/0/1/0/all/0/1\">Mao V. Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q.S. Quek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collapsing the Decision Tree: the Concurrent Data Predictor. (arXiv:2108.03887v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03887","description":"<p>A family of concurrent data predictors is derived from the decision tree\nclassifier by removing the limitation of sequentially evaluating attributes. By\nevaluating attributes concurrently, the decision tree collapses into a flat\nstructure. Experiments indicate improvements of the prediction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alb_C/0/1/0/all/0/1\">Cristian Alb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Hyperparameter Optimization for Differentially Private Deep Learning. (arXiv:2108.03888v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03888","description":"<p>Tuning the hyperparameters in the differentially private stochastic gradient\ndescent (DPSGD) is a fundamental challenge. Unlike the typical SGD, private\ndatasets cannot be used many times for hyperparameter search in DPSGD; e.g.,\nvia a grid search. Therefore, there is an essential need for algorithms that,\nwithin a given search space, can find near-optimal hyperparameters for the best\nachievable privacy-utility tradeoffs efficiently. We formulate this problem\ninto a general optimization framework for establishing a desirable\nprivacy-utility tradeoff, and systematically study three cost-effective\nalgorithms for being used in the proposed framework: evolutionary, Bayesian,\nand reinforcement learning. Our experiments, for hyperparameter tuning in DPSGD\nconducted on MNIST and CIFAR-10 datasets, show that these three algorithms\nsignificantly outperform the widely used grid search baseline. As this paper\noffers a first-of-a-kind framework for hyperparameter tuning in DPSGD, we\ndiscuss existing challenges and open directions for future studies. As we\nbelieve our work has implications to be utilized in the pipeline of private\ndeep learning, we open-source our code at\nhttps://github.com/AmanPriyanshu/DP-HyperparamTuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malekzadeh_M/0/1/0/all/0/1\">Mohammad Malekzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probabilistic Active Learning for Active Class Selection. (arXiv:2108.03891v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03891","description":"<p>In machine learning, active class selection (ACS) algorithms aim to actively\nselect a class and ask the oracle to provide an instance for that class to\noptimize a classifier's performance while minimizing the number of requests. In\nthis paper, we propose a new algorithm (PAL-ACS) that transforms the ACS\nproblem into an active learning task by introducing pseudo instances. These are\nused to estimate the usefulness of an upcoming instance for each class using\nthe performance gain model from probabilistic active learning. Our experimental\nevaluation (on synthetic and real data) shows the advantages of our algorithm\ncompared to state-of-the-art algorithms. It effectively prefers the sampling of\ndifficult classes and thereby improves the classification performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kottke_D/0/1/0/all/0/1\">Daniel Kottke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krempl_G/0/1/0/all/0/1\">Georg Krempl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stecklina_M/0/1/0/all/0/1\">Marianne Stecklina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekowski_C/0/1/0/all/0/1\">Cornelius Styp von Rekowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabsch_T/0/1/0/all/0/1\">Tim Sabsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_T/0/1/0/all/0/1\">Tuan Pham Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deliano_M/0/1/0/all/0/1\">Matthias Deliano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiliopoulou_M/0/1/0/all/0/1\">Myra Spiliopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIFA: Fast Inference Approximation for Action Segmentation. (arXiv:2108.03894v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03894","description":"<p>We introduce FIFA, a fast approximate inference method for action\nsegmentation and alignment. Unlike previous approaches, FIFA does not rely on\nexpensive dynamic programming for inference. Instead, it uses an approximate\ndifferentiable energy function that can be minimized using gradient-descent.\nFIFA is a general approach that can replace exact inference improving its speed\nby more than 5 times while maintaining its performance. FIFA is an anytime\ninference algorithm that provides a better speed vs. accuracy trade-off\ncompared to exact inference. We apply FIFA on top of state-of-the-art\napproaches for weakly supervised action segmentation and alignment as well as\nfully supervised action segmentation. FIFA achieves state-of-the-art results on\nmost metrics on two action segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souri_Y/0/1/0/all/0/1\">Yaser Souri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farha_Y/0/1/0/all/0/1\">Yazan Abu Farha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Despinoy_F/0/1/0/all/0/1\">Fabien Despinoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francesca_G/0/1/0/all/0/1\">Gianpiero Francesca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1\">Juergen Gall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Regularity Measures for Sample-wise Learning and Generalization. (arXiv:2108.03913v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03913","description":"<p>Fundamental machine learning theory shows that different samples contribute\nunequally both in learning and testing processes. Contemporary studies on DNN\nimply that such sample di?erence is rooted on the distribution of intrinsic\npattern information, namely sample regularity. Motivated by the recent\ndiscovery on network memorization and generalization, we proposed a pair of\nsample regularity measures for both processes with a formulation-consistent\nrepresentation. Specifically, cumulative binary training/generalizing loss\n(CBTL/CBGL), the cumulative number of correct classi?cations of the\ntraining/testing sample within training stage, is proposed to quantize the\nstability in memorization-generalization process; while\nforgetting/mal-generalizing events, i.e., the mis-classification of previously\nlearned or generalized sample, are utilized to represent the uncertainty of\nsample regularity with respect to optimization dynamics. Experiments validated\nthe effectiveness and robustness of the proposed approaches for mini-batch SGD\noptimization. Further applications on training/testing sample selection show\nthe proposed measures sharing the uni?ed computing procedure could benefit for\nboth tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoning Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuanqi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuehu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatticeNet: Fast Spatio-Temporal Point Cloud Segmentation Using Permutohedral Lattices. (arXiv:2108.03917v1 [cs.CV])","link":"http://arxiv.org/abs/2108.03917","description":"<p>Deep convolutional neural networks (CNNs) have shown outstanding performance\nin the task of semantically segmenting images. Applying the same methods on 3D\ndata still poses challenges due to the heavy memory requirements and the lack\nof structured data. Here, we propose LatticeNet, a novel approach for 3D\nsemantic segmentation, which takes raw point clouds as input. A PointNet\ndescribes the local geometry which we embed into a sparse permutohedral\nlattice. The lattice allows for fast convolutions while keeping a low memory\nfootprint. Further, we introduce DeformSlice, a novel learned data-dependent\ninterpolation for projecting lattice features back onto the point cloud. We\npresent results of 3D segmentation on multiple datasets where our method\nachieves state-of-the-art performance. We also extend and evaluate our network\nfor instance and dynamic object segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutt_P/0/1/0/all/0/1\">Peer Sch&#xfc;tt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quenzel_J/0/1/0/all/0/1\">Jan Quenzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some thoughts on catastrophic forgetting and how to learn an algorithm. (arXiv:2108.03940v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03940","description":"<p>The work of McCloskey and Cohen popularized the concept of catastrophic\ninterference. They used a neural network that tried to learn addition using two\ngroups of examples as two different tasks. In their case, learning the second\ntask rapidly deteriorated the acquired knowledge about the previous one. This\ncould be a symptom of a fundamental problem: addition is an algorithmic task\nthat should not be learned through pattern recognition. We propose to use a\nneural network with a different architecture that can be trained to recover the\ncorrect algorithm for the addition of binary numbers. We test it in the setting\nproposed by McCloskey and Cohen and training on random examples one by one. The\nneural network not only does not suffer from catastrophic forgetting but it\nimproves its predictive power on unseen pairs of numbers as training\nprogresses. This work emphasizes the importance that neural network\narchitecture has for the emergence of catastrophic forgetting and introduces a\nneural network that is able to learn an algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_M/0/1/0/all/0/1\">Miguel Ruiz-Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Approach for Detecting Morphological Analogies. (arXiv:2108.03945v1 [cs.CL])","link":"http://arxiv.org/abs/2108.03945","description":"<p>Analogical proportions are statements of the form \"A is to B as C is to D\"\nthat are used for several reasoning and classification tasks in artificial\nintelligence and natural language processing (NLP). For instance, there are\nanalogy based approaches to semantics as well as to morphology. In fact,\nsymbolic approaches were developed to solve or to detect analogies between\ncharacter strings, e.g., the axiomatic approach as well as that based on\nKolmogorov complexity. In this paper, we propose a deep learning approach to\ndetect morphological analogies, for instance, with reinflexion or conjugation.\nWe present empirical results that show that our framework is competitive with\nthe above-mentioned state of the art symbolic approaches. We also explore\nempirically its transferability capacity across languages, which highlights\ninteresting similarities between them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alsaidi_S/0/1/0/all/0/1\">Safa Alsaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_A/0/1/0/all/0/1\">Amandine Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lay_P/0/1/0/all/0/1\">Puthineath Lay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquer_E/0/1/0/all/0/1\">Esteban Marquer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murena_P/0/1/0/all/0/1\">Pierre-Alexandre Murena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1\">Miguel Couceiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Hyperparameters in Stochastic Gradient Descent with Momentum. (arXiv:2108.03947v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03947","description":"<p>Following the same routine as [SSJ20], we continue to present the theoretical\nanalysis for stochastic gradient descent with momentum (SGD with momentum) in\nthis paper. Differently, for SGD with momentum, we demonstrate it is the two\nhyperparameters together, the learning rate and the momentum coefficient, that\nplay the significant role for the linear rate of convergence in non-convex\noptimization. Our analysis is based on the use of a hyperparameters-dependent\nstochastic differential equation (hp-dependent SDE) that serves as a continuous\nsurrogate for SGD with momentum. Similarly, we establish the linear convergence\nfor the continuous-time formulation of SGD with momentum and obtain an explicit\nexpression for the optimal linear rate by analyzing the spectrum of the\nKramers-Fokker-Planck operator. By comparison, we demonstrate how the optimal\nlinear rate of convergence and the final gap for SGD only about the learning\nrate varies with the momentum coefficient increasing from zero to one when the\nmomentum is introduced. Then, we propose a mathematical interpretation why the\nSGD with momentum converges faster and more robust about the learning rate than\nthe standard SGD in practice. Finally, we show the Nesterov momentum under the\nexistence of noise has no essential difference with the standard momentum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bin Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Safe Deep Reinforcement Learning for Multi-Agent Systems with Continuous Action Spaces. (arXiv:2108.03952v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03952","description":"<p>Multi-agent control problems constitute an interesting area of application\nfor deep reinforcement learning models with continuous action spaces. Such\nreal-world applications, however, typically come with critical safety\nconstraints that must not be violated. In order to ensure safety, we enhance\nthe well-known multi-agent deep deterministic policy gradient (MADDPG)\nframework by adding a safety layer to the deep policy network. %which\nautomatically corrects invalid actions. In particular, we extend the idea of\nlinearizing the single-step transition dynamics, as was done for single-agent\nsystems in Safe DDPG (Dalal et al., 2018), to multi-agent settings. We\nadditionally propose to circumvent infeasibility problems in the action\ncorrection step using soft constraints (Kerrigan &amp; Maciejowski, 2000). Results\nfrom the theory of exact penalty functions can be used to guarantee constraint\nsatisfaction of the soft constraints under mild assumptions. We empirically\nfind that the soft formulation achieves a dramatic decrease in constraint\nviolations, making safety available even during the learning procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheebaelhamd_Z/0/1/0/all/0/1\">Ziyad Sheebaelhamd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisis_K/0/1/0/all/0/1\">Konstantinos Zisis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nisioti_A/0/1/0/all/0/1\">Athina Nisioti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkouletsos_D/0/1/0/all/0/1\">Dimitris Gkouletsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavllo_D/0/1/0/all/0/1\">Dario Pavllo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1\">Jonas Kohler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Joint Unsupervised Learning of Cluster-Aware Embedding for Heterogeneous Networks. (arXiv:2108.03953v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03953","description":"<p>Heterogeneous Information Network (HIN) embedding refers to the\nlow-dimensional projections of the HIN nodes that preserve the HIN structure\nand semantics. HIN embedding has emerged as a promising research field for\nnetwork analysis as it enables downstream tasks such as clustering and node\nclassification. In this work, we propose \\ours for joint learning of cluster\nembeddings as well as cluster-aware HIN embedding. We assume that the connected\nnodes are highly likely to fall in the same cluster, and adopt a variational\napproach to preserve the information in the pairwise relations in a\ncluster-aware manner. In addition, we deploy contrastive modules to\nsimultaneously utilize the information in multiple meta-paths, thereby\nalleviating the meta-path selection problem - a challenge faced by many of the\nfamous HIN embedding approaches. The HIN embedding, thus learned, not only\nimproves the clustering performance but also preserves pairwise proximity as\nwell as the high-order HIN structure. We show the effectiveness of our approach\nby comparing it with many competitive baselines on three real-world datasets on\nclustering and downstream node classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1\">Rayyan Ahmad Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinsteuber_M/0/1/0/all/0/1\">Martin Kleinsteuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VeRLPy: Python Library for Verification of Digital Designs with Reinforcement Learning. (arXiv:2108.03978v1 [cs.AR])","link":"http://arxiv.org/abs/2108.03978","description":"<p>Digital hardware is verified by comparing its behavior against a reference\nmodel on a range of randomly generated input signals. The random generation of\nthe inputs hopes to achieve sufficient coverage of the different parts of the\ndesign. However, such coverage is often difficult to achieve, amounting to\nlarge verification efforts and delays. An alternative is to use Reinforcement\nLearning (RL) to generate the inputs by learning to prioritize those inputs\nwhich can more efficiently explore the design under test. In this work, we\npresent VeRLPy an open-source library to allow RL-driven verification with\nlimited additional engineering overhead. This contributes to two broad\nmovements within the EDA community of (a) moving to open-source toolchains and\n(b) reducing barriers for development with Python support. We also demonstrate\nthe use of VeRLPy for a few designs and establish its value over randomly\ngenerated input signals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shibu_A/0/1/0/all/0/1\">Aebel Joe Shibu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Sadhana S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Shilpa N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Majority Voting in Digital Hardware. (arXiv:2108.03979v1 [eess.SP])","link":"http://arxiv.org/abs/2108.03979","description":"<p>In recent years, machine learning methods became increasingly important for a\nmanifold number of applications. However, they often suffer from high\ncomputational requirements impairing their efficient use in real-time systems,\neven when employing dedicated hardware accelerators. Ensemble learning methods\nare especially suitable for hardware acceleration since they can be constructed\nfrom individual learners of low complexity and thus offer large parallelization\npotential. For classification, the outputs of these learners are typically\ncombined by majority voting, which often represents the bottleneck of a\nhardware accelerator for ensemble inference. In this work, we present a novel\narchitecture that allows obtaining a majority decision in a number of clock\ncycles that is logarithmic in the number of inputs. We show, that for the\nexample application of handwritten digit recognition a random forest processing\nengine employing this majority decision architecture implemented on an FPGA\nallows the classification of more than seven million images per second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baumgartner_S/0/1/0/all/0/1\">Stefan Baumgartner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huemer_M/0/1/0/all/0/1\">Mario Huemer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lunglmayr_M/0/1/0/all/0/1\">Michael Lunglmayr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decentralized Deep Learning for Mobile Edge Computing: A Survey on Communication Efficiency and Trustworthiness. (arXiv:2108.03980v1 [cs.DC])","link":"http://arxiv.org/abs/2108.03980","description":"<p>A wider coverage and a better solution to latency reduction in 5G\nnecessitates its combination with mobile edge computing (MEC) technology.\nDecentralized deep learning (DDL) as a promising solution to privacy-preserving\ndata processing for millions of edge smart devices, it leverages federated\nlearning within the networking of local models, without disclosing a client's\nraw data. Especially, in industries such as finance and healthcare where\nsensitive data of transactions and personal medical records is cautiously\nmaintained, DDL facilitates the collaboration among these institutes to improve\nthe performance of local models, while protecting data privacy of participating\nclients. In this survey paper, we demonstrate technical fundamentals of DDL for\nbenefiting many walks of society through decentralized learning. Furthermore,\nwe offer a comprehensive overview of recent challenges of DDL and the most\nrelevant solutions from novel perspectives of communication efficiency and\ntrustworthiness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1\">Hideya Ochiai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esaki_H/0/1/0/all/0/1\">Hiroshi Esaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Credibility-aware Swarm-Federated Deep Learning Framework in Internet of Vehicles. (arXiv:2108.03981v1 [eess.SY])","link":"http://arxiv.org/abs/2108.03981","description":"<p>Federated Deep Learning (FDL) is helping to realize distributed machine\nlearning in the Internet of Vehicles (IoV). However, FDL's global model needs\nmultiple clients to upload learning model parameters, thus still existing\nunavoidable communication overhead and data privacy risks. The recently\nproposed Swarm Learning (SL) provides a decentralized machine-learning approach\nuniting edge computing and blockchain-based coordination without the need for a\ncentral coordinator. This paper proposes a Swarm-Federated Deep Learning\nframework in the IoV system (IoV-SFDL) that integrates SL into the FDL\nframework. The IoV-SFDL organizes vehicles to generate local SL models with\nadjacent vehicles based on the blockchain empowered SL, then aggregates the\nglobal FDL model among different SL groups with a proposed credibility weights\nprediction algorithm. Extensive experimental results demonstrate that compared\nwith the baseline frameworks, the proposed IoV-SFDL framework achieves a 16.72%\nreduction in edge-to-global communication overhead while improving about 5.02%\nin model performance with the same training iterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xinhang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Mechanically Driven Full-Field Quantities of Interest with Deep Learning-Based Metamodels. (arXiv:2108.03995v1 [cs.LG])","link":"http://arxiv.org/abs/2108.03995","description":"<p>Using simulation to predict the mechanical behavior of heterogeneous\nmaterials has applications ranging from topology optimization to multi-scale\nstructural analysis. However, full-fidelity simulation techniques such as\nFinite Element Analysis can be prohibitively computationally expensive when\nthey are used to explore the massive input parameter space of heterogeneous\nmaterials. Therefore, there has been significant recent interest in machine\nlearning-based models that, once trained, can predict mechanical behavior at a\nfraction of the computational cost. Over the past several years, research in\nthis area has been focused mainly on predicting single Quantities of Interest\n(QoIs). However, there has recently been an increased interest in a more\nchallenging problem: predicting full-field QoI (e.g., displacement/strain\nfields, damage fields) for mechanical problems. Due to the added complexity of\nfull-field information, network architectures that perform well on single QoI\nproblems may perform poorly in the full-field QoI problem setting. The work\npresented in this paper is twofold. First, we made a significant extension to\nthe Mechanical MNIST dataset designed to enable the investigation of full field\nQoI prediction. Specifically, we added Finite Element simulation results of\nquasi-static brittle fracture in a heterogeneous material captured with the\nphase-field method. Second, we established strong baseline performance for\npredicting full-field QoI with MultiRes-WNet architecture. In addition to\npresenting the results in this paper, we have released our model implementation\nand the Mechanical MNIST Crack Path dataset under open-source licenses. We\nanticipate that future researchers will directly use our model architecture on\nrelated datasets and potentially design models that exceed the baseline\nperformance for predicting full-field QoI established in this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadzadeh_S/0/1/0/all/0/1\">S. Mohammadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lejeune_E/0/1/0/all/0/1\">E. Lejeune</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty quantification for industrial design using dictionaries of reduced order models. (arXiv:2108.04012v1 [stat.ML])","link":"http://arxiv.org/abs/2108.04012","description":"<p>We consider the dictionary-based ROM-net (Reduced Order Model) framework [T.\nDaniel, F. Casenave, N. Akkari, D. Ryckelynck, Model order reduction assisted\nby deep neural networks (ROM-net), Advanced modeling and Simulation in\nEngineering Sciences 7 (16), 2020] and summarize the underlying methodologies\nand their recent improvements. The main contribution of this work is the\napplication of the complete workflow to a real-life industrial model of an\nelastoviscoplastic high-pressure turbine blade subjected to thermal,\ncentrifugal and pressure loadings, for the quantification of the uncertainty on\ndual quantities (such as the accumulated plastic strain and the stress tensor),\ngenerated by the uncertainty on the temperature loading field. The\ndictionary-based ROM-net computes predictions of dual quantities of interest\nfor 1008 Monte Carlo draws of the temperature loading field in 2 hours and 48\nminutes, which corresponds to a speedup greater than 600 with respect to a\nreference parallel solver using domain decomposition, with a relative error in\nthe order of 2%. Another contribution of this work consists in the derivation\nof a meta-model to reconstruct the dual quantities of interest over the\ncomplete mesh from their values on the reduced integration points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Daniel_T/0/1/0/all/0/1\">Thomas Daniel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Casenave_F/0/1/0/all/0/1\">Fabien Casenave</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Akkari_N/0/1/0/all/0/1\">Nissrine Akkari</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ryckelynck_D/0/1/0/all/0/1\">David Ryckelynck</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rey_C/0/1/0/all/0/1\">Christian Rey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DGEM: A New Dual-modal Graph Embedding Method in Recommendation System. (arXiv:2108.04031v1 [cs.IR])","link":"http://arxiv.org/abs/2108.04031","description":"<p>In the current deep learning based recommendation system, the embedding\nmethod is generally employed to complete the conversion from the\nhigh-dimensional sparse feature vector to the low-dimensional dense feature\nvector. However, as the dimension of the input vector of the embedding layer is\ntoo large, the addition of the embedding layer significantly slows down the\nconvergence speed of the entire neural network, which is not acceptable in\nreal-world scenarios. In addition, as the interaction between users and items\nincreases and the relationship between items becomes more complicated, the\nembedding method proposed for sequence data is no longer suitable for graphic\ndata in the current real environment. Therefore, in this paper, we propose the\nDual-modal Graph Embedding Method (DGEM) to solve these problems. DGEM includes\ntwo modes, static and dynamic. We first construct the item graph to extract the\ngraph structure and use random walk of unequal probability to capture the\nhigh-order proximity between the items. Then we generate the graph embedding\nvector through the Skip-Gram model, and finally feed the downstream deep neural\nnetwork for the recommendation task. The experimental results show that DGEM\ncan mine the high-order proximity between items and enhance the expression\nability of the recommendation model. Meanwhile it also improves the\nrecommendation performance by utilizing the time dependent relationship between\nitems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huimin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rongwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhuyun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reproducible Performance Optimization of Complex Applications on the Edge-to-Cloud Continuum. (arXiv:2108.04033v1 [cs.DC])","link":"http://arxiv.org/abs/2108.04033","description":"<p>In more and more application areas, we are witnessing the emergence of\ncomplex workflows that combine computing, analytics and learning. They often\nrequire a hybrid execution infrastructure with IoT devices interconnected to\ncloud/HPC systems (aka Computing Continuum). Such workflows are subject to\ncomplex constraints and requirements in terms of performance, resource usage,\nenergy consumption and financial costs. This makes it challenging to optimize\ntheir configuration and deployment. We propose a methodology to support the\noptimization of real-life applications on the Edge-to-Cloud Continuum. We\nimplement it as an extension of E2Clab, a previously proposed framework\nsupporting the complete experimental cycle across the Edge-to-Cloud Continuum.\nOur approach relies on a rigorous analysis of possible configurations in a\ncontrolled testbed environment to understand their behaviour and related\nperformance trade-offs. We illustrate our methodology by optimizing Pl@ntNet, a\nworld-wide plant identification application. Our methodology can be generalized\nto other applications in the Edge-to-Cloud Continuum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosendo_D/0/1/0/all/0/1\">Daniel Rosendo</a> (KerData), <a href=\"http://arxiv.org/find/cs/1/au:+Costan_A/0/1/0/all/0/1\">Alexandru Costan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antoniu_G/0/1/0/all/0/1\">Gabriel Antoniu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonin_M/0/1/0/all/0/1\">Matthieu Simonin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardo_J/0/1/0/all/0/1\">Jean-Christophe Lombardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joly_A/0/1/0/all/0/1\">Alexis Joly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valduriez_P/0/1/0/all/0/1\">Patrick Valduriez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture of Linear Models Co-supervised by Deep Neural Networks. (arXiv:2108.04035v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04035","description":"<p>Deep neural network (DNN) models have achieved phenomenal success for\napplications in many domains, ranging from academic research in science and\nengineering to industry and business. The modeling power of DNN is believed to\nhave come from the complexity and over-parameterization of the model, which on\nthe other hand has been criticized for the lack of interpretation. Although\ncertainly not true for every application, in some applications, especially in\neconomics, social science, healthcare industry, and administrative decision\nmaking, scientists or practitioners are resistant to use predictions made by a\nblack-box system for multiple reasons. One reason is that a major purpose of a\nstudy can be to make discoveries based upon the prediction function, e.g., to\nreveal the relationships between measurements. Another reason can be that the\ntraining dataset is not large enough to make researchers feel completely sure\nabout a purely data-driven result. Being able to examine and interpret the\nprediction function will enable researchers to connect the result with existing\nknowledge or gain insights about new directions to explore. Although classic\nstatistical models are much more explainable, their accuracy often falls\nconsiderably below DNN. In this paper, we propose an approach to fill the gap\nbetween relatively simple explainable models and DNN such that we can more\nflexibly tune the trade-off between interpretability and accuracy. Our main\nidea is a mixture of discriminative models that is trained with the guidance\nfrom a DNN. Although mixtures of discriminative models have been studied\nbefore, our way of generating the mixture is quite different.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_B/0/1/0/all/0/1\">Beomseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fed-BEV: A Federated Learning Framework for Modelling Energy Consumption of Battery Electric Vehicles. (arXiv:2108.04036v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04036","description":"<p>Recently, there has been an increasing interest in the roll-out of electric\nvehicles (EVs) in the global automotive market. Compared to conventional\ninternal combustion engine vehicles (ICEVs), EVs can not only help users reduce\nmonetary costs in their daily commuting, but also can effectively help mitigate\nthe increasing level of traffic emissions produced in cities. Among many\nothers, battery electric vehicles (BEVs) exclusively use chemical energy stored\nin their battery packs for propulsion. Hence, it becomes important to\nunderstand how much energy can be consumed by such vehicles in various traffic\nscenarios towards effective energy management. To address this challenge, we\npropose a novel framework in this paper by leveraging the federated learning\napproaches for modelling energy consumption for BEVs (Fed-BEV). More\nspecifically, a group of BEVs involved in the Fed-BEV framework can learn from\neach other to jointly enhance their energy consumption model. We present the\ndesign of the proposed system architecture and implementation details in a\nco-simulation environment. Finally, comparative studies and simulation results\nare discussed to illustrate the efficacy of our proposed framework for accurate\nenergy modelling of BEVs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Streamwise GAN Vocoder for Wideband Speech Coding at Very Low Bit Rate. (arXiv:2108.04051v1 [eess.AS])","link":"http://arxiv.org/abs/2108.04051","description":"<p>Recently, GAN vocoders have seen rapid progress in speech synthesis, starting\nto outperform autoregressive models in perceptual quality with much higher\ngeneration speed. However, autoregressive vocoders are still the common choice\nfor neural generation of speech signals coded at very low bit rates. In this\npaper, we present a GAN vocoder which is able to generate wideband speech\nwaveforms from parameters coded at 1.6 kbit/s. The proposed model is a modified\nversion of the StyleMelGAN vocoder that can run in frame-by-frame manner,\nmaking it suitable for streaming applications. The experimental results show\nthat the proposed model significantly outperforms prior autoregressive vocoders\nlike LPCNet for very low bit rate speech coding, with computational complexity\nof about 5 GMACs, providing a new state of the art in this domain. Moreover,\nthis streamwise adversarial vocoder delivers quality competitive to advanced\nspeech codecs such as EVS at 5.9 kbit/s on clean speech, which motivates\nfurther usage of feed-forward fully-convolutional models for low bit rate\nspeech coding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mustafa_A/0/1/0/all/0/1\">Ahmed Mustafa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buthe_J/0/1/0/all/0/1\">Jan B&#xfc;the</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korse_S/0/1/0/all/0/1\">Srikanth Korse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_K/0/1/0/all/0/1\">Kishan Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuchs_G/0/1/0/all/0/1\">Guillaume Fuchs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pia_N/0/1/0/all/0/1\">Nicola Pia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training of deep residual networks with stochastic MG/OPT. (arXiv:2108.04052v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04052","description":"<p>We train deep residual networks with a stochastic variant of the nonlinear\nmultigrid method MG/OPT. To build the multilevel hierarchy, we use the\ndynamical systems viewpoint specific to residual networks. We report\nsignificant speed-ups and additional robustness for training MNIST on deep\nresidual networks. Our numerical experiments also indicate that multilevel\ntraining can be used as a pruning technique, as many of the auxiliary networks\nhave accuracies comparable to the original network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Planta_C/0/1/0/all/0/1\">Cyrill von Planta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopanicakova_A/0/1/0/all/0/1\">Alena Kopanicakova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_R/0/1/0/all/0/1\">Rolf Krause</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Global Labels in Few-Shot Classification and How to Infer Them. (arXiv:2108.04055v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04055","description":"<p>Few-shot learning (FSL) is a central problem in meta-learning, where learners\nmust quickly adapt to new tasks given limited training data. Surprisingly,\nrecent works have outperformed meta-learning methods tailored to FSL by casting\nit as standard supervised learning to jointly classify all classes shared\nacross tasks. However, this approach violates the standard FSL setting by\nrequiring global labels shared across tasks, which are often unavailable in\npractice. In this paper, we show why solving FSL via standard classification is\ntheoretically advantageous. This motivates us to propose Meta Label Learning\n(MeLa), a novel algorithm that infers global labels and obtains robust few-shot\nmodels via standard classification. Empirically, we demonstrate that MeLa\noutperforms meta-learning competitors and is comparable to the oracle setting\nwhere ground truth labels are given. We provide extensive ablation studies to\nhighlight the key properties of the proposed strategy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1\">Massimiliano Pontil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciliberto_C/0/1/0/all/0/1\">Carlo Ciliberto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Interpretable Probabilistic Model for Short-Term Solar Power Forecasting Using Natural Gradient Boosting. (arXiv:2108.04058v1 [stat.AP])","link":"http://arxiv.org/abs/2108.04058","description":"<p>The stochastic nature of photovoltaic (PV) power has led both academia and\nindustry to a large amount of research work aiming at the development of\naccurate PV power forecasting models. However, most of those models are based\non machine learning algorithms and are considered as black boxes which do not\nprovide any insight or explanation about their predictions. Therefore, their\ndirect implementation in environments, where transparency is required, and the\ntrust associated with their predictions may be questioned. To this end, we\npropose a two stage probabilistic forecasting framework able to generate highly\naccurate, reliable, and sharp forecasts yet offering full transparency on both\nthe point forecasts and the prediction intervals (PIs). In the first stage, we\nexploit natural gradient boosting (NGBoost) for yielding probabilistic\nforecasts while in the second stage, we calculate the Shapley additive\nexplanation (SHAP) values in order to fully understand why a prediction was\nmade. To highlight the performance and the applicability of the proposed\nframework, real data from two PV parks located in Southern Germany are\nemployed. Initially, the natural gradient boosting is thoroughly compared with\ntwo state-of-the-art algorithms, namely Gaussian process and lower upper bound\nestimation, in a wide range of forecasting metrics. Secondly, a detailed\nanalysis of the model's complex nonlinear relationships and interaction effects\nbetween the various features is presented. The latter allows us to interpret\nthe model, identify some learned physical properties, explain individual\npredictions, reduce the computational requirements for the training without\njeopardizing the model accuracy, detect possible bugs, and gain trust in the\nmodel. Finally, we conclude that the model was able to develop nonlinear\nrelationships following human logic and intuition based on learned physical\nproperties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Mitrentsis_G/0/1/0/all/0/1\">Georgios Mitrentsis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lens_H/0/1/0/all/0/1\">Hendrik Lens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Aware Partitioning of Machine Learning Applications for Optimal Energy Use in Batteryless Systems. (arXiv:2108.04059v1 [cs.DC])","link":"http://arxiv.org/abs/2108.04059","description":"<p>Sensing systems powered by energy harvesting have traditionally been designed\nto tolerate long periods without energy. As the Internet of Things (IoT)\nevolves towards a more transient and opportunistic execution paradigm, reducing\nenergy storage costs will be key for its economic and ecologic viability.\nHowever, decreasing energy storage in harvesting systems introduces reliability\nissues. Transducers only produce intermittent energy at low voltage and current\nlevels, making guaranteed task completion a challenge. Existing ad hoc methods\novercome this by buffering enough energy either for single tasks, incurring\nlarge data-retention overheads, or for one full application cycle, requiring a\nlarge energy buffer. We present Julienning: an automated method for optimizing\nthe total energy cost of batteryless applications. Using a custom specification\nmodel, developers can describe transient applications as a set of atomically\nexecuted kernels with explicit data dependencies. Our optimization flow can\npartition data- and energy-intensive applications into multiple execution\ncycles with bounded energy consumption. By leveraging interkernel data\ndependencies, these energy-bounded execution cycles minimize the number of\nsystem activations and nonvolatile data transfers, and thus the total energy\noverhead. We validate our methodology with two batteryless cameras running\nenergy-intensive machine learning applications. Results demonstrate that\ncompared to ad hoc solutions, our method can reduce the required energy storage\nby over 94% while only incurring a 0.12% energy overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1\">Andres Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tretter_A/0/1/0/all/0/1\">Andreas Tretter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hager_P/0/1/0/all/0/1\">Pascal Alexander Hager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanmugarajah_P/0/1/0/all/0/1\">Praveenth Sanmugarajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1\">Luca Benini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1\">Lothar Thiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Householder Activations for Provable Robustness against Adversarial Attacks. (arXiv:2108.04062v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04062","description":"<p>Training convolutional neural networks (CNNs) with a strict Lipschitz\nconstraint under the l_{2} norm is useful for provable adversarial robustness,\ninterpretable gradients and stable training. While 1-Lipschitz CNNs can be\ndesigned by enforcing a 1-Lipschitz constraint on each layer, training such\nnetworks requires each layer to have an orthogonal Jacobian matrix (for all\ninputs) to prevent gradients from vanishing during backpropagation. A layer\nwith this property is said to be Gradient Norm Preserving (GNP). To construct\nexpressive GNP activation functions, we first prove that the Jacobian of any\nGNP piecewise linear function is only allowed to change via Householder\ntransformations for the function to be continuous. Building on this result, we\nintroduce a class of nonlinear GNP activations with learnable Householder\ntransformations called Householder activations. A householder activation\nparameterized by the vector $\\mathbf{v}$ outputs $(\\mathbf{I} -\n2\\mathbf{v}\\mathbf{v}^{T})\\mathbf{z}$ for its input $\\mathbf{z}$ if\n$\\mathbf{v}^{T}\\mathbf{z} \\leq 0$; otherwise it outputs $\\mathbf{z}$. Existing\nGNP activations such as $\\mathrm{MaxMin}$ can be viewed as special cases of\n$\\mathrm{HH}$ activations for certain settings of these transformations. Thus,\nnetworks with $\\mathrm{HH}$ activations have higher expressive power than those\nwith $\\mathrm{MaxMin}$ activations. Although networks with $\\mathrm{HH}$\nactivations have nontrivial provable robustness against adversarial attacks, we\nfurther boost their robustness by (i) introducing a certificate regularization\nand (ii) relaxing orthogonalization of the last layer of the network. Our\nexperiments on CIFAR-10 and CIFAR-100 show that our regularized networks with\n$\\mathrm{HH}$ activations lead to significant improvements in both the standard\nand provable robust accuracy over the prior works (gain of 3.65\\% and 4.46\\% on\nCIFAR-100 respectively).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Surbhi Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-learning: Learning from Noisy Labels with Self-supervision. (arXiv:2108.04063v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04063","description":"<p>Noisy labels, resulting from mistakes in manual labeling or webly data\ncollecting for supervised learning, can cause neural networks to overfit the\nmisleading information and degrade the generalization performance.\nSelf-supervised learning works in the absence of labels and thus eliminates the\nnegative impact of noisy labels. Motivated by co-training with both supervised\nlearning view and self-supervised learning view, we propose a simple yet\neffective method called Co-learning for learning with noisy labels. Co-learning\nperforms supervised learning and self-supervised learning in a cooperative way.\nThe constraints of intrinsic similarity with the self-supervised module and the\nstructural similarity with the noisily-supervised module are imposed on a\nshared common feature encoder to regularize the network to maximize the\nagreement between the two constraints. Co-learning is compared with peer\nmethods on corrupted data from benchmark datasets fairly, and extensive results\nare provided which demonstrate that Co-learning is superior to many\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-free inference of unseen attractors: Reconstructing phase space features from a single noisy trajectory using reservoir computing. (arXiv:2108.04074v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04074","description":"<p>Reservoir computers are powerful tools for chaotic time series prediction.\nThey can be trained to approximate phase space flows and can thus both predict\nfuture values to a high accuracy, as well as reconstruct the general properties\nof a chaotic attractor without requiring a model. In this work, we show that\nthe ability to learn the dynamics of a complex system can be extended to\nsystems with co-existing attractors, here a 4-dimensional extension of the\nwell-known Lorenz chaotic system. We demonstrate that a reservoir computer can\ninfer entirely unexplored parts of the phase space: a properly trained\nreservoir computer can predict the existence of attractors that were never\napproached during training and therefore are labelled as unseen. We provide\nexamples where attractor inference is achieved after training solely on a\nsingle noisy trajectory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1\">Andr&#xe9; R&#xf6;hm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gauthier_D/0/1/0/all/0/1\">Daniel J. Gauthier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1\">Ingo Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Uncertainty for Improved Static Malware Detection Under Extreme False Positive Constraints. (arXiv:2108.04081v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04081","description":"<p>The detection of malware is a critical task for the protection of computing\nenvironments. This task often requires extremely low false positive rates (FPR)\nof 0.01% or even lower, for which modern machine learning has no readily\navailable tools. We introduce the first broad investigation of the use of\nuncertainty for malware detection across multiple datasets, models, and feature\ntypes. We show how ensembling and Bayesian treatments of machine learning\nmethods for static malware detection allow for improved identification of model\nerrors, uncovering of new malware families, and predictive performance under\nextreme false positive constraints. In particular, we improve the true positive\nrate (TPR) at an actual realized FPR of 1e-5 from an expected 0.69 for previous\nmethods to 0.80 on the best performing model class on the Sophos industry scale\ndataset. We additionally demonstrate how previous works have used an evaluation\nprotocol that can lead to misleading results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Andre T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicholas_C/0/1/0/all/0/1\">Charles Nicholas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holt_J/0/1/0/all/0/1\">James Holt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Deep Learning for Partial Differential Equation Parameter Discovery with Sparse and Noisy Data. (arXiv:2108.04085v1 [math.NA])","link":"http://arxiv.org/abs/2108.04085","description":"<p>Scientific machine learning has been successfully applied to inverse problems\nand PDE discoveries in computational physics. One caveat of current methods\nhowever is the need for large amounts of (clean) data in order to recover full\nsystem responses or underlying physical models. Bayesian methods may be\nparticularly promising to overcome these challenges as they are naturally less\nsensitive to sparse and noisy data. In this paper, we propose to use Bayesian\nneural networks (BNN) in order to: 1) Recover the full system states from\nmeasurement data (e.g. temperature, velocity field, etc.). We use Hamiltonian\nMonte-Carlo to sample the posterior distribution of a deep and dense BNN, and\nshow that it is possible to accurately capture physics of varying complexity\nwithout overfitting. 2) Recover the parameters in the underlying partial\ndifferential equation (PDE) governing the physical system. Using the trained\nBNN as a surrogate of the system response, we generate datasets of derivatives\npotentially comprising the latent PDE of the observed system and perform a\nBayesian linear regression (BLR) between the successive derivatives in space\nand time to recover the original PDE parameters. We take advantage of the\nconfidence intervals on the BNN outputs and introduce the spatial derivative\nvariance into the BLR likelihood to discard the influence of highly uncertain\nsurrogate data points, which allows for more accurate parameter discovery. We\ndemonstrate our approach on a handful of example applied to physics and\nnon-linear dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Bonneville_C/0/1/0/all/0/1\">Christophe Bonneville</a>, <a href=\"http://arxiv.org/find/math/1/au:+Earls_C/0/1/0/all/0/1\">Christopher J. Earls</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning for Intelligent Healthcare Systems: A Comprehensive Survey. (arXiv:2108.04087v1 [cs.LG])","link":"http://arxiv.org/abs/2108.04087","description":"<p>The rapid increase in the percentage of chronic disease patients along with\nthe recent pandemic pose immediate threats on healthcare expenditure and\nelevate causes of death. This calls for transforming healthcare systems away\nfrom one-on-one patient treatment into intelligent health systems, to improve\nservices, access and scalability, while reducing costs. Reinforcement Learning\n(RL) has witnessed an intrinsic breakthrough in solving a variety of complex\nproblems for diverse applications and services. Thus, we conduct in this paper\na comprehensive survey of the recent models and techniques of RL that have been\ndeveloped/used for supporting Intelligent-healthcare (I-health) systems. This\npaper can guide the readers to deeply understand the state-of-the-art regarding\nthe use of RL in the context of I-health. Specifically, we first present an\noverview for the I-health systems challenges, architecture, and how RL can\nbenefit these systems. We then review the background and mathematical modeling\nof different RL, Deep RL (DRL), and multi-agent RL models. After that, we\nprovide a deep literature review for the applications of RL in I-health\nsystems. In particular, three main areas have been tackled, i.e., edge\nintelligence, smart core network, and dynamic treatment regimes. Finally, we\nhighlight emerging challenges and outline future research directions in driving\nthe future success of RL in I-health systems, which opens the door for\nexploring some interesting and unsolved problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdellatif_A/0/1/0/all/0/1\">Alaa Awad Abdellatif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mhaisen_N/0/1/0/all/0/1\">Naram Mhaisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chkirbene_Z/0/1/0/all/0/1\">Zina Chkirbene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Amr Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1\">Aiman Erbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1\">Mohsen Guizani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum principal component analysis only achieves an exponential speedup because of its state preparation assumptions. (arXiv:1811.00414v3 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/1811.00414","description":"<p>A central roadblock to analyzing quantum algorithms on quantum states is the\nlack of a comparable input model for classical algorithms. Inspired by recent\nwork of the author [E. Tang, STOC'19], we introduce such a model, where we\nassume we can efficiently perform $\\ell^2$-norm samples of input data, a\nnatural analogue to quantum algorithms that assume efficient state preparation\nof classical data. Though this model produces less practical algorithms than\nthe (stronger) standard model of classical computation, it captures versions of\nmany of the features and nuances of quantum linear algebra algorithms. With\nthis model, we describe classical analogues to Lloyd, Mohseni, and Rebentrost's\nquantum algorithms for principal component analysis [Nat. Phys. 10, 631 (2014)]\nand nearest-centroid clustering [<a href=\"/abs/1307.0411\">arXiv:1307.0411</a>]. Since they are only\npolynomially slower, these algorithms suggest that the exponential speedups of\ntheir quantum counterparts are simply an artifact of state preparation\nassumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_E/0/1/0/all/0/1\">Ewin Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing thermodynamic trajectories using evolutionary and gradient-based reinforcement learning. (arXiv:1903.08543v4 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/1903.08543","description":"<p>Using a model heat engine, we show that neural network-based reinforcement\nlearning can identify thermodynamic trajectories of maximal efficiency. We\nconsider both gradient and gradient-free reinforcement learning. We use an\nevolutionary learning algorithm to evolve a population of neural networks,\nsubject to a directive to maximize the efficiency of a trajectory composed of a\nset of elementary thermodynamic processes; the resulting networks learn to\ncarry out the maximally-efficient Carnot, Stirling, or Otto cycles. When given\nan additional irreversible process, this evolutionary scheme learns a\npreviously unknown thermodynamic cycle. Gradient-based reinforcement learning\nis able to learn the Stirling cycle, whereas an evolutionary approach achieves\nthe optimal Carnot cycle. Our results show how the reinforcement learning\nstrategies developed for game playing can be applied to solve physical problems\nconditioned upon path-extensive order parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beeler_C/0/1/0/all/0/1\">Chris Beeler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahorau_U/0/1/0/all/0/1\">Uladzimir Yahorau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coles_R/0/1/0/all/0/1\">Rory Coles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_K/0/1/0/all/0/1\">Kyle Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whitelam_S/0/1/0/all/0/1\">Stephen Whitelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamblyn_I/0/1/0/all/0/1\">Isaac Tamblyn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving high-dimensional optimal stopping problems using deep learning. (arXiv:1908.01602v3 [cs.CE] UPDATED)","link":"http://arxiv.org/abs/1908.01602","description":"<p>Nowadays many financial derivatives, such as American or Bermudan options,\nare of early exercise type. Often the pricing of early exercise options gives\nrise to high-dimensional optimal stopping problems, since the dimension\ncorresponds to the number of underlying assets. High-dimensional optimal\nstopping problems are, however, notoriously difficult to solve due to the\nwell-known curse of dimensionality. In this work, we propose an algorithm for\nsolving such problems, which is based on deep learning and computes, in the\ncontext of early exercise option pricing, both approximations of an optimal\nexercise strategy and the price of the considered option. The proposed\nalgorithm can also be applied to optimal stopping problems that arise in other\nareas where the underlying stochastic process can be efficiently simulated. We\npresent numerical results for a large number of example problems, which include\nthe pricing of many high-dimensional American and Bermudan options, such as\nBermudan max-call options in up to 5000 dimensions. Most of the obtained\nresults are compared to reference values computed by exploiting the specific\nproblem design or, where available, to reference values from the literature.\nThese numerical results suggest that the proposed algorithm is highly effective\nin the case of many underlyings, in terms of both accuracy and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Sebastian Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheridito_P/0/1/0/all/0/1\">Patrick Cheridito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jentzen_A/0/1/0/all/0/1\">Arnulf Jentzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welti_T/0/1/0/all/0/1\">Timo Welti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Oblique Random Forests: Towards Closing the Gap on Convolutional Deep Networks. (arXiv:1909.11799v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1909.11799","description":"<p>Decision forests (Forests), in particular random forests and gradient\nboosting trees, have demonstrated state-of-the-art accuracy compared to other\nmethods in many supervised learning scenarios. In particular, Forests dominate\nother methods in tabular data, that is, when the feature space is unstructured,\nso that the signal is invariant to a permutation of the feature indices.\nHowever, in structured data lying on a manifold (such as images, text, and\nspeech) deep networks (Networks), specifically convolutional deep networks\n(ConvNets), tend to outperform Forests. We conjecture that at least part of the\nreason for this is that the input to Networks is not simply the feature\nmagnitudes, but also their indices. In contrast, naive Forest implementations\nfail to explicitly consider feature indices. A recently proposed Forest\napproach demonstrates that Forests, for each node, implicitly sample a random\nmatrix from some specific distribution. These Forests, like some classes of\nNetworks, learn by partitioning the feature space into convex polytopes\ncorresponding to linear functions. We build on that approach and show that one\ncan choose distributions in a manifold-aware fashion to incorporate feature\nlocality. We demonstrate the empirical performance on data whose features live\non three different manifolds: a torus, images, and time-series. Moreover, we\ndemonstrate its strength in multivariate simulated settings and also show\nsuperiority in predicting surgical outcome in epilepsy patients and predicting\nmovement direction from raw stereotactic EEG data from non-motor brain regions.\nIn all simulations and real data, Manifold Oblique Random Forest (MORF)\nalgorithm outperforms approaches that ignore feature space structure and\nchallenges the performance of ConvNets. Moreover, MORF runs fast and maintains\ninterpretability and theoretical justification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perry_R/0/1/0/all/0/1\">Ronan Perry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Adam Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_C/0/1/0/all/0/1\">Chester Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomita_T/0/1/0/all/0/1\">Tyler M. Tomita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1\">Ronak Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arroyo_J/0/1/0/all/0/1\">Jesus Arroyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patsolic_J/0/1/0/all/0/1\">Jesse Patsolic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falk_B/0/1/0/all/0/1\">Benjamin Falk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Transformer: A Framework for Modeling Complex-Valued Sequence. (arXiv:1910.10202v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1910.10202","description":"<p>While deep learning has received a surge of interest in a variety of fields\nin recent years, major deep learning models barely use complex numbers.\nHowever, speech, signal and audio data are naturally complex-valued after\nFourier Transform, and studies have shown a potentially richer representation\nof complex nets. In this paper, we propose a Complex Transformer, which\nincorporates the transformer model as a backbone for sequence modeling; we also\ndevelop attention and encoder-decoder network operating for complex input. The\nmodel achieves state-of-the-art performance on the MusicNet dataset and an\nIn-phase Quadrature (IQ) signal dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Muqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Martin Q. Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yao-Hung Hubert Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weak Supervision Approach to Detecting Visual Anomalies for Automated Testing of Graphics Units. (arXiv:1912.04138v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1912.04138","description":"<p>We present a deep learning system for testing graphics units by detecting\nnovel visual corruptions in videos. Unlike previous work in which manual\ntagging was required to collect labeled training data, our weak supervision\nmethod is fully automatic and needs no human labelling. This is achieved by\nreproducing driver bugs that increase the probability of generating\ncorruptions, and by making use of ideas and methods from the Multiple Instance\nLearning (MIL) setting. In our experiments, we significantly outperform\nunsupervised methods such as GAN-based models and discover novel corruptions\nundetected by baselines, while adhering to strict requirements on accuracy and\nefficiency of our real-time system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szeskin_A/0/1/0/all/0/1\">Adi Szeskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faivishevsky_L/0/1/0/all/0/1\">Lev Faivishevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muppalla_A/0/1/0/all/0/1\">Ashwin K Muppalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armon_A/0/1/0/all/0/1\">Amitai Armon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maxmin Q-learning: Controlling the Estimation Bias of Q-learning. (arXiv:2002.06487v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2002.06487","description":"<p>Q-learning suffers from overestimation bias, because it approximates the\nmaximum action value using the maximum estimated action value. Algorithms have\nbeen proposed to reduce overestimation bias, but we lack an understanding of\nhow bias interacts with performance, and the extent to which existing\nalgorithms mitigate bias. In this paper, we 1) highlight that the effect of\noverestimation bias on learning efficiency is environment-dependent; 2) propose\na generalization of Q-learning, called \\emph{Maxmin Q-learning}, which provides\na parameter to flexibly control bias; 3) show theoretically that there exists a\nparameter choice for Maxmin Q-learning that leads to unbiased estimation with a\nlower approximation variance than Q-learning; and 4) prove the convergence of\nour algorithm in the tabular case, as well as convergence of several previous\nQ-learning variants, using a novel Generalized Q-learning framework. We\nempirically verify that our algorithm better controls estimation bias in toy\nenvironments, and that it achieves superior performance on several benchmark\nproblems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Q/0/1/0/all/0/1\">Qingfeng Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yangchen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fyshe_A/0/1/0/all/0/1\">Alona Fyshe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1\">Martha White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey of statistical learning techniques as applied to inexpensive pediatric Obstructive Sleep Apnea data. (arXiv:2002.07873v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2002.07873","description":"<p>Pediatric obstructive sleep apnea affects an estimated 1-5% of\nelementary-school aged children and can lead to other detrimental health\nproblems. Swift diagnosis and treatment are critical to a child's growth and\ndevelopment, but the variability of symptoms and the complexity of the\navailable data make this a challenge. We take a first step in streamlining the\nprocess by focusing on inexpensive data from questionnaires and craniofacial\nmeasurements. We apply correlation networks, the Mapper algorithm from\ntopological data analysis, and singular value decomposition in a process of\nexploratory data analysis. We then apply a variety of supervised and\nunsupervised learning techniques from statistics, machine learning, and\ntopology, ranging from support vector machines to Bayesian classifiers and\nmanifold learning. Finally, we analyze the results of each of these methods and\ndiscuss the implications for a multi-data-sourced algorithm moving forward.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Winn_E/0/1/0/all/0/1\">Emily T. Winn</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vazquez_M/0/1/0/all/0/1\">Marilyn Vazquez</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Loliencar_P/0/1/0/all/0/1\">Prachi Loliencar</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Taipale_K/0/1/0/all/0/1\">Kaisa Taipale</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Heo_G/0/1/0/all/0/1\">Giseon Heo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonconvex sparse regularization for deep neural networks and its optimality. (arXiv:2003.11769v2 [math.ST] UPDATED)","link":"http://arxiv.org/abs/2003.11769","description":"<p>Recent theoretical studies proved that deep neural network (DNN) estimators\nobtained by minimizing empirical risk with a certain sparsity constraint can\nattain optimal convergence rates for regression and classification problems.\nHowever, the sparsity constraint requires to know certain properties of the\ntrue model, which are not available in practice. Moreover, computation is\ndifficult due to the discrete nature of the sparsity constraint. In this paper,\nwe propose a novel penalized estimation method for sparse DNNs, which resolves\nthe aforementioned problems existing in the sparsity constraint. We establish\nan oracle inequality for the excess risk of the proposed sparse-penalized DNN\nestimator and derive convergence rates for several learning tasks. In\nparticular, we prove that the sparse-penalized estimator can adaptively attain\nminimax convergence rates for various nonparametric regression problems. For\ncomputation, we develop an efficient gradient-based optimization algorithm that\nguarantees the monotonic reduction of the objective function.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Ohn_I/0/1/0/all/0/1\">Ilsang Ohn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kim_Y/0/1/0/all/0/1\">Yongdai Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Convergence Rate of Projected Gradient Descent for a Back-Projection based Objective. (arXiv:2005.00959v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2005.00959","description":"<p>Ill-posed linear inverse problems appear in many scientific setups, and are\ntypically addressed by solving optimization problems, which are composed of\ndata fidelity and prior terms. Recently, several works have considered a\nback-projection (BP) based fidelity term as an alternative to the common least\nsquares (LS), and demonstrated excellent results for popular inverse problems.\nThese works have also empirically shown that using the BP term, rather than the\nLS term, requires fewer iterations of optimization algorithms. In this paper,\nwe examine the convergence rate of the projected gradient descent (PGD)\nalgorithm for the BP objective. Our analysis allows to identify an inherent\nsource for its faster convergence compared to using the LS objective, while\nmaking only mild assumptions. We also analyze the more general proximal\ngradient method under a relaxed contraction condition on the proximal mapping\nof the prior. This analysis further highlights the advantage of BP when the\nlinear measurement operator is badly conditioned. Numerical experiments with\nboth $\\ell_1$-norm and GAN-based priors corroborate our theoretical results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Tirer_T/0/1/0/all/0/1\">Tom Tirer</a>, <a href=\"http://arxiv.org/find/math/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and Variance Reduction. (arXiv:2006.03041v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2006.03041","description":"<p>Asynchronous Q-learning aims to learn the optimal action-value function (or\nQ-function) of a Markov decision process (MDP), based on a single trajectory of\nMarkovian samples induced by a behavior policy. Focusing on a\n$\\gamma$-discounted MDP with state space $\\mathcal{S}$ and action space\n$\\mathcal{A}$, we demonstrate that the $\\ell_{\\infty}$-based sample complexity\nof classical asynchronous Q-learning --- namely, the number of samples needed\nto yield an entrywise $\\varepsilon$-accurate estimate of the Q-function --- is\nat most on the order of $\\frac{1}{\\mu_{\\min}(1-\\gamma)^5\\varepsilon^2}+\n\\frac{t_{mix}}{\\mu_{\\min}(1-\\gamma)}$ up to some logarithmic factor, provided\nthat a proper constant learning rate is adopted. Here, $t_{mix}$ and\n$\\mu_{\\min}$ denote respectively the mixing time and the minimum state-action\noccupancy probability of the sample trajectory. The first term of this bound\nmatches the sample complexity in the synchronous case with independent samples\ndrawn from the stationary distribution of the trajectory. The second term\nreflects the cost taken for the empirical distribution of the Markovian\ntrajectory to reach a steady state, which is incurred at the very beginning and\nbecomes amortized as the algorithm runs. Encouragingly, the above bound\nimproves upon the state-of-the-art result \\cite{qu2020finite} by a factor of at\nleast $|\\mathcal{S}||\\mathcal{A}|$ for all scenarios, and by a factor of at\nleast $t_{mix}|\\mathcal{S}||\\mathcal{A}|$ for any sufficiently small accuracy\nlevel $\\varepsilon$. Further, we demonstrate that the scaling on the effective\nhorizon $\\frac{1}{1-\\gamma}$ can be improved by means of variance reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yuting Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1\">Yuejie Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuantao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fairness Under Feature Exemptions: Counterfactual and Observational Measures. (arXiv:2006.07986v2 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2006.07986","description":"<p>With the growing use of ML in highly consequential domains, quantifying\ndisparity with respect to protected attributes, e.g., gender, race, etc., is\nimportant. While quantifying disparity is essential, sometimes the needs of an\noccupation may require the use of certain features that are critical in a way\nthat any disparity that can be explained by them might need to be exempted.\nE.g., in hiring a software engineer for a safety-critical application,\ncoding-skills may be weighed strongly, whereas name, zip code, or reference\nletters may be used only to the extent that they do not add disparity. In this\nwork, we propose an information-theoretic decomposition of the total disparity\n(a quantification inspired from counterfactual fairness) into two components: a\nnon-exempt component which quantifies the part that cannot be accounted for by\nthe critical features, and an exempt component that quantifies the remaining\ndisparity. This decomposition allows one to check if the disparity arose purely\ndue to the critical features (inspired from the business necessity defense of\ndisparate impact law) and also enables selective removal of the non-exempt\ncomponent if desired. We arrive at this decomposition through canonical\nexamples that lead to a set of desirable properties (axioms) that a measure of\nnon-exempt disparity should satisfy. Our proposed measure satisfies all of\nthem. Our quantification bridges ideas of causality, Simpson's paradox, and a\nbody of work from information theory called Partial Information Decomposition.\nWe also obtain an impossibility result showing that no observational measure\ncan satisfy all the desirable properties, leading us to relax our goals and\nexamine observational measures that satisfy only some of them. We perform case\nstudies to show how one can audit/train models while reducing non-exempt\ndisparity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sanghamitra Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_P/0/1/0/all/0/1\">Praveen Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mardziel_P/0/1/0/all/0/1\">Piotr Mardziel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Anupam Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_P/0/1/0/all/0/1\">Pulkit Grover</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks. (arXiv:2006.09134v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.09134","description":"<p>Generative Adversarial Networks (GANs) are formulated as minimax game\nproblems, whereby generators attempt to approach real data distributions by\nvirtue of adversarial learning against discriminators. The intrinsic problem\ncomplexity poses the challenge to enhance the performance of generative\nnetworks. In this work, we aim to boost model learning from the perspective of\nnetwork architectures, by incorporating recent progress on automated\narchitecture search into GANs. To this end, we propose a fully differentiable\nsearch framework for generative adversarial networks, dubbed alphaGAN. The\nsearching process is formalized as solving a bi-level minimax optimization\nproblem, in which the outer-level objective aims for seeking a suitable network\narchitecture towards pure Nash Equilibrium conditioned on the generator and the\ndiscriminator network parameters optimized with a traditional GAN loss in the\ninner level. The entire optimization performs a first-order method by\nalternately minimizing the two-level objective in a fully differentiable\nmanner, enabling architecture search to be completed in an enormous search\nspace. Extensive experiments on CIFAR-10 and STL-10 datasets show that our\nalgorithm can obtain high-performing architectures only with 3-GPU hours on a\nsingle GPU in the search space comprised of approximate 2 ? 1011 possible\nconfigurations. We also provide a comprehensive analysis on the behavior of the\nsearching process and the properties of searched architectures, which would\nbenefit further research on architectures for generative models. Pretrained\nmodels and codes are available at https://github.com/yuesongtian/AlphaGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuesong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Guinan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilevel Graph Matching Networks for Deep Graph Similarity Learning. (arXiv:2007.04395v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.04395","description":"<p>While the celebrated graph neural networks yield effective representations\nfor individual nodes of a graph, there has been relatively less success in\nextending to the task of graph similarity learning. Recent work on graph\nsimilarity learning has considered either global-level graph-graph interactions\nor low-level node-node interactions, however ignoring the rich cross-level\ninteractions (e.g., between each node of one graph and the other whole graph).\nIn this paper, we propose a multi-level graph matching network (MGMN) framework\nfor computing the graph similarity between any pair of graph-structured objects\nin an end-to-end fashion. In particular, the proposed MGMN consists of a\nnode-graph matching network for effectively learning cross-level interactions\nbetween each node of one graph and the other whole graph, and a siamese graph\nneural network to learn global-level interactions between two input graphs.\nFurthermore, to compensate for the lack of standard benchmark datasets, we have\ncreated and collected a set of datasets for both the graph-graph classification\nand graph-graph regression tasks with different sizes in order to evaluate the\neffectiveness and robustness of our models. Comprehensive experiments\ndemonstrate that MGMN consistently outperforms state-of-the-art baseline models\non both the graph-graph classification and graph-graph regression tasks.\nCompared with previous work, MGMN also exhibits stronger robustness as the\nsizes of the two input graphs increase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiang Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Saizhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alex X. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chunming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Nature and Types of Anomalies: A Review of Deviations in Data. (arXiv:2007.15634v4 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2007.15634","description":"<p>Anomalies are occurrences in a dataset that are in some way unusual and do\nnot fit the general patterns. The concept of the anomaly is typically\nill-defined and perceived as vague and domain-dependent. Moreover, despite some\n250 years of publications on the topic, no comprehensive and concrete overviews\nof the different types of anomalies have hitherto been published. By means of\nan extensive literature review this study therefore offers the first\ntheoretically principled and domain-independent typology of data anomalies and\npresents a full overview of anomaly types and subtypes. To concretely define\nthe concept of the anomaly and its different manifestations, the typology\nemploys five dimensions: data type, cardinality of relationship, anomaly level,\ndata structure, and data distribution. These fundamental and data-centric\ndimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of\nanomalies. The typology facilitates the evaluation of the functional\ncapabilities of anomaly detection algorithms, contributes to explainable data\nscience, and provides insights into relevant topics such as local versus global\nanomalies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foorthuis_R/0/1/0/all/0/1\">Ralph Foorthuis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Pre-Conditioning for Expediting the Gradient-Descent Method: The Distributed Linear Least-Squares Problem. (arXiv:2008.02856v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2008.02856","description":"<p>This paper considers the multi-agent linear least-squares problem in a\nserver-agent network. In this problem, the system comprises multiple agents,\neach having a set of local data points, that are connected to a server. The\ngoal for the agents is to compute a linear mathematical model that optimally\nfits the collective data points held by all the agents, without sharing their\nindividual local data points. This goal can be achieved, in principle, using\nthe server-agent variant of the traditional iterative gradient-descent method.\nThe gradient-descent method converges linearly to a solution, and its rate of\nconvergence is lower bounded by the conditioning of the agents' collective data\npoints. If the data points are ill-conditioned, the gradient-descent method may\nrequire a large number of iterations to converge.\n</p>\n<p>We propose an iterative pre-conditioning technique that mitigates the\ndeleterious effect of the conditioning of data points on the rate of\nconvergence of the gradient-descent method. We rigorously show that the\nresulting pre-conditioned gradient-descent method, with the proposed iterative\npre-conditioning, achieves superlinear convergence when the least-squares\nproblem has a unique solution. In general, the convergence is linear with\nimproved rate of convergence in comparison to the traditional gradient-descent\nmethod and the state-of-the-art accelerated gradient-descent methods. We\nfurther illustrate the improved rate of convergence of our proposed algorithm\nthrough experiments on different real-world least-squares problems in both\nnoise-free and noisy computation environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Chakrabarti_K/0/1/0/all/0/1\">Kushal Chakrabarti</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gupta_N/0/1/0/all/0/1\">Nirupam Gupta</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chopra_N/0/1/0/all/0/1\">Nikhil Chopra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Bridge Sampling for Evaluating Safety-Critical Autonomous Systems. (arXiv:2008.10581v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2008.10581","description":"<p>Learning-based methodologies increasingly find applications in\nsafety-critical domains like autonomous driving and medical robotics. Due to\nthe rare nature of dangerous events, real-world testing is prohibitively\nexpensive and unscalable. In this work, we employ a probabilistic approach to\nsafety evaluation in simulation, where we are concerned with computing the\nprobability of dangerous events. We develop a novel rare-event simulation\nmethod that combines exploration, exploitation, and optimization techniques to\nfind failure modes and estimate their rate of occurrence. We provide rigorous\nguarantees for the performance of our method in terms of both statistical and\ncomputational efficiency. Finally, we demonstrate the efficacy of our approach\non a variety of scenarios, illustrating its usefulness as a tool for rapid\nsensitivity analysis and model comparison that are essential to developing and\ntesting safety-critical autonomous systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Aman Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OKelly_M/0/1/0/all/0/1\">Matthew O&#x27;Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedrake_R/0/1/0/all/0/1\">Russ Tedrake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1\">John Duchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to \"Improve\" Prediction Using Behavior Modification. (arXiv:2008.12138v2 [cs.CY] UPDATED)","link":"http://arxiv.org/abs/2008.12138","description":"<p>Many internet platforms that collect behavioral big data use it to predict\nuser behavior for internal purposes and for their business customers (e.g.,\nadvertisers, insurers, security forces, governments, political consulting\nfirms) who utilize the predictions for personalization, targeting, and other\ndecision-making. Improving predictive accuracy is therefore extremely valuable.\nData science researchers design algorithms, models, and approaches to improve\nprediction. Prediction is also improved with larger and richer data. Beyond\nimproving algorithms and data, platforms can stealthily achieve better\nprediction accuracy by \"pushing\" users' behaviors towards their predicted\nvalues, using behavior modification techniques, thereby demonstrating more\ncertain predictions. Such apparent \"improved\" prediction can unintentionally\nresult from employing reinforcement learning algorithms that combine prediction\nand behavior modification. This strategy is absent from the machine learning\nand statistics literature. Investigating its properties requires integrating\ncausal with predictive notation. To this end, we incorporate Pearl's causal\ndo(.) operator into the predictive vocabulary. We then decompose the expected\nprediction error given behavior modification, and identify the components\nimpacting predictive power. Our derivation elucidates implications of such\nbehavior modification to data scientists, platforms, their customers, and the\nhumans whose behavior is manipulated. Behavior modification can make users'\nbehavior more predictable and even more homogeneous; yet this apparent\npredictability might not generalize when customers use predictions in practice.\nOutcomes pushed towards their predictions can be at odds with customers'\nintentions, and harmful to manipulated users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shmueli_G/0/1/0/all/0/1\">Galit Shmueli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafti_A/0/1/0/all/0/1\">Ali Tafti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor Relational Algebra for Machine Learning System Design. (arXiv:2009.00524v3 [cs.DB] UPDATED)","link":"http://arxiv.org/abs/2009.00524","description":"<p>We consider the question: what is the abstraction that should be implemented\nby the computational engine of a machine learning system? Current machine\nlearning systems typically push whole tensors through a series of compute\nkernels such as matrix multiplications or activation functions, where each\nkernel runs on an AI accelerator (ASIC) such as a GPU. This implementation\nabstraction provides little built-in support for ML systems to scale past a\nsingle machine, or for handling large models with matrices or tensors that do\nnot easily fit into the RAM of an ASIC. In this paper, we present an\nalternative implementation abstraction called the tensor relational algebra\n(TRA). The TRA is a set-based algebra based on the relational algebra.\nExpressions in the TRA operate over binary tensor relations, where keys are\nmulti-dimensional arrays and values are tensors. The TRA is easily executed\nwith high efficiency in a parallel or distributed environment, and amenable to\nautomatic optimization. Our empirical study shows that the optimized TRA-based\nback-end can significantly outperform alternatives for running ML workflows in\ndistributed clusters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1\">Binhang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jankov_D/0/1/0/all/0/1\">Dimitrije Jankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Jia Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yuxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourgeois_D/0/1/0/all/0/1\">Daniel Bourgeois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Chris Jermaine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Negative Transfer. (arXiv:2009.00909v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.00909","description":"<p>Transfer learning (TL) utilizes data or knowledge from one or more source\ndomains to facilitate the learning in a target domain. It is particularly\nuseful when the target domain has very few or no labeled data, due to\nannotation expense, privacy concerns, etc. Unfortunately, the effectiveness of\nTL is not always guaranteed. Negative transfer (NT), i.e., leveraging source\ndomain data/knowledge undesirably reduces the learning performance in the\ntarget domain, has been a long-standing and challenging problem in TL. Various\napproaches have been proposed in the literature to handle it. However, there\ndoes not exist a systematic survey on the formulation of NT, the factors\nleading to NT, and the algorithms that mitigate NT. This paper fills this gap,\nby first introducing the definition of NT and its factors, then reviewing about\nfifty representative approaches for overcoming NT, according to four\ncategories: secure transfer, domain similarity estimation, distant transfer,\nand NT mitigation. NT in related fields, e.g., multi-task learning, lifelong\nlearning, and adversarial attacks, are also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lingfei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dongrui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!. (arXiv:2009.10684v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.10684","description":"<p>Despite efforts to distinguish three different evaluation setups (Bekoulis et\nal., 2018), numerous end-to-end Relation Extraction (RE) articles present\nunreliable performance comparison to previous work. In this paper, we first\nidentify several patterns of invalid comparisons in published papers and\ndescribe them to avoid their propagation. We then propose a small empirical\nstudy to quantify the impact of the most common mistake and evaluate it leads\nto overestimating the final RE performance by around 5% on ACE05. We also seize\nthis opportunity to study the unexplored ablations of two recent developments:\nthe use of language model pretraining (specifically BERT) and span-level NER.\nThis meta-analysis emphasizes the need for rigor in the report of both the\nevaluation setting and the datasets statistics and we call for unifying the\nevaluation setting in end-to-end RE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taille_B/0/1/0/all/0/1\">Bruno Taill&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guigue_V/0/1/0/all/0/1\">Vincent Guigue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning. (arXiv:2010.03110v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.03110","description":"<p>Animals exhibit an innate ability to learn regularities of the world through\ninteraction. By performing experiments in their environment, they are able to\ndiscern the causal factors of variation and infer how they affect the world's\ndynamics. Inspired by this, we attempt to equip reinforcement learning agents\nwith the ability to perform experiments that facilitate a categorization of the\nrolled-out trajectories, and to subsequently infer the causal factors of the\nenvironment in a hierarchical manner. We introduce {\\em causal curiosity}, a\nnovel intrinsic reward, and show that it allows our agents to learn optimal\nsequences of actions and discover causal factors in the dynamics of the\nenvironment. The learned behavior allows the agents to infer a binary quantized\nrepresentation for the ground-truth causal factors in every environment.\nAdditionally, we find that these experimental behaviors are semantically\nmeaningful (e.g., our agents learn to lift blocks to categorize them by\nweight), and are learnt in a self-supervised manner with approximately 2.5\ntimes less data than conventional supervised planners. We show that these\nbehaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or\nother downstream tasks). Finally, we show that the knowledge of causal factor\nrepresentations aids zero-shot learning for more complex tasks. Visit\nhttps://sites.google.com/usc.edu/causal-curiosity/home for website.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1\">Sumedh A. Sontakke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrjou_A/0/1/0/all/0/1\">Arash Mehrjou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1\">Laurent Itti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Classification Using Staked Ensembles: A Comprehensive Analysis. (arXiv:2010.05690v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.05690","description":"<p>The issue of COVID-19, increasing with a massive mortality rate. This led to\nthe WHO declaring it as a pandemic. In this situation, it is crucial to perform\nefficient and fast diagnosis. The reverse transcript polymerase chain reaction\n(RTPCR) test is conducted to detect the presence of SARS-CoV-2. This test is\ntime-consuming and instead chest CT (or Chest X-ray) can be used for a fast and\naccurate diagnosis. Automated diagnosis is considered to be important as it\nreduces human effort and provides accurate and low-cost tests. The\ncontributions of our research are three-fold. First, it is aimed to analyse the\nbehaviour and performance of variant vision models ranging from Inception to\nNAS networks with the appropriate fine-tuning procedure. Second, the behaviour\nof these models is visually analysed by plotting CAMs for individual networks\nand determining classification performance with AUCROC curves. Thirdly, stacked\nensembles techniques are imparted to provide higher generalisation on combining\nthe fine-tuned models, in which six ensemble neural networks are designed by\ncombining the existing fine-tuned networks. Implying these stacked ensembles\nprovides a great generalization to the models. The ensemble model designed by\ncombining all the fine-tuned networks obtained a state-of-the-art accuracy\nscore of 99.17%. The precision and recall for the COVID-19 class are 99.99% and\n89.79% respectively, which resembles the robustness of the stacked ensembles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+B_L/0/1/0/all/0/1\">Lalith Bharadwaj B</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeda_R/0/1/0/all/0/1\">Rohit Boddeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+K_S/0/1/0/all/0/1\">Sai Vardhan K</a>, <a href=\"http://arxiv.org/find/cs/1/au:+G_M/0/1/0/all/0/1\">Madhu G</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MGIC: Multigrid-in-Channels Neural Network Architectures. (arXiv:2011.09128v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.09128","description":"<p>We present a multigrid-in-channels (MGIC) approach that tackles the quadratic\ngrowth of the number of parameters with respect to the number of channels in\nstandard convolutional neural networks (CNNs). Thereby our approach addresses\nthe redundancy in CNNs that is also exposed by the recent success of\nlightweight CNNs. Lightweight CNNs can achieve comparable accuracy to standard\nCNNs with fewer parameters; however, the number of weights still scales\nquadratically with the CNN's width. Our MGIC architectures replace each CNN\nblock with an MGIC counterpart that utilizes a hierarchy of nested grouped\nconvolutions of small group size to address this.\n</p>\n<p>Hence, our proposed architectures scale linearly with respect to the\nnetwork's width while retaining full coupling of the channels as in standard\nCNNs.\n</p>\n<p>Our extensive experiments on image classification, segmentation, and point\ncloud classification show that applying this strategy to different\narchitectures like ResNet and MobileNetV3 reduces the number of parameters\nwhile obtaining similar or better accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1\">Moshe Eliasof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ephrath_J/0/1/0/all/0/1\">Jonathan Ephrath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruthotto_L/0/1/0/all/0/1\">Lars Ruthotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1\">Eran Treister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchored-STFT and GNAA: An extension of STFT in conjunction with an adversarial data augmentation technique for the decoding of neural signals. (arXiv:2011.14694v4 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2011.14694","description":"<p>Brain-computer interfaces (BCIs) enable communication between humans and\nmachines by translating brain activity into control commands.\nElectroencephalography (EEG) signals are one of the most used brain signals in\nnon-invasive BCI applications but are often contaminated with noise. Therefore,\nit is possible that meaningful patterns for classifying EEG signals are deeply\nhidden. State-of-the-art deep-learning algorithms are successful in learning\nhidden, meaningful patterns. However, the quality and the quantity of the\npresented inputs is pivotal. Here, we propose a novel feature extraction method\ncalled anchored Short Time Fourier Transform (anchored-STFT), which is an\nadvanced version of STFT, as it minimizes the trade-off between temporal and\nspectral resolution presented by STFT. In addition, we propose a novel\naugmentation method, called gradient norm adversarial augmentation (GNAA). GNAA\nis not only an augmentation method but is also used to harness adversarial\ninputs in EEG data, which not only improves the classification accuracy but\nalso enhances the robustness of the classifier. In addition, we also propose a\nnew CNN architecture, namely Skip-Net, for the classification of EEG signals.\nThe proposed pipeline outperforms all state-of-the-art methods and yields an\naverage classification accuracy of 90.7 % and 89.54 % on BCI competition II\ndataset III and BCI competition IV dataset 2b, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_O/0/1/0/all/0/1\">Omair Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saif_ur_Rehman_M/0/1/0/all/0/1\">Muhammad Saif-ur-Rehman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dyck_S/0/1/0/all/0/1\">Susanne Dyck</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Glasmachers_T/0/1/0/all/0/1\">Tobias Glasmachers</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Iossifidis_I/0/1/0/all/0/1\">Ioannis Iossifidis</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Klaes_C/0/1/0/all/0/1\">Christian Klaes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Gravity: enhancing mobility flows generation with deep neural networks and geographic information. (arXiv:2012.00489v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.00489","description":"<p>The movements of individuals within and among cities influence critical\naspects of our society, such as well-being, the spreading of epidemics, and the\nquality of the environment. When information about mobility flows is not\navailable for a particular region of interest, we must rely on mathematical\nmodels to generate them. In this work, we propose the Deep Gravity model, an\neffective method to generate flow probabilities that exploits many variables\n(e.g., land use, road network, transport, food, health facilities) extracted\nfrom voluntary geographic data, and uses deep neural networks to discover\nnon-linear relationships between those variables and mobility flows. Our\nexperiments, conducted on mobility flows in England, Italy, and New York State,\nshow that Deep Gravity has good geographic generalization capability, achieving\na significant increase in performance (especially in densely populated regions\nof interest) with respect to the classic gravity model and models that do not\nuse deep neural networks or geographic data. We also show how flows generated\nby Deep Gravity may be explained in terms of the geographic features using\nexplainable AI techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simini_F/0/1/0/all/0/1\">Filippo Simini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1\">Gianni Barlacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luca_M/0/1/0/all/0/1\">Massimiliano Luca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1\">Luca Pappalardo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChartPointFlow for Topology-Aware 3D Point Cloud Generation. (arXiv:2012.02346v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02346","description":"<p>A point cloud serves as a representation of the surface of a\nthree-dimensional (3D) shape. Deep generative models have been adapted to model\ntheir variations typically using a map from a ball-like set of latent\nvariables. However, previous approaches did not pay much attention to the\ntopological structure of a point cloud, despite that a continuous map cannot\nexpress the varying numbers of holes and intersections. Moreover, a point cloud\nis often composed of multiple subparts, and it is also difficult to express. In\nthis study, we propose ChartPointFlow, a flow-based generative model with\nmultiple latent labels for 3D point clouds. Each label is assigned to points in\nan unsupervised manner. Then, a map conditioned on a label is assigned to a\ncontinuous subset of a point cloud, similar to a chart of a manifold. This\nenables our proposed model to preserve the topological structure with clear\nboundaries, whereas previous approaches tend to generate blurry point clouds\nand fail to generate holes. The experimental results demonstrate that\nChartPointFlow achieves state-of-the-art performance in terms of generation and\nreconstruction compared with other point cloud generators. Moreover,\nChartPointFlow divides an object into semantic subparts using charts, and it\ndemonstrates superior performance in case of unsupervised segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kimura_T/0/1/0/all/0/1\">Takumi Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_T/0/1/0/all/0/1\">Takashi Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1\">Kuniaki Uehara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning Local Reduced Density Matrices for Many-body Hamiltonian Estimation. (arXiv:2012.03019v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2012.03019","description":"<p>Human experts cannot efficiently access the physical information of quantum\nmany-body states by simply \"reading\" the coefficients, but have to reply on the\nprevious knowledge such as order parameters and quantum measurements. In this\nwork, we demonstrate that convolutional neural network (CNN) can learn from the\ncoefficients of local reduced density matrices to estimate the physical\nparameters of the many-body Hamiltonians, such as coupling strengths and\nmagnetic fields, provided the states as the ground states. We propose QubismNet\nthat consists of two main parts: the Qubism map that visualizes the ground\nstates (or the purified reduced density matrices) as images, and a CNN that\nmaps the images to the target physical parameters. By assuming certain\nconstraints on the training set for the sake of balance, QubismNet exhibits\nimpressive powers of learning and generalization on several quantum spin\nmodels. While the training samples are restricted to the states from certain\nranges of the parameters, QubismNet can accurately estimate the parameters of\nthe states beyond such training regions. For instance, our results show that\nQubismNet can estimate the magnetic fields near the critical point by learning\nfrom the states away from the critical vicinity. Our work illuminates a\ndata-driven way to infer the Hamiltonians that give the designed ground states,\nand therefore would benefit the existing and future generalizations of quantum\ntechnologies such as Hamiltonian-based quantum simulations and state\ntomography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Ma_X/0/1/0/all/0/1\">Xinran Ma</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tu_Z/0/1/0/all/0/1\">Z. C. Tu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differential Evolution for Neural Architecture Search. (arXiv:2012.06400v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2012.06400","description":"<p>Neural architecture search (NAS) methods rely on a search strategy for\ndeciding which architectures to evaluate next and a performance estimation\nstrategy for assessing their performance (e.g., using full evaluations,\nmulti-fidelity evaluations, or the one-shot model). In this paper, we focus on\nthe search strategy. We introduce the simple yet powerful evolutionary\nalgorithm of differential evolution to the NAS community. Using the simplest\nperformance evaluation strategy of full evaluations, we comprehensively compare\nthis search strategy to regularized evolution and Bayesian optimization and\ndemonstrate that it yields improved and more robust results for 13 tabular NAS\nbenchmarks based on NAS-Bench-101, NAS-Bench-1Shot1, NAS-Bench-201 and NAS-HPO\nbench.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Awad_N/0/1/0/all/0/1\">Noor Awad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_N/0/1/0/all/0/1\">Neeratyoy Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1\">Frank Hutter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alternating linear scheme in a Bayesian framework for low-rank tensor approximation. (arXiv:2012.11228v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.11228","description":"<p>Multiway data often naturally occurs in a tensorial format which can be\napproximately represented by a low-rank tensor decomposition. This is useful\nbecause complexity can be significantly reduced and the treatment of\nlarge-scale data sets can be facilitated. In this paper, we find a low-rank\nrepresentation for a given tensor by solving a Bayesian inference problem. This\nis achieved by dividing the overall inference problem into sub-problems where\nwe sequentially infer the posterior distribution of one tensor decomposition\ncomponent at a time. This leads to a probabilistic interpretation of the\nwell-known iterative algorithm alternating linear scheme (ALS). In this way,\nthe consideration of measurement noise is enabled, as well as the incorporation\nof application-specific prior knowledge and the uncertainty quantification of\nthe low-rank tensor estimate. To compute the low-rank tensor estimate from the\nposterior distributions of the tensor decomposition components, we present an\nalgorithm that performs the unscented transform in tensor train format.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menzen_C/0/1/0/all/0/1\">Clara Menzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kok_M/0/1/0/all/0/1\">Manon Kok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batselier_K/0/1/0/all/0/1\">Kim Batselier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepHateExplainer: Explainable Hate Speech Detection in Under-resourced Bengali Language. (arXiv:2012.14353v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14353","description":"<p>The exponential growths of social media and micro-blogging sites not only\nprovide platforms for empowering freedom of expressions and individual voices,\nbut also enables people to express anti-social behaviour like online\nharassment, cyberbullying, and hate speech. Numerous works have been proposed\nto utilize textual data for social and anti-social behaviour analysis, by\npredicting the contexts mostly for highly-resourced languages like English.\nHowever, some languages are under-resourced, e.g., South Asian languages like\nBengali, that lack computational resources for accurate natural language\nprocessing (NLP). In this paper, we propose an explainable approach for hate\nspeech detection from the under-resourced Bengali language, which we called\nDeepHateExplainer. Bengali texts are first comprehensively preprocessed, before\nclassifying them into political, personal, geopolitical, and religious hates\nusing a neural ensemble method of transformer-based neural architectures (i.e.,\nmonolingual Bangla BERT-base, multilingual BERT-cased/uncased, and\nXLM-RoBERTa). Important(most and least) terms are then identified using\nsensitivity analysis and layer-wise relevance propagation(LRP), before\nproviding human-interpretable explanations. Finally, we compute\ncomprehensiveness and sufficiency scores to measure the quality of explanations\nw.r.t faithfulness. Evaluations against machine learning~(linear and tree-based\nmodels) and neural networks (i.e., CNN, Bi-LSTM, and Conv-LSTM with word\nembeddings) baselines yield F1-scores of 78%, 91%, 89%, and 84%, for political,\npersonal, geopolitical, and religious hates, respectively, outperforming both\nML and DNN baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Md. Rezaul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1\">Sumon Kanti Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tanhim Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1\">Sagor Sarker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_M/0/1/0/all/0/1\">Mehadi Hasan Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_K/0/1/0/all/0/1\">Kabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md. Azam Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_S/0/1/0/all/0/1\">Stefan Decker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Future Wireless Networks: A Comprehensive Survey. (arXiv:2102.07572v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.07572","description":"<p>With outstanding features, Machine Learning (ML) has been the backbone of\nnumerous applications in wireless networks. However, the conventional ML\napproaches have been facing many challenges in practical implementation, such\nas the lack of labeled data, the constantly changing wireless environments, the\nlong training process, and the limited capacity of wireless devices. These\nchallenges, if not addressed, will impede the effectiveness and applicability\nof ML in future wireless networks. To address these problems, Transfer Learning\n(TL) has recently emerged to be a very promising solution. The core idea of TL\nis to leverage and synthesize distilled knowledge from similar tasks as well as\nfrom valuable experiences accumulated from the past to facilitate the learning\nof new problems. Doing so, TL techniques can reduce the dependence on labeled\ndata, improve the learning speed, and enhance the ML methods' robustness to\ndifferent wireless environments. This article aims to provide a comprehensive\nsurvey on applications of TL in wireless networks. Particularly, we first\nprovide an overview of TL including formal definitions, classification, and\nvarious types of TL techniques. We then discuss diverse TL approaches proposed\nto address emerging issues in wireless networks. The issues include spectrum\nmanagement, localization, signal recognition, security, human activity\nrecognition and caching, which are all important to next-generation networks\nsuch as 5G and beyond. Finally, we highlight important challenges, open issues,\nand future research directions of TL in future wireless networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Cong T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1\">Nguyen Van Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_N/0/1/0/all/0/1\">Nam H. Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saputra_Y/0/1/0/all/0/1\">Yuris Mulya Saputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1\">Dinh Thai Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Diep N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quoc-Viet Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1\">Dusit Niyato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1\">Eryk Dutkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Won-Joo Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combinatorial Bandits under Strategic Manipulations. (arXiv:2102.12722v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.12722","description":"<p>Strategic behavior against sequential learning methods, such as \"click\nframing\" in real recommendation systems, has been widely observed. Motivated by\nsuch behavior we study the problem of combinatorial multi-armed bandits (CMAB)\nunder strategic manipulations of rewards, where each arm can modify the emitted\nreward signals for its own interest. This characterization of the adversarial\nbehavior is a relaxation of previously well-studied settings such as\nadversarial attacks and adversarial corruption. We propose a strategic variant\nof the combinatorial UCB algorithm, which has a regret of at most $O(m\\log T +\nm B_{max})$ under strategic manipulations, where $T$ is the time horizon, $m$\nis the number of arms, and $B_{max}$ is the maximum budget of an arm. We\nprovide lower bounds on the budget for arms to incur certain regret of the\nbandit algorithm. Extensive experiments on online worker selection for\ncrowdsourcing systems, online influence maximization and online recommendations\nwith both synthetic and real datasets corroborate our theoretical findings on\nrobustness and regret bounds, in a variety of regimes of manipulation budgets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Consistency Regularization for Semi-Supervised Transfer Learning. (arXiv:2103.02193v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02193","description":"<p>While recent studies on semi-supervised learning have shown remarkable\nprogress in leveraging both labeled and unlabeled data, most of them presume a\nbasic setting of the model is randomly initialized. In this work, we consider\nsemi-supervised learning and transfer learning jointly, leading to a more\npractical and competitive paradigm that can utilize both powerful pre-trained\nmodels from source domain as well as labeled/unlabeled data in the target\ndomain. To better exploit the value of both pre-trained weights and unlabeled\ntarget examples, we introduce adaptive consistency regularization that consists\nof two complementary components: Adaptive Knowledge Consistency (AKC) on the\nexamples between the source and target model, and Adaptive Representation\nConsistency (ARC) on the target model between labeled and unlabeled examples.\nExamples involved in the consistency regularization are adaptively selected\naccording to their potential contributions to the target task. We conduct\nextensive experiments on popular benchmarks including CIFAR-10, CUB-200, and\nMURA, by fine-tuning the ImageNet pre-trained ResNet-50 model. Results show\nthat our proposed adaptive consistency regularization outperforms\nstate-of-the-art semi-supervised learning techniques such as Pseudo Label, Mean\nTeacher, and FixMatch. Moreover, our algorithm is orthogonal to existing\nmethods and thus able to gain additional improvements on top of MixMatch and\nFixMatch. Our code is available at\nhttps://github.com/SHI-Labs/Semi-Supervised-Transfer-Learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolution Neural Network Hyperparameter Optimization Using Simplified Swarm Optimization. (arXiv:2103.03995v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2103.03995","description":"<p>Convolutional neural networks (CNNs) are widely used in image recognition.\nNumerous CNN models, such as LeNet, AlexNet, VGG, ResNet, and GoogLeNet, have\nbeen proposed by increasing the number of layers, to improve the performance of\nCNNs. However, performance deteriorates beyond a certain number of layers.\nHence, hyperparameter optimisation is a more efficient way to improve CNNs. To\nvalidate this concept, a new algorithm based on simplified swarm optimisation\nis proposed to optimise the hyperparameters of the simplest CNN model, which is\nLeNet. The results of experiments conducted on the MNIST, Fashion MNIST, and\nCifar10 datasets showed that the accuracy of the proposed algorithm is higher\nthan the original LeNet model and PSO-LeNet and that it has a high potential to\nbe extended to more complicated models, such as AlexNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_W/0/1/0/all/0/1\">Wei-Chang Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yi-Ping Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yun-Chia Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chyh-Ming Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Relative Order Attack in Deep Ranking. (arXiv:2103.05248v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.05248","description":"<p>Recent studies unveil the vulnerabilities of deep ranking models, where an\nimperceptible perturbation can trigger dramatic changes in the ranking result.\nWhile previous attempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order remains\nunder-explored. In this paper, we formulate a new adversarial attack against\ndeep ranking systems, i.e., the Order Attack, which covertly alters the\nrelative order among a selected set of candidates according to an\nattacker-specified permutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style loss imposing an\ninequality chain reflecting the specified permutation. However, direct\noptimization of such white-box objective is infeasible in a real-world attack\nscenario due to various black-box limitations. To cope with them, we propose a\nShort-range Ranking Correlation metric as a surrogate objective for black-box\nOrder Attack to approximate the white-box method. The Order Attack is evaluated\non the Fashion-MNIST and Stanford-Online-Products datasets under both white-box\nand black-box threat models. The black-box attack is also successfully\nimplemented on a major e-commerce platform. Comprehensive experimental\nevaluations demonstrate the effectiveness of the proposed methods, revealing a\nnew type of ranking model vulnerability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenxing Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Linear Policies for Robust Bipedal Locomotion on Terrains with Varying Slopes. (arXiv:2104.01662v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2104.01662","description":"<p>In this paper, with a view toward deployment of light-weight control\nframeworks for bipedal walking robots, we realize end-foot trajectories that\nare shaped by a single linear feedback policy. We learn this policy via a\nmodel-free and a gradient-free learning algorithm, Augmented Random Search\n(ARS), in the two robot platforms Rabbit and Digit. Our contributions are\ntwo-fold: a) By using torso and support plane orientation as inputs, we achieve\nrobust walking on slopes of up to 20 degrees in simulation. b) We demonstrate\nadditional behaviors like walking backwards, stepping-in-place, and recovery\nfrom external pushes of up to 120 N. The end result is a robust and a fast\nfeedback control law for bipedal walking on terrains with varying slopes.\nTowards the end, we also provide preliminary results of hardware transfer to\nDigit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_L/0/1/0/all/0/1\">Lokesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_U/0/1/0/all/0/1\">Utkarsh A. Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_G/0/1/0/all/0/1\">Guillermo A. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hereid_A/0/1/0/all/0/1\">Ayonga Hereid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolathaya_S/0/1/0/all/0/1\">Shishir Kolathaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting False Data Injection Attacks in Smart Grids with Modeling Errors: A Deep Transfer Learning Based Approach. (arXiv:2104.06307v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2104.06307","description":"<p>Most traditional false data injection attack (FDIA) detection approaches rely\non a key assumption, i.e., the power system can be accurately modeled. However,\nthe transmission line parameters are dynamic and cannot be accurately known\nduring operation and thus the involved modeling errors should not be neglected.\nIn this paper, an illustrative case has revealed that modeling errors in\ntransmission lines significantly weaken the detection effectiveness of\nconventional FDIA approaches. To tackle this issue, we propose an FDIA\ndetection mechanism from the perspective of transfer learning. Specifically,\nthe simulated power system is treated as a source domain, which provides\nabundant simulated normal and attack data. The real world's running system\nwhose transmission line parameters are unknown is taken as a target domain\nwhere sufficient real normal data are collected for tracking the latest system\nstates online. The designed transfer strategy that aims at making full use of\ndata in hand is divided into two optimization stages. In the first stage, a\ndeep neural network (DNN) is built by simultaneously optimizing several\nwell-designed objective terms with both simulated data and real data, and then\nit is fine-tuned via real data in the second stage. Several case studies on the\nIEEE 14-bus and 118-bus systems verify the effectiveness of the proposed\nmechanism.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Bowen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_F/0/1/0/all/0/1\">Fanghong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_C/0/1/0/all/0/1\">Changyun Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1\">Ruilong Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wen-An Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN. (arXiv:2104.06534v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06534","description":"<p>Existing thermal-to-visible face verification approaches expect the thermal\nand visible face images to be of similar resolution. This is unlikely in\nreal-world long-range surveillance systems, since humans are distant from the\ncameras. To address this issue, we introduce the task of thermal-to-visible\nface verification from low-resolution thermal images. Furthermore, we propose\nAxial-Generative Adversarial Network (Axial-GAN) to synthesize high-resolution\nvisible images for matching. In the proposed approach we augment the GAN\nframework with axial-attention layers which leverage the recent advances in\ntransformers for modelling long-range dependencies. We demonstrate the\neffectiveness of the proposed method by evaluating on two different\nthermal-visible face datasets. When compared to related state-of-the-art works,\nour results show significant improvements in both image quality and face\nverification performance, and are also much more efficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Immidisetti_R/0/1/0/all/0/1\">Rakhil Immidisetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shuowen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonlinear Level Set Learning for Function Approximation on Sparse Data with Applications to Parametric Differential Equations. (arXiv:2104.14072v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2104.14072","description":"<p>A dimension reduction method based on the \"Nonlinear Level set Learning\"\n(NLL) approach is presented for the pointwise prediction of functions which\nhave been sparsely sampled. Leveraging geometric information provided by the\nImplicit Function Theorem, the proposed algorithm effectively reduces the input\ndimension to the theoretical lower bound with minor accuracy loss, providing a\none-dimensional representation of the function which can be used for regression\nand sensitivity analysis. Experiments and applications are presented which\ncompare this modified NLL with the original NLL and the Active Subspaces (AS)\nmethod. While accommodating sparse input data, the proposed algorithm is shown\nto train quickly and provide a much more accurate and informative reduction\nthan either AS or the original NLL on two example functions with\nhigh-dimensional domains, as well as two state-dependent quantities depending\non the solutions to parametric differential equations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Gruber_A/0/1/0/all/0/1\">Anthony Gruber</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gunzburger_M/0/1/0/all/0/1\">Max Gunzburger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ju_L/0/1/0/all/0/1\">Lili Ju</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Teng_Y/0/1/0/all/0/1\">Yuankai Teng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1\">Zhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Series Forecasting of New Cases and New Deaths Rate for COVID-19 using Deep Learning Methods. (arXiv:2104.15007v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.15007","description":"<p>The first known case of Coronavirus disease 2019 (COVID-19) was identified in\nDecember 2019. It has spread worldwide, leading to an ongoing pandemic, imposed\nrestrictions and costs to many countries. Predicting the number of new cases\nand deaths during this period can be a useful step in predicting the costs and\nfacilities required in the future. The purpose of this study is to predict new\ncases and deaths rate one, three and seven-day ahead during the next 100 days.\nThe motivation for predicting every n days (instead of just every day) is the\ninvestigation of the possibility of computational cost reduction and still\nachieving reasonable performance. Such a scenario may be encountered real-time\nforecasting of time series. Six different deep learning methods are examined on\nthe data adopted from the WHO website. Three methods are LSTM, Convolutional\nLSTM, and GRU. The bidirectional extension is then considered for each method\nto forecast the rate of new cases and new deaths in Australia and Iran\ncountries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayoobi_N/0/1/0/all/0/1\">Nooshin Ayoobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1\">Hossein Moosaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chofreh_A/0/1/0/all/0/1\">Abdoulmohammad Gholamzadeh Chofreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goni_F/0/1/0/all/0/1\">Feybi Ariani Goni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemes_J/0/1/0/all/0/1\">Jiri Jaromir Klemes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Training Collaborative Object Detectors Adaptive to Bandwidth and Computation. (arXiv:2105.00591v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00591","description":"<p>In the past few years, mobile deep-learning deployment progressed by leaps\nand bounds, but solutions still struggle to accommodate its severe and\nfluctuating operational restrictions, which include bandwidth, latency,\ncomputation, and energy. In this work, we help to bridge that gap, introducing\nthe first configurable solution for object detection that manages the triple\ncommunication-computation-accuracy trade-off with a single set of weights. Our\nsolution shows state-of-the-art results on COCO-2017, adding only a minor\npenalty on the base EfficientDet-D2 architecture. Our design is robust to the\nchoice of base architecture and compressor and should adapt well for future\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Assine_J/0/1/0/all/0/1\">Juliano S. Assine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filho_J/0/1/0/all/0/1\">J. C. S. Santos Filho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Context-Aware Translation Models Pay the Right Attention?. (arXiv:2105.06977v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06977","description":"<p>Context-aware machine translation models are designed to leverage contextual\ninformation, but often fail to do so. As a result, they inaccurately\ndisambiguate pronouns and polysemous words that require context for resolution.\nIn this paper, we ask several questions: What contexts do human translators use\nto resolve ambiguous words? Are models paying large amounts of attention to the\nsame context? What if we explicitly train them to do so? To answer these\nquestions, we introduce SCAT (Supporting Context for Ambiguous Translations), a\nnew English-French dataset comprising supporting context words for 14K\ntranslations that professional translators found useful for pronoun\ndisambiguation. Using SCAT, we perform an in-depth analysis of the context used\nto disambiguate, examining positional and lexical characteristics of the\nsupporting words. Furthermore, we measure the degree of alignment between the\nmodel's attention scores and the supporting context from SCAT, and apply a\nguided attention strategy to encourage agreement between the two.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1\">Kayo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCAP: Deep Cross Attentional Product Network for User Response Prediction. (arXiv:2105.08649v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.08649","description":"<p>User response prediction, which aims to predict the probability that a user\nwill provide a predefined positive response in a given context such as clicking\non an ad or purchasing an item, is crucial to many industrial applications such\nas online advertising, recommender systems, and search ranking. However, due to\nthe high dimensionality and super sparsity of the data collected in these\ntasks, handcrafting cross features is inevitably time expensive. Prior studies\nin predicting user response leveraged the feature interactions by enhancing\nfeature vectors with products of features to model second-order or high-order\ncross features, either explicitly or implicitly. Nevertheless, these existing\nmethods can be hindered by not learning sufficient cross features due to model\narchitecture limitations or modeling all high-order feature interactions with\nequal weights. This work aims to fill this gap by proposing a novel\narchitecture Deep Cross Attentional Product Network (DCAP), which keeps cross\nnetwork's benefits in modeling high-order feature interactions explicitly at\nthe vector-wise level. Beyond that, it can differentiate the importance of\ndifferent cross features in each network layer inspired by the multi-head\nattention mechanism and Product Neural Network (PNN), allowing practitioners to\nperform a more in-depth analysis of user behaviors. Additionally, our proposed\nmodel can be easily implemented and train in parallel. We conduct comprehensive\nexperiments on three real-world datasets. The results have robustly\ndemonstrated that our proposed model DCAP achieves superior prediction\nperformance compared with the state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zekai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangtian Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pless_R/0/1/0/all/0/1\">Robert Pless</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiuzhen Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning and Certification under Instance-targeted Poisoning. (arXiv:2105.08709v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.08709","description":"<p>In this paper, we study PAC learnability and certification of predictions\nunder instance-targeted poisoning attacks, where the adversary who knows the\ntest instance may change a fraction of the training set with the goal of\nfooling the learner at the test instance. Our first contribution is to\nformalize the problem in various settings and to explicitly model subtle\naspects such as the proper or improper nature of the learning, learner's\nrandomness, and whether (or not) adversary's attack can depend on it. Our main\nresult shows that when the budget of the adversary scales sublinearly with the\nsample complexity, (improper) PAC learnability and certification are\nachievable; in contrast, when the adversary's budget grows linearly with the\nsample complexity, the adversary can potentially drive up the expected 0-1 loss\nto one. We also study distribution-specific PAC learning in the same attack\nmodel and show that proper learning with certification is possible for learning\nhalf spaces under natural distributions. Finally, we empirically study the\nrobustness of K nearest neighbour, logistic regression, multi-layer perceptron,\nand convolutional neural network on real data sets against targeted-poisoning\nattacks. Our experimental results show that many models, especially\nstate-of-the-art neural networks, are indeed vulnerable to these strong\nattacks. Interestingly, we observe that methods with high standard accuracy\nmight be more vulnerable to instance-targeted poisoning attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Ji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1\">Amin Karbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmoody_M/0/1/0/all/0/1\">Mohammad Mahmoody</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Perspective Anomaly Detection. (arXiv:2105.09903v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09903","description":"<p>Anomaly detection is a critical problem in the manufacturing industry. In\nmany applications, images of objects to be analyzed are captured from multiple\nperspectives which can be exploited to improve the robustness of anomaly\ndetection. In this work, we build upon the deep support vector data description\nalgorithm and address multi-perspective anomaly detection using three different\nfusion techniques, i.e., early fusion, late fusion, and late fusion with\nmultiple decoders. We employ different augmentation techniques with a denoising\nprocess to deal with scarce one-class data, which further improves the\nperformance (ROC AUC $= 80\\%$). Furthermore, we introduce the dices dataset,\nwhich consists of over 2000 grayscale images of falling dices from multiple\nperspectives, with 5\\% of the images containing rare anomalies (e.g., drill\nholes, sawing, or scratches). We evaluate our approach on the new dices dataset\nusing images from two different perspectives and also benchmark on the standard\nMNIST dataset. Extensive experiments demonstrate that our proposed\n{multi-perspective} approach exceeds the state-of-the-art {single-perspective\nanomaly detection on both the MNIST and dices datasets}. To the best of our\nknowledge, this is the first work that focuses on addressing multi-perspective\nanomaly detection in images by jointly using different perspectives together\nwith one single objective function for anomaly detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakob_P/0/1/0/all/0/1\">Peter Jakob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_M/0/1/0/all/0/1\">Manav Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_Schirling_T/0/1/0/all/0/1\">Tobias Schmid-Schirling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific Caching. (arXiv:2105.10554v2 [cs.AR] UPDATED)","link":"http://arxiv.org/abs/2105.10554","description":"<p>Graph neural networks (GNN) analysis engines are vital for real-world\nproblems that use large graph models. Challenges for a GNN hardware platform\ninclude the ability to (a) host a variety of GNNs, (b) handle high sparsity in\ninput vertex feature vectors and the graph adjacency matrix and the\naccompanying random memory access patterns, and (c) maintain load-balanced\ncomputation in the face of uneven workloads, induced by high sparsity and\npower-law vertex degree distributions. This paper proposes GNNIE, an\naccelerator designed to run a broad range of GNNs. It tackles workload\nimbalance by (i)~splitting vertex feature operands into blocks, (ii)~reordering\nand redistributing computations, (iii)~using a novel flexible MAC architecture.\nIt adopts a graph-specific, degree-aware caching policy that is well suited to\nreal-world graph characteristics. The policy enhances on-chip data reuse and\navoids random memory access to DRAM.\n</p>\n<p>GNNIE achieves average speedups of 21233x over a CPU and 699x over a GPU over\nmultiple datasets on graph attention networks (GATs), graph convolutional\nnetworks (GCNs), GraphSAGE, GINConv, and DiffPool. Compared to prior\napproaches, GNNIE achieves an average speedup of 35x over HyGCN (which cannot\nimplement GATs) for GCN, GraphSAGE, and GINConv, and, using 3.4x fewer\nprocessing units, an average speedup of 2.1x over AWB-GCN (which runs only\nGCNs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1\">Sudipta Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manasi_S/0/1/0/all/0/1\">Susmita Dey Manasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunal_K/0/1/0/all/0/1\">Kishor Kunal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramprasath_S/0/1/0/all/0/1\">S. Ramprasath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapatnekar_S/0/1/0/all/0/1\">Sachin S. Sapatnekar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a method to anticipate dark matter signals with deep learning at the LHC. (arXiv:2105.12018v2 [hep-ph] UPDATED)","link":"http://arxiv.org/abs/2105.12018","description":"<p>We study several simplified dark matter (DM) models and their signatures at\nthe LHC using neural networks. We focus on the usual monojet plus missing\ntransverse energy channel, but to train the algorithms we organize the data in\n2D histograms instead of event-by-event arrays. This results in a large\nperformance boost to distinguish between standard model (SM) only and SM plus\nnew physics signals. We use the kinematic monojet features as input data which\nallow us to describe families of models with a single data sample. We found\nthat the neural network performance does not depend on the simulated number of\nbackground events if they are presented as a function of $S/\\sqrt{B}$, where\n$S$ and $B$ are the number of signal and background events per histogram,\nrespectively. This provides flexibility to the method, since testing a\nparticular model in that case only requires knowing the new physics monojet\ncross section. Furthermore, we also discuss the network performance under\nincorrect assumptions about the true DM nature. Finally, we propose multimodel\nclassifiers to search and identify new signals in a more general way, for the\nnext LHC run.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/hep-ph/1/au:+Arganda_E/0/1/0/all/0/1\">Ernesto Arganda</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Medina_A/0/1/0/all/0/1\">Anibal D. Medina</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Perez_A/0/1/0/all/0/1\">Andres D. Perez</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Szynkman_A/0/1/0/all/0/1\">Alejandro Szynkman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-T: Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.15082","description":"<p>Mixture-of-Experts (MoE) models can achieve promising results with outrageous\nlarge amount of parameters but constant computation cost, and thus it has\nbecome a trend in model scaling. Still it is a mystery how MoE layers bring\nquality gains by leveraging the parameters with sparse activation. In this\nwork, we investigate several key factors in sparse expert models. We observe\nthat load imbalance may not be a significant problem affecting model quality,\ncontrary to the perspectives of recent studies, while the number of sparsely\nactivated experts $k$ and expert capacity $C$ in top-$k$ routing can\nsignificantly make a difference in this context. Furthermore, we take a step\nforward to propose a simple method called expert prototyping that splits\nexperts into different prototypes and applies $k$ top-$1$ routing. This\nstrategy improves the model quality but maintains constant computational costs,\nand our further exploration on extremely large-scale models reflects that it is\nmore effective in training larger models. We push the model scale to over $1$\ntrillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in\ncomparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model\nachieves substantial speedup in convergence over the same-size baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1\">Rui Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiamang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lin Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Quantum Potentials by Deep Neural Network and Metropolis Sampling. (arXiv:2106.03126v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2106.03126","description":"<p>The hybridizations of machine learning and quantum physics have caused\nessential impacts to the methodology in both fields. Inspired by quantum\npotential neural network, we here propose to solve the potential in the\nSchrodinger equation provided the eigenstate, by combining Metropolis sampling\nwith deep neural network, which we dub as Metropolis potential neural network\n(MPNN). A loss function is proposed to explicitly involve the energy in the\noptimization for its accurate evaluation. Benchmarking on the harmonic\noscillator and hydrogen atom, MPNN shows excellent accuracy and stability on\npredicting not just the potential to satisfy the Schrodinger equation, but also\nthe eigen-energy. Our proposal could be potentially applied to the ab-initio\nsimulations, and to inversely solving other partial differential equations in\nphysics and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Hong_R/0/1/0/all/0/1\">Rui Hong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhou_P/0/1/0/all/0/1\">Peng-Fei Zhou</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xi_B/0/1/0/all/0/1\">Bin Xi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ji_A/0/1/0/all/0/1\">An-Chun Ji</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Score Matching Model for Unbounded Data Score. (arXiv:2106.05527v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05527","description":"<p>Recent advance in diffusion models incorporates the Stochastic Differential\nEquation (SDE), which brings the state-of-the art performance on image\ngeneration tasks. This paper improves such diffusion models by analyzing the\nmodel at the zero diffusion time. In real datasets, the score function diverges\nas the diffusion time ($t$) decreases to zero, and this observation leads an\nargument that the score estimation fails at $t=0$ with any neural network\nstructure. Subsequently, we introduce Unbounded Diffusion Model (UDM) that\nresolves the score diverging problem with an easily applicable modification to\nany diffusion models. Additionally, we introduce a new SDE that overcomes the\ntheoretic and practical limitations of Variance Exploding SDE. On top of that,\nthe introduced Soft Truncation method improves the sample quality by mitigating\nthe loss scale issue that happens at $t=0$. We further provide a theoretic\nresult of the proposed method to uncover the behind mechanism of the diffusion\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongjun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungjae Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyungwoo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wanmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_I/0/1/0/all/0/1\">Il-Chul Moon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Based Reinforcement Learning via Latent-Space Collocation. (arXiv:2106.13229v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13229","description":"<p>The ability to plan into the future while utilizing only raw high-dimensional\nobservations, such as images, can provide autonomous agents with broad\ncapabilities. Visual model-based reinforcement learning (RL) methods that plan\nfuture actions directly have shown impressive results on tasks that require\nonly short-horizon reasoning, however, these methods struggle on temporally\nextended tasks. We argue that it is easier to solve long-horizon tasks by\nplanning sequences of states rather than just actions, as the effects of\nactions greatly compound over time and are harder to optimize. To achieve this,\nwe draw on the idea of collocation, which has shown good results on\nlong-horizon tasks in optimal control literature, and adapt it to the\nimage-based setting by utilizing learned latent state space models. The\nresulting latent collocation method (LatCo) optimizes trajectories of latent\nstates, which improves over previously proposed shooting methods for visual\nmodel-based RL on tasks with sparse rewards and long-term goals. Videos and\ncode at https://orybkin.github.io/latco/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rybkin_O/0/1/0/all/0/1\">Oleh Rybkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chuning Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagabandi_A/0/1/0/all/0/1\">Anusha Nagabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Limitations of machine learning for building energy prediction. (arXiv:2106.13475v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13475","description":"<p>Machine learning for building energy prediction has exploded in popularity in\nrecent years, yet understanding its limitations and potential for improvement\nare lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition\nwas the largest building energy meter machine learning competition ever held\nwith 4,370 participants who submitted 39,403 predictions. The test data set\nincluded two years of hourly electricity, hot water, chilled water, and steam\nreadings from 2,380 meters in 1,448 buildings at 16 locations. This paper\nanalyzes the various sources and types of residual model error from an\naggregation of the competition's top 50 solutions. This analysis reveals the\nlimitations for machine learning using the standard model inputs of historical\nmeter, weather, and basic building metadata. The types of error are classified\naccording to the amount of time errors occur in each instance, abrupt versus\ngradual behavior, the magnitude of error, and whether the error existed on\nsingle buildings or several buildings at once from a single location. The\nresults show machine learning models have errors within a range of\nacceptability on 79.1% of the test data. Lower magnitude model errors occur in\n16.1% of the test data. These discrepancies can likely be addressed through\nadditional training data sources or innovations in machine learning. Higher\nmagnitude errors occur in 4.8% of the test data and are unlikely to be\naccurately predicted regardless of innovation. There is a diversity of error\nbehavior depending on the energy meter type (electricity prediction models have\nunacceptable error in under 10% of test data, while hot water is over 60%) and\nbuilding use type (public service less than 14%, while technology/science is\njust over 46%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Clayton Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picchetti_B/0/1/0/all/0/1\">Bianca Picchetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantelic_J/0/1/0/all/0/1\">Jovan Pantelic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ranger21: a synergistic deep learning optimizer. (arXiv:2106.13731v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13731","description":"<p>As optimizers are critical to the performances of neural networks, every year\na large number of papers innovating on the subject are published. However,\nwhile most of these publications provide incremental improvements to existing\nalgorithms, they tend to be presented as new optimizers rather than composable\nalgorithms. Thus, many worthwhile improvements are rarely seen out of their\ninitial publication. Taking advantage of this untapped potential, we introduce\nRanger21, a new optimizer which combines AdamW with eight components, carefully\nselected after reviewing and testing ideas from the literature. We found that\nthe resulting optimizer provides significantly improved validation accuracy and\ntraining speed, smoother training curves, and is even able to train a ResNet50\non ImageNet2012 without Batch Normalization layers. A problem on which AdamW\nstays systematically stuck in a bad initial state.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wright_L/0/1/0/all/0/1\">Less Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeure_N/0/1/0/all/0/1\">Nestor Demeure</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.14463","description":"<p>Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Dimensionality Reduction for Comparative Analysis. (arXiv:2106.15481v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.15481","description":"<p>Finding the similarities and differences between groups of datasets is a\nfundamental analysis task. For high-dimensional data, dimensionality reduction\n(DR) methods are often used to find the characteristics of each group. However,\nexisting DR methods provide limited capability and flexibility for such\ncomparative analysis as each method is designed only for a narrow analysis\ntarget, such as identifying factors that most differentiate groups. This paper\npresents an interactive DR framework where we integrate our new DR method,\ncalled ULCA (unified linear comparative analysis), with an interactive visual\ninterface. ULCA unifies two DR schemes, discriminant analysis and contrastive\nlearning, to support various comparative analysis tasks. To provide flexibility\nfor comparative analysis, we develop an optimization algorithm that enables\nanalysts to interactively refine ULCA results. Additionally, the interactive\nvisualization interface facilitates interpretation and refinement of the ULCA\nresults. We evaluate ULCA and the optimization algorithm to show their\nefficiency as well as present multiple case studies using real-world datasets\nto demonstrate the usefulness of this framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_T/0/1/0/all/0/1\">Takanori Fujiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinhai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kwan-Liu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Source Separation in Polyphonic Music Recordings Using Deep Neural Networks Trained via Policy Gradients. (arXiv:2107.04235v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.04235","description":"<p>We propose a method for the blind separation of sounds of musical instruments\nin audio signals. We describe the individual tones via a parametric model,\ntraining a dictionary to capture the relative amplitudes of the harmonics. The\nmodel parameters are predicted via a U-Net, which is a type of deep neural\nnetwork. The network is trained without ground truth information, based on the\ndifference between the model prediction and the individual time frames of the\nshort-time Fourier transform. Since some of the model parameters do not yield a\nuseful backpropagation gradient, we model them stochastically and employ the\npolicy gradient instead. To provide phase information and account for\ninaccuracies in the dictionary-based representation, we also let the network\noutput a direct prediction, which we then use to resynthesize the audio signals\nfor the individual instruments. Due to the flexibility of the neural network,\ninharmonicity can be incorporated seamlessly and no preprocessing of the input\nspectra is required. Our algorithm yields high-quality separation results with\nparticularly low interference on a variety of different audio samples, both\nacoustic and synthetic, provided that the sample contains enough data for the\ntraining and that the spectral characteristics of the musical instruments are\nsufficiently stable to be approximated by the dictionary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schulze_S/0/1/0/all/0/1\">S&#xf6;ren Schulze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leuschner_J/0/1/0/all/0/1\">Johannes Leuschner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_E/0/1/0/all/0/1\">Emily J. King</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Accurate Human Activity Recognition for Embedded Devices Using Multi-level Distillation. (arXiv:2107.07331v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.07331","description":"<p>Human Activity Recognition (HAR) based on IMU sensors is a crucial area in\nubiquitous computing. Because of the trend of deploying AI on IoT devices or\nsmartphones, more researchers are designing different HAR models for embedded\ndevices. Deployment of models in embedded devices can help enhance the\nefficiency of HAR. We propose a multi-level HAR modeling pipeline called\nStage-Logits-Memory Distillation (SMLDist) for constructing deep convolutional\nHAR models with embedded hardware support. SMLDist includes stage distillation,\nmemory distillation, and logits distillation. Stage distillation constrains the\nlearning direction of the intermediate features. The teacher model teaches the\nstudent models how to explain and store the inner relationship among\nhigh-dimensional features based on Hopfield networks in memory distillation.\nLogits distillation builds logits distilled by a smoothed conditional rule to\npreserve the probability distribution and enhance the softer target accuracy.\nWe compare the accuracy, F1 macro score, and energy cost on embedded platforms\nof a MobileNet V3 model built by our SMLDist with those of various\nstate-of-the-art HAR frameworks. The product model has a good balance with\nrobustness and efficiency. SMLDist can also compress models with a minor\nperformance loss at an equal compression ratio to other advanced knowledge\ndistillation methods on seven public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haiyong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuechun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhiqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yida Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomy 2.0: Why is self-driving always 5 years away?. (arXiv:2107.08142v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2107.08142","description":"<p>Despite the numerous successes of machine learning over the past decade\n(image recognition, decision-making, NLP, image synthesis), self-driving\ntechnology has not yet followed the same trend. In this paper, we study the\nhistory, composition, and development bottlenecks of the modern self-driving\nstack. We argue that the slow progress is caused by approaches that require too\nmuch hand-engineering, an over-reliance on road testing, and high fleet\ndeployment costs. We observe that the classical stack has several bottlenecks\nthat preclude the necessary scale needed to capture the long tail of rare\nevents. To resolve these problems, we outline the principles of Autonomy 2.0,\nan ML-first approach to self-driving, as a viable alternative to the currently\nadopted state-of-the-art. This approach is based on (i) a fully differentiable\nAV stack trainable from human demonstrations, (ii) closed-loop data-driven\nreactive simulation, and (iii) large-scale, low-cost data collections as\ncritical solutions towards scalability issues. We outline the general\narchitecture, survey promising works in this direction and propose key\nchallenges to be addressed by the community in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ashesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pero_L/0/1/0/all/0/1\">Luca Del Pero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmett_H/0/1/0/all/0/1\">Hugo Grimmett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondruska_P/0/1/0/all/0/1\">Peter Ondruska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Undervolting as an On-Device Defense Against Adversarial Machine Learning Attacks. (arXiv:2107.09804v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2107.09804","description":"<p>Deep neural network (DNN) classifiers are powerful tools that drive a broad\nspectrum of important applications, from image recognition to autonomous\nvehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks\nthat affect virtually all state-of-the-art models. These attacks make small\nimperceptible modifications to inputs that are sufficient to induce the DNNs to\nproduce the wrong classification.\n</p>\n<p>In this paper we propose a novel, lightweight adversarial correction and/or\ndetection mechanism for image classifiers that relies on undervolting (running\na chip at a voltage that is slightly below its safe margin). We propose using\ncontrolled undervolting of the chip running the inference process in order to\nintroduce a limited number of compute errors. We show that these errors disrupt\nthe adversarial input in a way that can be used either to correct the\nclassification or detect the input as adversarial. We evaluate the proposed\nsolution in an FPGA design and through software simulation. We evaluate 10\nattacks and show average detection rates of 77% and 90% on two popular DNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Saikat Majumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1\">Mohammad Hossein Samavatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1\">Kristin Barber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1\">Radu Teodorescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.10234","description":"<p>Deep learning's performance has been extensively recognized recently. Graph\nneural networks (GNNs) are designed to deal with graph-structural data that\nclassical deep learning does not easily manage. Since most GNNs were created\nusing distinct theories, direct comparisons are impossible. Prior research has\nprimarily concentrated on categorizing existing models, with little attention\npaid to their intrinsic connections. The purpose of this study is to establish\na unified framework that integrates GNNs based on spectral graph and\napproximation theory. The framework incorporates a strong integration between\nspatial- and spectral-based GNNs while tightly associating approaches that\nexist within each respective domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fanglan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1\">Taoran Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kaiqun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1\">Charu Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chang-Tien Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Dimensional Differentially Private Stochastic Optimization with Heavy-tailed Data. (arXiv:2107.11136v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.11136","description":"<p>As one of the most fundamental problems in machine learning, statistics and\ndifferential privacy, Differentially Private Stochastic Convex Optimization\n(DP-SCO) has been extensively studied in recent years. However, most of the\nprevious work can only handle either regular data distribution or irregular\ndata in the low dimensional space case. To better understand the challenges\narising from irregular data distribution, in this paper we provide the first\nstudy on the problem of DP-SCO with heavy-tailed data in the high dimensional\nspace. In the first part we focus on the problem over some polytope constraint\n(such as the $\\ell_1$-norm ball). We show that if the loss function is smooth\nand its gradient has bounded second order moment, it is possible to get a (high\nprobability) error bound (excess population risk) of $\\tilde{O}(\\frac{\\log\nd}{(n\\epsilon)^\\frac{1}{3}})$ in the $\\epsilon$-DP model, where $n$ is the\nsample size and $d$ is the dimensionality of the underlying space. Next, for\nLASSO, if the data distribution that has bounded fourth-order moments, we\nimprove the bound to $\\tilde{O}(\\frac{\\log d}{(n\\epsilon)^\\frac{2}{5}})$ in the\n$(\\epsilon, \\delta)$-DP model. In the second part of the paper, we study sparse\nlearning with heavy-tailed data. We first revisit the sparse linear model and\npropose a truncated DP-IHT method whose output could achieve an error of\n$\\tilde{O}(\\frac{s^{*2}\\log d}{n\\epsilon})$, where $s^*$ is the sparsity of the\nunderlying parameter. Then we study a more general problem over the sparsity\n({\\em i.e.,} $\\ell_0$-norm) constraint, and show that it is possible to achieve\nan error of $\\tilde{O}(\\frac{s^{*\\frac{3}{2}}\\log d}{n\\epsilon})$, which is\nalso near optimal up to a factor of $\\tilde{O}{(\\sqrt{s^*})}$, if the loss\nfunction is smooth and strongly convex.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lijie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_S/0/1/0/all/0/1\">Shuo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Hanshen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11769","description":"<p>Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decentralized Federated Learning: Balancing Communication and Computing Costs. (arXiv:2107.12048v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.12048","description":"<p>Decentralized federated learning (DFL) is a powerful framework of distributed\nmachine learning and decentralized stochastic gradient descent (SGD) is a\ndriving engine for DFL. The performance of decentralized SGD is jointly\ninfluenced by communication-efficiency and convergence rate. In this paper, we\npropose a general decentralized federated learning framework to strike a\nbalance between communication-efficiency and convergence performance. The\nproposed framework performs both multiple local updates and multiple inter-node\ncommunications periodically, unifying traditional decentralized SGD methods. We\nestablish strong convergence guarantees for the proposed DFL algorithm without\nthe assumption of convex objective function. The balance of communication and\ncomputation rounds is essential to optimize decentralized federated learning\nunder constrained communication and computation resources. For further\nimproving communication-efficiency of DFL, compressed communication is applied\nto DFL, named DFL with compressed communication (C-DFL). The proposed C-DFL\nexhibits linear convergence for strongly convex objectives. Experiment results\nbased on MNIST and CIFAR-10 datasets illustrate the superiority of DFL over\ntraditional decentralized SGD methods and show that C-DFL further enhances\ncommunication-efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenyi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting. (arXiv:2107.12619v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12619","description":"<p>Recently, the problem of inaccurate learning targets in crowd counting draws\nincreasing attention. Inspired by a few pioneering work, we solve this problem\nby trying to predict the indices of pre-defined interval bins of counts instead\nof the count values themselves. However, an inappropriate interval setting\nmight make the count error contributions from different intervals extremely\nimbalanced, leading to inferior counting performance. Therefore, we propose a\nnovel count interval partition criterion called Uniform Error Partition (UEP),\nwhich always keeps the expected counting error contributions equal for all\nintervals to minimize the prediction risk. Then to mitigate the inevitably\nintroduced discretization errors in the count quantization process, we propose\nanother criterion called Mean Count Proxies (MCP). The MCP criterion selects\nthe best count proxy for each interval to represent its count value during\ninference, making the overall expected discretization error of an image nearly\nnegligible. As far as we are aware, this work is the first to delve into such a\nclassification task and ends up with a promising solution for count interval\npartition. Following the above two theoretically demonstrated criterions, we\npropose a simple yet effective model termed Uniform Error Partition Network\n(UEPNet), which achieves state-of-the-art performance on several challenging\ndatasets. The codes will be available at:\nhttps://github.com/TencentYoutuResearch/CrowdCounting-UEPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qingyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Boshen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yabiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12651","description":"<p>Language bias is a critical issue in Visual Question Answering (VQA), where\nmodels often exploit dataset biases for the final decision without considering\nthe image information. As a result, they suffer from performance drop on\nout-of-distribution data and inadequate visual explanation. Based on\nexperimental analysis for existing robust VQA methods, we stress the language\nbias in VQA that comes from two aspects, i.e., distribution bias and shortcut\nbias. We further propose a new de-bias framework, Greedy Gradient Ensemble\n(GGE), which combines multiple biased models for unbiased base model learning.\nWith the greedy strategy, GGE forces the biased models to over-fit the biased\ndata distribution in priority, thus makes the base model pay more attention to\nexamples that are hard to solve by biased models. The experiments demonstrate\nthat our method makes better use of visual information and achieves\nstate-of-the-art performance on diagnosing dataset VQA-CP without using extra\nannotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MARViN -- Multiple Arithmetic Resolutions Vacillating in Neural Networks. (arXiv:2107.13490v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.13490","description":"<p>Quantization is a technique for reducing deep neural networks (DNNs) training\nand inference times, which is crucial for training in resource constrained\nenvironments or time critical inference applications. State-of-the-art (SOTA)\nquantization approaches focus on post-training quantization, i.e. quantization\nof pre-trained DNNs for speeding up inference. Very little work on quantized\ntraining exists, which neither al-lows dynamic intra-epoch precision switches\nnor em-ploys an information theory based switching heuristic. Usually, existing\napproaches require full precision refinement afterwards and enforce a global\nword length across the whole DNN. This leads to suboptimal quantization\nmappings and resource usage. Recognizing these limits, we introduce MARViN, a\nnew quantized training strategy using information theory-based intra-epoch\nprecision switching, which decides on a per-layer basis which precision should\nbe used in order to minimize quantization-induced information loss. Note that\nany quantization must leave enough precision such that future learning steps do\nnot suffer from vanishing gradients. We achieve an average speedup of 1.86\ncompared to a float32 basis while limiting mean accuracy degradation on\nAlexNet/ResNet to only -0.075%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kummer_L/0/1/0/all/0/1\">Lorenz Kummer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sidak_K/0/1/0/all/0/1\">Kevin Sidak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichmann_T/0/1/0/all/0/1\">Tabea Reichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gansterer_W/0/1/0/all/0/1\">Wilfried Gansterer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills. (arXiv:2107.14483v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14483","description":"<p>Learning generalizable manipulation skills is central for robots to achieve\ntask automation in environments with endless scene and object variations.\nHowever, existing robot learning environments are limited in both scale and\ndiversity of 3D assets (especially of articulated objects), making it difficult\nto train and evaluate the generalization ability of agents over novel objects.\nIn this work, we focus on object-level generalization and propose SAPIEN\nManipulation Skill Benchmark (abbreviated as ManiSkill), a large-scale\nlearning-from-demonstrations benchmark for articulated object manipulation with\n3D visual input (point cloud and RGB-D image). ManiSkill supports object-level\nvariations by utilizing a rich and diverse set of articulated objects, and each\ntask is carefully designed for learning manipulations on a single category of\nobjects. We equip ManiSkill with a large number of high-quality demonstrations\nto facilitate learning-from-demonstrations approaches and perform evaluations\non baseline algorithms. We believe that ManiSkill can encourage the robot\nlearning community to explore more on learning generalizable object\nmanipulation skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporation of Deep Neural Network & Reinforcement Learning with Domain Knowledge. (arXiv:2107.14613v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14613","description":"<p>We present a study of the manners by which Domain information has been\nincorporated when building models with Neural Networks. Integrating space data\nis uniquely important to the development of Knowledge understanding model, as\nwell as other fields that aid in understanding information by utilizing the\nhuman-machine interface and Reinforcement Learning. On numerous such occasions,\nmachine-based model development may profit essentially from the human\ninformation on the world encoded in an adequately exact structure. This paper\ninspects expansive ways to affect encode such information as sensible and\nmathematical limitations and portrays methods and results that came to a couple\nof subcategories under all of those methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karn_A/0/1/0/all/0/1\">Aryan Karn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Ashutosh Acharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faster Rates of Differentially Private Stochastic Convex Optimization. (arXiv:2108.00331v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.00331","description":"<p>In this paper, we revisit the problem of Differentially Private Stochastic\nConvex Optimization (DP-SCO) and provide excess population risks for some\nspecial classes of functions that are faster than the previous results of\ngeneral convex and strongly convex functions. In the first part of the paper,\nwe study the case where the population risk function satisfies the Tysbakov\nNoise Condition (TNC) with some parameter $\\theta&gt;1$. Specifically, we first\nshow that under some mild assumptions on the loss functions, there is an\nalgorithm whose output could achieve an upper bound of\n$\\tilde{O}((\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\log\n\\frac{1}{\\delta}}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $(\\epsilon,\n\\delta)$-DP when $\\theta\\geq 2$, here $n$ is the sample size and $d$ is the\ndimension of the space. Then we address the inefficiency issue, improve the\nupper bounds by $\\text{Poly}(\\log n)$ factors and extend to the case where\n$\\theta\\geq \\bar{\\theta}&gt;1$ for some known $\\bar{\\theta}$. Next we show that\nthe excess population risk of population functions satisfying TNC with\nparameter $\\theta&gt;1$ is always lower bounded by\n$\\Omega((\\frac{d}{n\\epsilon})^\\frac{\\theta}{\\theta-1}) $ and\n$\\Omega((\\frac{\\sqrt{d\\log\n\\frac{1}{\\delta}}}{n\\epsilon})^\\frac{\\theta}{\\theta-1})$ for $\\epsilon$-DP and\n$(\\epsilon, \\delta)$-DP, respectively. In the second part, we focus on a\nspecial case where the population risk function is strongly convex. Unlike the\nprevious studies, here we assume the loss function is {\\em non-negative} and\n{\\em the optimal value of population risk is sufficiently small}. With these\nadditional assumptions, we propose a new method whose output could achieve an\nupper bound of\n$O(\\frac{d\\log\\frac{1}{\\delta}}{n^2\\epsilon^2}+\\frac{1}{n^{\\tau}})$ for any\n$\\tau\\geq 1$ in $(\\epsilon,\\delta)$-DP model if the sample size $n$ is\nsufficiently large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinyan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Piecewise Linear Units Improve Deep Neural Networks. (arXiv:2108.00700v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.00700","description":"<p>The activation function is at the heart of a deep neural networks\nnonlinearity; the choice of the function has great impact on the success of\ntraining. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)\ndue to its simplicity and reliability, despite its few drawbacks. While most\nprevious functions proposed to supplant ReLU have been hand-designed, recent\nwork on learning the function during training has shown promising results. In\nthis paper we propose an adaptive piecewise linear activation function, the\nPiecewise Linear Unit (PiLU), which can be learned independently for each\ndimension of the neural network. We demonstrate how PiLU is a generalised\nrectifier unit and note its similarities with the Adaptive Piecewise Linear\nUnits, namely adaptive and piecewise linear. Across a distribution of 30\nexperiments, we show that for the same model architecture, hyperparameters, and\npre-processing, PiLU significantly outperforms ReLU: reducing classification\nerror by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in\nthe number of neurons. Further work should be dedicated to exploring\ngeneralised piecewise linear units, as well as verifying these results across\nother challenging domains and larger problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inturrisi_J/0/1/0/all/0/1\">Jordan Inturrisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khoo_S/0/1/0/all/0/1\">Sui Yang Khoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1\">Abbas Kouzani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagliarella_R/0/1/0/all/0/1\">Riccardo Pagliarella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Machine learning approach for rapid disaster response based on multi-modal data. The case of housing & shelter needs. (arXiv:2108.00887v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.00887","description":"<p>Along with climate change, more frequent extreme events, such as flooding and\ntropical cyclones, threaten the livelihoods and wellbeing of poor and\nvulnerable populations. One of the most immediate needs of people affected by a\ndisaster is finding shelter. While the proliferation of data on disasters is\nalready helping to save lives, identifying damages in buildings, assessing\nshelter needs, and finding appropriate places to establish emergency shelters\nor settlements require a wide range of data to be combined rapidly. To address\nthis gap and make a headway in comprehensive assessments, this paper proposes a\nmachine learning workflow that aims to fuse and rapidly analyse multimodal\ndata. This workflow is built around open and online data to ensure scalability\nand broad accessibility. Based on a database of 19 characteristics for more\nthan 200 disasters worldwide, a fusion approach at the decision level was used.\nThis technique allows the collected multimodal data to share a common semantic\nspace that facilitates the prediction of individual variables. Each fused\nnumerical vector was fed into an unsupervised clustering algorithm called\nSelf-Organizing-Maps (SOM). The trained SOM serves as a predictor for future\ncases, allowing predicting consequences such as total deaths, total people\naffected, and total damage, and provides specific recommendations for\nassessments in the shelter and housing sector. To achieve such prediction, a\nsatellite image from before the disaster and the geographic and demographic\nconditions are shown to the trained model, which achieved a prediction accuracy\nof 62 %\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_K/0/1/0/all/0/1\">Karla Saldana Ochoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comes_T/0/1/0/all/0/1\">Tina Comes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GalaxAI: Machine learning toolbox for interpretable analysis of spacecraft telemetry data. (arXiv:2108.01407v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.01407","description":"<p>We present GalaxAI - a versatile machine learning toolbox for efficient and\ninterpretable end-to-end analysis of spacecraft telemetry data. GalaxAI employs\nvarious machine learning algorithms for multivariate time series analyses,\nclassification, regression and structured output prediction, capable of\nhandling high-throughput heterogeneous data. These methods allow for the\nconstruction of robust and accurate predictive models, that are in turn applied\nto different tasks of spacecraft monitoring and operations planning. More\nimportantly, besides the accurate building of models, GalaxAI implements a\nvisualisation layer, providing mission specialists and operators with a full,\ndetailed and interpretable view of the data analysis process. We show the\nutility and versatility of GalaxAI on two use-cases concerning two different\nspacecraft: i) analysis and planning of Mars Express thermal power consumption\nand ii) predicting of INTEGRAL's crossings through Van Allen belts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kostovska_A/0/1/0/all/0/1\">Ana Kostovska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petkovic_M/0/1/0/all/0/1\">Matej Petkovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepisnik_T/0/1/0/all/0/1\">Toma&#x17e; Stepi&#x161;nik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_L/0/1/0/all/0/1\">Luke Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1\">Timothy Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Heras_J/0/1/0/all/0/1\">Jos&#xe9; Mart&#xed;nez-Heras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panov_P/0/1/0/all/0/1\">Pan&#x10d;e Panov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1\">Sa&#x161;o D&#x17e;eroski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1\">Alessandro Donati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1\">Nikola Simidjievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1\">Dragi Kocev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Federated Edge Learning (MtFEEL) in Wireless Networks. (arXiv:2108.02517v2 [cs.IT] UPDATED)","link":"http://arxiv.org/abs/2108.02517","description":"<p>Federated Learning (FL) has evolved as a promising technique to handle\ndistributed machine learning across edge devices. A single neural network (NN)\nthat optimises a global objective is generally learned in most work in FL,\nwhich could be suboptimal for edge devices. Although works finding a NN\npersonalised for edge device specific tasks exist, they lack generalisation\nand/or convergence guarantees. In this paper, a novel communication efficient\nFL algorithm for personalised learning in a wireless setting with guarantees is\npresented. The algorithm relies on finding a ``better`` empirical estimate of\nlosses at each device, using a weighted average of the losses across different\ndevices. It is devised from a Probably Approximately Correct (PAC) bound on the\ntrue loss in terms of the proposed empirical loss and is bounded by (i) the\nRademacher complexity, (ii) the discrepancy, (iii) and a penalty term. Using a\nsigned gradient feedback to find a personalised NN at each device, it is also\nproven to converge in a Rayleigh flat fading (in the uplink) channel, at a rate\nof the order max{1/SNR,1/sqrt(T)} Experimental results show that the proposed\nalgorithm outperforms locally trained devices as well as the conventionally\nused FedAvg and FedSGD algorithms under practical SNR regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahara_S/0/1/0/all/0/1\">Sawan Singh Mahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M%2E_S/0/1/0/all/0/1\">Shruti M.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharath_B/0/1/0/all/0/1\">B. N. Bharath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.02574","description":"<p>Recently, much progress has been made in unsupervised restoration learning.\nHowever, existing methods more or less rely on some assumptions on the signal\nand/or degradation model, which limits their practical performance. How to\nconstruct an optimal criterion for unsupervised restoration learning without\nany prior knowledge on the degradation model is still an open question. Toward\nanswering this question, this work proposes a criterion for unsupervised\nrestoration learning based on the optimal transport theory. This criterion has\nfavorable properties, e.g., approximately maximal preservation of the\ninformation of the signal, whilst achieving perceptual reconstruction.\nFurthermore, though a relaxed unconstrained formulation is used in practical\nimplementation, we show that the relaxed formulation in theory has the same\nsolution as the original constrained formulation. Experiments on synthetic and\nreal-world data, including realistic photographic, microscopy, depth, and raw\ndepth images, demonstrate that the proposed method even compares favorably with\nsupervised methods, e.g., approaching the PSNR of supervised methods while\nhaving better perceptual quality. Particularly, for spatially correlated noise\nand realistic microscopy images, the proposed method not only achieves better\nperceptual quality but also has higher PSNR than supervised methods. Besides,\nit shows remarkable superiority in harsh practical conditions with complex\nnoise, e.g., raw depth images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1\">Zeyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1\">Rendong Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peilin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Elect. (arXiv:2108.02768v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.02768","description":"<p>Voting systems have a wide range of applications including recommender\nsystems, web search, product design and elections. Limited by the lack of\ngeneral-purpose analytical tools, it is difficult to hand-engineer desirable\nvoting rules for each use case. For this reason, it is appealing to\nautomatically discover voting rules geared towards each scenario. In this\npaper, we show that set-input neural network architectures such as Set\nTransformers, fully-connected graph networks and DeepSets are both\ntheoretically and empirically well-suited for learning voting rules. In\nparticular, we show that these network models can not only mimic a number of\nexisting voting rules to compelling accuracy --- both position-based (such as\nPlurality and Borda) and comparison-based (such as Kemeny, Copeland and\nMaximin) --- but also discover near-optimal voting rules that maximize\ndifferent social welfare functions. Furthermore, the learned voting rules\ngeneralize well to different voter utility distributions and election sizes\nunseen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1\">Cem Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1\">Xuchan Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02941","description":"<p>Disinformation through fake news is an ongoing problem in our society and has\nbecome easily spread through social media. The most cost and time effective way\nto filter these large amounts of data is to use a combination of human and\ntechnical interventions to identify it. From a technical perspective, Natural\nLanguage Processing (NLP) is widely used in detecting fake news. Social media\ncompanies use NLP techniques to identify the fake news and warn their users,\nbut fake news may still slip through undetected. It is especially a problem in\nmore localised contexts (outside the United States of America). How do we\nadjust fake news detection systems to work better for local contexts such as in\nSouth Africa. In this work we investigate fake news detection on South African\nwebsites. We curate a dataset of South African fake news and then train\ndetection models. We contrast this with using widely available fake news\ndatasets (from mostly USA website). We also explore making the datasets more\ndiverse by combining them and observe the differences in behaviour in writing\nbetween nations' fake news using interpretable machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1\">Harm de Wet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1\">Vukosi Marivate</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Modifications to Improve Tabular Neural Networks. (arXiv:2108.03214v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.03214","description":"<p>There is growing interest in neural network architectures for tabular data.\nMany general-purpose tabular deep learning models have been introduced\nrecently, with performance sometimes rivaling gradient boosted decision trees\n(GBDTs). These recent models draw inspiration from various sources, including\nGBDTs, factorization machines, and neural networks from other application\ndomains. Previous tabular neural networks are also drawn upon, but are possibly\nunder-considered, especially models associated with specific tabular problems.\nThis paper focuses on several such models, and proposes modifications for\nimproving their performance. When modified, these models are shown to be\ncompetitive with leading general-purpose tabular models, including GBDTs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fiedler_J/0/1/0/all/0/1\">James Fiedler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can a CNN trained on the Ising model detect the phase transition of the $q$-state Potts model?. (arXiv:2104.03632v3 [cond-mat.dis-nn] CROSS LISTED)","link":"http://arxiv.org/abs/2104.03632","description":"<p>Employing a deep convolutional neural network (deep CNN) trained on spin\nconfigurations of the 2D Ising model and the temperatures, we examine whether\nthe deep CNN can detect the phase transition of the 2D $q$-state Potts model.\nTo this end, we generate binarized images of spin configurations of the\n$q$-state Potts model ($q\\ge 3$) by replacing the spin variables\n$\\{0,1,\\dots,\\lfloor q/2\\rfloor-1\\}$ and $\\{\\lfloor q/2\\rfloor,\\dots,q-1\\}$\nwith $\\{0\\}$ and $\\{1\\}$, respectively. Then, we input these images to the\ntrained CNN to output the predicted temperatures. The binarized images of the\n$q$-state Potts model are entirely different from Ising spin configurations,\nparticularly at the transition temperature. Moreover, our CNN model is not\ntrained on the information about whether phases are ordered/disordered but is\nnaively trained by Ising spin configurations labeled with temperatures at which\nthey are generated. Nevertheless, the deep CNN can detect the transition point\nwith high accuracy, regardless of the type of transition. We also find that, in\nthe high-temperature region, the CNN outputs the temperature based on the\ninternal energy, whereas, in the low-temperature region, the output depends on\nthe magnetization and possibly the internal energy as well. However, in the\nvicinity of the transition point, the CNN may use more general factors to\ndetect the transition point.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Fukushima_K/0/1/0/all/0/1\">Kimihiko Fukushima</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Sakai_K/0/1/0/all/0/1\">Kazumitsu Sakai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Machine Learning"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}