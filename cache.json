{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Privacy Policy Question Answering Assistant: A Query-Guided Extractive Summarization Approach. (arXiv:2109.14638v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14638","description":"<p>Existing work on making privacy policies accessible has explored new\npresentation forms such as color-coding based on the risk factors or\nsummarization to assist users with conscious agreement. To facilitate a more\npersonalized interaction with the policies, in this work, we propose an\nautomated privacy policy question answering assistant that extracts a summary\nin response to the input user query. This is a challenging task because users\narticulate their privacy-related questions in a very different language than\nthe legal language of the policy, making it difficult for the system to\nunderstand their inquiry. Moreover, existing annotated data in this domain are\nlimited. We address these problems by paraphrasing to bring the style and\nlanguage of the user's question closer to the language of privacy policies. Our\ncontent scoring module uses the existing in-domain data to find relevant\ninformation in the policy and incorporates it in a summary. Our pipeline is\nable to find an answer for 89% of the user queries in the privacyQA dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keymanesh_M/0/1/0/all/0/1\">Moniba Keymanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsner_M/0/1/0/all/0/1\">Micha Elsner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classifying Tweet Sentiment Using the Hidden State and Attention Matrix of a Fine-tuned BERTweet Model. (arXiv:2109.14692v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14692","description":"<p>This paper introduces a study on tweet sentiment classification. Our task is\nto classify a tweet as either positive or negative. We approach the problem in\ntwo steps, namely embedding and classifying. Our baseline methods include\nseveral combinations of traditional embedding methods and classification\nalgorithms. Furthermore, we explore the current state-of-the-art tweet analysis\nmodel, BERTweet, and propose a novel approach in which features are engineered\nfrom the hidden states and attention matrices of the model, inspired by\nempirical study of the tweets. Using a multi-layer perceptron trained with a\nhigh dropout rate for classification, our proposed approach achieves a\nvalidation accuracy of 0.9111.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macri_T/0/1/0/all/0/1\">Tommaso Macr&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_F/0/1/0/all/0/1\">Freya Murphy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yunfan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zumbach_Y/0/1/0/all/0/1\">Yves Zumbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Approach For Sparse Representations Using The Locally Competitive Algorithm For Audio. (arXiv:2109.14705v1 [cs.SD])","link":"http://arxiv.org/abs/2109.14705","description":"<p>Gammachirp filterbank has been used to approximate the cochlea in sparse\ncoding algorithms. An oriented grid search optimization was applied to adapt\nthe gammachirp's parameters and improve the Matching Pursuit (MP) algorithm's\nsparsity along with the reconstruction quality. However, this combination of a\ngreedy algorithm with a grid search at each iteration is computationally\ndemanding and not suitable for real-time applications. This paper presents an\nadaptive approach to optimize the gammachirp's parameters but in the context of\nthe Locally Competitive Algorithm (LCA) that requires much fewer computations\nthan MP. The proposed method consists of taking advantage of the LCA's neural\narchitecture to automatically adapt the gammachirp's filterbank using the\nbackpropagation algorithm. Results demonstrate an improvement in the LCA's\nperformance with our approach in terms of sparsity, reconstruction quality, and\nconvergence time. This approach can yield a significant advantage over existing\napproaches for real-time applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahadi_S/0/1/0/all/0/1\">Soufiyan Bahadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouat_J/0/1/0/all/0/1\">Jean Rouat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plourde_E/0/1/0/all/0/1\">&#xc9;ric Plourde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief. (arXiv:2109.14723v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14723","description":"<p>Although pretrained language models (PTLMs) contain significant amounts of\nworld knowledge, they can still produce inconsistent answers to questions when\nprobed, even after specialized training. As a result, it can be hard to\nidentify what the model actually \"believes\" about the world, making it\nsusceptible to inconsistent behavior and simple errors. Our goal is to reduce\nthese problems. Our approach is to embed a PTLM in a broader system that also\nincludes an evolving, symbolic memory of beliefs -- a BeliefBank -- that\nrecords but then may modify the raw PTLM answers. We describe two mechanisms to\nimprove belief consistency in the overall system. First, a reasoning component\n-- a weighted MaxSAT solver -- revises beliefs that significantly clash with\nothers. Second, a feedback component issues future queries to the PTLM using\nknown beliefs as context. We show that, in a controlled experimental setting,\nthese two mechanisms result in more consistent beliefs in the overall system,\nimproving both the accuracy and consistency of its answers over time. This is\nsignificant as it is a first step towards PTLM-based architectures with a\nsystematic notion of belief, enabling them to construct a more coherent picture\nof the world, and improve over time without model retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kassner_N/0/1/0/all/0/1\">Nora Kassner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System. (arXiv:2109.14739v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14739","description":"<p>Pre-trained language models have been recently shown to benefit task-oriented\ndialogue (TOD) systems. Despite their success, existing methods often formulate\nthis task as a cascaded generation problem which can lead to error accumulation\nacross different sub-tasks and greater data annotation overhead. In this study,\nwe present PPTOD, a unified plug-and-play model for task-oriented dialogue. In\naddition, we introduce a new dialogue multi-task pre-training strategy that\nallows the model to learn the primary TOD task completion skills from\nheterogeneous dialog corpora. We extensively test our model on three benchmark\nTOD tasks, including end-to-end dialogue modelling, dialogue state tracking,\nand intent classification. Experimental results show that PPTOD achieves new\nstate of the art on all evaluated tasks in both high-resource and low-resource\nscenarios. Furthermore, comparisons against previous SOTA methods show that the\nresponses generated by PPTOD are more factually correct and semantically\ncoherent as judged by human annotators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arshit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yi-An Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications. (arXiv:2109.14776v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14776","description":"<p>Certainty and uncertainty are fundamental to science communication. Hedges\nhave widely been used as proxies for uncertainty. However, certainty is a\ncomplex construct, with authors expressing not only the degree but the type and\naspects of uncertainty in order to give the reader a certain impression of what\nis known. Here, we introduce a new study of certainty that models both the\nlevel and the aspects of certainty in scientific findings. Using a new dataset\nof 2167 annotated scientific findings, we demonstrate that hedges alone account\nfor only a partial explanation of certainty. We show that both the overall\ncertainty and individual aspects can be predicted with pre-trained language\nmodels, providing a more complete picture of the author's intended\ncommunication. Downstream analyses on 431K scientific findings from news and\nscientific abstracts demonstrate that modeling sentence-level and aspect-level\ncertainty is meaningful for areas like science communication. Both the model\nand datasets used in this paper are released at\nhttps://blablablab.si.umich.edu/projects/certainty/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiaxin Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tipping the Scales: A Corpus-Based Reconstruction of Adjective Scales in the McGill Pain Questionnaire. (arXiv:2109.14788v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14788","description":"<p>Modern medical diagnosis relies on precise pain assessment tools in\ntranslating clinical information from patient to physician. The McGill Pain\nQuestionnaire (MPQ) is a clinical pain assessment technique that utilizes 78\nadjectives of different intensities in 20 different categories to quantity a\npatient's pain. The questionnaire's efficacy depends on a predictable pattern\nof adjective use by patients experiencing pain. In this study, I recreate the\nMPQ's adjective intensity orderings using data gathered from patient forums and\nmodern NLP techniques. I extract adjective intensity relationships by searching\nfor key linguistic contexts, and then combine the relationship information to\nform robust adjective scales. Of 17 adjective relationships predicted by this\nresearch, 10 show agreement with the MPQ, which is statistically significant at\nthe .1 alpha level. The results suggest predictable patterns of adjective use\nby people experiencing pain, but call into question the MPQ's categories for\ngrouping adjectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stern_M/0/1/0/all/0/1\">Miriam Stern</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phonetic Word Embeddings. (arXiv:2109.14796v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14796","description":"<p>This work presents a novel methodology for calculating the phonetic\nsimilarity between words taking motivation from the human perception of sounds.\nThis metric is employed to learn a continuous vector embedding space that\ngroups similar sounding words together and can be used for various downstream\ncomputational phonology tasks. The efficacy of the method is presented for two\ndifferent languages (English, Hindi) and performance gains over previous\nreported works are discussed on established tests for predicting phonetic\nsimilarity. To address limited benchmarking mechanisms in this field, we also\nintroduce a heterographic pun dataset based evaluation methodology to compare\nthe effectiveness of acoustic similarity algorithms. Further, a visualization\nof the embedding space is presented with a discussion on the various possible\nuse-cases of this novel algorithm. An open-source implementation is also shared\nto aid reproducibility and enable adoption in related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rahul Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhawan_K/0/1/0/all/0/1\">Kunal Dhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pailla_B/0/1/0/all/0/1\">Balakrishna Pailla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Fake News Detection Using Bidirectiona lEncoder Representations from Transformers Based Models. (arXiv:2109.14816v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14816","description":"<p>Nowadays, the development of social media allows people to access the latest\nnews easily. During the COVID-19 pandemic, it is important for people to access\nthe news so that they can take corresponding protective measures. However, the\nfake news is flooding and is a serious issue especially under the global\npandemic. The misleading fake news can cause significant loss in terms of the\nindividuals and the society. COVID-19 fake news detection has become a novel\nand important task in the NLP field. However, fake news always contain the\ncorrect portion and the incorrect portion. This fact increases the difficulty\nof the classification task. In this paper, we fine tune the pre-trained\nBidirectional Encoder Representations from Transformers (BERT) model as our\nbase model. We add BiLSTM layers and CNN layers on the top of the finetuned\nBERT model with frozen parameters or not frozen parameters methods\nrespectively. The model performance evaluation results showcase that our best\nmodel (BERT finetuned model with frozen parameters plus BiLSTM layers) achieves\nstate-of-the-art results towards COVID-19 fake news detection task. We also\nexplore keywords evaluation methods using our best model and evaluate the model\nperformance after removing keywords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuebo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Process discovery on deviant traces and other stranger things. (arXiv:2109.14883v1 [cs.LG])","link":"http://arxiv.org/abs/2109.14883","description":"<p>As the need to understand and formalise business processes into a model has\ngrown over the last years, the process discovery research field has gained more\nand more importance, developing two different classes of approaches to model\nrepresentation: procedural and declarative. Orthogonally to this\nclassification, the vast majority of works envisage the discovery task as a\none-class supervised learning process guided by the traces that are recorded\ninto an input log. In this work instead, we focus on declarative processes and\nembrace the less-popular view of process discovery as a binary supervised\nlearning task, where the input log reports both examples of the normal system\nexecution, and traces representing \"stranger\" behaviours according to the\ndomain semantics. We therefore deepen how the valuable information brought by\nboth these two sets can be extracted and formalised into a model that is\n\"optimal\" according to user-defined goals. Our approach, namely NegDis, is\nevaluated w.r.t. other relevant works in this field, and shows promising\nresults as regards both the performance and the quality of the obtained\nsolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chesani_F/0/1/0/all/0/1\">Federico Chesani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francescomarino_C/0/1/0/all/0/1\">Chiara Di Francescomarino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidini_C/0/1/0/all/0/1\">Chiara Ghidini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loreti_D/0/1/0/all/0/1\">Daniela Loreti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1\">Fabrizio Maria Maggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_P/0/1/0/all/0/1\">Paola Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1\">Marco Montali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tessaris_S/0/1/0/all/0/1\">Sergio Tessaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems. (arXiv:2109.14895v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14895","description":"<p>In translating text where sentiment is the main message, human translators\ngive particular attention to sentiment-carrying words. The reason is that an\nincorrect translation of such words would miss the fundamental aspect of the\nsource text, i.e. the author's sentiment. In the online world, MT systems are\nextensively used to translate User-Generated Content (UGC) such as reviews,\ntweets, and social media posts, where the main message is often the author's\npositive or negative attitude towards the topic of the text. It is important in\nsuch scenarios to accurately measure how far an MT system can be a reliable\nreal-life utility in transferring the correct affect message. This paper\ntackles an under-recognised problem in the field of machine translation\nevaluation which is judging to what extent automatic metrics concur with the\ngold standard of human evaluation for a correct translation of sentiment. We\nevaluate the efficacy of conventional quality metrics in spotting a\nmistranslation of sentiment, especially when it is the sole error in the MT\noutput. We propose a numerical `sentiment-closeness' measure appropriate for\nassessing the accuracy of a translated affect message in UGC text by an MT\nsystem. We will show that incorporating this sentiment-aware measure can\nsignificantly enhance the correlation of some available quality metrics with\nthe human judgement of an accurate translation of sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Emad Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantavy_A/0/1/0/all/0/1\">Ashraf Tantavy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DICoE@FinSim-3: Financial Hypernym Detection using Augmented Terms and Distance-based Features. (arXiv:2109.14906v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14906","description":"<p>We present the submission of team DICoE for FinSim-3, the 3rd Shared Task on\nLearning Semantic Similarities for the Financial Domain. The task provides a\nset of terms in the financial domain and requires to classify them into the\nmost relevant hypernym from a financial ontology. After augmenting the terms\nwith their Investopedia definitions, our system employs a Logistic Regression\nclassifier over financial word embeddings and a mix of hand-crafted and\ndistance-based features. Also, for the first time in this task, we employ\ndifferent replacement methods for out-of-vocabulary terms, leading to improved\nperformance. Finally, we have also experimented with word representations\ngenerated from various financial corpora. Our best-performing submission ranked\n4th on the task's leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loukas_L/0/1/0/all/0/1\">Lefteris Loukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bougiatiotis_K/0/1/0/all/0/1\">Konstantinos Bougiatiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mavroeidis_D/0/1/0/all/0/1\">Dimitris Mavroeidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zavitsanos_E/0/1/0/all/0/1\">Elias Zavitsanos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14927","description":"<p>Temporal expressions in text play a significant role in language\nunderstanding and correctly identifying them is fundamental to various\nretrieval and natural language processing systems. Previous works have slowly\nshifted from rule-based to neural architectures, capable of tagging expressions\nwith higher accuracy. However, neural models can not yet distinguish between\ndifferent expression types at the same level as their rule-based counterparts.\nn this work, we aim to identify the most suitable transformer architecture for\njoint temporal tagging and type classification, as well as, investigating the\neffect of semi-supervised training on the performance of these systems. After\nstudying variants of token classification and encoder-decoder architectures, we\nultimately present a transformer encoder-decoder model using RoBERTa language\nmodel as our best performing system. By supplementing training resources with\nweakly labeled data from rule-based systems, our model surpasses previous works\nin temporal tagging and type classification, especially on rare classes.\nAdditionally, we make the code and pre-trained experiment publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almasian_S/0/1/0/all/0/1\">Satya Almasian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aumiller_D/0/1/0/all/0/1\">Dennis Aumiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_M/0/1/0/all/0/1\">Michael Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prose2Poem: The blessing of Transformer-based Language Models in translating Prose to Persian Poetry. (arXiv:2109.14934v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14934","description":"<p>Persian Poetry has consistently expressed its philosophy, wisdom, speech, and\nrationale on the basis of its couplets, making it an enigmatic language on its\nown to both native and non-native speakers. Nevertheless, the notice able gap\nbetween Persian prose and poem has left the two pieces of literature\nmedium-less. Having curated a parallel corpus of prose and their equivalent\npoems, we introduce a novel Neural Machine Translation (NMT) approach to\ntranslate prose to ancient Persian poetry using transformer-based Language\nModels in an extremely low-resource setting. More specifically, we trained a\nTransformer model from scratch to obtain initial translations and pretrained\ndifferent variations of BERT to obtain final translations. To address the\nchallenge of using masked language modelling under poeticness criteria, we\nheuristically joined the two models and generated valid poems in terms of\nautomatic and human assessments. Final results demonstrate the eligibility and\ncreativity of our novel heuristically aided approach among Literature\nprofessionals and non-professionals in generating novel Persian poems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirshafiee_M/0/1/0/all/0/1\">Mitra Sadat Mirshafiee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouryabi_Y/0/1/0/all/0/1\">Yazdan Rezaee Jouryabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic Persistence in Language Models: Priming as a Window into Abstract Language Representations. (arXiv:2109.14989v1 [cs.CL])","link":"http://arxiv.org/abs/2109.14989","description":"<p>We investigate the extent to which modern, neural language models are\nsusceptible to syntactic priming, the phenomenon where the syntactic structure\nof a sentence makes the same structure more probable in a follow-up sentence.\nWe explore how priming can be used to study the nature of the syntactic\nknowledge acquired by these models. We introduce a novel metric and release\nPrime-LM, a large corpus where we control for various linguistic factors which\ninteract with priming strength. We find that recent large Transformer models\nindeed show evidence of syntactic priming, but also that the syntactic\ngeneralisations learned by these models are to some extent modulated by\nsemantic information. We report surprisingly strong priming effects when\npriming with multiple sentences, each with different words and meaning but with\nidentical syntactic structure. We conclude that the syntactic priming paradigm\nis a highly useful, additional tool for gaining insights into the capacities of\nlanguage models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinclair_A/0/1/0/all/0/1\">Arabella Sinclair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jumelet_J/0/1/0/all/0/1\">Jaap Jumelet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1\">Willem Zuidema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1\">Raquel Fern&#xe1;ndez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A surprisal--duration trade-off across and within the world's languages. (arXiv:2109.15000v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15000","description":"<p>While there exist scores of natural languages, each with its unique features\nand idiosyncrasies, they all share a unifying theme: enabling human\ncommunication. We may thus reasonably predict that human cognition shapes how\nthese languages evolve and are used. Assuming that the capacity to process\ninformation is roughly constant across human populations, we expect a\nsurprisal--duration trade-off to arise both across and within languages. We\nanalyse this trade-off using a corpus of 600 languages and, after controlling\nfor several potential confounds, we find strong supporting evidence in both\nsettings. Specifically, we find that, on average, phones are produced faster in\nlanguages where they are less surprising, and vice versa. Further, we confirm\nthat more surprising phones are longer, on average, in 319 languages out of the\n600. We thus conclude that there is strong evidence of a surprisal--duration\ntrade-off in operation, both across and within the world's languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teufel_S/0/1/0/all/0/1\">Simone Teufel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1\">Dami&#xe1;n Blasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Compression Via Concurrent Pruning and Self-Distillation. (arXiv:2109.15014v1 [cs.LG])","link":"http://arxiv.org/abs/2109.15014","description":"<p>Pruning aims to reduce the number of parameters while maintaining performance\nclose to the original network. This work proposes a novel\n\\emph{self-distillation} based pruning strategy, whereby the representational\nsimilarity between the pruned and unpruned versions of the same network is\nmaximized. Unlike previous approaches that treat distillation and pruning\nseparately, we use distillation to inform the pruning criteria, without\nrequiring a separate student network as in knowledge distillation. We show that\nthe proposed {\\em cross-correlation objective for self-distilled pruning}\nimplicitly encourages sparse solutions, naturally complementing magnitude-based\npruning criteria. Experiments on the GLUE and XGLUE benchmarks show that\nself-distilled pruning increases mono- and cross-lingual language model\nperformance. Self-distilled pruned models also outperform smaller Transformers\nwith an equal number of parameters and are competitive against (6 times) larger\ndistilled networks. We also observe that self-distillation (1) maximizes class\nseparability, (2) increases the signal-to-noise ratio, and (3) converges faster\nafter pruning steps, providing further insights into why self-distilled pruning\nimproves generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neill_J/0/1/0/all/0/1\">James O&#x27; Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Sourav Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assem_H/0/1/0/all/0/1\">Haytham Assem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient Post-training Quantization of Pre-trained Language Models. (arXiv:2109.15082v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15082","description":"<p>Network quantization has gained increasing attention with the rapid growth of\nlarge pre-trained language models~(PLMs). However, most existing quantization\nmethods for PLMs follow quantization-aware training~(QAT) that requires\nend-to-end training with full access to the entire dataset. Therefore, they\nsuffer from slow training, large memory overhead, and data security issues. In\nthis paper, we study post-training quantization~(PTQ) of PLMs, and propose\nmodule-wise quantization error minimization~(MREM), an efficient solution to\nmitigate these issues. By partitioning the PLM into multiple modules, we\nminimize the reconstruction error incurred by quantization for each module. In\naddition, we design a new model parallel training strategy such that each\nmodule can be trained locally on separate computing devices without waiting for\npreceding modules, which brings nearly the theoretical training speed-up (e.g.,\n$4\\times$ on $4$ GPUs). Experiments on GLUE and SQuAD benchmarks show that our\nproposed PTQ solution not only performs close to QAT, but also enjoys\nsignificant reductions in training time, memory overhead, and data consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoli Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1\">Michael R. Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Key Point Analysis via Contrastive Learning and Extractive Argument Summarization. (arXiv:2109.15086v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15086","description":"<p>Key point analysis is the task of extracting a set of concise and high-level\nstatements from a given collection of arguments, representing the gist of these\narguments. This paper presents our proposed approach to the Key Point Analysis\nshared task, collocated with the 8th Workshop on Argument Mining. The approach\nintegrates two complementary components. One component employs contrastive\nlearning via a siamese neural network for matching arguments to key points; the\nother is a graph-based extractive summarization model for generating key\npoints. In both automatic and manual evaluation, our approach was ranked best\namong all submissions to the shared task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1\">Milad Alshomary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurke_T/0/1/0/all/0/1\">Timon Gurke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1\">Shahbaz Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_P/0/1/0/all/0/1\">Philipp Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spliethover_M/0/1/0/all/0/1\">Maximilian Splieth&#xf6;ver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cimiano_P/0/1/0/all/0/1\">Philipp Cimiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional generalization in semantic parsing with pretrained transformers. (arXiv:2109.15101v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15101","description":"<p>Large-scale pretraining instills large amounts of knowledge in deep neural\nnetworks. This, in turn, improves the generalization behavior of these models\nin downstream tasks. What exactly are the limits to the generalization benefits\nof large-scale pretraining? Here, we report observations from some simple\nexperiments aimed at addressing this question in the context of two semantic\nparsing tasks involving natural language, SCAN and COGS. We show that language\nmodels pretrained exclusively with non-English corpora, or even with\nprogramming language corpora, significantly improve out-of-distribution\ngeneralization in these benchmarks, compared with models trained from scratch,\neven though both benchmarks are English-based. This demonstrates the\nsurprisingly broad transferability of pretrained representations and knowledge.\nPretraining with a large-scale protein sequence prediction task, on the other\nhand, mostly deteriorates the generalization performance in SCAN and COGS,\nsuggesting that pretrained representations do not transfer universally and that\nthere are constraints on the similarity between the pretraining and downstream\ndomains for successful transfer. Finally, we show that larger models are harder\nto train from scratch and their generalization accuracy is lower when trained\nup to convergence on the relatively small SCAN and COGS datasets, but the\nbenefits of large-scale pretraining become much clearer with larger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact Verification Models. (arXiv:2109.15107v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15107","description":"<p>Fact verification datasets are typically constructed using crowdsourcing\ntechniques due to the lack of text sources with veracity labels. However, the\ncrowdsourcing process often produces undesired biases in data that cause models\nto learn spurious patterns. In this paper, we propose CrossAug, a contrastive\ndata augmentation method for debiasing fact verification models. Specifically,\nwe employ a two-stage augmentation pipeline to generate new claims and\nevidences from existing samples. The generated samples are then paired\ncross-wise with the original pair, forming contrastive samples that facilitate\nthe model to rely less on spurious patterns and learn more robust\nrepresentations. Experimental results show that our method outperforms the\nprevious state-of-the-art debiasing technique by 3.6% on the debiased extension\nof the FEVER dataset, with a total performance boost of 10.13% from the\nbaseline. Furthermore, we evaluate our approach in data-scarce settings, where\nmodels can be more susceptible to biases due to the lack of training data.\nExperimental results demonstrate that our approach is also effective at\ndebiasing in these low-resource conditions, exceeding the baseline performance\non the Symmetric dataset with just 1% of the original data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_S/0/1/0/all/0/1\">Seungpil Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Juae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Cheoneum Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the CLEF-2019 CheckThat!: Automatic Identification and Verification of Claims. (arXiv:2109.15118v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15118","description":"<p>We present an overview of the second edition of the CheckThat! Lab at CLEF\n2019. The lab featured two tasks in two different languages: English and\nArabic. Task 1 (English) challenged the participating systems to predict which\nclaims in a political debate or speech should be prioritized for fact-checking.\nTask 2 (Arabic) asked to (A) rank a given set of Web pages with respect to a\ncheck-worthy claim based on their usefulness for fact-checking that claim, (B)\nclassify these same Web pages according to their degree of usefulness for\nfact-checking the target claim, (C) identify useful passages from these pages,\nand (D) use the useful pages to predict the claim's factuality. CheckThat!\nprovided a full evaluation framework, consisting of data in English (derived\nfrom fact-checking sources) and Arabic (gathered and annotated from scratch)\nand evaluation based on mean average precision (MAP) and normalized discounted\ncumulative gain (nDCG) for ranking, and F1 for classification. A total of 47\nteams registered to participate in this lab, and fourteen of them actually\nsubmitted runs (compared to nine last year). The evaluation results show that\nthe most successful approaches to Task 1 used various neural networks and\nlogistic regression. As for Task 2, learning-to-rank was used by the highest\nscoring runs for subtask A, while different classifiers were used in the other\nsubtasks. We release to the research community all datasets from the lab as\nwell as the evaluation scripts, which should enable further research in the\nimportant tasks of check-worthiness estimation and automatic claim\nverification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_Cedeno_A/0/1/0/all/0/1\">Alberto Barr&#xf3;n-Cede&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanain_M/0/1/0/all/0/1\">Maram Hasanain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suwaileh_R/0/1/0/all/0/1\">Reem Suwaileh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved statistical machine translation using monolingual paraphrases. (arXiv:2109.15119v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15119","description":"<p>We propose a novel monolingual sentence paraphrasing method for augmenting\nthe training data for statistical machine translation systems \"for free\" -- by\ncreating it from data that is already available rather than having to create\nmore aligned data. Starting with a syntactic tree, we recursively generate new\nsentence variants where noun compounds are paraphrased using suitable\nprepositions, and vice-versa -- preposition-containing noun phrases are turned\ninto noun compounds. The evaluation shows an improvement equivalent to 33%-50%\nof that of doubling the amount of training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUper Team at SemEval-2016 Task 3: Building a feature-rich system for community question answering. (arXiv:2109.15120v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15120","description":"<p>We present the system we built for participating in SemEval-2016 Task 3 on\nCommunity Question Answering. We achieved the best results on subtask C, and\nstrong results on subtasks A and B, by combining a rich set of various types of\nfeatures: semantic, lexical, metadata, and user-related. The most important\ngroup turned out to be the metadata for the question and for the comment,\nsemantic vectors trained on QatarLiving data and similarities between the\nquestion and the comment for subtasks A and C, and between the original and the\nrelated question for Subtask B.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihaylova_T/0/1/0/all/0/1\">Tsvetomila Mihaylova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencheva_P/0/1/0/all/0/1\">Pepa Gencheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyanov_M/0/1/0/all/0/1\">Martin Boyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yovcheva_I/0/1/0/all/0/1\">Ivana Yovcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiprov_Y/0/1/0/all/0/1\">Yasen Kiprov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balchev_D/0/1/0/all/0/1\">Daniel Balchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolova_I/0/1/0/all/0/1\">Ivelina Nikolova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_G/0/1/0/all/0/1\">Galia Angelova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature-Rich Named Entity Recognition for Bulgarian Using Conditional Random Fields. (arXiv:2109.15121v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15121","description":"<p>The paper presents a feature-rich approach to the automatic recognition and\ncategorization of named entities (persons, organizations, locations, and\nmiscellaneous) in news text for Bulgarian. We combine well-established features\nused for other languages with language-specific lexical, syntactic and\nmorphological information. In particular, we make use of the rich tagset\nannotation of the BulTreeBank (680 morpho-syntactic tags), from which we derive\nsuitable task-specific tagsets (local and nonlocal). We further add\ndomain-specific gazetteers and additional unlabeled data, achieving F1=89.4%,\nwhich is comparable to the state-of-the-art results for English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganchev_K/0/1/0/all/0/1\">Kuzman Ganchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osenova_P/0/1/0/all/0/1\">Petya Osenova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simov_K/0/1/0/all/0/1\">Kiril Ivanov Simov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Text Style Transfer using Deep Learning. (arXiv:2109.15144v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15144","description":"<p>Style is an integral component of a sentence indicated by the choice of words\na person makes. Different people have different ways of expressing themselves,\nhowever, they adjust their speaking and writing style to a social context, an\naudience, an interlocutor or the formality of an occasion. Text style transfer\nis defined as a task of adapting and/or changing the stylistic manner in which\na sentence is written, while preserving the meaning of the original sentence.\n</p>\n<p>A systematic review of text style transfer methodologies using deep learning\nis presented in this paper. We point out the technological advances in deep\nneural networks that have been the driving force behind current successes in\nthe fields of natural language understanding and generation. The review is\nstructured around two key stages in the text style transfer process, namely,\nrepresentation learning and sentence generation in a new style. The discussion\nhighlights the commonalities and differences between proposed solutions as well\nas challenges and opportunities that are expected to direct and foster further\nresearch in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toshevska_M/0/1/0/all/0/1\">Martina Toshevska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gievska_S/0/1/0/all/0/1\">Sonja Gievska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Sarcasm Detection Based on Contrastive Attention Mechanism. (arXiv:2109.15153v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15153","description":"<p>In the past decade, sarcasm detection has been intensively conducted in a\ntextual scenario. With the popularization of video communication, the analysis\nin multi-modal scenarios has received much attention in recent years.\nTherefore, multi-modal sarcasm detection, which aims at detecting sarcasm in\nvideo conversations, becomes increasingly hot in both the natural language\nprocessing community and the multi-modal analysis community. In this paper,\nconsidering that sarcasm is often conveyed through incongruity between\nmodalities (e.g., text expressing a compliment while acoustic tone indicating a\ngrumble), we construct a Contras-tive-Attention-based Sarcasm Detection\n(ConAttSD) model, which uses an inter-modality contrastive attention mechanism\nto extract several contrastive features for an utterance. A contrastive feature\nrepresents the incongruity of information between two modalities. Our\nexperiments on MUStARD, a benchmark multi-modal sarcasm dataset, demonstrate\nthe effectiveness of the proposed ConAttSD model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guangyuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focused Contrastive Training for Test-based Constituency Analysis. (arXiv:2109.15159v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15159","description":"<p>We propose a scheme for self-training of grammaticality models for\nconstituency analysis based on linguistic tests. A pre-trained language model\nis fine-tuned by contrastive estimation of grammatical sentences from a corpus,\nand ungrammatical sentences that were perturbed by a syntactic test, a\ntransformation that is motivated by constituency theory. We show that\nconsistent gains can be achieved if only certain positive instances are chosen\nfor training, depending on whether they could be the result of a test\ntransformation. This way, the positives, and negatives exhibit similar\ncharacteristics, which makes the objective more challenging for the language\nmodel, and also allows for additional markup that indicates the position of the\ntest application within the sentence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cano_E/0/1/0/all/0/1\">Erion &#xc7;ano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual AMR Parsing with Noisy Knowledge Distillation. (arXiv:2109.15196v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15196","description":"<p>We study multilingual AMR parsing from the perspective of knowledge\ndistillation, where the aim is to learn and improve a multilingual AMR parser\nby using an existing English parser as its teacher. We constrain our\nexploration in a strict multilingual setting: there is but one model to parse\nall different languages including English. We identify that noisy input and\nprecise output are the key to successful distillation. Together with extensive\npre-training, we obtain an AMR parser whose performances surpass all previously\npublished results on four different foreign languages, including German,\nSpanish, Italian, and Chinese, by large margins (up to 18.8 \\textsc{Smatch}\npoints on Chinese and on average 11.3 \\textsc{Smatch} points). Our parser also\nachieves comparable performance on English to the latest state-of-the-art\nEnglish-only parser.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jackie Chun-Sing Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments. (arXiv:2109.15207v1 [cs.CV])","link":"http://arxiv.org/abs/2109.15207","description":"<p>In the Vision-and-Language Navigation (VLN) task an embodied agent navigates\na 3D environment, following natural language instructions. A challenge in this\ntask is how to handle 'off the path' scenarios where an agent veers from a\nreference path. Prior work supervises the agent with actions based on the\nshortest path from the agent's location to the goal, but such goal-oriented\nsupervision is often not in alignment with the instruction. Furthermore, the\nevaluation metrics employed by prior work do not measure how much of a language\ninstruction the agent is able to follow. In this work, we propose a simple and\neffective language-aligned supervision scheme, and a new metric that measures\nthe number of sub-instructions the agent has completed during navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raychaudhuri_S/0/1/0/all/0/1\">Sonia Raychaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wani_S/0/1/0/all/0/1\">Saim Wani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shivansh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_U/0/1/0/all/0/1\">Unnat Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel X. Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A formal model for ledger management systems based on contracts and temporal logic. (arXiv:2109.15212v1 [cs.CR])","link":"http://arxiv.org/abs/2109.15212","description":"<p>A key component of blockchain technology is the ledger, viz., a database\nthat, unlike standard databases, keeps in memory the complete history of past\ntransactions as in a notarial archive for the benefit of any future test. In\nsecond-generation blockchains such as Ethereum the ledger is coupled with smart\ncontracts, which enable the automation of transactions associated with\nagreements between the parties of a financial or commercial nature. The\ncoupling of smart contracts and ledgers provides the technological background\nfor very innovative application areas, such as Decentralized Autonomous\nOrganizations (DAOs), Initial Coin Offerings (ICOs) and Decentralized Finance\n(DeFi), which propelled blockchains beyond cryptocurrencies that were the only\nfocus of first generation blockchains such as the Bitcoin. However, the\ncurrently used implementation of smart contracts as arbitrary programming\nconstructs has made them susceptible to dangerous bugs that can be exploited\nmaliciously and has moved their semantics away from that of legal contracts. We\npropose here to recompose the split and recover the reliability of databases by\nformalizing a notion of contract modelled as a finite-state automaton with\nwell-defined computational characteristics derived from an encoding in terms of\nallocations of resources to actors, as an alternative to the approach based on\nprogramming. To complete the work, we use temporal logic as the basis for an\nabstract query language that is effectively suited to the historical nature of\nthe information kept in the ledger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bottoni_P/0/1/0/all/0/1\">Paolo Bottoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labella_A/0/1/0/all/0/1\">Anna Labella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pareschi_R/0/1/0/all/0/1\">Remo Pareschi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SlovakBERT: Slovak Masked Language Model. (arXiv:2109.15254v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15254","description":"<p>We introduce a new Slovak masked language model called SlovakBERT in this\npaper. It is the first Slovak-only transformers-based model trained on a\nsizeable corpus. We evaluate the model on several NLP tasks and achieve\nstate-of-the-art results. We publish the masked language model, as well as the\nsubsequently fine-tuned models for part-of-speech tagging, sentiment analysis\nand semantic textual similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pikuliak_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Pikuliak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grivalsky_S/0/1/0/all/0/1\">&#x160;tefan Grivalsk&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopka_M/0/1/0/all/0/1\">Martin Kon&#xf4;pka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blstak_M/0/1/0/all/0/1\">Miroslav Bl&#x161;t&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamajka_M/0/1/0/all/0/1\">Martin Tamajka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachraty_V/0/1/0/all/0/1\">Viktor Bachrat&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simko_M/0/1/0/all/0/1\">Mari&#xe1;n &#x160;imko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazik_P/0/1/0/all/0/1\">Pavol Bal&#xe1;&#x17e;ik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trnka_M/0/1/0/all/0/1\">Michal Trnka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhlarik_F/0/1/0/all/0/1\">Filip Uhl&#xe1;rik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inducing Transformer's Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks. (arXiv:2109.15256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15256","description":"<p>Systematic compositionality is an essential mechanism in human language,\nallowing the recombination of known parts to create novel expressions. However,\nexisting neural models have been shown to lack this basic ability in learning\nsymbolic structures. Motivated by the failure of a Transformer model on the\nSCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing\na command into actions, we propose two auxiliary sequence prediction tasks that\ntrack the progress of function and argument semantics, as additional training\nsupervision. These automatically-generated sequences are more representative of\nthe underlying compositional symbolic structures of the input data. During\ninference, the model jointly predicts the next action and the next tokens in\nthe auxiliary sequences at each step. Experiments on the SCAN dataset show that\nour method encourages the Transformer to understand compositional structures of\nthe command, improving its accuracy on multiple challenging splits from &lt;= 10%\nto 100%. With only 418 (5%) training instances, our approach still achieves\n97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can\nbe induced in Transformers given minimal but proper guidance. We also show that\na better result is achieved using less contextualized vectors as the\nattention's query, providing insights into architecture choices in achieving\nsystematic compositionality. Finally, we show positive generalization results\non the groundedSCAN task (Ruis et al., 2020). Our code is publicly available\nat: https://github.com/jiangycTarheel/compositional-auxseq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yichen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MatSciBERT: A Materials Domain Language Model for Text Mining and Information Extraction. (arXiv:2109.15290v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15290","description":"<p>An overwhelmingly large amount of knowledge in the materials domain is\ngenerated and stored as text published in peer-reviewed scientific literature.\nRecent developments in natural language processing, such as bidirectional\nencoder representations from transformers (BERT) models, provide promising\ntools to extract information from these texts. However, direct application of\nthese models in the materials domain may yield suboptimal results as the models\nthemselves may not be trained on notations and jargon that are specific to the\ndomain. Here, we present a materials-aware language model, namely, MatSciBERT,\nwhich is trained on a large corpus of scientific literature published in the\nmaterials domain. We further evaluate the performance of MatSciBERT on three\ndownstream tasks, namely, abstract classification, named entity recognition,\nand relation extraction, on different materials datasets. We show that\nMatSciBERT outperforms SciBERT, a language model trained on science corpus, on\nall the tasks. Further, we discuss some of the applications of MatSciBERT in\nthe materials domain for extracting information, which can, in turn, contribute\nto materials discovery or optimization. Finally, to make the work accessible to\nthe larger materials community, we make the pretrained and finetuned weights\nand the models of MatSciBERT freely accessible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_T/0/1/0/all/0/1\">Tanishq Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohd Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1\">N. M. Anoop Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-granular Legal Topic Classification on Greek Legislation. (arXiv:2109.15298v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15298","description":"<p>In this work, we study the task of classifying legal texts written in the\nGreek language. We introduce and make publicly available a novel dataset based\non Greek legislation, consisting of more than 47 thousand official, categorized\nGreek legislation resources. We experiment with this dataset and evaluate a\nbattery of advanced methods and classifiers, ranging from traditional machine\nlearning and RNN-based methods to state-of-the-art Transformer-based methods.\nWe show that recurrent architectures with domain-specific word embeddings offer\nimproved overall performance while being competitive even to transformer-based\nmodels. Finally, we show that cutting-edge multilingual and monolingual\ntransformer-based models brawl on the top of the classifiers' ranking, making\nus question the necessity of training monolingual transfer learning models as a\nrule of thumb. To the best of our knowledge, this is the first time the task of\nGreek legal text classification is considered in an open research project,\nwhile also Greek is a language with very limited NLP resources in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papaloukas_C/0/1/0/all/0/1\">Christos Papaloukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athinaios_K/0/1/0/all/0/1\">Konstantinos Athinaios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantazi_D/0/1/0/all/0/1\">Despina-Athanasia Pantazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koubarakis_M/0/1/0/all/0/1\">Manolis Koubarakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Text Classification via Self-Pretraining. (arXiv:2109.15300v1 [cs.CL])","link":"http://arxiv.org/abs/2109.15300","description":"<p>We present a neural semi-supervised learning model termed Self-Pretraining.\nOur model is inspired by the classic self-training algorithm. However, as\nopposed to self-training, Self-Pretraining is threshold-free, it can\npotentially update its belief about previously labeled documents, and can cope\nwith the semantic drift problem. Self-Pretraining is iterative and consists of\ntwo classifiers. In each iteration, one classifier draws a random set of\nunlabeled documents and labels them. This set is used to initialize the second\nclassifier, to be further trained by the set of labeled documents. The\nalgorithm proceeds to the next iteration and the classifiers' roles are\nreversed. To improve the flow of information across the iterations and also to\ncope with the semantic drift problem, Self-Pretraining employs an iterative\ndistillation process, transfers hypotheses across the iterations, utilizes a\ntwo-stage training model, uses an efficient learning rate schedule, and employs\na pseudo-label transformation heuristic. We have evaluated our model in three\npublicly available social media datasets. Our experiments show that\nSelf-Pretraining outperforms the existing state-of-the-art semi-supervised\nclassifiers across multiple settings. Our code is available at\nhttps://github.com/p-karisani/self_pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karisani_N/0/1/0/all/0/1\">Negin Karisani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Graph Contextualized Knowledge into Pre-trained Language Models. (arXiv:1912.00147v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1912.00147","description":"<p>Complex node interactions are common in knowledge graphs, and these\ninteractions also contain rich knowledge information. However, traditional\nmethods usually treat a triple as a training unit during the knowledge\nrepresentation learning (KRL) procedure, neglecting contextualized information\nof the nodes in knowledge graphs (KGs). We generalize the modeling object to a\nvery general form, which theoretically supports any subgraph extracted from the\nknowledge graph, and these subgraphs are fed into a novel transformer-based\nmodel to learn the knowledge embeddings. To broaden usage scenarios of\nknowledge, pre-trained language models are utilized to build a model that\nincorporates the learned knowledge representations. Experimental results\ndemonstrate that our model achieves the state-of-the-art performance on several\nmedical NLP tasks, and improvement above TransE indicates that our KRL method\ncaptures the graph contextualized information effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Di Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+jiang_X/0/1/0/all/0/1\">Xin jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Jing Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Sequence Learning for Generating Adequate Question-Answer Pairs. (arXiv:2010.01620v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.01620","description":"<p>Creating multiple-choice questions to assess reading comprehension of a given\narticle involves generating question-answer pairs (QAPs) on the main points of\nthe document. We present a learning scheme to generate adequate QAPs via\nmeta-sequence representations of sentences. A meta sequence is a sequence of\nvectors comprising semantic and syntactic tags. In particular, we devise a\nscheme called MetaQA to learn meta sequences from training data to form pairs\nof a meta sequence for a declarative sentence (MD) and a corresponding\ninterrogative sentence (MIs). On a given declarative sentence, a trained MetaQA\nmodel converts it to a meta sequence, finds a matched MD, and uses the\ncorresponding MIs and the input sentence to generate QAPs. We implement MetaQA\nfor the English language using semantic-role labeling, part-of-speech tagging,\nand named-entity recognition, and show that trained on a small dataset, MetaQA\ngenerates efficiently over the official SAT practice reading tests a large\nnumber of syntactically and semantically correct QAPs with over 97\\% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.09697","description":"<p>The capacity of neural networks like the widely adopted transformer is known\nto be very high. Evidence is emerging that they learn successfully due to\ninductive bias in the training routine, typically a variant of gradient descent\n(GD). To better understand this bias, we study the tendency for transformer\nparameters to grow in magnitude ($\\ell_2$ norm) during training, and its\nimplications for the emergent representations within self attention layers.\nEmpirically, we document norm growth in the training of transformer language\nmodels, including T5 during its pretraining. As the parameters grow in\nmagnitude, we prove that the network approximates a discretized network with\nsaturated activation functions. Such \"saturated\" networks are known to have a\nreduced capacity compared to the full network family that can be described in\nterms of formal languages and automata. Our results suggest saturation is a new\ncharacterization of an inductive bias implicit in GD of particular interest for\nNLP. We leverage the emergent discrete structure in a saturated transformer to\nanalyze the role of different attention heads, finding that some focus locally\non a small number of positions, while other heads compute global averages,\nallowing counting. We believe understanding the interplay between these two\ncapabilities may shed further light on the structure of computation within\nlarge transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does My Representation Capture X? Probe-Ably. (arXiv:2104.05807v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.05807","description":"<p>Probing (or diagnostic classification) has become a popular strategy for\ninvestigating whether a given set of intermediate features is present in the\nrepresentations of neural models. Probing studies may have misleading results,\nbut various recent works have suggested more reliable methodologies that\ncompensate for the possible pitfalls of probing. However, these best practices\nare numerous and fast-evolving. To simplify the process of running a set of\nprobing experiments in line with suggested methodologies, we introduce\nProbe-Ably: an extendable probing framework which supports and automates the\napplication of probing methods to the user's inputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayaparan_M/0/1/0/all/0/1\">Mokanarangan Thayaparan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valentino_M/0/1/0/all/0/1\">Marco Valentino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media. (arXiv:2104.06999v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06999","description":"<p>Online social media platforms increasingly rely on Natural Language\nProcessing (NLP) techniques to detect abusive content at scale in order to\nmitigate the harms it causes to their users. However, these techniques suffer\nfrom various sampling and association biases present in training data, often\nresulting in sub-par performance on content relevant to marginalized groups,\npotentially furthering disproportionate harms towards them. Studies on such\nbiases so far have focused on only a handful of axes of disparities and\nsubgroups that have annotations/lexicons available. Consequently, biases\nconcerning non-Western contexts are largely ignored in the literature. In this\npaper, we introduce a weakly supervised method to robustly detect lexical\nbiases in broader geocultural contexts. Through a case study on a publicly\navailable toxicity detection model, we demonstrate that our method identifies\nsalient groups of cross-geographic errors, and, in a follow up, demonstrate\nthat these groupings reflect human judgments of offensive and inoffensive\nlanguage in those geographic contexts. We also conduct analysis of a model\ntrained on a dataset with ground truth labels to better understand these\nbiases, and present preliminary mitigation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_D/0/1/0/all/0/1\">Dylan Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. (arXiv:2104.08758v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08758","description":"<p>Large language models have led to remarkable progress on many NLP tasks, and\nresearchers are turning to ever-larger text corpora to train them. Some of the\nlargest corpora available are made by scraping significant portions of the\ninternet, and are frequently introduced with only minimal documentation. In\nthis work we provide some of the first documentation for the Colossal Clean\nCrawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set\nof filters to a single snapshot of Common Crawl. We begin by investigating\nwhere the data came from, and find a significant amount of text from unexpected\nsources like patents and US military websites. Then we explore the content of\nthe text itself, and find machine-generated text (e.g., from machine\ntranslation systems) and evaluation examples from other benchmark NLP datasets.\nTo understand the impact of the filters applied to create this dataset, we\nevaluate the text that was removed, and show that blocklist filtering\ndisproportionately removes text from and about minority individuals. Finally,\nwe conclude with some recommendations for how to created and document web-scale\ndatasets from a scrape of the internet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1\">William Agnew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groeneveld_D/0/1/0/all/0/1\">Dirk Groeneveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1\">Margaret Mitchell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01542","description":"<p>Machine reading comprehension (MRC) is a sub-field in natural language\nprocessing that aims to assist computers understand unstructured texts and then\nanswer questions related to them. In practice, the conversation is an essential\nway to communicate and transfer information. To help machines understand\nconversation texts, we present UIT-ViCoQA, a new corpus for conversational\nmachine reading comprehension in the Vietnamese language. This corpus consists\nof 10,000 questions with answers over 2,000 conversations about health news\narticles. Then, we evaluate several baseline approaches for conversational\nmachine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1\nscore of 45.27%, which is 30.91 points behind human performance (76.18%),\nindicating that there is ample room for improvement. Our dataset is available\nat our website: <a href=\"http://nlp.uit.edu.vn/datasets/\">this http URL</a> for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Mao Nguyen Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Loi Duc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khiem Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphFormers: GNN-nested Transformers for Representation Learning on Textual Graph. (arXiv:2105.02605v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.02605","description":"<p>The representation learning on textual graph is to generate low-dimensional\nembeddings for the nodes based on the individual textual features and the\nneighbourhood information. Recent breakthroughs on pretrained language models\nand graph neural networks push forward the development of corresponding\ntechniques. The existing works mainly rely on the cascaded model architecture:\nthe textual features of nodes are independently encoded by language models at\nfirst; the textual embeddings are aggregated by graph neural networks\nafterwards. However, the above architecture is limited due to the independent\nmodeling of textual features. In this work, we propose GraphFormers, where\nlayerwise GNN components are nested alongside the transformer blocks of\nlanguage models. With the proposed architecture, the text encoding and the\ngraph aggregation are fused into an iterative workflow, making each node's\nsemantic accurately comprehended from the global perspective. In addition, a\nprogressive learning strategy is introduced, where the model is successively\ntrained on manipulated data and original data to reinforce its capability of\nintegrating information on graph. Extensive evaluations are conducted on three\nlarge-scale benchmark datasets, where GraphFormers outperform the SOTA\nbaselines with comparable running efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaozhuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sanjay Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Amit Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Rank Question Answer Pairs with Bilateral Contrastive Data Augmentation. (arXiv:2106.11096v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11096","description":"<p>In this work, we propose a novel and easy-to-apply data augmentation\nstrategy, namely Bilateral Generation (BiG), with a contrastive training\nobjective for improving the performance of ranking question answer pairs with\nexisting labeled data. In specific, we synthesize pseudo-positive QA pairs in\ncontrast to the original negative QA pairs with two pre-trained generation\nmodels, one for question generation, the other for answer generation, which are\nfine-tuned on the limited positive QA pairs from the original dataset. With the\naugmented dataset, we design a contrastive training objective for learning to\nrank question answer pairs. Experimental results on three benchmark datasets\nshow that our method significantly improves the performance of ranking models\nby making full use of existing labeled data and can be easily applied to\ndifferent ranking models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13658","description":"<p>The understanding of time expressions includes two sub-tasks: recognition and\nnormalization. In recent years, significant progress has been made in the\nrecognition of time expressions while research on normalization has lagged\nbehind. Existing SOTA normalization methods highly rely on rules or grammars\ndesigned by experts, which limits their performance on emerging corpora, such\nas social media texts. In this paper, we model time expression normalization as\na sequence of operations to construct the normalized temporal value, and we\npresent a novel method called ARTime, which can automatically generate\nnormalization rules from training data without expert interventions.\nSpecifically, ARTime automatically captures possible operation sequences from\nannotated data and generates normalization rules on time expressions with\ncommon surface forms. The experimental results show that ARTime can\nsignificantly surpass SOTA methods on the Tweets benchmark, and achieves\ncompetitive results with existing expert-engineered rule methods on the\nTempEval-3 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wentao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Bias in NLP -- Application to Hate Speech Classification using transfer learning techniques. (arXiv:2109.09725v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09725","description":"<p>In this paper, a BERT based neural network model is applied to the JIGSAW\ndata set in order to create a model identifying hateful and toxic comments\n(strictly seperated from offensive language) in online social platforms\n(English language), in this case Twitter. Three other neural network\narchitectures and a GPT-2 model are also applied on the provided data set in\norder to compare these different models. The trained BERT model is then applied\non two different data sets to evaluate its generalisation power, namely on\nanother Twitter data set and the data set HASOC 2019 which includes Twitter and\nalso Facebook comments; we focus on the English HASOC 2019 data. In addition,\nit can be shown that by fine-tuning the trained BERT model on these two data\nsets by applying different transfer learning scenarios via retraining partial\nor all layers the predictive scores improve compared to simply applying the\nmodel pre-trained on the JIGSAW data set. With our results, we get precisions\nfrom 64% to around 90% while still achieving acceptable recall values of at\nleast lower 60s%, proving that BERT is suitable for real use cases in social\nplatforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patoulidis_G/0/1/0/all/0/1\">Georgios Patoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zagidullina_A/0/1/0/all/0/1\">Aygul Zagidullina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?. (arXiv:2109.11321v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11321","description":"<p>Large language models are known to suffer from the hallucination problem in\nthat they are prone to output statements that are false or inconsistent,\nindicating a lack of knowledge. A proposed solution to this is to provide the\nmodel with additional data modalities that complements the knowledge obtained\nthrough text. We investigate the use of visual data to complement the knowledge\nof large language models by proposing a method for evaluating visual knowledge\ntransfer to text for uni- or multimodal language models. The method is based on\ntwo steps, 1) a novel task querying for knowledge of memory colors, i.e.\ntypical colors of well-known objects, and 2) filtering of model training data\nto clearly separate knowledge contributions. Additionally, we introduce a model\narchitecture that involves a visual imagination step and evaluate it with our\nproposed method. We find that our method can successfully be used to measure\nvisual knowledge transfer capabilities in models and that our novel model\narchitecture shows promising results for leveraging multimodal knowledge in a\nunimodal setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1\">Tobias Norlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_L/0/1/0/all/0/1\">Lovisa Hagstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1\">Richard Johansson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12212","description":"<p>Online conversations include more than just text. Increasingly, image-based\nresponses such as memes and animated gifs serve as culturally recognized and\noften humorous responses in conversation. However, while NLP has broadened to\nmultimodal models, conversational dialog systems have largely focused only on\ngenerating text replies. Here, we introduce a new dataset of 1.56M text-gif\nconversation turns and introduce a new multimodal conversational model Pepe the\nKing Prawn for selecting gif-based replies. We demonstrate that our model\nproduces relevant and high-quality gif responses and, in a large randomized\ncontrol trial of multiple models replying to real users, we show that our model\nreplies with gifs that are significantly better received by the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions. (arXiv:2109.13066v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13066","description":"<p>Existing text-to-SQL research only considers complete questions as the input,\nbut lay-users might strive to formulate a complete question. To build a smarter\nnatural language interface to database systems (NLIDB) that also processes\nincomplete questions, we propose a new task, prefix-to-SQL which takes question\nprefix from users as the input and predicts the intended SQL. We construct a\nnew benchmark called PAGSAS that contains 124K user question prefixes and the\nintended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.\nAdditionally, we propose a new metric SAVE to measure how much effort can be\nsaved by users. Experimental results show that PAGSAS is challenging even for\nstrong baseline models such as T5. As we observe the difficulty of\nprefix-to-SQL is related to the number of omitted tokens, we incorporate\ncurriculum learning of feeding examples with an increasing number of omitted\ntokens. This improves scores on various sub-tasks by as much as 9% recall\nscores on sub-task GeoQuery in PAGSAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaichen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP). (arXiv:2109.13123v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13123","description":"<p>Digital learning platforms enable students to learn on a flexible and\nindividual schedule as well as providing instant feedback mechanisms. The field\nof STEM education requires students to solve numerous training exercises to\ngrasp underlying concepts. It is apparent that there are restrictions in\ncurrent online education in terms of exercise diversity and individuality. Many\nexercises show little variance in structure and content, hindering the adoption\nof abstraction capabilities by students. This thesis proposes an approach to\ngenerate diverse, context rich word problems. In addition to requiring the\ngenerated language to be grammatically correct, the nature of word problems\nimplies additional constraints on the validity of contents. The proposed\napproach is proven to be effective in generating valid word problems for\nmathematical statistics. The experimental results present a tradeoff between\ngeneration time and exercise validity. The system can easily be parametrized to\nhandle this tradeoff according to the requirements of specific use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keller_S/0/1/0/all/0/1\">Stanley Uros Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v2 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2109.13662","description":"<p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- Hinge Loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. We evaluate DeepPSL on a zero shot learning problem in\nimage classification. State of the art results demonstrate the utility and\nflexibility of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puranam_S/0/1/0/all/0/1\">Sai Akhil Puranam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dasaratha_S/0/1/0/all/0/1\">Sridhar Dasaratha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phogat_K/0/1/0/all/0/1\">Karmvir Singh Phogat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiyyagura_S/0/1/0/all/0/1\">Sunil Reddy Tiyyagura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}