{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Leveraging Pretrained Models for Automatic Summarization of Doctor-Patient Conversations. (arXiv:2109.12174v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12174","description":"<p>Fine-tuning pretrained models for automatically summarizing doctor-patient\nconversation transcripts presents many challenges: limited training data,\nsignificant domain shift, long and noisy transcripts, and high target summary\nvariability. In this paper, we explore the feasibility of using pretrained\ntransformer models for automatically summarizing doctor-patient conversations\ndirectly from transcripts. We show that fluent and adequate summaries can be\ngenerated with limited training data by fine-tuning BART on a specially\nconstructed dataset. The resulting models greatly surpass the performance of an\naverage human annotator and the quality of previous published work for the\ntask. We evaluate multiple methods for handling long conversations, comparing\nthem to the obvious baseline of truncating the conversation to fit the\npretrained model length limit. We introduce a multistage approach that tackles\nthe task by learning two fine-tuned models: one for summarizing conversation\nchunks into partial summaries, followed by one for rewriting the collection of\npartial summaries into a complete summary. Using a carefully chosen fine-tuning\ndataset, this method is shown to be effective at handling longer conversations,\nimproving the quality of generated summaries. We conduct both an automatic\nevaluation (through ROUGE and two concept-based metrics focusing on medical\nfindings) and a human evaluation (through qualitative examples from literature,\nassessing hallucination, generalization, fluency, and general quality of the\ngenerated summaries).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrinho_R/0/1/0/all/0/1\">Renato Negrinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Arindam Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagannathan_V/0/1/0/all/0/1\">Vasudevan Jagannathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassanzadeh_H/0/1/0/all/0/1\">Hamid Reza Hassanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaaf_T/0/1/0/all/0/1\">Thomas Schaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gormley_M/0/1/0/all/0/1\">Matthew R. Gormley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Attention Sparsity in Transformers. (arXiv:2109.12188v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12188","description":"<p>A bottleneck in transformer architectures is their quadratic complexity with\nrespect to the input sequence, which has motivated a body of work on efficient\nsparse approximations to softmax. An alternative path, used by entmax\ntransformers, consists of having built-in exact sparse attention; however this\napproach still requires quadratic computation. In this paper, we propose\nSparsefinder, a simple model trained to identify the sparsity pattern of entmax\nattention before computing it. We experiment with three variants of our method,\nbased on distances, quantization, and clustering, on two tasks: machine\ntranslation (attention in the decoder) and masked language modeling\n(encoder-only). Our work provides a new angle to study model efficiency by\ndoing extensive analysis of the tradeoff between the sparsity and recall of the\npredicted attention graph. This allows for detailed comparison between\ndifferent models, and may guide future benchmarks for sparse models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gois_A/0/1/0/all/0/1\">Ant&#xf3;nio G&#xf3;is</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Erick Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Truly Matters? Using Linguistic Cues for Analyzing the #BlackLivesMatter Movement and its Counter Protests: 2013 to 2020. (arXiv:2109.12192v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12192","description":"<p>Since the fatal shooting of 17-year old Black teenager Trayvon Martin in\nFebruary 2012 by a White neighborhood watchman, George Zimmerman in Sanford,\nFlorida, there has been a significant increase in digital activism addressing\npolice-brutality related and racially-motivated incidents in the United States.\nIn this work, we administer an innovative study of digital activism by\nexploiting social media as an authoritative tool to examine and analyze the\nlinguistic cues and thematic relationships in these three mediums. We conduct a\nmulti-level text analysis on 36,984,559 tweets to investigate users' behaviors\nto examine the language used and understand the impact of digital activism on\nsocial media within each social movement on a sentence-level, word-level, and\ntopic-level. Our results show that excessive use of racially-related or\nprejudicial hashtags were used by the counter protests which portray potential\ndiscriminatory tendencies. Consequently, our findings highlight that social\nactivism done by Black Lives Matter activists does not diverge from the social\nissues and topics involving police-brutality related and racially-motivated\nkillings of Black individuals due to the shape of its topical graph that topics\nand conversations encircling the largest component directly relate to the topic\nof Black Lives Matter. Finally, we see that both Blue Lives Matter and All\nLives Matter movements depict a different directive, as the topics of Blue\nLives Matter or All Lives Matter do not reside in the center. These findings\nsuggest that topics and conversations within each social movement are skewed,\nrandom or possessed racially-related undertones, and thus, deviating from the\nprominent social injustice issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dacon_J/0/1/0/all/0/1\">Jamell Dacon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiliang Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style Control for Schema-Guided Natural Language Generation. (arXiv:2109.12211v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12211","description":"<p>Natural Language Generation (NLG) for task-oriented dialogue systems focuses\non communicating specific content accurately, fluently, and coherently. While\nthese attributes are crucial for a successful dialogue, it is also desirable to\nsimultaneously accomplish specific stylistic goals, such as response length,\npoint-of-view, descriptiveness, sentiment, formality, and empathy. In this\nwork, we focus on stylistic control and evaluation for schema-guided NLG, with\njoint goals of achieving both semantic and stylistic control. We experiment in\ndetail with various controlled generation methods for large pretrained language\nmodels: specifically, conditional training, guided fine-tuning, and guided\ndecoding. We discuss their advantages and limitations, and evaluate them with a\nbroad range of automatic and human evaluation metrics. Our results show that\nwhile high style accuracy and semantic correctness are easier to achieve for\nmore lexically-defined styles with conditional training, stylistic control is\nalso achievable for more semantically complex styles using discriminator-based\nguided decoding methods. The results also suggest that methods that are more\nscalable (with less hyper-parameters tuning) and that disentangle content\ngeneration and stylistic variations are more effective at achieving semantic\ncorrectness and style accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsai_A/0/1/0/all/0/1\">Alicia Y. Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oraby_S/0/1/0/all/0/1\">Shereen Oraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perera_V/0/1/0/all/0/1\">Vittorio Perera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_J/0/1/0/all/0/1\">Jiun-Yu Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog. (arXiv:2109.12212v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12212","description":"<p>Online conversations include more than just text. Increasingly, image-based\nresponses such as memes and animated gifs serve as culturally recognized and\noften humorous responses in conversation. However, while NLP has broadened to\nmultimodal models, conversational dialog systems have largely focused only on\ngenerating text replies. Here, we introduce a new dataset of 1.56M text-gif\nconversation turns and introduce a new multimodal conversational model Pepe the\nKing Prawn for selecting gif-based replies. We demonstrate that our model\nproduces relevant and high-quality gif responses and, in a large randomized\ncontrol trial of multiple models replying to real users, we show that our model\nreplies with gifs that are significantly better received by the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Contrastive Learning for Chest X-Ray Report Generation. (arXiv:2109.12242v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12242","description":"<p>Radiology report generation aims at generating descriptive text from\nradiology images automatically, which may present an opportunity to improve\nradiology reporting and interpretation. A typical setting consists of training\nencoder-decoder models on image-report pairs with a cross entropy loss, which\nstruggles to generate informative sentences for clinical diagnoses since normal\nfindings dominate the datasets. To tackle this challenge and encourage more\nclinically-accurate text outputs, we propose a novel weakly supervised\ncontrastive loss for medical report generation. Experimental results\ndemonstrate that our method benefits from contrasting target reports with\nincorrect but semantically-close ones. It outperforms previous work on both\nclinical correctness and text generation metrics for two public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zexue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Eric Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentili_A/0/1/0/all/0/1\">Amilcare Gentili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chun-Nan Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematic Generalization on gSCAN: What is Nearly Solved and What is Next?. (arXiv:2109.12243v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12243","description":"<p>We analyze the grounded SCAN (gSCAN) benchmark, which was recently proposed\nto study systematic generalization for grounded language understanding. First,\nwe study which aspects of the original benchmark can be solved by commonly used\nmethods in multi-modal research. We find that a general-purpose\nTransformer-based model with cross-modal attention achieves strong performance\non a majority of the gSCAN splits, surprisingly outperforming more specialized\napproaches from prior work. Furthermore, our analysis suggests that many of the\nremaining errors reveal the same fundamental challenge in systematic\ngeneralization of linguistic constructs regardless of visual context. Second,\ninspired by this finding, we propose challenging new tasks for gSCAN by\ngenerating data to incorporate relations between objects in the visual\nenvironment. Finally, we find that current models are surprisingly data\ninefficient given the narrow scope of commands in gSCAN, suggesting another\nchallenge for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_F/0/1/0/all/0/1\">Fei Sha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features. (arXiv:2109.12258v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12258","description":"<p>We report two essential improvements in readability assessment: 1. three\nnovel features in advanced semantics and 2. the timely evidence that\ntraditional ML models (e.g. Random Forest, using handcrafted features) can\ncombine with transformers (e.g. RoBERTa) to augment model performance. First,\nwe explore suitable transformers and traditional ML models. Then, we extract\n255 handcrafted linguistic features using self-developed extraction software.\nFinally, we assemble those to create several hybrid models, achieving\nstate-of-the-art (SOTA) accuracy on popular datasets in readability assessment.\nThe use of handcrafted features help model performance on smaller datasets.\nNotably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification\naccuracy of 99%, a 20.3% increase from the previous SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bruce W. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Yoo Sung Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jason Hyung-Jong Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering. (arXiv:2109.12264v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12264","description":"<p>Textual Question Answering (QA) aims to provide precise answers to user's\nquestions in natural language using unstructured data. One of the most popular\napproaches to this goal is machine reading comprehension(MRC). In recent years,\nmany novel datasets and evaluation metrics based on classical MRC tasks have\nbeen proposed for broader textual QA tasks. In this paper, we survey 47 recent\ntextual QA benchmark datasets and propose a new taxonomy from an application\npoint of view. In addition, We summarize 8 evaluation metrics of textual QA\ntasks. Finally, we discuss current trends in constructing textual QA benchmarks\nand suggest directions for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jointly Learning to Repair Code and Generate Commit Message. (arXiv:2109.12296v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12296","description":"<p>We propose a novel task of jointly repairing program codes and generating\ncommit messages. Code repair and commit message generation are two essential\nand related tasks for software development. However, existing work usually\nperforms the two tasks independently. We construct a multilingual triple\ndataset including buggy code, fixed code, and commit messages for this novel\ntask. We provide the cascaded models as baseline, which are enhanced with\ndifferent training approaches, including the teacher-student method, the\nmulti-task method, and the back-translation method. To deal with the error\npropagation problem of the cascaded method, the joint model is proposed that\ncan both repair the code and generate the commit message in a unified\nframework. Experimental results show that the enhanced cascaded model with\nteacher-student method and multitask-learning method achieves the best score on\ndifferent metrics of automated code repair, and the joint model behaves better\nthan the cascaded model on commit message generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaqi Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanco_A/0/1/0/all/0/1\">Ambrosio Blanco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning Transformer Models to Build ASAG System. (arXiv:2109.12300v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12300","description":"<p>Research towards creating systems for automatic grading of student answers to\nquiz and exam questions in educational settings has been ongoing since 1966.\nOver the years, the problem was divided into many categories. Among them,\ngrading text answers were divided into short answer grading, and essay grading.\nThe goal of this work was to develop an ML-based short answer grading system. I\nhence built a system which uses finetuning on Roberta Large Model pretrained on\nSTS benchmark dataset and have also created an interface to show the production\nreadiness of the system. I evaluated the performance of the system on the\nMohler extended dataset and SciEntsBank Dataset. The developed system achieved\na Pearsons Correlation of 0.82 and RMSE of 0.7 on the Mohler Dataset which\nbeats the SOTA performance on this dataset which is correlation of 0.805 and\nRMSE of 0.793. Additionally, Pearsons Correlation of 0.79 and RMSE of 0.56 was\nachieved on the SciEntsBank Dataset, which only reconfirms the robustness of\nthe system. A few observations during achieving these results included usage of\nbatch size of 1 produced better results than using batch size of 16 or 32 and\nusing huber loss as loss function performed well on this regression task. The\nsystem was tried and tested on train and validation splits using various random\nseeds and still has been tweaked to achieve a minimum of 0.76 of correlation\nand a maximum 0.15 (out of 1) RMSE on any dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Mithun Thakkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Neural Templates for Recommender Dialogue System. (arXiv:2109.12302v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12302","description":"<p>Though recent end-to-end neural models have shown promising progress on\nConversational Recommender System (CRS), two key challenges still remain.\nFirst, the recommended items cannot be always incorporated into the generated\nreplies precisely and appropriately. Second, only the items mentioned in the\ntraining corpus have a chance to be recommended in the conversation. To tackle\nthese challenges, we introduce a novel framework called NTRD for recommender\ndialogue system that decouples the dialogue generation from the item\nrecommendation. NTRD has two key components, i.e., response template generator\nand item selector. The former adopts an encoder-decoder model to generate a\nresponse template with slot locations tied to target items, while the latter\nfills in slot locations with the proper items using a sufficient attention\nmechanism. Our approach combines the strengths of both classical slot filling\napproaches (that are generally controllable) and modern neural NLG approaches\n(that are generally more natural and accurate). Extensive experiments on the\nbenchmark ReDial show our NTRD significantly outperforms the previous\nstate-of-the-art methods. Besides, our approach has the unique advantage to\nproduce novel items that do not appear in the training set of dialogue corpus.\nThe code is available at \\url{https://github.com/jokieleung/NTRD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zujie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jian Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yingying He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1\">Fan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Graph-Based Neural Model for End-to-End Frame Semantic Parsing. (arXiv:2109.12319v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12319","description":"<p>Frame semantic parsing is a semantic analysis task based on FrameNet which\nhas received great attention recently. The task usually involves three subtasks\nsequentially: (1) target identification, (2) frame classification and (3)\nsemantic role labeling. The three subtasks are closely related while previous\nstudies model them individually, which ignores their intern connections and\nmeanwhile induces error propagation problem. In this work, we propose an\nend-to-end neural model to tackle the task jointly. Concretely, we exploit a\ngraph-based method, regarding frame semantic parsing as a graph construction\nproblem. All predicates and roles are treated as graph nodes, and their\nrelations are taken as graph edges. Experiment results on two benchmark\ndatasets of frame semantic parsing show that our method is highly competitive,\nresulting in better performance than pipeline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhichao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DziriBERT: a Pre-trained Language Model for the Algerian Dialect. (arXiv:2109.12346v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12346","description":"<p>Pre-trained transformers are now the de facto models in Natural Language\nProcessing given their state-of-the-art results in many tasks and languages.\nHowever, most of the current models have been trained on languages for which\nlarge text resources are already available (such as English, French, Arabic,\netc.). Therefore, there is still a number of low-resource languages that need\nmore attention from the community. In this paper, we study the Algerian dialect\nwhich has several specificities that make the use of Arabic or multilingual\nmodels inappropriate. To address this issue, we collected more than one Million\nAlgerian tweets, and pre-trained the first Algerian language model: DziriBERT.\nWhen compared to existing models, DziriBERT achieves the best results on two\nAlgerian downstream datasets. The obtained results show that pre-training a\ndedicated model on a small dataset (150 MB) can outperform existing models that\nhave been trained on much more data (hundreds of GB). Finally, our model is\npublicly available to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdaoui_A/0/1/0/all/0/1\">Amine Abdaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berrimi_M/0/1/0/all/0/1\">Mohamed Berrimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1\">Mourad Oussalah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussaoui_A/0/1/0/all/0/1\">Abdelouahab Moussaoui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Reasoning with Context-Aware Linearization for Interpretable Fact Extraction and Verification. (arXiv:2109.12349v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12349","description":"<p>This paper presents an end-to-end system for fact extraction and verification\nusing textual and tabular evidence, the performance of which we demonstrate on\nthe FEVEROUS dataset. We experiment with both a multi-task learning paradigm to\njointly train a graph attention network for both the task of evidence\nextraction and veracity prediction, as well as a single objective graph model\nfor solely learning veracity prediction and separate evidence extraction. In\nboth instances, we employ a framework for per-cell linearization of tabular\nevidence, thus allowing us to treat evidence from tables as sequences. The\ntemplates we employ for linearizing tables capture the context as well as the\ncontent of table data. We furthermore provide a case study to show the\ninterpretability our approach. Our best performing system achieves a FEVEROUS\nscore of 0.23 and 53% label accuracy on the blind test data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotonya_N/0/1/0/all/0/1\">Neema Kotonya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1\">Thomas Spooner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1\">Daniele Magazzeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Model Priming for Cross-Lingual Event Extraction. (arXiv:2109.12383v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12383","description":"<p>We present a novel, language-agnostic approach to \"priming\" language models\nfor the task of event extraction, providing particularly effective performance\nin low-resource and zero-shot cross-lingual settings. With priming, we augment\nthe input to the transformer stack's language model differently depending on\nthe question(s) being asked of the model at runtime. For instance, if the model\nis being asked to identify arguments for the trigger \"protested\", we will\nprovide that trigger as part of the input to the language model, allowing it to\nproduce different representations for candidate arguments than when it is asked\nabout arguments for the trigger \"arrest\" elsewhere in the same sentence. We\nshow that by enabling the language model to better compensate for the deficits\nof sparse and noisy training data, our approach improves both trigger and\nargument detection and classification significantly over the state of the art\nin a zero-shot cross-lingual setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fincke_S/0/1/0/all/0/1\">Steven Fincke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shantanu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sorting through the noise: Testing robustness of information processing in pre-trained language models. (arXiv:2109.12393v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12393","description":"<p>Pre-trained LMs have shown impressive performance on downstream NLP tasks,\nbut we have yet to establish a clear understanding of their sophistication when\nit comes to processing, retaining, and applying information presented in their\ninput. In this paper we tackle a component of this question by examining\nrobustness of models' ability to deploy relevant context information in the\nface of distracting content. We present models with cloze tasks requiring use\nof critical context information, and introduce distracting content to test how\nrobustly the models retain and use that critical information for prediction. We\nalso systematically manipulate the nature of these distractors, to shed light\non dynamics of models' use of contextual cues. We find that although models\nappear in simple contexts to make predictions based on understanding and\napplying relevant facts from prior context, the presence of distracting but\nirrelevant content has clear impact in confusing model predictions. In\nparticular, models appear particularly susceptible to factors of semantic\nsimilarity and word position. The findings are consistent with the conclusion\nthat LM predictions are driven in large part by superficial contextual cues,\nrather than by robust representations of context meaning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandia_L/0/1/0/all/0/1\">Lalchand Pandia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1\">Allyson Ettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Latent Space Clustering in Multi-filter Seq2Seq Model: A Reinforcement Learning Approach. (arXiv:2109.12399v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12399","description":"<p>In sequence-to-sequence language processing tasks, sentences with\nheterogeneous semantics or grammatical structures may increase the difficulty\nof convergence while training the network. To resolve this problem, we\nintroduce a model that concentrates the each of the heterogeneous features in\nthe input-output sequences. Build upon the encoder-decoder architecture, we\ndesign a latent-enhanced multi-filter seq2seq model (LMS2S) that analyzes the\nlatent space representations using a clustering algorithm. The representations\nare generated from an encoder and a latent space enhancer. A cluster classifier\nis applied to group the representations into clusters. A soft actor-critic\nreinforcement learning algorithm is applied to the cluster classifier to\nenhance the clustering quality by maximizing the Silhouette score. Then,\nmultiple filters are trained by the features only from their corresponding\nclusters, the heterogeneity of the training data can be resolved accordingly.\nOur experiments on semantic parsing and machine translation demonstrate the\npositive correlation between the clustering quality and the model's\nperformance, as well as show the enhancement our model has made with respect to\nthe ordinary encoder-decoder model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhaokun Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINIMAL: Mining Models for Data Free Universal Adversarial Triggers. (arXiv:2109.12406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12406","description":"<p>It is well known that natural language models are vulnerable to adversarial\nattacks, which are mostly input-specific in nature. Recently, it has been shown\nthat there also exist input-agnostic attacks in NLP models, called universal\nadversarial triggers. However, existing methods to craft universal triggers are\ndata intensive. They require large amounts of data samples to generate\nadversarial triggers, which are typically inaccessible by attackers. For\ninstance, previous works take 3000 data samples per class for the SNLI dataset\nto generate adversarial triggers. In this paper, we present a novel data-free\napproach, MINIMAL, to mine input-agnostic adversarial triggers from models.\nUsing the triggers produced with our data-free algorithm, we reduce the\naccuracy of Stanford Sentiment Treebank's positive class from 93.6% to 9.6%.\nSimilarly, for the Stanford Natural Language Inference (SNLI), our single-word\ntrigger reduces the accuracy of the entailment class from 90.95% to less than\n0.6\\%. Despite being completely data-free, we get equivalent accuracy drops as\ndata-dependent methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1\">Yaman Singla Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Somesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coreference Resolution for the Biomedical Domain: A Survey. (arXiv:2109.12424v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12424","description":"<p>Issues with coreference resolution are one of the most frequently mentioned\nchallenges for information extraction from the biomedical literature. Thus, the\nbiomedical genre has long been the second most researched genre for coreference\nresolution after the news domain, and the subject of a great deal of research\nfor NLP in general. In recent years this interest has grown enormously leading\nto the development of a number of substantial datasets, of domain-specific\ncontextual language models, and of several architectures. In this paper we\nreview the state-of-the-art of coreference in the biomedical domain with a\nparticular attention on these most recent developments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pengcheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poesio_M/0/1/0/all/0/1\">Massimo Poesio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deciding Whether to Ask Clarifying Questions in Large-Scale Spoken Language Understanding. (arXiv:2109.12451v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12451","description":"<p>A large-scale conversational agent can suffer from understanding user\nutterances with various ambiguities such as ASR ambiguity, intent ambiguity,\nand hypothesis ambiguity. When ambiguities are detected, the agent should\nengage in a clarifying dialog to resolve the ambiguities before committing to\nactions. However, asking clarifying questions for all the ambiguity occurrences\ncould lead to asking too many questions, essentially hampering the user\nexperience. To trigger clarifying questions only when necessary for the user\nsatisfaction, we propose a neural self-attentive model that leverages the\nhypotheses with ambiguities and contextual signals. We conduct extensive\nexperiments on five common ambiguity types using real data from a large-scale\ncommercial conversational agent and demonstrate significant improvement over a\nset of baseline approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joo-Kyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Bum Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Selectively Learn for Weakly-supervised Paraphrase Generation. (arXiv:2109.12457v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12457","description":"<p>Paraphrase generation is a longstanding NLP task that has diverse\napplications for downstream NLP tasks. However, the effectiveness of existing\nefforts predominantly relies on large amounts of golden labeled data. Though\nunsupervised endeavors have been proposed to address this issue, they may fail\nto generate meaningful paraphrases due to the lack of supervision signals. In\nthis work, we go beyond the existing paradigms and propose a novel approach to\ngenerate high-quality paraphrases with weak supervision data. Specifically, we\ntackle the weakly-supervised paraphrase generation problem by: (1) obtaining\nabundant weakly-labeled parallel sentences via retrieval-based pseudo\nparaphrase expansion; and (2) developing a meta-learning framework to\nprogressively select valuable samples for fine-tuning a pre-trained language\nmodel, i.e., BART, on the sentential paraphrasing task. We demonstrate that our\napproach achieves significant improvements over existing unsupervised\napproaches, and is even comparable in performance with supervised\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kaize Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alexander Hanbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Refinements for Lexically Constrained Text Generation with BART. (arXiv:2109.12487v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12487","description":"<p>Lexically constrained text generation aims to control the generated text by\nincorporating some pre-specified keywords into the output. Previous work\ninjects lexical constraints into the output by controlling the decoding process\nor refining the candidate output iteratively, which tends to generate generic\nor ungrammatical sentences, and has high computational complexity. To address\nthese challenges, we propose Constrained BART (CBART) for lexically constrained\ntext generation. CBART leverages the pre-trained model BART and transfers part\nof the generation burden from the decoder to the encoder by decomposing this\ntask into two sub-tasks, thereby improving the sentence quality. Concretely, we\nextend BART by adding a token-level classifier over the encoder, aiming at\ninstructing the decoder where to replace and insert. Guided by the encoder, the\ndecoder refines multiple tokens of the input in one step by inserting tokens\nbefore specific positions and re-predicting tokens with low confidence. To\nfurther reduce the inference latency, the decoder predicts all tokens in\nparallel. Experiment results on One-Billion-Word and Yelp show that CBART can\ngenerate plausible text with high quality and diversity while significantly\naccelerating inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingwei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Electoral Programs of German Parties 2021: A Computational Analysis Of Their Comprehensibility and Likeability Based On SentiArt. (arXiv:2109.12500v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12500","description":"<p>The electoral programs of six German parties issued before the parliamentary\nelections of 2021 are analyzed using state-of-the-art computational tools for\nquantitative narrative, topic and sentiment analysis. We compare different\nmethods for computing the textual similarity of the programs, Jaccard Bag\nsimilarity, Latent Semantic Analysis, doc2vec, and sBERT, the representational\nand computational complexity increasing from the 1st to the 4th method. A new\nsimilarity measure for entire documents derived from the Fowlkes Mallows Score\nis applied to kmeans clustering of sBERT transformed sentences. Using novel\nindices of the readability and emotion potential of texts computed via SentiArt\n(Jacobs, 2019), our data shed light on the similarities and differences of the\nprograms regarding their length, main ideas, comprehensibility, likeability,\nand semantic complexity. Among others, they reveal that the programs of the SPD\nand CDU have the best chances to be comprehensible and likeable -all other\nthings being equal-, and they raise the important issue of which similarity\nmeasure is optimal for comparing texts such as electoral programs which\nnecessarily share a lot of words. While such analyses can not replace\nqualitative analyses or a deep reading of the texts, they offer predictions\nthat can be verified in empirical studies and may serve as a motivation for\nchanging aspects of future electoral programs potentially making them more\ncomprehensible and/or likeable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_A/0/1/0/all/0/1\">Arthur M. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinder_A/0/1/0/all/0/1\">Annette Kinder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Linking Meets Deep Learning: Techniques and Solutions. (arXiv:2109.12520v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12520","description":"<p>Entity linking (EL) is the process of linking entity mentions appearing in\nweb text with their corresponding entities in a knowledge base. EL plays an\nimportant role in the fields of knowledge engineering and data mining,\nunderlying a variety of downstream applications such as knowledge base\npopulation, content analysis, relation extraction, and question answering. In\nrecent years, deep learning (DL), which has achieved tremendous success in\nvarious domains, has also been leveraged in EL methods to surpass traditional\nmachine learning based methods and yield the state-of-the-art performance. In\nthis survey, we present a comprehensive review and analysis of existing DL\nbased EL methods. First of all, we propose a new taxonomy, which organizes\nexisting DL based EL methods using three axes: embedding, feature, and\nalgorithm. Then we systematically survey the representative EL methods along\nthe three axes of the taxonomy. Later, we introduce ten commonly used EL data\nsets and give a quantitative performance analysis of DL based EL methods over\nthese data sets. Finally, we discuss the remaining limitations of existing\nmethods and highlight some promising future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojie Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioCopy: A Plug-And-Play Span Copy Mechanism in Seq2Seq Models. (arXiv:2109.12533v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12533","description":"<p>Copy mechanisms explicitly obtain unchanged tokens from the source (input)\nsequence to generate the target (output) sequence under the neural seq2seq\nframework. However, most of the existing copy mechanisms only consider single\nword copying from the source sentences, which results in losing essential\ntokens while copying long spans. In this work, we propose a plug-and-play\narchitecture, namely BioCopy, to alleviate the problem aforementioned.\nSpecifically, in the training stage, we construct a BIO tag for each token and\ntrain the original model with BIO tags jointly. In the inference stage, the\nmodel will firstly predict the BIO tag at each time step, then conduct\ndifferent mask strategies based on the predicted BIO label to diminish the\nscope of the probability distributions over the vocabulary list. Experimental\nresults on two separate generative tasks show that they all outperform the\nbaseline models by adding our BioCopy to the original model structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guoan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Puning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge. (arXiv:2109.12573v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12573","description":"<p>Cross-lingual pre-training has achieved great successes using monolingual and\nbilingual plain text corpora. However, existing pre-trained models neglect\nmultilingual knowledge, which is language agnostic but comprises abundant\ncross-lingual structure alignment. In this paper, we propose XLM-K, a\ncross-lingual language model incorporating multilingual knowledge in\npre-training. XLM-K augments existing multilingual pre-training with two\nknowledge tasks, namely Masked Entity Prediction Task and Object Entailment\nTask. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly\ndemonstrate significant improvements over existing multilingual language\nmodels. The results on MLQA and NER exhibit the superiority of XLM-K in\nknowledge related tasks. The success in XNLI shows a better cross-lingual\ntransferability obtained in XLM-K. What is more, we provide a detailed probing\nanalysis to confirm the desired knowledge captured in our pre-training regimen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoze Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Paradigm Shift in Natural Language Processing. (arXiv:2109.12575v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12575","description":"<p>In the era of deep learning, modeling for most NLP tasks has converged to\nseveral mainstream paradigms. For example, we usually adopt the sequence\nlabeling paradigm to solve a bundle of tasks such as POS-tagging, NER,\nChunking, and adopt the classification paradigm to solve tasks like sentiment\nanalysis. With the rapid progress of pre-trained language models, recent years\nhave observed a rising trend of Paradigm Shift, which is solving one NLP task\nby reformulating it as another one. Paradigm shift has achieved great success\non many tasks, becoming a promising way to improve model performance. Moreover,\nsome of these paradigms have shown great potential to unify a large number of\nNLP tasks, making it possible to build a single model to handle diverse tasks.\nIn this paper, we review such phenomenon of paradigm shifts in recent years,\nhighlighting several paradigms that have the potential to solve different NLP\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12584","description":"<p>In recent times, there has been definitive progress in the field of NLP, with\nits applications growing as the utility of our language models increases with\nadvances in their performance. However, these models require a large amount of\ncomputational power and data to train, consequently leading to large carbon\nfootprints. Therefore, is it imperative that we study the carbon efficiency and\nlook for alternatives to reduce the overall environmental impact of training\nmodels, in particular large language models. In our work, we assess the\nperformance of models for machine translation, across multiple language pairs\nto assess the difference in computational power required to train these models\nfor each of these language pairs and examine the various components of these\nmodels to analyze aspects of our pipeline that can be optimized to reduce these\ncarbon emissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_M/0/1/0/all/0/1\">Mirza Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surana_P/0/1/0/all/0/1\">Praatibh Surana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents. (arXiv:2109.12595v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12595","description":"<p>We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented\ndialogues grounded in multiple documents. Most previous works treat\ndocument-grounded dialogue modeling as a machine reading comprehension task\nbased on a single given document or passage. In this work, we aim to address\nmore realistic scenarios where a goal-oriented information-seeking conversation\ninvolves multiple topics, and hence is grounded on different documents. To\nfacilitate such a task, we introduce a new dataset that contains dialogues\ngrounded in multiple documents from four different domains. We also explore\nmodeling the dialogue-based and document-based context in the dataset. We\npresent strong baseline approaches and various experimental results, aiming to\nsupport further research efforts on such a task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Song Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Siva Sankalp Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_H/0/1/0/all/0/1\">Hui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachindra Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings. (arXiv:2109.12599v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12599","description":"<p>Learning sentence embeddings from dialogues has drawn increasing attention\ndue to its low annotation cost and high domain adaptability. Conventional\napproaches employ the siamese-network for this task, which obtains the sentence\nembeddings through modeling the context-response semantic relevance by applying\na feed-forward network on top of the sentence encoders. However, as the\nsemantic textual similarity is commonly measured through the element-wise\ndistance metrics (e.g. cosine and L2 distance), such architecture yields a\nlarge gap between training and evaluating. In this paper, we propose\nDialogueCSE, a dialogue-based contrastive learning approach to tackle this\nissue. DialogueCSE first introduces a novel matching-guided embedding (MGE)\nmechanism, which generates a context-aware embedding for each candidate\nresponse embedding (i.e. the context-free embedding) according to the guidance\nof the multi-turn context-response matching matrices. Then it pairs each\ncontext-aware embedding with its corresponding context-free embedding and\nfinally minimizes the contrastive loss across all pairs. We evaluate our model\non three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing\nDong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results\nshow that our approach significantly outperforms the baselines across all three\ndatasets in terms of MAP and Spearman's correlation measures, demonstrating its\neffectiveness. Further quantitative experiments show that our approach achieves\nbetter performance when leveraging more dialogue context and remains robust\nwhen less training data is provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Che Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Analysis of Euclidean vs. Graph-Based Framing for Bilingual Lexicon Induction from Word Embedding Spaces. (arXiv:2109.12640v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12640","description":"<p>Much recent work in bilingual lexicon induction (BLI) views word embeddings\nas vectors in Euclidean space. As such, BLI is typically solved by finding a\nlinear transformation that maps embeddings to a common space. Alternatively,\nword embeddings may be understood as nodes in a weighted graph. This framing\nallows us to examine a node's graph neighborhood without assuming a linear\ntransform, and exploits new techniques from the graph matching optimization\nliterature. These contrasting approaches have not been compared in BLI so far.\nIn this work, we study the behavior of Euclidean versus graph-based approaches\nto BLI under differing data conditions and show that they complement each other\nwhen combined. We release our code at\nhttps://github.com/kellymarchisio/euc-v-graph-bli.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Youngser Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_Eldin_A/0/1/0/all/0/1\">Ali Saad-Eldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyakin_A/0/1/0/all/0/1\">Anton Alyakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1\">Carey Priebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions. (arXiv:2109.12655v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12655","description":"<p>Multi-text applications, such as multi-document summarization, are typically\nrequired to model redundancies across related texts. Current methods\nconfronting consolidation struggle to fuse overlapping information. In order to\nexplicitly represent content overlap, we propose to align predicate-argument\nrelations across texts, providing a potential scaffold for information\nconsolidation. We go beyond clustering coreferring mentions, and instead model\noverlap with respect to redundancy at a propositional level, rather than merely\ndetecting shared referents. Our setting exploits QA-SRL, utilizing\nquestion-answer pairs to capture predicate-argument relations, facilitating\nlaymen annotation of cross-text alignments. We employ crowd-workers for\nconstructing a dataset of QA-based alignments, and present a baseline QA\nalignment model trained over our dataset. Analyses show that our new task is\nsemantically challenging, capturing content overlap beyond lexical similarity\nand complements cross-document coreference with proposition-level links,\noffering potential use for downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weiss_D/0/1/0/all/0/1\">Daniela Brook Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1\">Ayal Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Ori Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Question Answering Performance Using Knowledge Distillation and Active Learning. (arXiv:2109.12662v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12662","description":"<p>Contemporary question answering (QA) systems, including transformer-based\narchitectures, suffer from increasing computational and model complexity which\nrender them inefficient for real-world applications with limited resources.\nFurther, training or even fine-tuning such models requires a vast amount of\nlabeled data which is often not available for the task at hand. In this\nmanuscript, we conduct a comprehensive analysis of the mentioned challenges and\nintroduce suitable countermeasures. We propose a novel knowledge distillation\n(KD) approach to reduce the parameter and model complexity of a pre-trained\nBERT system and utilize multiple active learning (AL) strategies for immense\nreduction in annotation efforts. In particular, we demonstrate that our model\nachieves the performance of a 6-layer TinyBERT and DistilBERT, whilst using\nonly 2% of their total parameters. Finally, by the integration of our AL\napproaches into the BERT framework, we show that state-of-the-art results on\nthe SQuAD dataset can be achieved when we only use 20% of the training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boreshban_Y/0/1/0/all/0/1\">Yasaman Boreshban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirbostani_S/0/1/0/all/0/1\">Seyed Morteza Mirbostani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassem_Sani_G/0/1/0/all/0/1\">Gholamreza Ghassem-Sani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Prunability of Attention Heads in Multilingual BERT. (arXiv:2109.12683v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12683","description":"<p>Large multilingual models, such as mBERT, have shown promise in crosslingual\ntransfer. In this work, we employ pruning to quantify the robustness and\ninterpret layer-wise importance of mBERT. On four GLUE tasks, the relative\ndrops in accuracy due to pruning have almost identical results on mBERT and\nBERT suggesting that the reduced attention capacity of the multilingual models\ndoes not affect robustness to pruning. For the crosslingual task XNLI, we\nreport higher drops in accuracy with pruning indicating lower robustness in\ncrosslingual transfer. Also, the importance of the encoder layers sensitively\ndepends on the language family and the pre-training corpus size. The top\nlayers, which are relatively more influenced by fine-tuning, encode important\ninformation for languages similar to English (SVO) while the bottom layers,\nwhich are relatively less influenced by fine-tuning, are particularly important\nfor agglutinative and low-resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Budhraja_A/0/1/0/all/0/1\">Aakriti Budhraja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pande_M/0/1/0/all/0/1\">Madhura Pande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting and Inferring Personal Attributes from Dialogue. (arXiv:2109.12702v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12702","description":"<p>Personal attributes represent structured information about a person, such as\ntheir hobbies, pets, family, likes and dislikes. In this work, we introduce the\ntasks of extracting and inferring personal attributes from human-human\ndialogue. We first demonstrate the benefit of incorporating personal attributes\nin a social chit-chat dialogue model and task-oriented dialogue setting. Thus\nmotivated, we propose the tasks of personal attribute extraction and inference,\nand then analyze the linguistic demands of these tasks. To meet these\nchallenges, we introduce a simple and extensible model that combines an\nautoregressive language model utilizing constrained attribute generation with a\ndiscriminative reranker. Our model outperforms strong baselines on extracting\npersonal attributes as well as inferring personal attributes that are not\ncontained verbatim in utterances and instead requires commonsense reasoning and\nlexical inferences, which occur frequently in everyday conversation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_A/0/1/0/all/0/1\">Alex Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding. (arXiv:2109.12742v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12742","description":"<p>The few-shot natural language understanding (NLU) task has attracted much\nrecent attention. However, prior methods have been evaluated under a disparate\nset of protocols, which hinders fair comparison and measuring progress of the\nfield. To address this issue, we introduce an evaluation framework that\nimproves previous evaluation procedures in three key aspects, i.e., test\nperformance, dev-test correlation, and stability. Under this new evaluation\nframework, we re-evaluate several state-of-the-art few-shot methods for NLU\ntasks. Our framework reveals new insights: (1) both the absolute performance\nand relative gap of the methods were not accurately estimated in prior\nliterature; (2) no single method dominates most tasks with consistent\nperformance; (3) improvements of some methods diminish with a larger pretrained\nmodel; and (4) gains from different methods are often complementary and the\nbest combined model performs close to a strong fully-supervised baseline. We\nopen-source our toolkit, FewNLU, that implements our evaluation framework along\nwith a number of state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yanan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yujie Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhilin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text to Insight: Accelerating Organic Materials Knowledge Extraction via Deep Learning. (arXiv:2109.12758v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12758","description":"<p>Scientific literature is one of the most significant resources for sharing\nknowledge. Researchers turn to scientific literature as a first step in\ndesigning an experiment. Given the extensive and growing volume of literature,\nthe common approach of reading and manually extracting knowledge is too time\nconsuming, creating a bottleneck in the research cycle. This challenge spans\nnearly every scientific domain. For the materials science, experimental data\ndistributed across millions of publications are extremely helpful for\npredicting materials properties and the design of novel materials. However,\nonly recently researchers have explored computational approaches for knowledge\nextraction primarily for inorganic materials. This study aims to explore\nknowledge extraction for organic materials. We built a research dataset\ncomposed of 855 annotated and 708,376 unannotated sentences drawn from 92,667\nabstracts. We used named-entity-recognition (NER) with BiLSTM-CNN-CRF deep\nlearning model to automatically extract key knowledge from literature.\nEarly-phase results show a high potential for automated knowledge extraction.\nThe paper presents our findings and a framework for supervised knowledge\nextraction that can be adapted to other scientific domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xintong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_S/0/1/0/all/0/1\">Steven Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikin_S/0/1/0/all/0/1\">Semion Saikin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaohua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_J/0/1/0/all/0/1\">Jane Greenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts. (arXiv:2109.12761v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12761","description":"<p>In order to better simulate the real human conversation process, models need\nto generate dialogue utterances based on not only preceding textual contexts\nbut also visual contexts. However, with the development of multi-modal dialogue\nlearning, the dataset scale gradually becomes a bottleneck. In this report, we\nrelease OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset\ncompared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a\ntotal number of 5.6 million dialogue turns extracted from either movies or TV\nseries from different resources, and each dialogue turn is paired with its\ncorresponding visual context. We hope this large-scale dataset can help\nfacilitate future researches on open-domain multi-modal dialog generation,\ne.g., multi-modal pretraining for dialogue generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_R/0/1/0/all/0/1\">Rongbin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rumour Detection via Zero-shot Cross-lingual Transfer Learning. (arXiv:2109.12773v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12773","description":"<p>Most rumour detection models for social media are designed for one specific\nlanguage (mostly English). There are over 40 languages on Twitter and most\nlanguages lack annotated resources to build rumour detection models. In this\npaper we propose a zero-shot cross-lingual transfer learning framework that can\nadapt a rumour detection model trained for a source language to another target\nlanguage. Our framework utilises pretrained multilingual language models (e.g.\\\nmultilingual BERT) and a self-training loop to iteratively bootstrap the\ncreation of ''silver labels'' in the target language to adapt the model from\nthe source language to the target language. We evaluate our methodology on\nEnglish and Chinese rumour datasets and demonstrate that our model\nsubstantially outperforms competitive benchmarks in both source and target\nlanguage rumour detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiuzhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReINTEL Challenge 2020: A Comparative Study of Hybrid Deep Neural Network for Reliable Intelligence Identification on Vietnamese SNSs. (arXiv:2109.12777v1 [cs.LG])","link":"http://arxiv.org/abs/2109.12777","description":"<p>The overwhelming abundance of data has created a misinformation crisis.\nUnverified sensationalism that is designed to grab the readers' short attention\nspan, when crafted with malice, has caused irreparable damage to our society's\nstructure. As a result, determining the reliability of an article has become a\ncrucial task. After various ablation studies, we propose a multi-input model\nthat can effectively leverage both tabular metadata and post content for the\ntask. Applying state-of-the-art finetuning techniques for the pretrained\ncomponent and training strategies for our complete model, we have achieved a\n0.9462 ROC-score on the VLSP private test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trinh_H/0/1/0/all/0/1\">Hoang Viet Trinh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tung Tien Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_H/0/1/0/all/0/1\">Huy Quang Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quang Huu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Ngoc N. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thanh_T/0/1/0/all/0/1\">Ta Minh Thanh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Use of Graph Convolution Network and Contextual Sub-Tree forCommodity News Event Extraction. (arXiv:2109.12781v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12781","description":"<p>Event extraction in commodity news is a less researched area as compared to\ngeneric event extraction. However, accurate event extraction from commodity\nnews is useful in abroad range of applications such as under-standing event\nchains and learning event-event relations, which can then be used for commodity\nprice prediction. The events found in commodity news exhibit characteristics\ndifferent from generic events, hence posing a unique challenge in event\nextraction using existing methods. This paper proposes an effective use of\nGraph Convolutional Networks(GCN) with a pruned dependency parse tree, termed\ncontextual sub-tree, for better event ex-traction in commodity news. The event\nex-traction model is trained using feature embed-dings from ComBERT, a\nBERT-based masked language model that was produced through domain-adaptive\npre-training on a commodity news corpus. Experimental results show the\nefficiency of the proposed solution, which out-performs existing methods with\nF1 scores as high as 0.90. Furthermore, our pre-trained language model\noutperforms GloVe by 23%, and BERT and RoBERTa by 7% in terms of argument roles\nclassification. For the goal of re-producibility, the code and trained models\nare made publicly available1.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Meisin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1\">Lay-Ki Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siew_E/0/1/0/all/0/1\">Eu-Gene Siew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiplicative Position-aware Transformer Models for Language Understanding. (arXiv:2109.12788v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12788","description":"<p>Transformer models, which leverage architectural improvements like\nself-attention, perform remarkably well on Natural Language Processing (NLP)\ntasks. The self-attention mechanism is position agnostic. In order to capture\npositional ordering information, various flavors of absolute and relative\nposition embeddings have been proposed. However, there is no systematic\nanalysis on their contributions and a comprehensive comparison of these methods\nis missing in the literature. In this paper, we review major existing position\nembedding methods and compare their accuracy on downstream NLP tasks, using our\nown implementations. We also propose a novel multiplicative embedding method\nwhich leads to superior accuracy when compared to existing methods. Finally, we\nshow that our proposed embedding method, served as a drop-in replacement of the\ndefault absolute position embedding, can improve the RoBERTa-base and\nRoBERTa-large models on SQuAD1.1 and SQuAD2.0 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Davis Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates. (arXiv:2109.12804v1 [eess.AS])","link":"http://arxiv.org/abs/2109.12804","description":"<p>The multi-decoder (MD) end-to-end speech translation model has demonstrated\nhigh translation quality by searching for better intermediate automatic speech\nrecognition (ASR) decoder states as hidden intermediates (HI). It is a two-pass\ndecoding model decomposing the overall task into ASR and machine translation\nsub-tasks. However, the decoding speed is not fast enough for real-world\napplications because it conducts beam search for both sub-tasks during\ninference. We propose Fast-MD, a fast MD model that generates HI by\nnon-autoregressive (NAR) decoding based on connectionist temporal\nclassification (CTC) outputs followed by an ASR decoder. We investigated two\ntypes of NAR HI: (1) parallel HI by using an autoregressive Transformer ASR\ndecoder and (2) masked HI by using Mask-CTC, which combines CTC and the\nconditional masked language model. To reduce a mismatch in the ASR decoder\nbetween teacher-forcing during training and conditioning on CTC outputs during\ntesting, we also propose sampling CTC outputs during training. Experimental\nevaluations on three corpora show that Fast-MD achieved about 2x and 4x faster\ndecoding speed than that of the na\\\"ive MD model on GPU and CPU with comparable\ntranslation quality. Adopting the Conformer encoder and intermediate CTC loss\nfurther boosts its quality without sacrificing decoding speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Non-local Features for Neural Constituency Parsing. (arXiv:2109.12814v1 [cs.CL])","link":"http://arxiv.org/abs/2109.12814","description":"<p>Thanks to the strong representation power of neural encoders, neural\nchart-based parsers have achieved highly competitive performance by using local\nfeatures. Recently, it has been shown that non-local features in CRF structures\nlead to improvements. In this paper, we investigate injecting non-local\nfeatures into the training process of a local span-based parser, by predicting\nconstituent n-gram non-local patterns and ensuring consistency between\nnon-local patterns and local constituents. Results show that our simple method\ngives better results than the CRF parser on both PTB and CTB. Besides, our\nmethod achieves state-of-the-art BERT-based performance on PTB (95.92 F1) and\nstrong performance on CTB (92.31 F1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Negative Statements Considered Useful. (arXiv:2001.04425v6 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2001.04425","description":"<p>Knowledge bases (KBs) about notable entities and their properties are an\nimportant asset in applications such as search, question answering and\ndialogue. All popular KBs capture virtually only positive statements, and\nabstain from taking any stance on statements not stored in the KB. This paper\nmakes the case for explicitly stating salient statements that do not hold.\nNegative statements are useful to overcome limitations of question answering\nsystems that are mainly geared for positive questions; they can also contribute\nto informative summaries of entities. Due to the abundance of such invalid\nstatements, any effort to compile them needs to address ranking by saliency. We\npresent a statisticalinference method for compiling and ranking negative\nstatements, based on expectations from positive statements of related entities\nin peer groups. Experimental results, with a variety of datasets, show that the\nmethod can effectively discover notable negative statements, and extrinsic\nstudies underline their usefulness for entity summarization. Datasets and code\nare released as resources for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1\">Hiba Arnaout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jeff Z. Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VirTex: Learning Visual Representations from Textual Annotations. (arXiv:2006.06666v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.06666","description":"<p>The de-facto approach to many vision tasks is to start from pretrained visual\nrepresentations, typically learned via supervised training on ImageNet. Recent\nmethods have explored unsupervised pretraining to scale to vast quantities of\nunlabeled images. In contrast, we aim to learn high-quality visual\nrepresentations from fewer images. To this end, we revisit supervised\npretraining, and seek data-efficient alternatives to classification-based\npretraining. We propose VirTex -- a pretraining approach using semantically\ndense captions to learn visual representations. We train convolutional networks\nfrom scratch on COCO Captions, and transfer them to downstream recognition\ntasks including image classification, object detection, and instance\nsegmentation. On all tasks, VirTex yields features that match or exceed those\nlearned on ImageNet -- supervised or unsupervised -- despite using up to ten\ntimes fewer images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desai_K/0/1/0/all/0/1\">Karan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-19 Twitter Dataset with Latent Topics, Sentiments and Emotions Attributes. (arXiv:2007.06954v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.06954","description":"<p>This paper describes a large global dataset on people's social media\nresponses to the COVID-19 pandemic over the Twitter platform. From 28 January\n2020 to 1 September 2021, we collected over 198 million Twitter posts from more\nthan 25 million unique users using four keywords: \"corona\", \"wuhan\", \"nCov\" and\n\"covid\". Leveraging topic modeling techniques and pre-trained machine\nlearning-based emotion analytic algorithms, we labeled each tweet with\nseventeen semantic attributes, including a) ten binary attributes indicating\nthe tweet's relevance or irrelevance to the top ten detected topics, b) five\nquantitative emotion attributes indicating the degree of intensity of the\nvalence or sentiment (from 0: very negative to 1: very positive), and the\ndegree of intensity of fear, anger, happiness and sadness emotions (from 0: not\nat all to 1: extremely intense), and c) two qualitative attributes indicating\nthe sentiment category (very negative, negative, neutral or mixed, positive,\nvery positive) and the dominant emotion category (fear, anger, happiness,\nsadness, no specific emotion) the tweet is mainly expressing. We report the\ndescriptive statistics around these new attributes, their temporal\ndistributions, and the overall geographic representation of the dataset. The\npaper concludes with an outline of the dataset's possible usage in\ncommunication, psychology, public health, economics, and epidemiology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raj Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishwanath_A/0/1/0/all/0/1\">Ajay Vishwanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Pretrained Language Models for Graph-to-Text Generation. (arXiv:2007.08426v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.08426","description":"<p>Graph-to-text generation aims to generate fluent texts from graph-based data.\nIn this paper, we investigate two recently proposed pretrained language models\n(PLMs) and analyze the impact of different task-adaptive pretraining strategies\nfor PLMs in graph-to-text generation. We present a study across three graph\ndomains: meaning representations, Wikipedia knowledge graphs (KGs) and\nscientific KGs. We show that the PLMs BART and T5 achieve new state-of-the-art\nresults and that task-adaptive pretraining strategies improve their performance\neven further. In particular, we report new state-of-the-art BLEU scores of\n49.72 on LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative\nimprovement of 31.8%, 4.5%, and 42.4%, respectively. In an extensive analysis,\nwe identify possible reasons for the PLMs' success on graph-to-text tasks. We\nfind evidence that their knowledge about true facts helps them perform well\neven when the input graph representation is reduced to a simple bag of node and\nedge labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Conflicting Data when Updating a Neural Semantic Parser. (arXiv:2010.12675v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12675","description":"<p>In this paper, we explore how to use a small amount of new data to update a\ntask-oriented semantic parsing model when the desired output for some examples\nhas changed. When making updates in this way, one potential problem that arises\nis the presence of conflicting data, or out-of-date labels in the original\ntraining set. To evaluate the impact of this understudied problem, we propose\nan experimental setup for simulating changes to a neural semantic parser. We\nshow that the presence of conflicting data greatly hinders learning of an\nupdate, then explore several methods to mitigate its effect. Our multi-task and\ndata selection methods lead to large improvements in model accuracy compared to\na naive data-mixing strategy, and our best method closes 86% of the accuracy\ngap between this baseline and an oracle upper bound.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaddy_D/0/1/0/all/0/1\">David Gaddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzemtchenko_A/0/1/0/all/0/1\">Alex Kouzemtchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muddireddy_P/0/1/0/all/0/1\">Pavankumar Reddy Muddireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolhar_P/0/1/0/all/0/1\">Prateek Kolhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rushin Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Panoramic Survey of Natural Language Processing in the Arab World. (arXiv:2011.12631v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.12631","description":"<p>The term natural language refers to any system of symbolic communication\n(spoken, signed or written) without intentional human planning and design. This\ndistinguishes natural languages such as Arabic and Japanese from artificially\nconstructed languages such as Esperanto or Python. Natural language processing\n(NLP) is the sub-field of artificial intelligence (AI) focused on modeling\nnatural languages to build applications such as speech recognition and\nsynthesis, machine translation, optical character recognition (OCR), sentiment\nanalysis (SA), question answering, dialogue systems, etc. NLP is a highly\ninterdisciplinary field with connections to computer science, linguistics,\ncognitive science, psychology, mathematics and others. Some of the earliest AI\napplications were in NLP (e.g., machine translation); and the last decade\n(2010-2020) in particular has witnessed an incredible increase in quality,\nmatched with a rise in public awareness, use, and expectations of what may have\nseemed like science fiction in the past. NLP researchers pride themselves on\ndeveloping language independent models and tools that can be applied to all\nhuman languages, e.g. machine translation systems can be built for a variety of\nlanguages using the same basic mechanisms and models. However, the reality is\nthat some languages do get more attention (e.g., English and Chinese) than\nothers (e.g., Hindi and Swahili). Arabic, the primary language of the Arab\nworld and the religious language of millions of non-Arab Muslims is somewhere\nin the middle of this continuum. Though Arabic NLP has many challenges, it has\nseen many successes and developments. Next we discuss Arabic's main challenges\nas a necessary background, and we present a brief history of Arabic NLP. We\nthen survey a number of its research areas, and close with a critical\ndiscussion of the future of Arabic NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1\">Mourad Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khalifa_H/0/1/0/all/0/1\">Hend Al-Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Natsheh_H/0/1/0/all/0/1\">Huseein T. Al-Natsheh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Beltagy_S/0/1/0/all/0/1\">Samhaa R. El-Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouzoubaa_K/0/1/0/all/0/1\">Karim Bouzoubaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavalli_Sforza_V/0/1/0/all/0/1\">Violetta Cavalli-Sforza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Hajj_W/0/1/0/all/0/1\">Wassim El-Hajj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarrar_M/0/1/0/all/0/1\">Mustafa Jarrar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of Language Model Interpretability via Sentence Editing. (arXiv:2011.14039v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.14039","description":"<p>Pre-trained language models (PLMs) like BERT are being used for almost all\nlanguage-related tasks, but interpreting their behavior still remains a\nsignificant challenge and many important questions remain largely unanswered.\nIn this work, we re-purpose a sentence editing dataset, where faithful\nhigh-quality human rationales can be automatically extracted and compared with\nextracted model rationales, as a new testbed for interpretability. This enables\nus to conduct a systematic investigation on an array of questions regarding\nPLMs' interpretability, including the role of pre-training procedure,\ncomparison of rationale extraction methods, and different layers in the PLM.\nThe investigation generates new insights, for example, contrary to the common\nunderstanding, we find that attention weights correlate well with human\nrationales and work better than gradient-based saliency in extracting model\nrationales. Both the dataset and code are available at\nhttps://github.com/samuelstevens/sentence-editing-interpretability to\nfacilitate future interpretability research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stevens_S/0/1/0/all/0/1\">Samuel Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Variable Models for Visual Question Answering. (arXiv:2101.06399v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06399","description":"<p>Current work on Visual Question Answering (VQA) explore deterministic\napproaches conditioned on various types of image and question features. We\nposit that, in addition to image and question pairs, other modalities are\nuseful for teaching machine to carry out question answering. Hence in this\npaper, we propose latent variable models for VQA where extra information (e.g.\ncaptions and answer categories) are incorporated as latent variables, which are\nobserved during training but in turn benefit question-answering performance at\ntest time. Experiments on the VQA v2.0 benchmarking dataset demonstrate the\neffectiveness of our proposed models: they improve over strong baselines,\nespecially those that do not rely on extensive language-vision pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AuGPT: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models. (arXiv:2102.05126v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.05126","description":"<p>Attention-based pre-trained language models such as GPT-2 brought\nconsiderable progress to end-to-end dialogue modelling. However, they also\npresent considerable risks for task-oriented dialogue, such as lack of\nknowledge grounding or diversity. To address these issues, we introduce\nmodified training objectives for language model finetuning, and we employ\nmassive data augmentation via back-translation to increase the diversity of the\ntraining data. We further examine the possibilities of combining data from\nmultiples sources to improve performance on the target dataset. We carefully\nevaluate our contributions with both human and automatic methods. Our model\nsubstantially outperforms the baseline on the MultiWOZ data and shows\ncompetitive performance with state of the art in both automatic and human\nevaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulhanek_J/0/1/0/all/0/1\">Jon&#xe1;&#x161; Kulh&#xe1;nek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudecek_V/0/1/0/all/0/1\">Vojt&#x11b;ch Hude&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nekvinda_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Nekvinda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1\">Ond&#x159;ej Du&#x161;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent Recognition and Unsupervised Slot Identification for Low Resourced Spoken Dialog Systems. (arXiv:2104.01287v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01287","description":"<p>Intent Recognition and Slot Identification are crucial components in spoken\nlanguage understanding (SLU) systems. In this paper, we present a novel\napproach towards both these tasks in the context of low resourced and unwritten\nlanguages. We present an acoustic based SLU system that converts speech to its\nphonetic transcription using a universal phone recognition system. We build a\nword-free natural language understanding module that does intent recognition\nand slot identification from these phonetic transcription. Our proposed SLU\nsystem performs competitively for resource rich scenarios and significantly\noutperforms existing approaches as the amount of available data reduces. We\nobserve more than 10% improvement for intent classification in Tamil and more\nthan 5% improvement for intent classification in Sinhala. We also present a\nnovel approach towards unsupervised slot identification using normalized\nattention scores. This approach can be used for unsupervised slot labelling,\ndata augmentation and to generate data for a new slot in a one-shot way with\nonly one speech recording\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_O/0/1/0/all/0/1\">Olivia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushwaha_A/0/1/0/all/0/1\">Akruti Kushwaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Saloni Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">William Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sai Krishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce. (arXiv:2104.06960v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06960","description":"<p>Existing pre-trained language models (PLMs) have demonstrated the\neffectiveness of self-supervised learning for a broad range of natural language\nprocessing (NLP) tasks. However, most of them are not explicitly aware of\ndomain-specific knowledge, which is essential for downstream tasks in many\ndomains, such as tasks in e-commerce scenarios. In this paper, we propose\nK-PLUG, a knowledge-injected pre-trained language model based on the\nencoder-decoder transformer that can be transferred to both natural language\nunderstanding and generation tasks. We verify our method in a diverse range of\ne-commerce scenarios that require domain-specific knowledge. Specifically, we\npropose five knowledge-aware self-supervised pre-training objectives to\nformulate the learning of domain-specific knowledge, including e-commerce\ndomain-specific knowledge-bases, aspects of product entities, categories of\nproduct entities, and unique selling propositions of product entities. K-PLUG\nachieves new state-of-the-art results on a suite of domain-specific NLP tasks,\nincluding product knowledge base completion, abstractive product summarization,\nand multi-turn dialogue, significantly outperforms baselines across the board,\nwhich demonstrates that the proposed method effectively learns a diverse set of\ndomain-specific knowledge for both language understanding and generation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Song Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_P/0/1/0/all/0/1\">Peng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujia Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explanation-Based Human Debugging of NLP Models: A Survey. (arXiv:2104.15135v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.15135","description":"<p>Debugging a machine learning model is hard since the bug usually involves the\ntraining data and the learning process. This becomes even harder for an opaque\ndeep learning model if we have no clue about how the model actually works. In\nthis survey, we review papers that exploit explanations to enable humans to\ngive feedback and debug NLP models. We call this problem explanation-based\nhuman debugging (EBHD). In particular, we categorize and discuss existing work\nalong three dimensions of EBHD (the bug context, the workflow, and the\nexperimental setting), compile findings on how EBHD components affect the\nfeedback providers, and highlight open problems that could be future research\ndirections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lertvittayakumjorn_P/0/1/0/all/0/1\">Piyawat Lertvittayakumjorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1\">Francesca Toni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separate but Together: Unsupervised Federated Learning for Speech Enhancement from Non-IID Data. (arXiv:2105.04727v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2105.04727","description":"<p>We propose FEDENHANCE, an unsupervised federated learning (FL) approach for\nspeech enhancement and separation with non-IID distributed data across multiple\nclients. We simulate a real-world scenario where each client only has access to\na few noisy recordings from a limited and disjoint number of speakers (hence\nnon-IID). Each client trains their model in isolation using mixture invariant\ntraining while periodically providing updates to a central server. Our\nexperiments show that our approach achieves competitive enhancement performance\ncompared to IID training on a single device and that we can further facilitate\nthe convergence speed and the overall performance using transfer learning on\nthe server-side. Moreover, we show that we can effectively combine updates from\nclients trained locally with supervised and unsupervised losses. We also\nrelease a new dataset LibriFSD50K and its creation recipe in order to\nfacilitate FL research for source separation problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tzinis_E/0/1/0/all/0/1\">Efthymios Tzinis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casebeer_J/0/1/0/all/0/1\">Jonah Casebeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smaragdis_P/0/1/0/all/0/1\">Paris Smaragdis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits. (arXiv:2106.14371v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2106.14371","description":"<p>Target speech separation is the process of filtering a certain speaker's\nvoice out of speech mixtures according to the additional speaker identity\ninformation provided. Recent works have made considerable improvement by\nprocessing signals in the time domain directly. The majority of them take fully\noverlapped speech mixtures for training. However, since most real-life\nconversations occur randomly and are sparsely overlapped, we argue that\ntraining with different overlap ratio data benefits. To do so, an unavoidable\nproblem is that the popularly used SI-SNR loss has no definition for silent\nsources. This paper proposes the weighted SI-SNR loss, together with the joint\nlearning of target speech separation and personal VAD. The weighted SI-SNR loss\nimposes a weight factor that is proportional to the target speaker's duration\nand returns zero when the target speaker is absent. Meanwhile, the personal VAD\ngenerates masks and sets non-target speech to silence. Experiments show that\nour proposed method outperforms the baseline by 1.73 dB in terms of SDR on\nfully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely\noverlapped speech of clean and noisy conditions. Besides, with slight\ndegradation in performance, our model could reduce the time costs in inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Luyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.03200","description":"<p>The increasing use of social media sites in countries like India has given\nrise to large volumes of code-mixed data. Sentiment analysis of this data can\nprovide integral insights into people's perspectives and opinions. Developing\nrobust explainability techniques which explain why models make their\npredictions becomes essential. In this paper, we propose an adequate\nmethodology to integrate explainable approaches into code-mixed sentiment\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1\">Sudarshan Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_N/0/1/0/all/0/1\">Nipuna Chhabra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fuzzy Attention for Structured Sentiment Analysis. (arXiv:2109.06719v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06719","description":"<p>Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructured sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structured sentiment analysis. We further explore the\nparsing modeling on structured sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.08535","description":"<p>Open-domain question answering has exploded in popularity recently due to the\nsuccess of dense retrieval models, which have surpassed sparse models using\nonly a few supervised training examples. However, in this paper, we demonstrate\ncurrent dense models are not yet the holy grail of retrieval. We first\nconstruct EntityQuestions, a set of simple, entity-rich questions based on\nfacts from Wikidata (e.g., \"Where was Arve Furset born?\"), and observe that\ndense retrievers drastically underperform sparse methods. We investigate this\nissue and uncover that dense retrievers can only generalize to common entities\nunless the question pattern is explicitly observed during training. We discuss\ntwo simple solutions towards addressing this critical problem. First, we\ndemonstrate that data augmentation is unable to fix the generalization problem.\nSecond, we argue a more robust passage encoder helps facilitate better question\nadaptation using specialized question encoders. We hope our work can shed light\non the challenges in creating a robust, universal dense retriever that works\nwell across different input distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sciavolino_C/0/1/0/all/0/1\">Christopher Sciavolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10282","description":"<p>Text recognition is a long-standing research problem for document\ndigitalization. Existing approaches for text recognition are usually built\nbased on CNN for image understanding and RNN for char-level text generation. In\naddition, another language model is usually needed to improve the overall\naccuracy as a post-processing step. In this paper, we propose an end-to-end\ntext recognition approach with pre-trained image Transformer and text\nTransformer models, namely TrOCR, which leverages the Transformer architecture\nfor both image understanding and wordpiece-level text generation. The TrOCR\nmodel is simple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets. Experiments show\nthat the TrOCR model outperforms the current state-of-the-art models on both\nprinted and handwritten text recognition tasks. The code and models will be\npublicly available at https://aka.ms/TrOCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}