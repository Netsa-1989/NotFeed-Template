{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-05T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Sentiment and structure in word co-occurrence networks on Twitter. (arXiv:2110.00587v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00587","description":"<p>We explore the relationship between context and happiness scores in political\ntweets using word co-occurrence networks, where nodes in the network are the\nwords, and the weight of an edge is the number of tweets in the corpus for\nwhich the two connected words co-occur. In particular, we consider tweets with\nhashtags #imwithher and #crookedhillary, both relating to Hillary Clinton's\npresidential bid in 2016. We then analyze the network properties in conjunction\nwith the word scores by comparing with null models to separate the effects of\nthe network structure and the score distribution. Neutral words are found to be\ndominant and most words, regardless of polarity, tend to co-occur with neutral\nwords. We do not observe any score homophily among positive and negative words.\nHowever, when we perform network backboning, community detection results in\nword groupings with meaningful narratives, and the happiness scores of the\nwords in each group correspond to its respective theme. Thus, although we\nobserve no clear relationship between happiness scores and co-occurrence at the\nnode or edge level, a community-centric approach can isolate themes of\ncompeting sentiments in a corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fudolig_M/0/1/0/all/0/1\">Mikaela Irene Fudolig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alshaabi_T/0/1/0/all/0/1\">Thayer Alshaabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Michael V. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danforth_C/0/1/0/all/0/1\">Christopher M. Danforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodds_P/0/1/0/all/0/1\">Peter Sheridan Dodds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expected Validation Performance and Estimation of a Random Variable's Maximum. (arXiv:2110.00613v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00613","description":"<p>Research in NLP is often supported by experimental results, and improved\nreporting of such results can lead to better understanding and more\nreproducible science. In this paper we analyze three statistical estimators for\nexpected validation performance, a tool used for reporting performance (e.g.,\naccuracy) as a function of computational budget (e.g., number of hyperparameter\ntuning experiments). Where previous work analyzing such estimators focused on\nthe bias, we also examine the variance and mean squared error (MSE). In both\nsynthetic and realistic scenarios, we evaluate three estimators and find the\nunbiased estimator has the highest variance, and the estimator with the\nsmallest variance has the largest bias; the estimator with the smallest MSE\nstrikes a balance between bias and variance, displaying a classic bias-variance\ntradeoff. We use expected validation performance to compare between different\nmodels, and analyze how frequently each estimator leads to drawing incorrect\nconclusions about which of two models performs best. We find that the two\nbiased estimators lead to the fewest incorrect conclusions, which hints at the\nimportance of minimizing variance and MSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dodge_J/0/1/0/all/0/1\">Jesse Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Attentive Constituency Parsing for UCCA-based Semantic Parsing. (arXiv:2110.00621v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00621","description":"<p>Semantic parsing provides a way to extract the semantic structure of a text\nthat could be understood by machines. It is utilized in various NLP\napplications that require text comprehension such as summarization and question\nanswering. Graph-based representation is one of the semantic representation\napproaches to express the semantic structure of a text. Such representations\ngenerate expressive and adequate graph-based target structures. In this paper,\nwe focus primarily on UCCA graph-based semantic representation. The paper not\nonly presents the existing approaches proposed for UCCA representation, but\nalso proposes a novel self-attentive neural parsing model for the UCCA\nrepresentation. We present the results for both single-lingual and\ncross-lingual tasks using zero-shot and few-shot learning for low-resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bolucu_N/0/1/0/all/0/1\">Necva B&#xf6;l&#xfc;c&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_B/0/1/0/all/0/1\">Burcu Can</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALBU: An approximate Loopy Belief message passing algorithm for LDA to improve performance on small data sets. (arXiv:2110.00635v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00635","description":"<p>Variational Bayes (VB) applied to latent Dirichlet allocation (LDA) has\nbecome the most popular algorithm for aspect modeling. While sufficiently\nsuccessful in text topic extraction from large corpora, VB is less successful\nin identifying aspects in the presence of limited data. We present a novel\nvariational message passing algorithm as applied to Latent Dirichlet Allocation\n(LDA) and compare it with the gold standard VB and collapsed Gibbs sampling. In\nsituations where marginalisation leads to non-conjugate messages, we use ideas\nfrom sampling to derive approximate update equations. In cases where conjugacy\nholds, Loopy Belief update (LBU) (also known as Lauritzen-Spiegelhalter) is\nused. Our algorithm, ALBU (approximate LBU), has strong similarities with\nVariational Message Passing (VMP) (which is the message passing variant of VB).\nTo compare the performance of the algorithms in the presence of limited data,\nwe use data sets consisting of tweets and news groups. Additionally, to perform\nmore fine grained evaluations and comparisons, we use simulations that enable\ncomparisons with the ground truth via Kullback-Leibler divergence (KLD). Using\ncoherence measures for the text corpora and KLD with the simulations we show\nthat ALBU learns latent distributions more accurately than does VB, especially\nfor smaller data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Rebecca M.C. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preez_J/0/1/0/all/0/1\">Johan A. du Preez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models. (arXiv:2110.00672v1 [cs.CY])","link":"http://arxiv.org/abs/2110.00672","description":"<p>We use a dataset of U.S. first names with labels based on predominant gender\nand racial group to examine the effect of training corpus frequency on\ntokenization, contextualization, similarity to initial representation, and bias\nin BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white\nnames are less frequent in the training corpora of these four language models.\nWe find that infrequent names are more self-similar across contexts, with\nSpearman's r between frequency and self-similarity as low as -.763. Infrequent\nnames are also less similar to initial representation, with Spearman's r\nbetween frequency and linear centered kernel alignment (CKA) similarity to\ninitial representation as high as .702. Moreover, we find Spearman's r between\nracial bias and name frequency in BERT of .492, indicating that lower-frequency\nminority group names are more associated with unpleasantness. Representations\nof infrequent names undergo more processing, but are more self-similar,\nindicating that models rely on less context-informed representations of\nuncommon and minority names which are overfit to a lower number of observed\ncontexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_R/0/1/0/all/0/1\">Robert Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caliskan_A/0/1/0/all/0/1\">Aylin Caliskan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning. (arXiv:2110.00678v1 [eess.AS])","link":"http://arxiv.org/abs/2110.00678","description":"<p>To address the performance gap of English ASR models on L2 English speakers,\nwe evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;\nXu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,\n2018) under different training settings. We compare \\textbf{(a)} models trained\nwith a combination of diverse accents to ones trained with only specific\naccents and \\textbf{(b)} results from different single-accent models. Our\nexperiments demonstrate the promise of developing ASR models for non-native\nEnglish speakers, even with small amounts of L2 training data and even without\na language model. Our models also excel in the zero-shot setting where we train\non multiple L2 datasets and test on a blind L2 test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shibano_T/0/1/0/all/0/1\">Toshiko Shibano</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyi Zhang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mia Taige Li</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Haejin Cho</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Sullivan_P/0/1/0/all/0/1\">Peter Sullivan</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a> (1) ((1) University of British Columbia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Robustness of Dialog Models to Popular Figurative Language Constructs. (arXiv:2110.00687v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00687","description":"<p>Humans often employ figurative language use in communication, including\nduring interactions with dialog systems. Thus, it is important for real-world\ndialog systems to be able to handle popular figurative language constructs like\nmetaphor and simile. In this work, we analyze the performance of existing\ndialog models in situations where the input dialog context exhibits use of\nfigurative language. We observe large gaps in handling of figurative language\nwhen evaluating the models on two open domain dialog datasets. When faced with\ndialog contexts consisting of figurative language, some models show very large\ndrops in performance compared to contexts without figurative language. We\nencourage future research in dialog modeling to separately analyze and report\nresults on figurative language in order to better test model capabilities\nrelevant to real-world use. Finally, we propose lightweight solutions to help\nexisting models become more robust to figurative language by simply using an\nexternal resource to translate figurative language to literal (non-figurative)\nforms while preserving the meaning to the best extent possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering and Network Analysis for the Embedding Spaces of Sentences and Sub-Sentences. (arXiv:2110.00697v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00697","description":"<p>Sentence embedding methods offer a powerful approach for working with short\ntextual constructs or sequences of words. By representing sentences as dense\nnumerical vectors, many natural language processing (NLP) applications have\nimproved their performance. However, relatively little is understood about the\nlatent structure of sentence embeddings. Specifically, research has not\naddressed whether the length and structure of sentences impact the sentence\nembedding space and topology. This paper reports research on a set of\ncomprehensive clustering and network analyses targeting sentence and\nsub-sentence embedding spaces. Results show that one method generates the most\nclusterable embeddings. In general, the embeddings of span sub-sentences have\nbetter clustering properties than the original sentences. The results have\nimplications for future sentence embedding models and applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yuan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinowski_A/0/1/0/all/0/1\">Alexander Kalinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_J/0/1/0/all/0/1\">Jane Greenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-shot Multilingual Neural Machine Translation for Low-Resource Languages. (arXiv:2110.00712v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00712","description":"<p>Although the multilingual Neural Machine Translation(NMT), which extends\nGoogle's multilingual NMT, has ability to perform zero-shot translation and the\niterative self-learning algorithm can improve the quality of zero-shot\ntranslation, it confronts with two problems: the multilingual NMT model is\nprone to generate wrong target language when implementing zero-shot\ntranslation; the self-learning algorithm, which uses beam search to generate\nsynthetic parallel data, demolishes the diversity of the generated source\nlanguage and amplifies the impact of the same noise during the iterative\nlearning process. In this paper, we propose the tagged-multilingual NMT model\nand improve the self-learning algorithm to handle these two problems. Firstly,\nwe extend the Google's multilingual NMT model and add target tokens to the\ntarget languages, which associates the start tag with the target language to\nensure that the source language can be translated to the required target\nlanguage. Secondly, we improve the self-learning algorithm by replacing beam\nsearch with random sample to increases the diversity of the generated data and\nmakes it properly cover the true data distribution. Experimental results on\nIWSLT show that the adjusted tagged-multilingual NMT separately obtains 9.41\nand 7.85 BLEU scores over the multilingual NMT on 2010 and 2017\nRomanian-Italian test sets. Similarly, it obtains 9.08 and 7.99 BLEU scores on\nItalian-Romanian zero-shot translation. Furthermore, the improved self-learning\nalgorithm shows its superiorities over the conventional self-learning algorithm\non zero-shot translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gongxu Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is There More Pattern in Knowledge Graph? Exploring Proximity Pattern for Knowledge Graph Embedding. (arXiv:2110.00720v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00720","description":"<p>Modeling of relation pattern is the core focus of previous Knowledge Graph\nEmbedding works, which represents how one entity is related to another\nsemantically by some explicit relation. However, there is a more natural and\nintuitive relevancy among entities being always ignored, which is that how one\nentity is close to another semantically, without the consideration of any\nexplicit relation. We name such semantic phenomenon in knowledge graph as\nproximity pattern. In this work, we explore the problem of how to define and\nrepresent proximity pattern, and how it can be utilized to help knowledge graph\nembedding. Firstly, we define the proximity of any two entities according to\ntheir statistically shared queries, then we construct a derived graph structure\nand represent the proximity pattern from global view. Moreover, with the\noriginal knowledge graph, we design a Chained couPle-GNN (CP-GNN) architecture\nto deeply merge the two patterns (graphs) together, which can encode a more\ncomprehensive knowledge embedding. Being evaluated on FB15k-237 and WN18RR\ndatasets, CP-GNN achieves state-of-the-art results for Knowledge Graph\nCompletion task, and can especially boost the modeling capacity for complex\nqueries that contain multiple answer entities, proving the effectiveness of\nintroduced proximity pattern.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiannan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplify Your Law: Using Information Theory to Deduplicate Legal Documents. (arXiv:2110.00735v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00735","description":"<p>Textual redundancy is one of the main challenges to ensuring that legal texts\nremain comprehensible and maintainable. Drawing inspiration from the\nrefactoring literature in software engineering, which has developed methods to\nexpose and eliminate duplicated code, we introduce the duplicated phrase\ndetection problem for legal texts and propose the Dupex algorithm to solve it.\nLeveraging the Minimum Description Length principle from information theory,\nDupex identifies a set of duplicated phrases, called patterns, that together\nbest compress a given input text. Through an extensive set of experiments on\nthe Titles of the United States Code, we confirm that our algorithm works well\nin practice: Dupex will help you simplify your law.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coupette_C/0/1/0/all/0/1\">Corinna Coupette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jyotsna Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spamann_H/0/1/0/all/0/1\">Holger Spamann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopiOCQA: Open-domain Conversational Question Answeringwith Topic Switching. (arXiv:2110.00768v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00768","description":"<p>In a conversational question answering scenario, a questioner seeks to\nextract information about a topic through a series of interdependent questions\nand answers. As the conversation progresses, they may switch to related topics,\na phenomenon commonly observed in information-seeking search sessions. However,\ncurrent datasets for conversational question answering are limiting in two\nways: 1) they do not contain topic switches; and 2) they assume the reference\ntext for the conversation is given, i.e., the setting is not open-domain. We\nintroduce TopiOCQA (pronounced Tapioca), an open-domain conversational dataset\nwith topic switches on Wikipedia. TopiOCQA contains 3,920 conversations with\ninformation-seeking questions and free-form answers. TopiOCQA poses a\nchallenging test-bed for models, where efficient retrieval is required on\nmultiple turns of the same conversation, in conjunction with constructing valid\nresponses using conversational history. We evaluate several baselines, by\ncombining state-of-the-art document retrieval methods with neural reader\nmodels. Our best models achieves F1 of 51.9, and BLEU score of 42.1 which falls\nshort of human performance by 18.3 points and 17.6 points respectively,\nindicating the difficulty of our dataset. Our dataset and code will be\navailable at https://mcgill-nlp.github.io/topiocqa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adlakha_V/0/1/0/all/0/1\">Vaibhav Adlakha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suleman_K/0/1/0/all/0/1\">Kaheer Suleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimizing LR(1) State Machines is NP-Hard. (arXiv:2110.00776v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00776","description":"<p>LR(1) parsing was a focus of extensive research in the past 50 years. Though\nmost fundamental mysteries have been resolved, a few remain hidden in the dark\ncorners. The one we bumped into is the minimization of the LR(1) state\nmachines, which we prove is NP-hard. It is the node-coloring problem that is\nreduced to the minimization puzzle. The reduction makes use of two technique:\nindirect reduction and incremental construction. Indirect reduction means the\ngraph to be colored is not reduced to an LR(1) state machine directly. Instead,\nit is reduced to a context-free grammar from which an LR(1) state machine is\nderived. Furthermore, by considering the nodes in the graph to be colored one\nat a time, the context-free grammar is incrementally extended from a template\ncontext-free grammar that is for a two-node graph. The extension is done by\nadding new grammar symbols and rules. A minimized LR(1) machine can be used to\nrecover a minimum coloring of the original graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wuu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect Sentiment Quad Prediction as Paraphrase Generation. (arXiv:2110.00796v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00796","description":"<p>Aspect-based sentiment analysis (ABSA) has been extensively studied in recent\nyears, which typically involves four fundamental sentiment elements, including\nthe aspect category, aspect term, opinion term, and sentiment polarity.\nExisting studies usually consider the detection of partial sentiment elements,\ninstead of predicting the four elements in one shot. In this work, we introduce\nthe Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all\nsentiment elements in quads for a given opinionated sentence, which can reveal\na more comprehensive and complete aspect-level sentiment structure. We further\npropose a novel \\textsc{Paraphrase} modeling paradigm to cast the ASQP task to\na paraphrase generation process. On one hand, the generation formulation allows\nsolving ASQP in an end-to-end manner, alleviating the potential error\npropagation in the pipeline solution. On the other hand, the semantics of the\nsentiment elements can be fully exploited by learning to generate them in the\nnatural language form. Extensive experiments on benchmark datasets show the\nsuperiority of our proposed method and the capacity of cross-task transfer with\nthe proposed unified \\textsc{Paraphrase} modeling framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yifei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swiss-Judgment-Prediction: A Multilingual Legal Judgment Prediction Benchmark. (arXiv:2110.00806v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00806","description":"<p>In many jurisdictions, the excessive workload of courts leads to high delays.\nSuitable predictive AI models can assist legal professionals in their work, and\nthus enhance and speed up the process. So far, Legal Judgment Prediction (LJP)\ndatasets have been released in English, French, and Chinese. We publicly\nrelease a multilingual (German, French, and Italian), diachronic (2000-2020)\ncorpus of 85K cases from the Federal Supreme Court of Switzerland (FSCS). We\nevaluate state-of-the-art BERT-based methods including two variants of BERT\nthat overcome the BERT input (text) length limitation (up to 512 tokens).\nHierarchical BERT has the best performance (approx. 68-70% Macro-F1-Score in\nGerman and French). Furthermore, we study how several factors (canton of\norigin, year of publication, text length, legal area) affect performance. We\nrelease both the benchmark dataset and our code to accelerate future research\nand ensure reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1\">Joel Niklaus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1\">Matthias St&#xfc;rmer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning. (arXiv:2110.00842v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00842","description":"<p>Mapping natural language instructions to programs that computers can process\nis a fundamental challenge. Existing approaches focus on likelihood-based\ntraining or using reinforcement learning to fine-tune models based on a single\nreward. In this paper, we pose program generation from language as Inverse\nReinforcement Learning. We introduce several interpretable reward components\nand jointly learn (1) a reward function that linearly combines them, and (2) a\npolicy for program generation. Fine-tuning with our approach achieves\nsignificantly better performance than competitive methods using Reinforcement\nLearning (RL). On the VirtualHome framework, we get improvements of up to 9.0%\non the Longest Common Subsequence metric and 14.7% on recall-based metrics over\nprevious work on this framework (Puig et al., 2018). The approach is\ndata-efficient, showing larger gains in performance in the low-data regime.\nGenerated programs are also preferred by human evaluators over an RL-based\napproach, and rated higher on relevance, completeness, and human-likeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Sentiment Analysis Using NLP and Different Machine Learning Techniques on US Airline Twitter Data. (arXiv:2110.00859v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00859","description":"<p>Today's business ecosystem has become very competitive. Customer satisfaction\nhas become a major focus for business growth. Business organizations are\nspending a lot of money and human resources on various strategies to understand\nand fulfill their customer's needs. But, because of defective manual analysis\non multifarious needs of customers, many organizations are failing to achieve\ncustomer satisfaction. As a result, they are losing customer's loyalty and\nspending extra money on marketing. We can solve the problems by implementing\nSentiment Analysis. It is a combined technique of Natural Language Processing\n(NLP) and Machine Learning (ML). Sentiment Analysis is broadly used to extract\ninsights from wider public opinion behind certain topics, products, and\nservices. We can do it from any online available data. In this paper, we have\nintroduced two NLP techniques (Bag-of-Words and TF-IDF) and various ML\nclassification algorithms (Support Vector Machine, Logistic Regression,\nMultinomial Naive Bayes, Random Forest) to find an effective approach for\nSentiment Analysis on a large, imbalanced, and multi-classed dataset. Our best\napproaches provide 77% accuracy using Support Vector Machine and Logistic\nRegression with Bag-of-Words technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tusar_M/0/1/0/all/0/1\">Md. Taufiqul Haque Khan Tusar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Touhidul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Case Study to Reveal if an Area of Interest has a Trend in Ongoing Tweets Using Word and Sentence Embeddings. (arXiv:2110.00866v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00866","description":"<p>In the field of Natural Language Processing, information extraction from\ntexts has been the objective of many researchers for years. Many different\ntechniques have been applied in order to reveal the opinion that a tweet might\nhave, thus understanding the sentiment of the small writing up to 280\ncharacters. Other than figuring out the sentiment of a tweet, a study can also\nfocus on finding the correlation of the tweets with a certain area of interest,\nwhich constitutes the purpose of this study. In order to reveal if an area of\ninterest has a trend in ongoing tweets, we have proposed an easily applicable\nautomated methodology in which the Daily Mean Similarity Scores that show the\nsimilarity between the daily tweet corpus and the target words representing our\narea of interest is calculated by using a na\\\"ive correlation-based technique\nwithout training any Machine Learning Model. The Daily Mean Similarity Scores\nhave mainly based on cosine similarity and word/sentence embeddings computed by\nMultilanguage Universal Sentence Encoder and showed main opinion stream of the\ntweets with respect to a certain area of interest, which proves that an ongoing\ntrend of a specific subject on Twitter can easily be captured in almost real\ntime by using the proposed methodology in this study. We have also compared the\neffectiveness of using word versus sentence embeddings while applying our\nmethodology and realized that both give almost the same results, whereas using\nword embeddings requires less computational time than sentence embeddings, thus\nbeing more effective. This paper will start with an introduction followed by\nthe background information about the basics, then continue with the explanation\nof the proposed methodology and later on finish by interpreting the results and\nconcluding the findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aslan_I/0/1/0/all/0/1\">&#x130;smail Aslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_Y/0/1/0/all/0/1\">Y&#xfc;cel Top&#xe7;u</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subtractive mountain clustering algorithm applied to a chatbot to assist elderly people in medication intake. (arXiv:2110.00933v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00933","description":"<p>Errors in medication intake among elderly people are very common. One of the\nmain causes for this is their loss of ability to retain information. The high\namount of medicine intake required by the advanced age is another limiting\nfactor. Thence, the design of an interactive aid system, preferably using\nnatural language, to help the older population with medication is in demand. A\nchatbot based on a subtractive cluster algorithm, included in unsupervised\nlearned models, is the chosen solution since the processing of natural\nlanguages is a necessary step in view to construct a chatbot able to answer\nquestions that older people may pose upon themselves concerning a particular\ndrug. In this work, the subtractive mountain clustering algorithm has been\nadapted to the problem of natural languages processing. This algorithm version\nallows for the association of a set of words into clusters. After finding the\ncentre of every cluster -- the most relevant word, all the others are\naggregated according to a defined metric adapted to the language processing\nrealm. All the relevant stored information is processed, as well as the\nquestions, by the algorithm. The correct processing of the text enables the\nchatbot to produce answers that relate to the posed queries. To validate the\nmethod, we use the package insert of a drug as the available information and\nformulate associated questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clar_N/0/1/0/all/0/1\">Neuza Clar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgado_P/0/1/0/all/0/1\">Paulo A. Salgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdicoulis_T/0/1/0/all/0/1\">T-P Azevedo Perdico&#xfa;lis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Likelihood Ratio Estimation for High- to Zero-frequency N-grams. (arXiv:2110.00946v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00946","description":"<p>Likelihood ratios (LRs), which are commonly used for probabilistic data\nprocessing, are often estimated based on the frequency counts of individual\nelements obtained from samples. In natural language processing, an element can\nbe a continuous sequence of $N$ items, called an $N$-gram, in which each item\nis a word, letter, etc. In this paper, we attempt to estimate LRs based on\n$N$-gram frequency information. A naive estimation approach that uses only\n$N$-gram frequencies is sensitive to low-frequency (rare) $N$-grams and not\napplicable to zero-frequency (unobserved) $N$-grams; these are known as the\nlow- and zero-frequency problems, respectively. To address these problems, we\npropose a method for decomposing $N$-grams into item units and then applying\ntheir frequencies along with the original $N$-gram frequencies. Our method can\nobtain the estimates of unobserved $N$-grams by using the unit frequencies.\nAlthough using only unit frequencies ignores dependencies between items, our\nmethod takes advantage of the fact that certain items often co-occur in\npractice and therefore maintains their dependencies by using the relevant\n$N$-gram frequencies. We also introduce a regularization to achieve robust\nestimation for rare $N$-grams. Our experimental results demonstrate that our\nmethod is effective at solving both problems and can effectively control\ndependencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kikuchi_M/0/1/0/all/0/1\">Masato Kikuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawakami_K/0/1/0/all/0/1\">Kento Kawakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1\">Kazuho Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_M/0/1/0/all/0/1\">Mitsuo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umemura_K/0/1/0/all/0/1\">Kyoji Umemura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LexGLUE: A Benchmark Dataset for Legal Language Understanding in English. (arXiv:2110.00976v1 [cs.CL])","link":"http://arxiv.org/abs/2110.00976","description":"<p>Law, interpretations of law, legal arguments, agreements, etc. are typically\nexpressed in writing, leading to the production of vast corpora of legal text.\nTheir analysis, which is at the center of legal practice, becomes increasingly\nelaborate as these collections grow in size. Natural language understanding\n(NLU) technologies can be a valuable tool to support legal practitioners in\nthese endeavors. Their usefulness, however, largely depends on whether current\nstate-of-the-art models can generalize across various tasks in the legal\ndomain. To answer this currently open question, we introduce the Legal General\nLanguage Understanding Evaluation (LexGLUE) benchmark, a collection of datasets\nfor evaluating model performance across a diverse set of legal NLU tasks in a\nstandardized way. We also provide an evaluation and analysis of several generic\nand legal-oriented models demonstrating that the latter consistently offer\nperformance improvements across multiple tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1\">Abhik Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_D/0/1/0/all/0/1\">Dirk Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bommarito_M/0/1/0/all/0/1\">Michael Bommarito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Recurrent Neural Networks is all we need for clinical events predictions using EHR data. (arXiv:2110.00998v1 [cs.LG])","link":"http://arxiv.org/abs/2110.00998","description":"<p>Recently, there is great interest to investigate the application of deep\nlearning models for the prediction of clinical events using electronic health\nrecords (EHR) data. In EHR data, a patient's history is often represented as a\nsequence of visits, and each visit contains multiple events. As a result, deep\nlearning models developed for sequence modeling, like recurrent neural networks\n(RNNs) are common architecture for EHR-based clinical events predictive models.\nWhile a large variety of RNN models were proposed in the literature, it is\nunclear if complex architecture innovations will offer superior predictive\nperformance. In order to move this field forward, a rigorous evaluation of\nvarious methods is needed. In this study, we conducted a thorough benchmark of\nRNN architectures in modeling EHR data. We used two prediction tasks: the risk\nfor developing heart failure and the risk of early readmission for inpatient\nhospitalization. We found that simple gated RNN models, including GRUs and\nLSTMs, often offer competitive results when properly tuned with Bayesian\nOptimization, which is in line with similar to findings in the natural language\nprocessing (NLP) domain. For reproducibility, Our codebase is shared at\nhttps://github.com/ZhiGroup/pytorch_ehr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasmy_L/0/1/0/all/0/1\">Laila Rasmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_X/0/1/0/all/0/1\">Xin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hong Thoai Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yujia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiryaki_F/0/1/0/all/0/1\">Firat Tiryaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhi_D/0/1/0/all/0/1\">Degui Zhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v1 [cs.CV])","link":"http://arxiv.org/abs/2110.01013","description":"<p>Today's VQA models still tend to capture superficial linguistic correlations\nin the training set and fail to generalize to the test set with different QA\ndistributions. To reduce these language biases, recent VQA works introduce an\nauxiliary question-only model to regularize the training of targeted VQA model,\nand achieve dominating performance on diagnostic benchmarks for\nout-of-distribution testing. However, due to complex model design, these\nensemble-based methods are unable to equip themselves with two indispensable\ncharacteristics of an ideal VQA model: 1) Visual-explainable: The model should\nrely on the right visual regions when making decisions. 2) Question-sensitive:\nThe model should be sensitive to the linguistic variations in questions. To\nthis end, we propose a novel model-agnostic Counterfactual Samples Synthesizing\nand Training (CSST) strategy. After training with CSST, VQA models are forced\nto focus on all critical objects and words, which significantly improves both\nvisual-explainable and question-sensitive abilities. Specifically, CSST is\ncomposed of two parts: Counterfactual Samples Synthesizing (CSS) and\nCounterfactual Samples Training (CST). CSS generates counterfactual samples by\ncarefully masking critical objects in images or words in questions and\nassigning pseudo ground-truth answers. CST not only trains the VQA models with\nboth complementary samples to predict respective ground-truth answers, but also\nurges the VQA models to further distinguish the original samples and\nsuperficially similar counterfactual ones. To facilitate the CST training, we\npropose two variants of supervised contrastive loss for VQA, and design an\neffective positive and negative sample selection mechanism based on CSS.\nExtensive experiments have shown the effectiveness of CSST. Particularly, by\nbuilding on top of model LMH+SAR, we achieve record-breaking performance on all\nOOD benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuhang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yulei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Project Debater APIs: Decomposing the AI Grand Challenge. (arXiv:2110.01029v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01029","description":"<p>Project Debater was revealed in 2019 as the first AI system that can debate\nhuman experts on complex topics. Engaging in a live debate requires a diverse\nset of skills, and Project Debater has been developed accordingly as a\ncollection of components, each designed to perform a specific subtask. Project\nDebater APIs provide access to many of these capabilities, as well as to more\nrecently developed ones. This diverse set of web services, publicly available\nfor academic use, includes core NLP services, argument mining and analysis\ncapabilities, and higher-level services for content summarization. We describe\nthese APIs and their performance, and demonstrate how they can be used for\nbuilding practical solutions. In particular, we will focus on Key Point\nAnalysis, a novel technology that identifies the main points and their\nprevalence in a collection of texts such as survey responses and user reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Haim_R/0/1/0/all/0/1\">Roy Bar-Haim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantor_Y/0/1/0/all/0/1\">Yoav Kantor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1\">Elad Venezian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Document Keyphrase Extraction: A Literature Review and the First Dataset. (arXiv:2110.01073v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01073","description":"<p>Keyphrase extraction has been comprehensively researched within the\nsingle-document setting, with an abundance of methods and a wealth of datasets.\nIn contrast, multi-document keyphrase extraction has been infrequently studied,\ndespite its utility for describing sets of documents, and its use in\nsummarization. Moreover, no dataset existed for multi-document keyphrase\nextraction, hindering the progress of the task. Recent advances in multi-text\nprocessing make the task an even more appealing challenge to pursue. To\ninitiate this pursuit, we present here the first literature review and the\nfirst dataset for the task, MK-DUC-01, which can serve as a new benchmark. We\ntest several keyphrase extraction baselines on our data and show their results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shapira_O/0/1/0/all/0/1\">Ori Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amsterdamer_Y/0/1/0/all/0/1\">Yael Amsterdamer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Voice-Activated Framework using Self-supervised Learning. (arXiv:2110.01077v1 [eess.AS])","link":"http://arxiv.org/abs/2110.01077","description":"<p>Self-supervised learning methods such as wav2vec 2.0 have shown promising\nresults in learning speech representations from unlabelled and untranscribed\nspeech data that are useful for speech recognition. Since these representations\nare learned without any task-specific supervision, they can also be useful for\nother voice-activated tasks like speaker verification, keyword spotting,\nemotion classification etc. In our work, we propose a general purpose framework\nfor adapting a pre-trained wav2vec 2.0 model for different voice-activated\ntasks. We develop downstream network architectures that operate on the\ncontextualized speech representations of wav2vec 2.0 to adapt the\nrepresentations for solving a given task. Finally, we extend our framework to\nperform multi-task learning by jointly optimizing the network parameters on\nmultiple voice activated tasks using a shared transformer backbone. Both of our\nsingle and multi-task frameworks achieve state-of-the-art results in speaker\nverification and keyword spotting benchmarks. Our best performing models\nachieve 1.98% and 3.15% EER on VoxCeleb1 test set when trained on VoxCeleb2 and\nVoxCeleb1 respectively, and 98.23% accuracy on Google Speech Commands v1.0\nkeyword spotting dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Shehzeen Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_V/0/1/0/all/0/1\">Van Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visser_E/0/1/0/all/0/1\">Erik Visser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Persuasion in Computational Argumentation. (arXiv:2110.01078v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01078","description":"<p>Opinion formation and persuasion in argumentation are affected by three major\nfactors: the argument itself, the source of the argument, and the properties of\nthe audience. Understanding the role of each and the interplay between them is\ncrucial for obtaining insights regarding argument interpretation and\ngeneration. It is particularly important for building effective argument\ngeneration systems that can take both the discourse and the audience\ncharacteristics into account. Having such personalized argument generation\nsystems would be helpful to expose individuals to different viewpoints and help\nthem make a more fair and informed decision on an issue. Even though studies in\nSocial Sciences and Psychology have shown that source and audience effects are\nessential components of the persuasion process, most research in computational\npersuasion has focused solely on understanding the characteristics of\npersuasive language. In this thesis, we make several contributions to\nunderstand the relative effect of the source, audience, and language in\ncomputational persuasion. We first introduce a large-scale dataset with\nextensive user information to study these factors' effects simultaneously.\nThen, we propose models to understand the role of the audience's prior beliefs\non their perception of arguments. We also investigate the role of social\ninteractions and engagement in understanding users' success in online debating\nover time. We find that the users' prior beliefs and social interactions play\nan essential role in predicting their success in persuasion. Finally, we\nexplore the importance of incorporating contextual information to predict\nargument impact and show improvements compared to encoding only the text of the\narguments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Examples Generation for Reducing Implicit Gender Bias in Pre-trained Models. (arXiv:2110.01094v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01094","description":"<p>Over the last few years, Contextualized Pre-trained Neural Language Models,\nsuch as BERT, GPT, have shown significant gains in various NLP tasks. To\nenhance the robustness of existing pre-trained models, one way is adversarial\nexamples generation and evaluation for conducting data augmentation or\nadversarial learning. In the meanwhile, gender bias embedded in the models\nseems to be a serious problem in practical applications. Many researches have\ncovered the gender bias produced by word-level information(e.g.\ngender-stereotypical occupations), while few researchers have investigated the\nsentence-level cases and implicit cases.\n</p>\n<p>In this paper, we proposed a method to automatically generate implicit gender\nbias samples at sentence-level and a metric to measure gender bias. Samples\ngenerated by our method will be evaluated in terms of accuracy. The metric will\nbe used to guide the generation of examples from Pre-trained models. Therefore,\nthose examples could be used to impose attacks on Pre-trained Models. Finally,\nwe discussed the evaluation efficacy of our generated examples on reducing\ngender bias for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wenqian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaojia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cassie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+A_J/0/1/0/all/0/1\">Ji A</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Language Models for Understanding of Temporal Expressions. (arXiv:2110.01113v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01113","description":"<p>We present three Natural Language Inference (NLI) challenge sets that can\nevaluate NLI models on their understanding of temporal expressions. More\nspecifically, we probe these models for three temporal properties: (a) the\norder between points in time, (b) the duration between two points in time, (c)\nthe relation between the magnitude of times specified in different units. We\nfind that although large language models fine-tuned on MNLI have some basic\nperception of the order between points in time, at large, these models do not\nhave a thorough understanding of the relation between temporal expressions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thukral_S/0/1/0/all/0/1\">Shivin Thukral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukreja_K/0/1/0/all/0/1\">Kunal Kukreja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavouras_C/0/1/0/all/0/1\">Christian Kavouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured abbreviation expansion in context. (arXiv:2110.01140v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01140","description":"<p>Ad hoc abbreviations are commonly found in informal communication channels\nthat favor shorter messages. We consider the task of reversing these\nabbreviations in context to recover normalized, expanded versions of\nabbreviated messages. The problem is related to, but distinct from, spelling\ncorrection, in that ad hoc abbreviations are intentional and may involve\nsubstantial differences from the original words. Ad hoc abbreviations are\nproductively generated on-the-fly, so they cannot be resolved solely by\ndictionary lookup. We generate a large, open-source data set of ad hoc\nabbreviations. This data is used to study abbreviation strategies and to\ndevelop two strong baselines for abbreviation expansion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorman_K/0/1/0/all/0/1\">Kyle Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirov_C/0/1/0/all/0/1\">Christo Kirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roark_B/0/1/0/all/0/1\">Brian Roark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sproat_R/0/1/0/all/0/1\">Richard Sproat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Interplay Between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis. (arXiv:2110.01147v1 [cs.SD])","link":"http://arxiv.org/abs/2110.01147","description":"<p>Are end-to-end text-to-speech (TTS) models over-parametrized? To what extent\ncan these models be pruned, and what happens to their synthesis capabilities?\nThis work serves as a starting point to explore pruning both spectrogram\nprediction networks and vocoders. We thoroughly investigate the tradeoffs\nbetween sparstiy and its subsequent effects on synthetic speech. Additionally,\nwe explored several aspects of TTS pruning: amount of finetuning data versus\nsparsity, TTS-Augmentation to utilize unspoken text, and combining knowledge\ndistillation and pruning. Our findings suggest that not only are end-to-end TTS\nmodels highly prunable, but also, perhaps surprisingly, pruned TTS models can\nproduce synthetic speech with equal or higher naturalness and intelligibility,\nwith similar prosody. All of our experiments are conducted on publicly\navailable models, and findings in this work are backed by large-scale\nsubjective tests and objective measures. Code and 200 pruned models are made\navailable to facilitate future research on efficiency in TTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Cheng-I Jeff Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_E/0/1/0/all/0/1\">Erica Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yi-Lun Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yung-Sung Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Alexander H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cox_D/0/1/0/all/0/1\">David Cox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts. (arXiv:2110.01159v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01159","description":"<p>Recent models in developing summarization systems consist of millions of\nparameters and the model performance is highly dependent on the abundance of\ntraining data. While most existing summarization corpora contain data in the\norder of thousands to one million, generation of large-scale summarization\ndatasets in order of couple of millions is yet to be explored. Practically,\nmore data is better at generalizing the training patterns to unseen data. In\nthis paper, we introduce TLDR9+ -- a large-scale summarization dataset --\ncontaining over 9 million training instances extracted from Reddit discussion\nforum (https://github.com/sajastu/reddit_collector). This dataset is\nspecifically gathered to perform extreme summarization (i.e., generating\none-sentence summary in high compression and abstraction) and is more than\ntwice larger than the previously proposed dataset. We go one step further and\nwith the help of human annotations, we distill a more fine-grained dataset by\nsampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We\nfurther pinpoint different state-of-the-art summarization models on our\nproposed datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1\">Sajad Sotudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deilamsalehi_H/0/1/0/all/0/1\">Hanieh Deilamsalehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1\">Nazli Goharian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Topics: Discovering Latent Healthcare Objectives from Event Sequences. (arXiv:2110.01160v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01160","description":"<p>A meaningful understanding of clinical protocols and patient pathways helps\nimprove healthcare outcomes. Electronic health records (EHR) reflect real-world\ntreatment behaviours that are used to enhance healthcare management but present\nchallenges; protocols and pathways are often loosely defined and with elements\nfrequently not recorded in EHRs, complicating the enhancement. To solve this\nchallenge, healthcare objectives associated with healthcare management\nactivities can be indirectly observed in EHRs as latent topics. Topic models,\nsuch as Latent Dirichlet Allocation (LDA), are used to identify latent patterns\nin EHR data. However, they do not examine the ordered nature of EHR sequences,\nnor do they appraise individual events in isolation. Our novel approach, the\nCategorical Sequence Encoder (CaSE) addresses these shortcomings. The\nsequential nature of EHRs is captured by CaSE's event-level representations,\nrevealing latent healthcare objectives. In synthetic EHR sequences, CaSE\noutperforms LDA by up to 37% at identifying healthcare objectives. In the\nreal-world MIMIC-III dataset, CaSE identifies meaningful representations that\ncould critically enhance protocol and pathway development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caruana_A/0/1/0/all/0/1\">Adrian Caruana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandara_M/0/1/0/all/0/1\">Madhushi Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catchpoole_D/0/1/0/all/0/1\">Daniel Catchpoole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_P/0/1/0/all/0/1\">Paul J Kennedy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Metric for Evaluating Semantics Preservation. (arXiv:2110.01176v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01176","description":"<p>In this paper, we leverage pre-trained language models (PLMs) to precisely\nevaluate the semantics preservation of edition process on sentences. Our\nmetric, Neighbor Distribution Divergence (NDD), evaluates the disturbance on\npredicted distribution of neighboring words from mask language model (MLM). NDD\nis capable of detecting precise changes in semantics which are easily ignored\nby text similarity. By exploiting the property of NDD, we implement a\nunsupervised and even training-free algorithm for extractive sentence\ncompression. We show that our NDD-based algorithm outperforms previous\nperplexity-based unsupervised algorithm by a large margin. For further\nexploration on interpretability, we evaluate NDD by pruning on syntactic\ndependency treebanks and apply NDD for predicate detection as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The state-of-the-art in text-based automatic personality prediction. (arXiv:2110.01186v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01186","description":"<p>Personality detection is an old topic in psychology and Automatic Personality\nPrediction (or Perception) (APP) is the automated (computationally) forecasting\nof the personality on different types of human generated/exchanged contents\n(such as text, speech, image, video). The principal objective of this study is\nto offer a shallow (overall) review of natural language processing approaches\non APP since 2010. With the advent of deep learning and following it\ntransfer-learning and pre-trained model in NLP, APP research area has been a\nhot topic, so in this review, methods are categorized into three; pre-trained\nindependent, pre-trained model based, multimodal approaches. Also, to achieve a\ncomprehensive comparison, reported results are informed by datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_A/0/1/0/all/0/1\">Ali-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1\">Mohammad-Reza Feizi-Derakhshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Majid Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikzad_Khasmakhi_N/0/1/0/all/0/1\">Narjes Nikzad-Khasmakhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_Chenaghlu_M/0/1/0/all/0/1\">Meysam Asgari-Chenaghlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akan_T/0/1/0/all/0/1\">Taymaz Akan</a> (Rahkar-Farshi), <a href=\"http://arxiv.org/find/cs/1/au:+Ranjbar_Khadivi_M/0/1/0/all/0/1\">Mehrdad Ranjbar-Khadivi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafarni_Moattar_E/0/1/0/all/0/1\">Elnaz Zafarni-Moattar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanbakhsh_Naghadeh_Z/0/1/0/all/0/1\">Zoleikha Jahanbakhsh-Naghadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LawSum: A weakly supervised approach for Indian Legal Document Summarization. (arXiv:2110.01188v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01188","description":"<p>Unlike the courts in western countries, public records of Indian judiciary\nare completely unstructured and noisy. No large scale publicly available\nannotated datasets of Indian legal documents exist till date. This limits the\nscope for legal analytics research. In this work, we propose a new dataset\nconsisting of over 10,000 judgements delivered by the supreme court of India\nand their corresponding hand written summaries. The proposed dataset is\npre-processed by normalising common legal abbreviations, handling spelling\nvariations in named entities, handling bad punctuations and accurate sentence\ntokenization. Each sentence is tagged with their rhetorical roles. We also\nannotate each judgement with several attributes like date, names of the\nplaintiffs, defendants and the people representing them, judges who delivered\nthe judgement, acts/statutes that are cited and the most common citations used\nto refer the judgement. Further, we propose an automatic labelling technique\nfor identifying sentences which have summary worthy information. We demonstrate\nthat this auto labeled data can be used effectively to train a weakly\nsupervised sentence extractor with high accuracy. Some possible applications of\nthis dataset besides legal document summarization can be in retrieval, citation\nanalysis and prediction of decisions by a particular judge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_V/0/1/0/all/0/1\">Vedant Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_V/0/1/0/all/0/1\">Vidit Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metha_P/0/1/0/all/0/1\">Parth Metha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_N/0/1/0/all/0/1\">Nimita Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention. (arXiv:2006.03654v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.03654","description":"<p>Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Named Entity Recognition for Kazakh. (arXiv:2007.13626v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2007.13626","description":"<p>We present several neural networks to address the task of named entity\nrecognition for morphologically complex languages (MCL). Kazakh is a\nmorphologically complex language in which each root/stem can produce hundreds\nor thousands of variant word forms. This nature of the language could lead to a\nserious data sparsity problem, which may prevent the deep learning models from\nbeing well trained for under-resourced MCLs. In order to model the MCLs' words\neffectively, we introduce root and entity tag embedding plus tensor layer to\nthe neural networks. The effects of those are significant for improving NER\nmodel performance of MCLs. The proposed models outperform state-of-the-art\nincluding character-based approaches, and can be potentially applied to other\nmorphologically complex languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tolegen_G/0/1/0/all/0/1\">Gulmira Tolegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toleu_A/0/1/0/all/0/1\">Alymzhan Toleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamyrbayev_O/0/1/0/all/0/1\">Orken Mamyrbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mussabayev_R/0/1/0/all/0/1\">Rustam Mussabayev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Text Generation with Pattern-Exploiting Training. (arXiv:2012.11926v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.11926","description":"<p>Providing pretrained language models with simple task descriptions in natural\nlanguage enables them to solve some tasks in a fully unsupervised fashion.\nMoreover, when combined with regular learning from examples, this idea yields\nimpressive few-shot results for a wide range of text classification tasks. It\nis also a promising direction to improve data efficiency in generative\nsettings, but there are several challenges to using a combination of task\ndescriptions and example-based learning for text generation. In particular, it\nis crucial to find task descriptions that are easy to understand for the\npretrained model and to ensure that it actually makes good use of them;\nfurthermore, effective measures against overfitting have to be implemented. In\nthis paper, we show how these challenges can be tackled: We introduce GenPET, a\nmethod for text generation that is based on pattern-exploiting training, a\nrecent approach for combining textual instructions with supervised learning\nthat only works for classification tasks. On several summarization and headline\ngeneration datasets, GenPET gives consistent improvements over strong baselines\nin few-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast WordPiece Tokenization. (arXiv:2012.15524v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15524","description":"<p>Tokenization is a fundamental preprocessing step for almost all NLP tasks. In\nthis paper, we propose efficient algorithms for the WordPiece tokenization used\nin BERT, from single-word tokenization to general text (e.g., sentence)\ntokenization. When tokenizing a single word, WordPiece uses a\nlongest-match-first strategy, known as maximum matching. The best known\nalgorithms so far are O(n^2) (where n is the input length) or O(nm) (where m is\nthe maximum vocabulary token length). We propose a novel algorithm whose\ntokenization complexity is strictly O(n). Our method is inspired by the\nAho-Corasick algorithm. We introduce additional linkages on top of the trie\nbuilt from the vocabulary, allowing smart transitions when the trie matching\ncannot continue. For general text, we further propose an algorithm that\ncombines pre-tokenization (splitting the text into words) and our linear-time\nWordPiece method into a single pass. Experimental results show that our method\nis 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text\non average for general text tokenization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinying Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salcianu_A/0/1/0/all/0/1\">Alex Salcianu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dopson_D/0/1/0/all/0/1\">Dave Dopson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vy\\=akarana: A Colorless Green Benchmark for Syntactic Evaluation in Indic Languages. (arXiv:2103.00854v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.00854","description":"<p>While there has been significant progress towards developing NLU resources\nfor Indic languages, syntactic evaluation has been relatively less explored.\nUnlike English, Indic languages have rich morphosyntax, grammatical genders,\nfree linear word-order, and highly inflectional morphology. In this paper, we\nintroduce Vy\\=akarana: a benchmark of Colorless Green sentences in Indic\nlanguages for syntactic evaluation of multilingual language models. The\nbenchmark comprises four syntax-related tasks: PoS Tagging, Syntax Tree-depth\nPrediction, Grammatical Case Marking, and Subject-Verb Agreement. We use the\ndatasets from the evaluation tasks to probe five multilingual language models\nof varying architectures for syntax in Indic languages. Due to its prevalence,\nwe also include a code-switching setting in our experiments. Our results show\nthat the token-level and sentence-level representations from the Indic language\nmodels (IndicBERT and MuRIL) do not capture the syntax in Indic languages as\nefficiently as the other highly multilingual language models. Further, our\nlayer-wise probing experiments reveal that while mBERT, DistilmBERT, and XLM-R\nlocalize the syntax in middle layers, the Indic language models do not show\nsuch syntactic localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhillon_J/0/1/0/all/0/1\">Jasleen Dhillon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahurkar_S/0/1/0/all/0/1\">Siddhant Mahurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_S/0/1/0/all/0/1\">Saumitra Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malhotra_M/0/1/0/all/0/1\">Manav Malhotra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1\">Veeky Baths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Mandarin-English Code-switching Speech Recognition. (arXiv:2104.02258v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.02258","description":"<p>Mandarin-English code-switching (CS) is frequently used among East and\nSoutheast Asian people. However, the intra-sentence language switching of the\ntwo very different languages makes recognizing CS speech challenging.\nMeanwhile, the recent successful non-autoregressive (NAR) ASR models remove the\nneed for left-to-right beam decoding in autoregressive (AR) models and achieved\noutstanding performance and fast inference speed, but it has not been applied\nto Mandarin-English CS speech recognition. This paper takes advantage of the\nMask-CTC NAR ASR framework to tackle the CS speech recognition issue. We\nfurther propose to change the Mandarin output target of the encoder to Pinyin\nfor faster encoder training and introduce the Pinyin-to-Mandarin decoder to\nlearn contextualized information. Moreover, we use word embedding label\nsmoothing to regularize the decoder with contextualized information and\nprojection matrix regularization to bridge that gap between the encoder and\ndecoder. We evaluate these methods on the SEAME corpus and achieved exciting\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chuang_S/0/1/0/all/0/1\">Shun-Po Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sung-Feng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Datasets with Pretrained Language Models. (arXiv:2104.07540v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07540","description":"<p>To obtain high-quality sentence embeddings from pretrained language models\n(PLMs), they must either be augmented with additional pretraining objectives or\nfinetuned on a large set of labeled text pairs. While the latter approach\ntypically outperforms the former, it requires great human effort to generate\nsuitable datasets of sufficient size. In this paper, we show how PLMs can be\nleveraged to obtain high-quality sentence embeddings without the need for\nlabeled data, finetuning or modifications to the pretraining objective: We\nutilize the generative abilities of large and high-performing PLMs to generate\nentire datasets of labeled text pairs from scratch, which we then use for\nfinetuning much smaller and more efficient models. Our fully unsupervised\napproach outperforms strong baselines on several semantic textual similarity\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation. (arXiv:2104.08200v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08200","description":"<p>A benchmark provides an ecosystem to measure the advancement of models with\nstandard datasets and automatic and human evaluation metrics. We introduce\nIndoNLG, the first such benchmark for the Indonesian language for natural\nlanguage generation (NLG). It covers six tasks: summarization, question\nanswering, open chitchat, as well as three different language-pairs of machine\ntranslation tasks. We provide a vast and clean pre-training corpus of\nIndonesian, Sundanese, and Javanese datasets called Indo4B-Plus, which is used\nto train our pre-trained NLG model, IndoBART. We evaluate the effectiveness and\nefficiency of IndoBART by conducting extensive evaluation on all IndoNLG tasks.\nOur findings show that IndoBART achieves competitive performance on Indonesian\ntasks with five times fewer parameters compared to the largest multilingual\nmodel in our benchmark, mBART-LARGE (Liu et al., 2020), and an almost 4x and\n2.5x faster inference time on the CPU and GPU respectively. We additionally\ndemonstrate the ability of IndoBART to learn Javanese and Sundanese, and it\nachieves decent performance on machine translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilie_B/0/1/0/all/0/1\">Bryan Wilie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincentio_K/0/1/0/all/0/1\">Karissa Vincentio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1\">Zhi Yuan Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahar_S/0/1/0/all/0/1\">Syafri Bahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodra_M/0/1/0/all/0/1\">Masayu Leylia Khodra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1\">Ayu Purwarianti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Imitating Metrics for Training and Evaluating Privacy Preserving Emotion Recognition Models Using Sociolinguistic Knowledge. (arXiv:2104.08792v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08792","description":"<p>Privacy preservation is a crucial component of any real-world application.\nBut, in applications relying on machine learning backends, privacy is\nchallenging because models often capture more than what the model was initially\ntrained for, resulting in the potential leakage of sensitive information. In\nthis paper, we propose an automatic and quantifiable metric that allows us to\nevaluate humans' perception of a model's ability to preserve privacy with\nrespect to sensitive variables. In this paper, we focus on saliency-based\nexplanations, explanations that highlight regions of the input text, to infer\ninternal workings of a black box model. We use the degree with which\ndifferences in interpretation of general vs privacy preserving models correlate\nwith sociolinguistic biases to inform metric design. We show how certain\ncommonly-used methods that seek to preserve privacy do not align with human\nperception of privacy preservation leading to distrust about model's claims. We\ndemonstrate the versatility of our proposed metric by validating its utility\nfor measuring cross corpus generalization for both privacy and emotion.\nFinally, we conduct crowdsourcing experiments to evaluate the inclination of\nthe evaluators to choose a particular model for a given purpose when model\nexplanations are provided, and show a positive relationship with the proposed\nmetric. To the best of our knowledge, we take the first step in proposing\nautomatic and quantifiable metrics that best align with human perception of\nmodel's ability for privacy preservation, allowing for cost-effective model\ndevelopment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_M/0/1/0/all/0/1\">Mimansa Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Provost_E/0/1/0/all/0/1\">Emily Mower Provost</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks. (arXiv:2104.08815v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08815","description":"<p>Increasing concerns and regulations about data privacy and sparsity\nnecessitate the study of privacy-preserving, decentralized learning methods for\nnatural language processing (NLP) tasks. Federated learning (FL) provides\npromising approaches for a large number of clients (e.g., personal devices or\norganizations) to collaboratively learn a shared global model to benefit all\nclients while allowing users to keep their data locally. Despite interest in\nstudying FL methods for NLP tasks, a systematic comparison and analysis is\nlacking in the literature. Herein, we present the FedNLP, a benchmarking\nframework for evaluating federated learning methods on four different task\nformulations: text classification, sequence tagging, question answering, and\nseq2seq. We propose a universal interface between Transformer-based language\nmodels (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under\nvarious non-IID partitioning strategies. Our extensive experiments with FedNLP\nprovide empirical comparisons between FL methods and helps us better understand\nthe inherent challenges of this direction. The comprehensive analysis points to\nintriguing and exciting future research aimed at developing FL methods for NLP\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zihang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Gender by First Name Using Character-level Machine Learning. (arXiv:2106.10156v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10156","description":"<p>Predicting gender by the first name is not a simple task. In many\napplications, especially in the natural language processing (NLP) field, this\ntask may be necessary, mainly when considering foreign names. In this paper, we\nexamined and implemented several machine learning algorithms, such as extra\ntrees, KNN, Naive Bayes, SVM, random forest, gradient boosting, light GBM,\nlogistic regression, ridge classifier, and deep neural network models, such as\nMLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first name. A\ndataset of Brazilian names is used to train and evaluate the models. We\nanalyzed the accuracy, recall, precision, f1 score, and confusion matrix to\nmeasure the models' performances. The results indicate that the gender\nprediction can be performed from the feature extraction strategy looking at the\nnames as a set of strings. Some models accurately predict gender in more than\n95% of the cases. The recurrent models overcome the feedforward models in this\nbinary classification problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rego_R/0/1/0/all/0/1\">Rosana C. B. Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1\">Ver&#xf4;nica M. L. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_V/0/1/0/all/0/1\">Victor M. Fernandes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03228","description":"<p>Recent studies on compression of pretrained language models (e.g., BERT)\nusually use preserved accuracy as the metric for evaluation. In this paper, we\npropose two new metrics, label loyalty and probability loyalty that measure how\nclosely a compressed model (i.e., student) mimics the original model (i.e.,\nteacher). We also explore the effect of compression with regard to robustness\nunder adversarial attacks. We benchmark quantization, pruning, knowledge\ndistillation and progressive module replacing with loyalty and robustness. By\ncombining multiple compression techniques, we provide a practical strategy to\nachieve better accuracy, loyalty and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Search for a Search Method -- Simple Heuristics Suffice for Adversarial Text Attacks. (arXiv:2109.07926v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07926","description":"<p>Recently more attention has been given to adversarial attacks on neural\nnetworks for natural language processing (NLP). A central research topic has\nbeen the investigation of search algorithms and search constraints, accompanied\nby benchmark algorithms and tasks. We implement an algorithm inspired by zeroth\norder optimization-based attacks and compare with the benchmark results in the\nTextAttack framework. Surprisingly, we find that optimization-based methods do\nnot yield any improvement in a constrained setup and slightly benefit from\napproximate gradient information only in unconstrained setups where search\nspaces are larger. In contrast, simple heuristics exploiting nearest neighbors\nwithout querying the target function yield substantial success rates in\nconstrained setups, and nearly full success rate in unconstrained setups, at an\norder of magnitude fewer queries. We conclude from these results that current\nTextAttack benchmark tasks are too easy and constraints are too strict,\npreventing meaningful research on black-box adversarial text attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berger_N/0/1/0/all/0/1\">Nathaniel Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riezler_S/0/1/0/all/0/1\">Stefan Riezler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_S/0/1/0/all/0/1\">Sebastian Ebert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Variational Graph Autoencoders for Unsupervised Cross-domain Prerequisite Chains. (arXiv:2109.08722v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.08722","description":"<p>Prerequisite chain learning helps people acquire new knowledge efficiently.\nWhile people may quickly determine learning paths over concepts in a domain,\nfinding such paths in other domains can be challenging. We introduce\nDomain-Adversarial Variational Graph Autoencoders (DAVGAE) to solve this\ncross-domain prerequisite chain learning task efficiently. Our novel model\nconsists of a variational graph autoencoder (VGAE) and a domain discriminator.\nThe VGAE is trained to predict concept relations through link prediction, while\nthe domain discriminator takes both source and target domain data as input and\nis trained to predict domain labels. Most importantly, this method only needs\nsimple homogeneous graphs as input, compared with the current state-of-the-art\nmodel. We evaluate our model on the LectureBankCD dataset, and results show\nthat our model outperforms recent graph-based benchmarks while using only 1/10\nof graph scale and 1/3 computation time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_V/0/1/0/all/0/1\">Vanessa Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From None to Severe: Predicting Severity in Movie Scripts. (arXiv:2109.09276v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09276","description":"<p>In this paper, we introduce the task of predicting severity of age-restricted\naspects of movie content based solely on the dialogue script. We first\ninvestigate categorizing the ordinal severity of movies on 5 aspects: Sex,\nViolence, Profanity, Substance consumption, and Frightening scenes. The problem\nis handled using a siamese network-based multitask framework which concurrently\nimproves the interpretability of the predictions. The experimental results show\nthat our method outperforms the previous state-of-the-art model and provides\nuseful information to interpret model predictions. The proposed dataset and\nsource code are publicly available at our GitHub repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yigeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_M/0/1/0/all/0/1\">Mahsa Shafaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation. (arXiv:2109.10164v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10164","description":"<p>Intermediate layer knowledge distillation (KD) can improve the standard KD\ntechnique (which only targets the output of teacher and student models)\nespecially over large pre-trained language models. However, intermediate layer\ndistillation suffers from excessive computational burdens and engineering\nefforts required for setting up a proper layer mapping. To address these\nproblems, we propose a RAndom Intermediate Layer Knowledge Distillation\n(RAIL-KD) approach in which, intermediate layers from the teacher model are\nselected randomly to be distilled into the intermediate layers of the student\nmodel. This randomized selection enforce that: all teacher layers are taken\ninto account in the training process, while reducing the computational cost of\nintermediate layer distillation. Also, we show that it act as a regularizer for\nimproving the generalizability of the student model. We perform extensive\nexperiments on GLUE tasks as well as on out-of-domain test sets. We show that\nour proposed RAIL-KD approach outperforms other state-of-the-art intermediate\nlayer KD methods considerably in both performance and training-time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haidar_M/0/1/0/all/0/1\">Md Akmal Haidar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anchuri_N/0/1/0/all/0/1\">Nithin Anchuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. (arXiv:2109.11491v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11491","description":"<p>We present a method for exploring regions around individual points in a\ncontextualized vector space (particularly, BERT space), as a way to investigate\nhow these regions correspond to word senses. By inducing a contextualized\n\"pseudoword\" as a stand-in for a static embedding in the input layer, and then\nperforming masked prediction of a word in the sentence, we are able to\ninvestigate the geometry of the BERT-space in a controlled manner around\nindividual instances. Using our method on a set of carefully constructed\nsentences targeting ambiguous English words, we find substantial regularity in\nthe contextualized space, with regions that correspond to distinct word senses;\nbut between these regions there are occasionally \"sense voids\" -- regions that\ndo not correspond to any intelligible sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1\">Taelin Karidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT got a Date: Introducing Transformers to Temporal Tagging. (arXiv:2109.14927v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14927","description":"<p>Temporal expressions in text play a significant role in language\nunderstanding and correctly identifying them is fundamental to various\nretrieval and natural language processing systems. Previous works have slowly\nshifted from rule-based to neural architectures, capable of tagging expressions\nwith higher accuracy. However, neural models can not yet distinguish between\ndifferent expression types at the same level as their rule-based counterparts.\nIn this work, we aim to identify the most suitable transformer architecture for\njoint temporal tagging and type classification, as well as, investigating the\neffect of semi-supervised training on the performance of these systems. Based\non our study of token classification variants and encoder-decoder\narchitectures, we present a transformer encoder-decoder model using the RoBERTa\nlanguage model as our best performing system. By supplementing training\nresources with weakly labeled data from rule-based systems, our model surpasses\nprevious works in temporal tagging and type classification, especially on rare\nclasses. Our code and pre-trained experiments are available at:\nhttps://github.com/satya77/Transformer_Temporal_Tagger\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Almasian_S/0/1/0/all/0/1\">Satya Almasian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aumiller_D/0/1/0/all/0/1\">Dennis Aumiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_M/0/1/0/all/0/1\">Michael Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-04T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}}]}]}