{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Privacy enabled Financial Text Classification using Differential Privacy and Federated Learning. (arXiv:2110.01643v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01643","description":"<p>Privacy is important considering the financial Domain as such data is highly\nconfidential and sensitive. Natural Language Processing (NLP) techniques can be\napplied for text classification and entity detection purposes in financial\ndomains such as customer feedback sentiment analysis, invoice entity detection,\ncategorisation of financial documents by type etc. Due to the sensitive nature\nof such data, privacy measures need to be taken for handling and training large\nmodels with such data. In this work, we propose a contextualized transformer\n(BERT and RoBERTa) based text classification model integrated with privacy\nfeatures such as Differential Privacy (DP) and Federated Learning (FL). We\npresent how to privately train NLP models and desirable privacy-utility\ntradeoffs and evaluate them on the Financial Phrase Bank dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rerunning OCR -- A Machine Learning Approach to Quality Assessment and Enhancement Prediction. (arXiv:2110.01661v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01661","description":"<p>Iterating with new and improved OCR solutions enforces decisions to be taken\nwhen it comes to targeting the right reprocessing candidates. This especially\napplies when the underlying data collection is of considerable size and rather\ndiverse in terms of fonts, languages, periods of publication and consequently\nOCR quality. This article captures the efforts of the National Library of\nLuxembourg to support those exact decisions. They are crucial in order to\nguarantee low computational overhead and reduced quality degradation risks,\ncombined with a more quantifiable OCR improvement. In particular, this work\nexplains the methodology of the library with respect to text block level\nquality assessment. As an extension of this technique, another contribution\ncomes in the form of a regression model that takes the enhancement potential of\na new OCR engine into account. They both mark promising approaches, especially\nfor cultural institutions dealing with historic data of lower quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. (arXiv:2110.01691v1 [cs.HC])","link":"http://arxiv.org/abs/2110.01691","description":"<p>Although large language models (LLMs) have demonstrated impressive potential\non simple tasks, their breadth of scope, lack of transparency, and insufficient\ncontrollability can make them less effective when assisting humans on more\ncomplex tasks. In response, we introduce the concept of Chaining LLM steps\ntogether, where the output of one step becomes the input for the next, thus\naggregating the gains per step. We first define a set of LLM primitive\noperations useful for Chain construction, then present an interactive system\nwhere users can modify these Chains, along with their intermediate results, in\na modular way. In a 20-person user study, we found that Chaining not only\nimproved the quality of task outcomes, but also significantly enhanced system\ntransparency, controllability, and sense of collaboration. Additionally, we saw\nthat users developed new ways of interacting with LLMs through Chains: they\nleveraged sub-tasks to calibrate model expectations, compared and contrasted\nalternative strategies by observing parallel downstream effects, and debugged\nunexpected model outputs by \"unit-testing\" sub-components of a Chain. In two\ncase studies, we further explore how LLM Chains may be used in future\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Carrie J. Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoEfication: Conditional Computation of Transformer Models for Efficient Inference. (arXiv:2110.01786v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01786","description":"<p>Transformer-based pre-trained language models can achieve superior\nperformance on most NLP tasks due to large parameter capacity, but also lead to\nhuge computation cost. Fortunately, we find by empirical study that, most\ninputs only activate a tiny ratio of neurons during inference. Hence, we\nexplore to accelerate large-model inference by conditional computation based on\nthe sparse activation phenomenon. We propose to transform a large model into\nits mixture-of-experts (MoE) version with equal model size, namely MoEfication.\nModel MoEfication consists of two steps: (1) splitting the parameters of\nfeed-forward neural networks (FFNs) into multiple parts as experts, and (2)\nbuilding expert routers to decide which experts will be used for each input. To\nfurther improve the performance of MoEfied models, we can also fine-tune the\nmodels on downstream tasks, namely parameter calibration. Experimental results\nshow that the MoEfied models can significantly reduce computation cost, e.g.,\nonly activating 20% FFN parameters of a 700-million-parameter model without\nperformance degradation on several downstream tasks including text\nclassification and reading comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts. (arXiv:2110.01799v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01799","description":"<p>Reviewing contracts is a time-consuming procedure that incurs large expenses\nto companies and social inequality to those who cannot afford it. In this work,\nwe propose \"document-level natural language inference (NLI) for contracts\", a\nnovel, real-world application of NLI that addresses such problems. In this\ntask, a system is given a set of hypotheses (such as \"Some obligations of\nAgreement may survive termination.\") and a contract, and it is asked to\nclassify whether each hypothesis is \"entailed by\", \"contradicting to\" or \"not\nmentioned by\" (neutral to) the contract as well as identifying \"evidence\" for\nthe decision as spans in the contract. We annotated and release the largest\ncorpus to date consisting of 607 annotated contracts. We then show that\nexisting models fail badly on our task and introduce a strong baseline, which\n(1) models evidence identification as multi-label classification over spans\ninstead of trying to predict start and end tokens, and (2) employs more\nsophisticated context segmentation for dealing with long documents. We also\nshow that linguistic characteristics of contracts, such as negations by\nexceptions, are contributing to the difficulty of this task and that there is\nmuch room for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koreeda_Y/0/1/0/all/0/1\">Yuta Koreeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey On Neural Word Embeddings. (arXiv:2110.01804v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01804","description":"<p>Understanding human language has been a sub-challenge on the way of\nintelligent machines. The study of meaning in natural language processing (NLP)\nrelies on the distributional hypothesis where language elements get meaning\nfrom the words that co-occur within contexts. The revolutionary idea of\ndistributed representation for a concept is close to the working of a human\nmind in that the meaning of a word is spread across several neurons, and a loss\nof activation will only slightly affect the memory retrieval process.\n</p>\n<p>Neural word embeddings transformed the whole field of NLP by introducing\nsubstantial improvements in all NLP tasks. In this survey, we provide a\ncomprehensive literature review on neural word embeddings. We give theoretical\nfoundations and describe existing work by an interplay between word embeddings\nand language modelling. We provide broad coverage on neural word embeddings,\nincluding early word embeddings, embeddings targeting specific semantic\nrelations, sense embeddings, morpheme embeddings, and finally, contextual\nrepresentations. Finally, we describe benchmark datasets in word embeddings'\nperformance evaluation and downstream tasks along with the performance results\nof/due to word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sezerer_E/0/1/0/all/0/1\">Erhan Sezerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekir_S/0/1/0/all/0/1\">Selma Tekir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation. (arXiv:2110.01811v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01811","description":"<p>Pre-training (PT) and back-translation (BT) are two simple and powerful\nmethods to utilize monolingual data for improving the model performance of\nneural machine translation (NMT). This paper takes the first step to\ninvestigate the complementarity between PT and BT. We introduce two probing\ntasks for PT and BT respectively and find that PT mainly contributes to the\nencoder module while BT brings more benefits to the decoder. Experimental\nresults show that PT and BT are nicely complementary to each other,\nestablishing state-of-the-art performances on the WMT16 English-Romanian and\nEnglish-Russian benchmarks. Through extensive analyses on sentence originality\nand word frequency, we also demonstrate that combining Tagged BT with PT is\nmore helpful to their complementarity, leading to better translation quality.\nSource code is freely available at https://github.com/SunbowLiu/PTvsBT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Truth-Conditional Captioning of Time Series Data. (arXiv:2110.01839v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01839","description":"<p>In this paper, we explore the task of automatically generating natural\nlanguage descriptions of salient patterns in a time series, such as stock\nprices of a company over a week. A model for this task should be able to\nextract high-level patterns such as presence of a peak or a dip. While typical\ncontemporary neural models with attention mechanisms can generate fluent output\ndescriptions for this task, they often generate factually incorrect\ndescriptions. We propose a computational model with a truth-conditional\narchitecture which first runs small learned programs on the input time series,\nthen identifies the programs/patterns which hold true for the given input, and\nfinally conditions on only the chosen valid program (rather than the input time\nseries) to generate the output text description. A program in our model is\nconstructed from modules, which are small neural networks that are designed to\ncapture numerical patterns and temporal information. The modules are shared\nacross multiple programs, enabling compositionality as well as efficient\nlearning of module parameters. The modules, as well as the composition of the\nmodules, are unobserved in data, and we learn them in an end-to-end fashion\nwith the only training signal coming from the accompanying natural language\ntext descriptions. We find that the proposed model is able to generate\nhigh-precision captions even though we consider a small and simple space of\nmodule types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1\">Harsh Jhamtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation Approaches in Natural Language Processing: A Survey. (arXiv:2110.01852v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01852","description":"<p>As an effective strategy, data augmentation (DA) alleviates data scarcity\nscenarios where deep learning techniques may fail. It is widely applied in\ncomputer vision then introduced to natural language processing and achieves\nimprovements in many tasks. One of the main focuses of the DA methods is to\nimprove the diversity of training data, thereby helping the model to better\ngeneralize to unseen testing data. In this survey, we frame DA methods into\nthree categories based on the diversity of augmented data, including\nparaphrasing, noising, and sampling. Our paper sets out to analyze DA methods\nin detail according to the above categories. Further, we also introduce their\napplications in NLP tasks as well as the challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yutai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASR Rescoring and Confidence Estimation with ELECTRA. (arXiv:2110.01857v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01857","description":"<p>In automatic speech recognition (ASR) rescoring, the hypothesis with the\nfewest errors should be selected from the n-best list using a language model\n(LM). However, LMs are usually trained to maximize the likelihood of correct\nword sequences, not to detect ASR errors. We propose an ASR rescoring method\nfor directly detecting errors with ELECTRA, which is originally a pre-training\nmethod for NLP tasks. ELECTRA is pre-trained to predict whether each word is\nreplaced by BERT or not, which can simulate ASR error detection on large text\ncorpora. To make this pre-training closer to ASR error detection, we further\npropose an extended version of ELECTRA called phone-attentive ELECTRA\n(P-ELECTRA). In the pre-training of P-ELECTRA, each word is replaced by a\nphone-to-word conversion model, which leverages phone information to generate\nacoustically similar words. Since our rescoring method is optimized for\ndetecting errors, it can also be used for word-level confidence estimation.\nExperimental evaluations on the Librispeech and TED-LIUM2 corpora show that our\nrescoring method with ELECTRA is competitive with conventional rescoring\nmethods with faster inference. ELECTRA also performs better in confidence\nestimation than BERT because it can learn to detect inappropriate words not\nonly in fine-tuning but also in pre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Futami_H/0/1/0/all/0/1\">Hayato Futami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mimura_M/0/1/0/all/0/1\">Masato Mimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakai_S/0/1/0/all/0/1\">Shinsuke Sakai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating the Impact of Pre-trained Language Models on Dialog Evaluation. (arXiv:2110.01895v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01895","description":"<p>Recently, there is a surge of interest in applying pre-trained language\nmodels (Pr-LM) in automatic open-domain dialog evaluation. Pr-LMs offer a\npromising direction for addressing the multi-domain evaluation challenge. Yet,\nthe impact of different Pr-LMs on the performance of automatic metrics is not\nwell-understood. This paper examines 8 different Pr-LMs and studies their\nimpact on three typical automatic dialog evaluation metrics across three\ndifferent dialog evaluation benchmarks. Specifically, we analyze how the choice\nof Pr-LMs affects the performance of automatic metrics. Extensive correlation\nanalyses on each of the metrics are performed to assess the effects of\ndifferent Pr-LMs along various axes, including pre-training objectives, dialog\nevaluation criteria, model size, and cross-dataset robustness. This study\nserves as the first comprehensive assessment of the effects of different Pr-LMs\non automatic dialog evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrichs_T/0/1/0/all/0/1\">Thomas Friedrichs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT. (arXiv:2110.01900v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01900","description":"<p>Self-supervised speech representation learning methods like wav2vec 2.0 and\nHidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and\noffer good representations for numerous speech processing tasks. Despite the\nsuccess of these methods, they require large memory and high pre-training\ncosts, making them inaccessible for researchers in academia and small\ncompanies. Therefore, this paper introduces DistilHuBERT, a novel multi-task\nlearning framework to distill hidden representations from a HuBERT model\ndirectly. This method reduces HuBERT's size by 75% and 73% faster while\nretaining most performance in ten different tasks. Moreover, DistilHuBERT\nrequired little training time and data, opening the possibilities of\npre-training personal and on-device SSL models for speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shu-wen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sicilian Translator: A Recipe for Low-Resource NMT. (arXiv:2110.01938v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01938","description":"<p>With 17,000 pairs of Sicilian-English translated sentences, Arba Sicula\ndeveloped the first neural machine translator for the Sicilian language. Using\nsmall subword vocabularies, we trained small Transformer models with high\ndropout parameters and achieved BLEU scores in the upper 20s. Then we\nsupplemented our dataset with backtranslation and multilingual translation and\npushed our scores into the mid 30s. We also attribute our success to\nincorporating theoretical information in our dataset. Prior to training, we\nbiased the subword vocabulary towards the desinences one finds in a textbook.\nAnd we included textbook exercises in our dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wdowiak_E/0/1/0/all/0/1\">Eryk Wdowiak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AraCOVID19-SSD: Arabic COVID-19 Sentiment and Sarcasm Detection Dataset. (arXiv:2110.01948v1 [cs.CL])","link":"http://arxiv.org/abs/2110.01948","description":"<p>Coronavirus disease (COVID-19) is an infectious respiratory disease that was\nfirst discovered in late December 2019, in Wuhan, China, and then spread\nworldwide causing a lot of panic and death. Users of social networking sites\nsuch as Facebook and Twitter have been focused on reading, publishing, and\nsharing novelties, tweets, and articles regarding the newly emerging pandemic.\nA lot of these users often employ sarcasm to convey their intended meaning in a\nhumorous, funny, and indirect way making it hard for computer-based\napplications to automatically understand and identify their goal and the harm\nlevel that they can inflect. Motivated by the emerging need for annotated\ndatasets that tackle these kinds of problems in the context of COVID-19, this\npaper builds and releases AraCOVID19-SSD a manually annotated Arabic COVID-19\nsarcasm and sentiment detection dataset containing 5,162 tweets. To confirm the\npractical utility of the built dataset, it has been carefully analyzed and\ntested using several classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ameur_M/0/1/0/all/0/1\">Mohamed Seghir Hadj Ameur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aliane_H/0/1/0/all/0/1\">Hassina Aliane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Objective Few-shot Learning for Fair Classification. (arXiv:2110.01951v1 [cs.LG])","link":"http://arxiv.org/abs/2110.01951","description":"<p>In this paper, we propose a general framework for mitigating the disparities\nof the predicted classes with respect to secondary attributes within the data\n(e.g., race, gender etc.). Our proposed method involves learning a\nmulti-objective function that in addition to learning the primary objective of\npredicting the primary class labels from the data, also employs a\nclustering-based heuristic to minimize the disparities of the class label\ndistribution with respect to the cluster memberships, with the assumption that\neach cluster should ideally map to a distinct combination of attribute values.\nExperiments demonstrate effective mitigation of cognitive biases on a benchmark\ndataset without the use of annotations of secondary attribute values (the\nzero-shot case) or with the use of a small number of attribute value\nannotations (the few-shot case).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1\">Ishani Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Procheta Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_D/0/1/0/all/0/1\">Debasis Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Transition System for End-to-End Opinion Role Labeling. (arXiv:2110.02001v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02001","description":"<p>Unified opinion role labeling (ORL) aims to detect all possible opinion\nstructures of `opinion-holder-target' in one shot, given a text. The existing\ntransition-based unified method, unfortunately, is subject to longer opinion\nterms and fails to solve the term overlap issue. Current top performance has\nbeen achieved by employing the span-based graph model, which however still\nsuffers from both high model complexity and insufficient interaction among\nopinions and roles. In this work, we investigate a novel solution by revisiting\nthe transition architecture, and augment it with a pointer network (PointNet).\nThe framework parses out all opinion structures in linear-time complexity,\nmeanwhile breaks through the limitation of any length of terms with PointNet.\nTo achieve the explicit opinion-role interactions, we further propose a unified\ndependency-opinion graph (UDOG), co-modeling the syntactic dependency structure\nand the partial opinion-role structure. We then devise a relation-centered\ngraph aggregator (RCGA) to encode the multi-relational UDOG, where the\nresulting high-order representations are used to promote the predictions in the\nvanilla transition system. Our model achieves new state-of-the-art results on\nthe MPQA benchmark. Analyses further demonstrate the superiority of our methods\non both efficacy and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FoodChem: A food-chemical relation extraction model. (arXiv:2110.02019v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02019","description":"<p>In this paper, we present FoodChem, a new Relation Extraction (RE) model for\nidentifying chemicals present in the composition of food entities, based on\ntextual information provided in biomedical peer-reviewed scientific literature.\nThe RE task is treated as a binary classification problem, aimed at identifying\nwhether the contains relation exists between a food-chemical entity pair. This\nis accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models.\nFor evaluation purposes, a novel dataset with annotated contains relations in\nfood-chemical entity pairs is generated, in a golden and silver version. The\nmodels are integrated into a voting scheme in order to produce the silver\nversion of the dataset which we use for augmenting the individual models, while\nthe manually annotated golden version is used for their evaluation. Out of the\nthree evaluated models, the BioBERT model achieves the best results, with a\nmacro averaged F1 score of 0.902 in the unbalanced augmentation setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cenikj_G/0/1/0/all/0/1\">Gjorgjina Cenikj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seljak_B/0/1/0/all/0/1\">Barbara Korou&#x161;i&#x107; Seljak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eftimov_T/0/1/0/all/0/1\">Tome Eftimov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings. (arXiv:2110.02030v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02030","description":"<p>Semantic sentence embeddings are usually supervisedly built minimizing\ndistances between pairs of embeddings of sentences labelled as semantically\nsimilar by annotators. Since big labelled datasets are rare, in particular for\nnon-English languages, and expensive, recent studies focus on unsupervised\napproaches that require not-paired input sentences. We instead propose a\nlanguage-independent approach to build large datasets of pairs of informal\ntexts weakly similar, without manual human effort, exploiting Twitter's\nintrinsic powerful signals of relatedness: replies and quotes of tweets. We use\nthe collected pairs to train a Transformer model with triplet-like structures,\nand we test the generated embeddings on Twitter NLP similarity tasks (PIT and\nTURL) and STSb. We also introduce four new sentence ranking evaluation\nbenchmarks of informal texts, carefully extracted from the initial collections\nof tweets, proving not only that our best model learns classical Semantic\nTextual Similarity, but also excels on tasks where pairs of sentences are not\nexact paraphrases. Ablation studies reveal how increasing the corpus size\ninfluences positively the results, even at 2M samples, suggesting that bigger\ncollections of Tweets still do not contain redundant information about semantic\nsimilarities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giovanni_M/0/1/0/all/0/1\">Marco Di Giovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brambilla_M/0/1/0/all/0/1\">Marco Brambilla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FooDI-ML: a large multi-language dataset of food, drinks and groceries images and descriptions. (arXiv:2110.02035v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02035","description":"<p>In this paper we introduce the Food Drinks and groceries Images Multi Lingual\n(FooDI-ML) dataset. This dataset contains over 1.5M unique images and over 9.5M\nstore names, product names descriptions, and collection sections gathered from\nthe Glovo application. The data made available corresponds to food, drinks and\ngroceries products from 37 countries in Europe, the Middle East, Africa and\nLatin America. The dataset comprehends 33 languages, including 870K samples of\nlanguages of countries from Eastern Europe and Western Asia such as Ukrainian\nand Kazakh, which have been so far underrepresented in publicly available\nvisio-linguistic datasets. The dataset also includes widely spoken languages\nsuch as Spanish and English. To assist further research, we include a benchmark\nover the text-image retrieval task using ADAPT, a SotA existing technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Olondriz_D/0/1/0/all/0/1\">David Amat Ol&#xf3;ndriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puigdevall_P/0/1/0/all/0/1\">Pon&#xe7; Palau Puigdevall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palau_A/0/1/0/all/0/1\">Adri&#xe0; Salvador Palau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ur-iw-hnt at GermEval 2021: An Ensembling Strategy with Multiple BERT Models. (arXiv:2110.02042v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02042","description":"<p>This paper describes our approach (ur-iw-hnt) for the Shared Task of\nGermEval2021 to identify toxic, engaging, and fact-claiming comments. We\nsubmitted three runs using an ensembling strategy by majority (hard) voting\nwith multiple different BERT models of three different types: German-based,\nTwitter-based, and multilingual models. All ensemble models outperform single\nmodels, while BERTweet is the winner of all individual models in every subtask.\nTwitter-based models perform better than GermanBERT models, and multilingual\nmodels perform worse but by a small margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hoai Nam Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruschwitz_U/0/1/0/all/0/1\">Udo Kruschwitz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TENT: Text Classification Based on ENcoding Tree Learning. (arXiv:2110.02047v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02047","description":"<p>Text classification is a primary task in natural language processing (NLP).\nRecently, graph neural networks (GNNs) have developed rapidly and been applied\nto text classification tasks. Although more complex models tend to achieve\nbetter performance, research highly depends on the computing power of the\ndevice used. In this article, we propose TENT (https://github.com/Daisean/TENT)\nto obtain better text classification performance and reduce the reliance on\ncomputing power. Specifically, we first establish a dependency analysis graph\nfor each text and then convert each graph into its corresponding encoding tree.\nThe representation of the entire graph is obtained by updating the\nrepresentation of the non-leaf nodes in the encoding tree. Experimental results\nshow that our method outperforms other baselines on several datasets while\nhaving a simple structure and few parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">He Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning for Multi-lingual Tasks -- a Survey. (arXiv:2110.02052v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02052","description":"<p>These days different platforms such as social media provide their clients\nfrom different backgrounds and languages the possibility to connect and\nexchange information. It is not surprising anymore to see comments from\ndifferent languages in posts published by international celebrities or data\nproviders. In this era, understanding cross languages content and\nmultilingualism in natural language processing (NLP) are hot topics, and\nmultiple efforts have tried to leverage existing technologies in NLP to tackle\nthis challenging research problem. In this survey, we provide a comprehensive\noverview of the existing literature with a focus on transfer learning\ntechniques in multilingual tasks. We also identify potential opportunities for\nfurther research in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jafari_A/0/1/0/all/0/1\">Amir Reza Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidary_B/0/1/0/all/0/1\">Behnam Heidary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahbakhsh_R/0/1/0/all/0/1\">Reza Farahbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mostafa Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalili_M/0/1/0/all/0/1\">Mahdi Jalili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NoiER: An Approach for Training more Reliable Fine-TunedDownstream Task Models. (arXiv:2110.02054v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02054","description":"<p>The recent development in pretrained language models trained in a\nself-supervised fashion, such as BERT, is driving rapid progress in the field\nof NLP. However, their brilliant performance is based on leveraging syntactic\nartifacts of the training data rather than fully understanding the intrinsic\nmeaning of language. The excessive exploitation of spurious artifacts causes a\nproblematic issue: The distribution collapse problem, which is the phenomenon\nthat the model fine-tuned on downstream tasks is unable to distinguish\nout-of-distribution (OOD) sentences while producing a high confidence score. In\nthis paper, we argue that distribution collapse is a prevalent issue in\npretrained language models and propose noise entropy regularisation (NoiER) as\nan efficient learning paradigm that solves the problem without auxiliary models\nand additional~data. The proposed approach improved traditional OOD detection\nevaluation metrics by 55% on average compared to the original fine-tuned\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Training Resources Insufficient? Predict First Then Explain!. (arXiv:2110.02056v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02056","description":"<p>Natural language free-text explanation generation is an efficient approach to\ntrain explainable language processing models for\ncommonsense-knowledge-requiring tasks. The most predominant form of these\nmodels is the explain-then-predict (EtP) structure, which first generates\nexplanations and uses them for making decisions. The performance of EtP models\nis highly dependent on that of the explainer by the nature of their structure.\nTherefore, large-sized explanation data are required to train a good explainer\nmodel. However, annotating explanations is expensive. Also, recent works reveal\nthat free-text explanations might not convey sufficient information for\ndecision making. These facts cast doubts on the effectiveness of EtP models. In\nthis paper, we argue that the predict-then-explain (PtE) architecture is a more\nefficient approach in terms of the modelling perspective. Our main contribution\nis twofold. First, we show that the PtE structure is the most data-efficient\napproach when explanation data are lacking. Second, we reveal that the PtE\nstructure is always more training-efficient than the EtP structure. We also\nprovide experimental results that confirm the theoretical advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1\">Myeongjun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Prediction in NLP -- A survey. (arXiv:2110.02057v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02057","description":"<p>Over the last several years, the field of Structured prediction in NLP has\nhad seen huge advancements with sophisticated probabilistic graphical models,\nenergy-based networks, and its combination with deep learning-based approaches.\nThis survey provides a brief of major techniques in structured prediction and\nits applications in the NLP domains like parsing, sequence labeling, text\ngeneration, and sequence to sequence tasks. We also deep-dived into\nenergy-based and attention-based techniques in structured prediction,\nidentified some relevant open issues and gaps in the current state-of-the-art\nresearch, and have come up with some detailed ideas for future research in\nthese fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_C/0/1/0/all/0/1\">Chauhan Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biyani_N/0/1/0/all/0/1\">Naman Biyani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthar_N/0/1/0/all/0/1\">Nirmal P. Suthar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prashant Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1\">Priyanshu Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactively Generating Explanations for Transformer-based Language Models. (arXiv:2110.02058v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02058","description":"<p>Transformer language models are state-of-the-art in a multitude of NLP tasks.\nDespite these successes, their opaqueness remains problematic. Recent methods\naiming to provide interpretability and explainability to black-box models\nprimarily focus on post-hoc explanations of (sometimes spurious) input-output\ncorrelations. Instead, we emphasize using prototype networks directly\nincorporated into the model architecture and hence explain the reasoning\nprocess behind the network's decisions. Moreover, while our architecture\nperforms on par with several language models, it enables one to learn from user\ninteractions. This not only offers a better understanding of language models,\nbut uses human capabilities to incorporate knowledge outside of the rigid range\nof purely data-driven approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1\">Felix Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tauchmann_C/0/1/0/all/0/1\">Christopher Tauchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Relational Graph based Heterogeneous Multi-Task Learning in Community Question Answering. (arXiv:2110.02059v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02059","description":"<p>Various data mining tasks have been proposed to study Community Question\nAnswering (CQA) platforms like Stack Overflow. The relatedness between some of\nthese tasks provides useful learning signals to each other via Multi-Task\nLearning (MTL). However, due to the high heterogeneity of these tasks, few\nexisting works manage to jointly solve them in a unified framework. To tackle\nthis challenge, we develop a multi-relational graph based MTL model called\nHeterogeneous Multi-Task Graph Isomorphism Network (HMTGIN) which efficiently\nsolves heterogeneous CQA tasks. In each training forward pass, HMTGIN embeds\nthe input CQA forum graph by an extension of Graph Isomorphism Network and skip\nconnections. The embeddings are then shared across all task-specific output\nlayers to compute respective losses. Moreover, two cross-task constraints based\non the domain knowledge about tasks' relationships are used to regularize the\njoint learning. In the evaluation, the embeddings are shared among different\ntask-specific output layers to make corresponding predictions. To the best of\nour knowledge, HMTGIN is the first MTL model capable of tackling CQA tasks from\nthe aspect of multi-relational graphs. To evaluate HMTGIN's effectiveness, we\nbuild a novel large-scale multi-relational graph CQA dataset with over two\nmillion nodes from Stack Overflow. Extensive experiments show that: $(1)$\nHMTGIN is superior to all baselines on five tasks; $(2)$ The proposed MTL\nstrategy and cross-task constraints have substantial advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zizheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_H/0/1/0/all/0/1\">Haowen Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngo-Yin Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Huan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Junpeng Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teach Me What to Say and I Will Learn What to Pick: Unsupervised Knowledge Selection Through Response Generation with Pretrained Generative Models. (arXiv:2110.02067v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02067","description":"<p>Knowledge Grounded Conversation Models (KGCM) are usually based on a\nselection/retrieval module and a generation module, trained separately or\nsimultaneously, with or without having access to a gold knowledge option. With\nthe introduction of large pre-trained generative models, the selection and\ngeneration part have become more and more entangled, shifting the focus towards\nenhancing knowledge incorporation (from multiple sources) instead of trying to\npick the best knowledge option. These approaches however depend on knowledge\nlabels and/or a separate dense retriever for their best performance. In this\nwork we study the unsupervised selection abilities of pre-trained generative\nmodels (e.g. BART) and show that by adding a score-and-aggregate module between\nencoder and decoder, they are capable of learning to pick the proper knowledge\nthrough minimising the language modelling loss (i.e. without having access to\nknowledge labels). Trained as such, our model - K-Mine - shows competitive\nselection and generation performance against models that benefit from knowledge\nlabels and/or separate dense retriever.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1\">Ehsan Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1\">Maxime De Bruyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1\">Jeska Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1\">Walter Daelemans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OPAD: An Optimized Policy-based Active Learning Framework for Document Content Analysis. (arXiv:2110.02069v1 [cs.IR])","link":"http://arxiv.org/abs/2110.02069","description":"<p>Documents are central to many business systems, and include forms, reports,\ncontracts, invoices or purchase orders. The information in documents is\ntypically in natural language, but can be organized in various layouts and\nformats. There have been recent spurt of interest in understanding document\ncontent with novel deep learning architectures. However, document understanding\ntasks need dense information annotations, which are costly to scale and\ngeneralize. Several active learning techniques have been proposed to reduce the\noverall budget of annotation while maintaining the performance of the\nunderlying deep learning model. However, most of these techniques work only for\nclassification problems. But content detection is a more complex task, and has\nbeen scarcely explored in active learning literature. In this paper, we propose\n\\textit{OPAD}, a novel framework using reinforcement policy for active learning\nin content detection tasks for documents. The proposed framework learns the\nacquisition function to decide the samples to be selected while optimizing\nperformance metrics that the tasks typically have. Furthermore, we extend to\nweak labelling scenarios to further reduce the cost of annotation\nsignificantly. We propose novel rewards to account for class imbalance and user\nfeedback in the annotation interface, to improve the active learning method. We\nshow superior performance of the proposed \\textit{OPAD} framework for active\nlearning for various tasks related to document understanding like layout\nparsing, object detection and named entity recognition. Ablation studies for\nhuman feedback and class imbalance rewards are presented, along with a\ncomparison of annotation times for different approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Sumit Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guda_B/0/1/0/all/0/1\">Bhanu Prakash Reddy Guda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaubey_A/0/1/0/all/0/1\">Ashutosh Chaubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_I/0/1/0/all/0/1\">Ishan Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Avanish Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaRLE: Natural Language Models using Reinforcement Learning with Emotion Feedback. (arXiv:2110.02148v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02148","description":"<p>Current research in dialogue systems is focused on conversational assistants\nworking on short conversations in either task-oriented or open domain settings.\nIn this paper, we focus on improving task-based conversational assistants\nonline, primarily those working on document-type conversations (e.g., emails)\nwhose contents may or may not be completely related to the assistant's task. We\npropose \"NARLE\" a deep reinforcement learning (RL) framework for improving the\nnatural language understanding (NLU) component of dialogue systems online\nwithout the need to collect human labels for customer data. The proposed\nsolution associates user emotion with the assistant's action and uses that to\nimprove NLU models using policy gradients. For two intent classification\nproblems, we empirically show that using reinforcement learning to fine tune\nthe pre-trained supervised learning models improves performance up to 43%.\nFurthermore, we demonstrate the robustness of the method to partial and noisy\nimplicit feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ruijie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1\">Soham Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greer_J/0/1/0/all/0/1\">Jeremiah Greer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Charles Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Impact of COVID-19 on Economy from the Perspective of Users Reviews. (arXiv:2110.02198v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02198","description":"<p>One of the most important incidents in the world in 2020 is the outbreak of\nthe Coronavirus. Users on social networks publish a large number of comments\nabout this event. These comments contain important hidden information of public\nopinion regarding this pandemic. In this research, a large number of\nCoronavirus-related tweets are considered and analyzed using natural language\nprocessing and information retrieval science. Initially, the location of the\ntweets is determined using a dictionary prepared through the Geo-Names\ngeographic database, which contains detailed and complete information of places\nsuch as city names, streets, and postal codes. Then, using a large dictionary\nprepared from the terms of economics, related tweets are extracted and\nsentiments corresponded to tweets are analyzed with the help of the RoBERTa\nlanguage-based model, which has high accuracy and good performance. Finally,\nthe frequency chart of tweets related to the economy and their sentiment scores\n(positive and negative tweets) is plotted over time for the entire world and\nthe top 10 economies. From the analysis of the charts, we learn that the reason\nfor publishing economic tweets is not only the increase in the number of people\ninfected with the Coronavirus but also imposed restrictions and lockdowns in\ncountries. The consequences of these restrictions include the loss of millions\nof jobs and the economic downturn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salmani_F/0/1/0/all/0/1\">Fatemeh Salmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vahdat_Nejad_H/0/1/0/all/0/1\">Hamed Vahdat-Nejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiabadi_H/0/1/0/all/0/1\">Hamideh Hajiabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Psuedolabels for training Sentiment Classifiers makes the model generalize better across datasets. (arXiv:2110.02200v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02200","description":"<p>The problem statement addressed in this work is : For a public sentiment\nclassification API, how can we set up a classifier that works well on different\ntypes of data, having limited ability to annotate data from across domains. We\nshow that given a large amount of unannotated data from across different\ndomains and pseudolabels on this dataset generated by a classifier trained on a\nsmall annotated dataset from one domain, we can train a sentiment classifier\nthat generalizes better across different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reddy_N/0/1/0/all/0/1\">Natesh Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1\">Muktabh Mayank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sense-Specific Static Embeddings using Contextualised Word Embeddings as a Proxy. (arXiv:2110.02204v1 [cs.CL])","link":"http://arxiv.org/abs/2110.02204","description":"<p>Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Waypoint Models for Instruction-guided Navigation in Continuous Environments. (arXiv:2110.02207v1 [cs.CV])","link":"http://arxiv.org/abs/2110.02207","description":"<p>Little inquiry has explicitly addressed the role of action spaces in\nlanguage-guided visual navigation -- either in terms of its effect on\nnavigation success or the efficiency with which a robotic agent could execute\nthe resulting trajectory. Building on the recently released VLN-CE setting for\ninstruction following in continuous environments, we develop a class of\nlanguage-conditioned waypoint prediction networks to examine this question. We\nvary the expressivity of these models to explore a spectrum between low-level\nactions and continuous waypoint prediction. We measure task performance and\nestimated execution time on a profiled LoCoBot robot. We find more expressive\nmodels result in simpler, faster to execute trajectories, but lower-level\nactions can achieve better navigation metrics by approximating shortest paths\nbetter. Further, our models outperform prior work in VLN-CE and set a new\nstate-of-the-art on the public leaderboard -- increasing success rate by 4%\nwith our best model on this challenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1\">Jacob Krantz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Legal Approach to Hate Speech: Operationalizing the EU's Legal Framework against the Expression of Hatred as an NLP Task. (arXiv:2004.03422v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.03422","description":"<p>We propose a 'legal approach' to hate speech detection by operationalization\nof the decision as to whether a post is subject to criminal law into an NLP\ntask. Comparing existing regulatory regimes for hate speech, we base our\ninvestigation on the European Union's framework as it provides a widely\napplicable legal minimum standard. Accurately judging whether a post is\npunishable or not usually requires legal training. We show that, by breaking\nthe legal assessment down into a series of simpler sub-decisions, even\nlaypersons can annotate consistently. Based on a newly annotated dataset, our\nexperiments show that directly learning an automated model of punishable\ncontent is challenging. However, learning the two sub-tasks of `target group'\nand `targeting conduct' instead of an end-to-end approach to punishability\nyields better results. Overall, our method also provides decisions that are\nmore transparent than those of end-to-end models, which is a crucial point in\nlegal decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zufall_F/0/1/0/all/0/1\">Frederike Zufall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamacher_M/0/1/0/all/0/1\">Marius Hamacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kloppenborg_K/0/1/0/all/0/1\">Katharina Kloppenborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zesch_T/0/1/0/all/0/1\">Torsten Zesch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriched Pre-trained Transformers for Joint Slot Filling and Intent Detection. (arXiv:2004.14848v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14848","description":"<p>Detecting the user's intent and finding the corresponding slots among the\nutterance's words are important tasks in natural language understanding. Their\ninterconnected nature makes their joint modeling a standard part of training\nsuch models. Moreover, data scarceness and specialized vocabularies pose\nadditional challenges. Recently, the advances in pre-trained language models,\nnamely contextualized models such as ELMo and BERT have revolutionized the\nfield by tapping the potential of training very large models with just a few\nsteps of fine-tuning on a task-specific dataset. Here, we leverage such models,\nnamely BERT and RoBERTa, and we design a novel architecture on top of them.\nMoreover, we propose an intent pooling attention mechanism, and we reinforce\nthe slot filling task by fusing intent distributions, word features, and token\nrepresentations. The experimental results on standard datasets show that our\nmodel outperforms both the current non-BERT state of the art as well as some\nstronger BERT-based baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset for Automatic Summarization of Russian News. (arXiv:2006.11063v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.11063","description":"<p>Automatic text summarization has been studied in a variety of domains and\nlanguages. However, this does not hold for the Russian language. To overcome\nthis issue, we present Gazeta, the first dataset for summarization of Russian\nnews. We describe the properties of this dataset and benchmark several\nextractive and abstractive models. We demonstrate that the dataset is a valid\ntask for methods of text summarization for Russian. Additionally, we prove the\npretrained mBART model to be useful for Russian text summarization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1\">Ilya Gusev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Book Success Prediction with Pretrained Sentence Embeddings and Readability Scores. (arXiv:2007.11073v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.11073","description":"<p>Predicting the potential success of a book in advance is vital in many\napplications. This could help both publishers and readers in their\ndecision-making process whether or not a book is worth publishing and reading,\nrespectively. In this paper, we propose a model that leverages pretrained\nsentence embeddings along with various readability scores for book success\nprediction. Unlike previous methods, the proposed method requires no\ncount-based, lexical, or syntactic features. Instead, we use a convolutional\nneural network over pretrained sentence embeddings and leverage different\nreadability scores through a simple concatenation operation. Our proposed model\noutperforms strong baselines for this task by as large as 6.4\\% F1-score\npoints. Moreover, our experiments show that according to our model, only the\nfirst 1K sentences are good enough to predict the potential success of books.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1\">Aminul Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The optimality of syntactic dependency distances. (arXiv:2007.15342v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.15342","description":"<p>It is often stated that human languages, as other biological systems, are\nshaped by cost-cutting pressures but, to what extent? Attempts to quantify the\ndegree of optimality of languages by means of an optimality score have been\nscarce and focused mostly on English. Here we recast the problem of the\noptimality of the word order of a sentence as an optimization problem on a\nspatial network where the vertices are words, arcs indicate syntactic\ndependencies and the space is defined by the linear order of the words in the\nsentence. We introduce a new score to quantify the cognitive pressure to reduce\nthe distance between linked words in a sentence. The analysis of sentences from\n93 languages representing 19 linguistic families reveals that half of languages\nare optimized to a 70% or more. The score indicates that distances are not\nsignificantly reduced in a few languages and confirms two theoretical\npredictions, i.e. that longer sentences are more optimized and that distances\nare more likely to be longer than expected by chance in short sentences. We\npresent a new hierarchical ranking of languages by their degree of\noptimization. The new score has implications for various fields of language\nresearch (dependency linguistics, typology, historical linguistics, clinical\nlinguistics and cognitive science). Finally, the principles behind the design\nof the score have implications for network science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Quality Assessment of Cognitive Behavioral Therapy Sessions Through Highly Contextualized Language Representations. (arXiv:2102.11573v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.11573","description":"<p>During a psychotherapy session, the counselor typically adopts techniques\nwhich are codified along specific dimensions (e.g., 'displays warmth and\nconfidence', or 'attempts to set up collaboration') to facilitate the\nevaluation of the session. Those constructs, traditionally scored by trained\nhuman raters, reflect the complex nature of psychotherapy and highly depend on\nthe context of the interaction. Recent advances in deep contextualized language\nmodels offer an avenue for accurate in-domain linguistic representations which\ncan lead to robust recognition and scoring of such psychotherapy-relevant\nbehavioral constructs, and support quality assurance and supervision. In this\nwork, we propose a BERT-based model for automatic behavioral scoring of a\nspecific type of psychotherapy, called Cognitive Behavioral Therapy (CBT),\nwhere prior work is limited to frequency-based language features and/or short\ntext excerpts which do not capture the unique elements involved in a\nspontaneous long conversational interaction. The model focuses on the\nclassification of therapy sessions with respect to the overall score achieved\non the widely-used Cognitive Therapy Rating Scale (CTRS), but is trained in a\nmulti-task manner in order to achieve higher interpretability. BERT-based\nrepresentations are further augmented with available therapy metadata,\nproviding relevant non-linguistic context and leading to consistent performance\nimprovements. We train and evaluate our models on a set of 1,118 real-world\ntherapy sessions, recorded and automatically transcribed. Our best model\nachieves an F1 score equal to 72.61% on the binary classification task of low\nvs. high total CTRS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flemotomos_N/0/1/0/all/0/1\">Nikolaos Flemotomos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_V/0/1/0/all/0/1\">Victor R. Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuohao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creed_T/0/1/0/all/0/1\">Torrey A. Creed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1\">David C. Atkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shrikanth Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability. (arXiv:2103.07162v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07162","description":"<p>This paper investigates whether the power of the models pre-trained on text\ndata, such as BERT, can be transferred to general token sequence classification\napplications. To verify pre-trained models' transferability, we test the\npre-trained models on text classification tasks with meanings of tokens\nmismatches, and real-world non-text token sequence classification data,\nincluding amino acid, DNA, and music. We find that even on non-text data, the\nmodels pre-trained on text converge faster, perform better than the randomly\ninitialized models, and only slightly worse than the models using task-specific\nknowledge. We also find that the representations of the text and non-text\npre-trained models share non-trivial similarities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kao_W/0/1/0/all/0/1\">Wei-Tsung Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integer-only Zero-shot Quantization for Efficient Speech Recognition. (arXiv:2103.16827v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2103.16827","description":"<p>End-to-end neural network models achieve improved performance on various\nautomatic speech recognition (ASR) tasks. However, these models perform poorly\non edge hardware due to large memory and computation requirements. While\nquantizing model weights and/or activations to low-precision can be a promising\nsolution, previous research on quantizing ASR models is limited. In particular,\nthe previous approaches use floating-point arithmetic during inference and thus\nthey cannot fully exploit efficient integer processing units. Moreover, they\nrequire training/validation data during quantization, which may not be\navailable due to security/privacy concerns. To address these limitations, we\npropose an integer-only, zero shot quantization scheme for ASR models. In\nparticular, we generate synthetic data whose runtime statistics resemble the\nreal data, and we use it to calibrate models during quantization. We apply our\nmethod to quantize QuartzNet, Jasper, and Conformer and show negligible WER\nchange as compared to the full-precision baseline models, even without using\nany training data. Moreover, we achieve up to 2.35x speedup on a T4 GPU and 4x\ncompression rate, with a modest WER degradation of &lt;1% with INT8 quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1\">Nicholas Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1\">Patrick Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nrusimha_A/0/1/0/all/0/1\">Anirudda Nrusimha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_B/0/1/0/all/0/1\">Bohan Zhai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_T/0/1/0/all/0/1\">Tianren Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Architectures and Training for Raw Waveform Feature Extraction in ASR. (arXiv:2104.04298v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2104.04298","description":"<p>With the success of neural network based modeling in automatic speech\nrecognition (ASR), many studies investigated acoustic modeling and learning of\nfeature extractors directly based on the raw waveform. Recently, one line of\nresearch has focused on unsupervised pre-training of feature extractors on\naudio-only data to improve downstream ASR performance. In this work, we\ninvestigate the usefulness of one of these front-end frameworks, namely\nwav2vec, in a setting without additional untranscribed data for hybrid ASR\nsystems. We compare this framework both to the manually defined standard\nGammatone feature set, as well as to features extracted as part of the acoustic\nmodel of an ASR system trained supervised. We study the benefits of using the\npre-trained feature extractor and explore how to additionally exploit an\nexisting acoustic model trained with different features. Finally, we\nsystematically examine combinations of the described features in order to\nfurther advance the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vieting_P/0/1/0/all/0/1\">Peter Vieting</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luscher_C/0/1/0/all/0/1\">Christoph L&#xfc;scher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Michel_W/0/1/0/all/0/1\">Wilfried Michel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empathetic Dialog Generation with Fine-Grained Intents. (arXiv:2105.06829v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06829","description":"<p>Empathetic dialog generation aims at generating coherent responses following\nprevious dialog turns and, more importantly, showing a sense of caring and a\ndesire to help. Existing models either rely on pre-defined emotion labels to\nguide the response generation, or use deterministic rules to decide the emotion\nof the response. With the advent of advanced language models, it is possible to\nlearn subtle interactions directly from the dataset, providing that the emotion\ncategories offer sufficient nuances and other non-emotional but emotional\nregulating intents are included. In this paper, we describe how to incorporate\na taxonomy of 32 emotion categories and 8 additional emotion regulating intents\nto succeed the task of empathetic response generation. To facilitate the\ntraining, we also curated a large-scale emotional dialog dataset from movie\nsubtitles. Through a carefully designed crowdsourcing experiment, we evaluated\nand demonstrated how our model produces more empathetic dialogs compared with\nits baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yubo Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_P/0/1/0/all/0/1\">Pearl Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence. (arXiv:2107.02173v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.02173","description":"<p>Topic model evaluation, like evaluation of other unsupervised methods, can be\ncontentious. However, the field has coalesced around automated estimates of\ntopic coherence, which rely on the frequency of word co-occurrences in a\nreference corpus. Recent models relying on neural components surpass classical\ntopic models according to these metrics. At the same time, unlike classical\nmodels, the practice of neural topic model evaluation suffers from a validation\ngap: automatic coherence for neural models has not been validated using human\nexperimentation. In addition, as we show via a meta-analysis of topic modeling\nliterature, there is a substantial standardization gap in the use of automated\ntopic modeling benchmarks. We address both the standardization gap and the\nvalidation gap. Using two of the most widely used topic model evaluation\ndatasets, we assess a dominant classical model and two state-of-the-art neural\nmodels in a systematic, clearly documented, reproducible way. We use automatic\ncoherence along with the two most widely accepted human judgment tasks, namely,\ntopic rating and word intrusion. Automated evaluation will declare one model\nsignificantly different from another when corresponding human evaluations do\nnot, calling into question the validity of fully automatic evaluations\nindependent of human judgments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyle_A/0/1/0/all/0/1\">Alexander Hoyle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Pranav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peskov_D/0/1/0/all/0/1\">Denis Peskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hian_Cheong_A/0/1/0/all/0/1\">Andrew Hian-Cheong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnik_P/0/1/0/all/0/1\">Philip Resnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuned Language Models Are Zero-Shot Learners. (arXiv:2109.01652v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01652","description":"<p>This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially boosts zero-shot performance on unseen tasks.\n</p>\n<p>We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof tasks and model scale are key components to the success of instruction\ntuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Y. Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine Translation. (arXiv:2109.12584v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12584","description":"<p>In recent times, there has been definitive progress in the field of NLP, with\nits applications growing as the utility of our language models increases with\nadvances in their performance. However, these models require a large amount of\ncomputational power and data to train, consequently leading to large carbon\nfootprints. Therefore, is it imperative that we study the carbon efficiency and\nlook for alternatives to reduce the overall environmental impact of training\nmodels, in particular large language models. In our work, we assess the\nperformance of models for machine translation, across multiple language pairs\nto assess the difference in computational power required to train these models\nfor each of these language pairs and examine the various components of these\nmodels to analyze aspects of our pipeline that can be optimized to reduce these\ncarbon emissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_M/0/1/0/all/0/1\">Mirza Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surana_P/0/1/0/all/0/1\">Praatibh Surana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gauri Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_K/0/1/0/all/0/1\">Krithika Ramesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFAQ: a Multilingual FAQ Dataset. (arXiv:2109.12870v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12870","description":"<p>In this paper, we present the first multilingual FAQ dataset publicly\navailable. We collected around 6M FAQ pairs from the web, in 21 different\nlanguages. Although this is significantly larger than existing FAQ retrieval\ndatasets, it comes with its own challenges: duplication of content and uneven\ndistribution of topics. We adopt a similar setup as Dense Passage Retrieval\n(DPR) and test various bi-encoders on this dataset. Our experiments reveal that\na multilingual model based on XLM-RoBERTa achieves the best results, except for\nEnglish. Lower resources languages seem to learn from one another as a\nmultilingual model achieves a higher MRR than language-specific ones. Our\nqualitative analysis reveals the brittleness of the model on simple word\nchanges. We publicly release our dataset, model and training script.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1\">Maxime De Bruyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1\">Ehsan Lotfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1\">Jeska Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1\">Walter Daelemans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment-Aware Measure (SAM) for Evaluating Sentiment Transfer by Machine Translation Systems. (arXiv:2109.14895v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.14895","description":"<p>In translating text where sentiment is the main message, human translators\ngive particular attention to sentiment-carrying words. The reason is that an\nincorrect translation of such words would miss the fundamental aspect of the\nsource text, i.e. the author's sentiment. In the online world, MT systems are\nextensively used to translate User-Generated Content (UGC) such as reviews,\ntweets, and social media posts, where the main message is often the author's\npositive or negative attitude towards the topic of the text. It is important in\nsuch scenarios to accurately measure how far an MT system can be a reliable\nreal-life utility in transferring the correct affect message. This paper\ntackles an under-recognised problem in the field of machine translation\nevaluation which is judging to what extent automatic metrics concur with the\ngold standard of human evaluation for a correct translation of sentiment. We\nevaluate the efficacy of conventional quality metrics in spotting a\nmistranslation of sentiment, especially when it is the sole error in the MT\noutput. We propose a numerical `sentiment-closeness' measure appropriate for\nassessing the accuracy of a translated affect message in UGC text by an MT\nsystem. We will show that incorporating this sentiment-aware measure can\nsignificantly enhance the correlation of some available quality metrics with\nthe human judgement of an accurate translation of sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Orasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_E/0/1/0/all/0/1\">Emad Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantawy_A/0/1/0/all/0/1\">Ashraf Tantawy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional generalization in semantic parsing with pretrained transformers. (arXiv:2109.15101v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.15101","description":"<p>Large-scale pretraining instills large amounts of knowledge in deep neural\nnetworks. This, in turn, improves the generalization behavior of these models\nin downstream tasks. What exactly are the limits to the generalization benefits\nof large-scale pretraining? Here, we report observations from some simple\nexperiments aimed at addressing this question in the context of two semantic\nparsing tasks involving natural language, SCAN and COGS. We show that language\nmodels pretrained exclusively with non-English corpora, or even with\nprogramming language corpora, significantly improve out-of-distribution\ngeneralization in these benchmarks, compared with models trained from scratch,\neven though both benchmarks are English-based. This demonstrates the\nsurprisingly broad transferability of pretrained representations and knowledge.\nPretraining with a large-scale protein sequence prediction task, on the other\nhand, mostly deteriorates the generalization performance in SCAN and COGS,\nsuggesting that pretrained representations do not transfer universally and that\nthere are constraints on the similarity between the pretraining and downstream\ndomains for successful transfer. Finally, we show that larger models are harder\nto train from scratch and their generalization accuracy is lower when trained\nup to convergence on the relatively small SCAN and COGS datasets, but the\nbenefits of large-scale pretraining become much clearer with larger models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts. (arXiv:2110.01159v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01159","description":"<p>Recent models in developing summarization systems consist of millions of\nparameters and the model performance is highly dependent on the abundance of\ntraining data. While most existing summarization corpora contain data in the\norder of thousands to one million, generation of large-scale summarization\ndatasets in order of couple of millions is yet to be explored. Practically,\nmore data is better at generalizing the training patterns to unseen data. In\nthis paper, we introduce TLDR9+ -- a large-scale summarization dataset --\ncontaining over 9 million training instances extracted from Reddit discussion\nforum (https://github.com/sajastu/reddit_collector). This dataset is\nspecifically gathered to perform extreme summarization (i.e., generating\none-sentence summary in high compression and abstraction) and is more than\ntwice larger than the previously proposed dataset. We go one step further and\nwith the help of human annotations, we distill a more fine-grained dataset by\nsampling High-Quality instances from TLDR9+ and call it TLDRHQ dataset. We\nfurther pinpoint different state-of-the-art summarization models on our\nproposed datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1\">Sajad Sotudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deilamsalehy_H/0/1/0/all/0/1\">Hanieh Deilamsalehy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1\">Nazli Goharian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LawSum: A weakly supervised approach for Indian Legal Document Summarization. (arXiv:2110.01188v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01188","description":"<p>Unlike the courts in western countries, public records of Indian judiciary\nare completely unstructured and noisy. No large scale publicly available\nannotated datasets of Indian legal documents exist till date. This limits the\nscope for legal analytics research. In this work, we propose a new dataset\nconsisting of over 10,000 judgements delivered by the supreme court of India\nand their corresponding hand written summaries. The proposed dataset is\npre-processed by normalising common legal abbreviations, handling spelling\nvariations in named entities, handling bad punctuations and accurate sentence\ntokenization. Each sentence is tagged with their rhetorical roles. We also\nannotate each judgement with several attributes like date, names of the\nplaintiffs, defendants and the people representing them, judges who delivered\nthe judgement, acts/statutes that are cited and the most common citations used\nto refer the judgement. Further, we propose an automatic labelling technique\nfor identifying sentences which have summary worthy information. We demonstrate\nthat this auto labeled data can be used effectively to train a weakly\nsupervised sentence extractor with high accuracy. Some possible applications of\nthis dataset besides legal document summarization can be in retrieval, citation\nanalysis and prediction of decisions by a particular judge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_V/0/1/0/all/0/1\">Vedant Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_V/0/1/0/all/0/1\">Vidit Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metha_P/0/1/0/all/0/1\">Parth Metha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_N/0/1/0/all/0/1\">Namita Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factorized Neural Transducer for Efficient Language Model Adaptation. (arXiv:2110.01500v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01500","description":"<p>In recent years, end-to-end (E2E) based automatic speech recognition (ASR)\nsystems have achieved great success due to their simplicity and promising\nperformance. Neural Transducer based models are increasingly popular in\nstreaming E2E based ASR systems and have been reported to outperform the\ntraditional hybrid system in some scenarios. However, the joint optimization of\nacoustic model, lexicon and language model in neural Transducer also brings\nabout challenges to utilize pure text for language model adaptation. This\ndrawback might prevent their potential applications in practice. In order to\naddress this issue, in this paper, we propose a novel model, factorized neural\nTransducer, by factorizing the blank and vocabulary prediction, and adopting a\nstandalone language model for the vocabulary prediction. It is expected that\nthis factorization can transfer the improvement of the standalone language\nmodel to the Transducer for speech recognition, which allows various language\nmodel adaptation techniques to be applied. We demonstrate that the proposed\nfactorized neural Transducer yields 15% to 20% WER improvements when\nout-of-domain text data is used for language model adaptation, at the cost of a\nminor degradation in WER on a general test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Sarangarajan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-scale ASR Domain Adaptation using Self- and Semi-supervised Learning. (arXiv:2110.00165v2 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2110.00165","description":"<p>Self- and semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Misra_A/0/1/0/all/0/1\">Ananya Misra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddhartha_N/0/1/0/all/0/1\">Nikhil Siddhartha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garg_S/0/1/0/all/0/1\">Shefali Garg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_D/0/1/0/all/0/1\">David Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beaufays_F/0/1/0/all/0/1\">Fran&#xe7;oise Beaufays</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}